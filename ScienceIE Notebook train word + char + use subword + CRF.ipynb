{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract ScienceIE data\n",
    "- Training set: https://drive.google.com/open?id=0B2Z1kbILu3YtYjkwMHd3TmNPWDQ\n",
    "- Development set: https://drive.google.com/open?id=0B2Z1kbILu3YtNDE1R0h5c2tQclU\n",
    "- Test set: https://drive.google.com/open?id=0B2Z1kbILu3YtMUlfaWZDN0FSUms\n",
    "\n",
    "Place scienceie2017_dev.zip, scienceie2017_train.zip, and semeval_articles_test.zip in repo root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir data/temp\n",
    "# !unzip -o ./scienceie2017_dev.zip -d data/temp\n",
    "# !unzip -o ./scienceie2017_train.zip -d data/temp\n",
    "# !unzip -o ./semeval_articles_test.zip -d data/temp\n",
    "# !mv data/temp/semeval_articles_test data/test\n",
    "# !mv data/temp/train2 data/train\n",
    "# !mv data/temp/dev data/dev\n",
    "# !rm -rf data/temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert annotations to BIO (tag unannotated words as O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "def convert_anns(path):\n",
    "    annfiles = [f for f in os.listdir(path) if f.endswith('.ann')]\n",
    "    \n",
    "    tag_out = []\n",
    "    \n",
    "    for af in annfiles:\n",
    "        \n",
    "        annfile = os.path.join(path, af)\n",
    "        txtfile = annfile.replace(\".ann\", \".txt\")\n",
    "        paragraph_ = codecs.open(txtfile, \"r\", \"utf-8\")\n",
    "        paragraph = paragraph_.read()\n",
    "        paragraph = paragraph.strip()\n",
    "\n",
    "        lastind = 0\n",
    "        last_offset = 0\n",
    "        \n",
    "        with open(annfile) as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    \n",
    "                    lsplit = line.split('\\t')\n",
    "                    tid, t_cinds, t_words = lsplit\n",
    "                    t_words=t_words.strip()\n",
    "                    try:\n",
    "                        tsplit = t_cinds.split(' ')\n",
    "                        tag = tsplit[0]\n",
    "                        start = tsplit[1]\n",
    "                        end = tsplit[-1]\n",
    "                    except:\n",
    "                        print t_cinds\n",
    "                        print annfile\n",
    "                    start=int(start)\n",
    "                    end=int(end)\n",
    "                    if start!=lastind:\n",
    "                        owords=paragraph[lastind:start+last_offset].split(' ')\n",
    "                        for oword in owords:\n",
    "                            if oword.strip()!='':\n",
    "                                tag_out+=[oword +' O']\n",
    "                    lastind=end+last_offset\n",
    "                    offset = 0\n",
    "                    while(not paragraph[(start+offset):(end+offset)]==t_words):\n",
    "                        if offset>50:\n",
    "                            break\n",
    "                        offset+=1\n",
    "                    last_offset = offset\n",
    "                    words=paragraph[(start+offset):(end+offset)].split(' ')\n",
    "                    for wind in range(len(words)):\n",
    "                        if words[wind].strip() == '':\n",
    "                                next\n",
    "                        else:\n",
    "                            if wind==0:\n",
    "                                tag_out+=[words[wind] +' B-'+tag]\n",
    "                            else:\n",
    "                                tag_out+=[words[wind] +' I-'+tag]\n",
    "\n",
    "        owords=paragraph[lastind+1+offset:len(paragraph)].split(' ')\n",
    "        for oword in owords:\n",
    "                if oword.strip()!='':\n",
    "                    tag_out+=[oword +' O']\n",
    "        tag_out+=['']\n",
    "                    \n",
    "#     with open(path+'.txt', 'w') as out:\n",
    "    with codecs.open(path+'.txt', 'w', 'utf-8') as out:\n",
    "        out.write('\\n'.join(tag_out))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'data/dev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd54299ad695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'data/dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# for p in ['data/test']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconvert_anns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b1e65113b521>\u001b[0m in \u001b[0;36mconvert_anns\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_anns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mannfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ann'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtag_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'data/dev'"
     ]
    }
   ],
   "source": [
    "for p in ['data/dev', 'data/train', 'data/test']:\n",
    "# for p in ['data/test']:\n",
    "    convert_anns(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20131 data/dev.txt\n",
      "106918 data/train.txt\n",
      "37755 data/test.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/dev.txt\n",
    "!wc -l data/train.txt\n",
    "!wc -l data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### process full texts from training set for subword embeddings\n",
    "import nltk\n",
    "import codecs\n",
    "\n",
    "import sys\n",
    "stdout=sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "sys.stdout=stdout\n",
    "\n",
    "from model.xml_utils import parseXML\n",
    "\n",
    "\n",
    "def convert_xmls(path):\n",
    "    xmlfiles = [f for f in os.listdir(path) if f.endswith('.xml')]\n",
    "    doclist = []\n",
    "    for f in xmlfiles:\n",
    "        _, _, sents = parseXML(os.path.join(path, f))\n",
    "        doclist+=nltk.sent_tokenize(sents)\n",
    "        \n",
    "    with codecs.open('data/'+os.path.basename(path)+'_full.txt', 'w', 'utf-8') as out:\n",
    "        out.write('\\n'.join(doclist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_xmls('data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_xmls('data/dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM assignment 5 formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "corpus=[]\n",
    "current_sent=[]\n",
    "last_token=''\n",
    "err='none'\n",
    "\n",
    "with codecs.open(\"data/test.txt\", 'r', 'utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if not line in [u'\\n', u'\\r\\n']:\n",
    "            token, tag = line.split(' ')\n",
    "            if last_token.endswith('.') and not last_token in ['e.g.', 'i.e.']:\n",
    "                if token[0].isupper():\n",
    "                    corpus.append(current_sent)\n",
    "                    current_sent=[]\n",
    "            current_sent.append((token, tag.strip()))\n",
    "        else:\n",
    "            corpus.append(current_sent)\n",
    "            current_sent=[]\n",
    "    corpus.append(current_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.\n",
      "[u'O', u'O', u'O', u'O', u'O', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'I-Task', u'I-Process', u'O', u'O', u'O', u'O', u'B-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Process', u'I-Process', u'I-Process', u'I-Process', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Process', u'I-Process', u'I-Process', u'I-Process', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Material', u'I-Material', u'B-Task', u'I-Task', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'I-Task', u'I-Process', u'O', u'O', u'O', u'O', u'B-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'I-Task', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Process', u'I-Process', u'I-Process', u'I-Process', u'O', u'O', u'O', u'O', u'O', u'O', u'B-Process', u'I-Process', u'I-Process', u'I-Process', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O', u'O']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir('../2017-fall-main/assignment/a5/part1')\n",
    "\n",
    "import pos\n",
    "import nltk\n",
    "\n",
    "hmm = pos.HMM()\n",
    "for sentence in corpus:\n",
    "    hmm.update_counts(sentence)\n",
    "hmm.compute_logprobs()\n",
    "\n",
    "def pretty_print_fb(sentence):\n",
    "    print sentence\n",
    "    print hmm.forward_backward(sentence.split())\n",
    "    \n",
    "pretty_print_fb(' '.join([i for i,j in corpus[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acquire GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-15 02:38:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2017-12-15 02:38:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘./data/glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  50.5MB/s    in 21s     \n",
      "\n",
      "2017-12-15 02:38:58 (39.3 MB/s) - ‘./data/glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  ./data/glove.6B.zip\n",
      "  inflating: data/glove.6B/glove.6B.50d.txt  \n",
      "  inflating: data/glove.6B/glove.6B.100d.txt  \n",
      "  inflating: data/glove.6B/glove.6B.200d.txt  \n",
      "  inflating: data/glove.6B/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# !wget -P ./data/ \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "# !unzip ./data/glove.6B.zip -d data/glove.6B/\n",
    "# !rm ./data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/glove.840B.300d.zip\n",
      "  inflating: data/glove.840B/glove.840B.300d.txt  \n",
      "rm: cannot remove './data/glove.840B.zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !wget -P ./data/ \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "!unzip ./data/glove.840B.300d.zip -d data/glove.840B/\n",
    "!rm ./data/glove.840B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct subword vectors from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...........\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /home/beatspace9/anaconda2:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    bzip2:        1.0.6-h6d464ef_2             \n",
      "    cmake:        3.9.4-h142f0e9_0             \n",
      "    libgcc-ng:    7.2.0-h7cc24e2_2             \n",
      "    libprotobuf:  3.4.1-h5b8497f_0             \n",
      "    libstdcxx-ng: 7.2.0-h7a57d05_2             \n",
      "    libuv:        1.14.0-h56b52c2_0            \n",
      "    ncurses:      6.0-h9df7e31_2               \n",
      "    rhash:        1.3.5-hbf7ad62_1             \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    conda:        4.3.29-py27_0     conda-forge --> 4.3.30-py27h6ae6dc7_0\n",
      "    conda-env:    2.6.0-0           conda-forge --> 2.6.0-h36134e3_1     \n",
      "    protobuf:     3.4.0-py27_0      conda-forge --> 3.4.1-py27h2ba6a9c_0 \n",
      "    xz:           5.2.2-1                       --> 5.2.3-h55aa19d_2     \n",
      "    zlib:         1.2.8-3                       --> 1.2.11-ha838bed_2    \n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "Fetching package metadata ...^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/beatspace9/anaconda2/bin/conda\", line 6, in <module>\n",
      "    sys.exit(conda.cli.main())\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/cli/main.py\", line 182, in main\n",
      "    return conda_exception_handler(_main, *args)\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/exceptions.py\", line 640, in conda_exception_handler\n",
      "    return_value = func(*args, **kwargs)\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/cli/main.py\", line 140, in _main\n",
      "    exit_code = args.func(args, p)\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/cli/main_install.py\", line 80, in execute\n",
      "    install(args, parser, 'install')\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/cli/install.py\", line 231, in install\n",
      "    unknown=index_args['unknown'], prefix=prefix)\n",
      "  File \"/home/beatspace9/anaconda2/lib/python2.7/site-packages/conda/core/index.py\", line 101, in get_index\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/facebookresearch/fastText.git fasttext\n",
    "!conda install cmake\n",
    "!conda install -c conda-forge pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd fasttext\n",
    "!mkdir build && cd build && cmake ..\n",
    "!make && make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminate called after throwing an instance of 'std::invalid_argument'\n",
      "  what():  fulltxts.txt cannot be opened for training!\n",
      "Aborted (core dumped)\n"
     ]
    }
   ],
   "source": [
    "# !cd ../data\n",
    "# !mkdir subwords\n",
    "# !./fasttext skipgram -input fulltxts.txt -output subwords/fasttext.100d.txt -dim 100\n",
    "# !./fasttext skipgram -input fulltxts.txt -output subwords/fasttext.300d.txt -dim 300\n",
    "# !sed 1,1d subwords/fasttext.100d.txt > subwords/fasttext.100d.txt\n",
    "# !sed 1,1d subwords/fasttext.300d.txt > subwords/fasttext.300d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract vocab from data and matching GloVe vectors\n",
    "Bi-LSTM code adapted from https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.config import Config\n",
    "from model.data_utils import CoNLLDataset, get_vocabs, UNK, NUM, \\\n",
    "    get_glove_vocab, write_vocab, load_vocab, get_char_vocab, \\\n",
    "    export_trimmed_glove_vectors, get_processing_word\n",
    "    \n",
    "config = Config(load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config.dim_word = 300\n",
    "config.dim_char = 100\n",
    "config.dim_subword = 300\n",
    "\n",
    "# config.filename_glove = \"data/glove.6B/glove.6B.{}d.txt\".format(config.dim_word)\n",
    "# config.filename_trimmed = \"data/glove.6B.{}d.trimmed.npz\".format(config.dim_word)\n",
    "# config.use_pretrained = True\n",
    "\n",
    "config.filename_glove = \"data/glove.840B/glove.840B.{}d.txt\".format(config.dim_word)\n",
    "config.filename_trimmed = \"data/glove.840B.{}d.trimmed.npz\".format(config.dim_word)\n",
    "config.use_pretrained = False\n",
    "\n",
    "config.filename_subwords = \"data/subwords/fasttext.{}d.txt\".format(config.dim_subword)\n",
    "config.filename_swtrimmed = \"data/fasttext.{}d.trimmed.npz\".format(config.dim_subword)\n",
    "config.use_subwords = True\n",
    "\n",
    "# dataset\n",
    "config.filename_dev = \"data/dev.txt\"\n",
    "config.filename_test = \"data/test.txt\"\n",
    "config.filename_train = \"data/train.txt\"\n",
    "# config.filename_dev = config.filename_test = config.filename_train = \"data/test.txt\" # test\n",
    "\n",
    "max_iter = None # if not None, max number of examples in Dataset\n",
    "\n",
    "config.filename_words = \"data/words.txt\"\n",
    "config.filename_tags = \"data/tags.txt\"\n",
    "config.filename_chars = \"data/chars.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "config.train_embeddings = False\n",
    "config.nepochs          = 15\n",
    "config.dropout          = 0.5\n",
    "config.batch_size       = 20\n",
    "config.lr_method        = \"adam\"\n",
    "config.lr               = 0.001\n",
    "config.lr_decay         = 0.9\n",
    "config.clip             = -1 # if negative, no clipping\n",
    "config.nepoch_no_imprv  = 4\n",
    "\n",
    "# model hyperparameters\n",
    "config.hidden_size_char = 100 # lstm on chars\n",
    "config.hidden_size_lstm = 300 # lstm on word embeddings\n",
    "\n",
    "config.use_crf = True\n",
    "config.use_chars = True\n",
    "config.use_glove = False\n",
    "\n",
    "# character CNN parameters\n",
    "# config.feature_maps=[50, 100, 150, 200, 200, 200, 200]\n",
    "# config.kernels=[1,2,3,4,5,6,7]\n",
    "# config.num_filters=128\n",
    "# config.filter_sizes=[3,5,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab...\n",
      "- done. 15346 tokens\n",
      "Building vocab...\n",
      "- done. 2196016 tokens\n",
      "Building vocab...\n",
      "- done. 18614 tokens\n",
      "Writing vocab...\n",
      "- done. 7913 tokens\n",
      "Writing vocab...\n",
      "- done. 7 tokens\n"
     ]
    }
   ],
   "source": [
    "processing_word = get_processing_word(lowercase=True)\n",
    "\n",
    "# Generators\n",
    "dev   = CoNLLDataset(config.filename_dev, processing_word)\n",
    "test  = CoNLLDataset(config.filename_test, processing_word)\n",
    "train = CoNLLDataset(config.filename_train, processing_word)\n",
    "\n",
    "# Build Word and Tag vocab\n",
    "vocab_words, vocab_tags = get_vocabs([train, dev, test])\n",
    "vocab_glove = get_glove_vocab(config.filename_glove)\n",
    "vocab_subwords = get_glove_vocab(config.filename_subwords)\n",
    "\n",
    "vocab = vocab_words & vocab_subwords\n",
    "vocab.add(UNK)\n",
    "vocab.add(NUM)\n",
    "\n",
    "# Save vocab\n",
    "write_vocab(vocab, config.filename_words)\n",
    "write_vocab(vocab_tags, config.filename_tags)\n",
    "\n",
    "# Trim GloVe Vectors\n",
    "vocab = load_vocab(config.filename_words)\n",
    "export_trimmed_glove_vectors(vocab, config.filename_glove,\n",
    "                            config.filename_trimmed, config.dim_word)\n",
    "export_trimmed_glove_vectors(vocab, config.filename_subwords,\n",
    "                            config.filename_swtrimmed, config.dim_subword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab...\n",
      "- done. 154 tokens\n"
     ]
    }
   ],
   "source": [
    "# Build and save char vocab\n",
    "train = CoNLLDataset(config.filename_train)\n",
    "vocab_chars = get_char_vocab(train)\n",
    "write_vocab(vocab_chars, config.filename_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: randomly initializing word vectors\n",
      "/home/beatspace9/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Initializing tf session\n",
      "Epoch 1 out of 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 116s - train loss: 248.6373     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 85.41 - f1 0.00 - precision 0.00\n",
      "- new best score!\n",
      "Epoch 2 out of 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 117s - train loss: 206.7256     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 85.41 - f1 0.00 - precision 0.00\n",
      "- new best score!\n",
      "Epoch 3 out of 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 119s - train loss: 188.3187     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 85.43 - f1 0.77 - precision 26.83\n",
      "- new best score!\n",
      "Epoch 4 out of 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 117s - train loss: 173.2285     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc 85.78 - f1 11.66 - precision 34.31\n",
      "- new best score!\n",
      "Epoch 5 out of 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/18 [===================>..........] - ETA: 41s - train loss: 162.6103 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "from model.ner_model import NERModel\n",
    "\n",
    "config.load()\n",
    "\n",
    "model = NERModel(config)\n",
    "model.build()\n",
    "# model.restore_session(\"results/crf/model.weights/\") # optional, restore weights\n",
    "# model.reinitialize_weights(\"proj\")\n",
    "\n",
    "# create datasets\n",
    "dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n",
    "                     config.processing_tag, config.max_iter)\n",
    "train = CoNLLDataset(config.filename_train, config.processing_word,\n",
    "                     config.processing_tag, config.max_iter)\n",
    "\n",
    "# train model\n",
    "model.train(train, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test  = CoNLLDataset(config.filename_test, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model.data_utils import CoNLLDataset\n",
    "from model.ner_model import NERModel\n",
    "from model.config import Config\n",
    "\n",
    "\n",
    "def align_data(data):\n",
    "    \"\"\"Given dict with lists, creates aligned strings\n",
    "    Adapted from Assignment 3 of CS224N\n",
    "    Args:\n",
    "        data: (dict) data[\"x\"] = [\"I\", \"love\", \"you\"]\n",
    "              (dict) data[\"y\"] = [\"O\", \"O\", \"O\"]\n",
    "    Returns:\n",
    "        data_aligned: (dict) data_align[\"x\"] = \"I love you\"\n",
    "                           data_align[\"y\"] = \"O O    O  \"\n",
    "    \"\"\"\n",
    "    spacings = [max([len(seq[i]) for seq in data.values()])\n",
    "                for i in range(len(data[list(data.keys())[0]]))]\n",
    "    data_aligned = dict()\n",
    "\n",
    "    # for each entry, create aligned string\n",
    "    for key, seq in data.items():\n",
    "        str_aligned = \"\"\n",
    "        for token, spacing in zip(seq, spacings):\n",
    "            str_aligned += token + \" \" * (spacing - len(token) + 1)\n",
    "\n",
    "        data_aligned[key] = str_aligned\n",
    "\n",
    "    return data_aligned\n",
    "\n",
    "\n",
    "\n",
    "def tag_text(model, text):\n",
    "    words_raw = text.strip().split(\" \")\n",
    "\n",
    "    preds = model.predict(words_raw)\n",
    "    to_print = align_data({\"input\": words_raw, \"output\": preds})\n",
    "\n",
    "#     for key, seq in to_print.items():\n",
    "#         model.logger.info(seq)\n",
    "    return words_raw, preds\n",
    "        \n",
    "# test_sent=\"First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries. Multiplicities and their evolution with centrality are successfully reproduced. Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality. Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/onepunchdan/bf83950297e9b8ec8a50d33d3a1ecf59/raw/0bea2b99f76e751606bbf08972b91ce75e1dab3a/gistfile1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import codecs\n",
    "# ps=[]\n",
    "\n",
    "# with codecs.open(\"gistfile1.txt\", 'r', 'utf-8') as f:\n",
    "#     for l in f.readlines():\n",
    "#         if l.strip()!='':\n",
    "#             ps.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plist=[]\n",
    "# for p in ps:\n",
    "#     words, preds = tag_text(model, p)\n",
    "#     tagnum=sum([1 for i in preds if i!='O'])\n",
    "#     plist.append([tagnum, words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tag_text(model, ' '.join(max(plist)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
