Zirconium hydride precipitation kinetics in Zircaloy-4 observed with synchrotron X-ray diffraction High-energy synchrotron X-ray diffraction was used to investigate the isothermal precipitation of δ-hydride platelets in Zircaloy-4 at a range of temperatures relevant to reactor conditions, during both normal operation and thermal transients.
From an examination of the rate kinetics of the precipitation process, precipitation slows with increasing temperature above 200°C, due to a reduction in the thermodynamic driving force.
A model for nucleation rate as a function of temperature was developed, to interpret the precipitation rates seen experimentally.
While the strain energy associated with the misfit between hydrides and the matrix makes a significant contribution to the energy barrier for nucleation, a larger contribution arises from the interfacial energy.
Diffusion distance calculations show that hydrogen is highly mobile in the considered thermal range and on the scale of inter-hydride spacing and it is not expected to be significantly rate limiting on the precipitation process that takes place under reactor operating conditions.
Zirconium alloys have been widely adopted as cladding material for fuel rods in Western light water reactors (LWRs) ever since nuclear reactor technology was first conceived at the end of the 1940s [1].
The function of this cladding is to act as a structural component, containing fuel pellets, holding them in place within a fuel assembly and allowing the desired turbulent flow of coolant between rods, while conducting heat energy from the fuel into the fluid [2].
Additionally, it also acts as a buffer between the fuel pellets and the encompassing reactor environment by containing fission products that escape the fuel, whilst preventing degradation of the fuel from exposure to the coolant medium [2–4].
This means that the structural stability of these components is of paramount importance, as it is linked intrinsically to the safety, efficiency and operability of the fuel assembly [2].
The choice of available materials is limited, as fuel assembly components are expected to last years under typical operating conditions – where temperatures range from 280°C to 400°C and coolant pressures are in excess of 15MPa – as well as performing adequately during potential accident scenarios, where conditions are significantly more harsh [2,5,6].
The selection of zirconium alloys as cladding stems from the advantageous mechanical properties, good corrosion behaviour and, critically, the low neutron absorption cross-section that they demonstrate [1,4,7].
However, the common commercially deployed alloys of zirconium display a high affinity for both oxygen and hydrogen [8,9].
When exposed to the light water coolant, some of the hydrogen by-product of the oxidation reaction is absorbed into the cladding, though the mechanism for this ingress through the oxide is still debated [10].
While the solubility limit of oxygen in α-zirconium is as high as 28.5at.% under typical light water reactor operating conditions [11], the hydrogen solubility in the same environment is at most 3at.% [12].
Once the local hydrogen concentration exceeds the terminal solid solubility of the alloy for the local temperature, any excess will then precipitate into hydride phase.
Given the general tendency for hydrogen to diffuse down thermal and concentration gradients, and up hydrostatic stress gradients, macroscopic hydrogen distribution can often be non-uniform within components [13].
This is further compounded in fuel cladding, where the coolant removes heat from the outer surface of the material, while the fuel itself heats the inner surface.
Together, these mechanisms lead to dramatic differences in concentration and solubility between different regions, leading to phenomena like the formation of hydride rims [14].
Since the early days of nuclear technology, researchers have investigated the impact of hydrogen on zirconium alloys, as well as various aspects of the formation and properties of hydrides [15–21].
When precipitating, hydrides take the morphology of either needles or platelets, based on the cooling rate and the availability of hydrogen [21].
These variables influence which of the hydride phases precipitate, where the most commonly observed form is the non-stoichiometric face-centred cubic delta (δ) phase [22].
Less commonly, the stable/metastable (based on temperature and interpretation [23]) gamma (γ) or stable epsilon (ε) hydrides, both taking a face centred tetragonal crystal structure, may also precipitate [23].
The physical shape that each of these different phases take, on a fine scale and in isolation, is primarily ascribed to the anisotropic directionality of the stress-free transformation strains of each, generated during phase change [15].
This misfit is the product of an increase in volume that occurs in newly developed hydrides due to their lower density, when compared to that of the parent matrix [24].
In the case of δ-hydride, where the face-centered cubic unit cell is isotropic in the three principle axes, the anisotropy of the hexagonal close packed cell of the parent α-zirconium matrix leads to similar anisotropy in the transformation strain [15].
When δ precipitates, the misfit normal to the basal plane is calculated by Carpenter to be 7.2%, while that normal to the prismatic plane is only 4.58% [15].
This encourages isotropic growth in the basal plane while retarding that in the prismatic plane, potentially yielding a ‘micro’-hydride with a platelet type morphology.
These precipitates will be oriented with platelet normals parallel to the matrix hexagonal unit cell basal normal.
More recently, Barrow measured transformation strains of 5.5%, 3.1% and 0.5% in the [0001]α, [112¯0]α and [1¯100]α directions, respectively [25].
Interestingly, these values imply that δ-hydrides would not precipitate with the commonly described platelet morphology, and are instead described as needle-like structures by the author [25].
During their evolution, micro-hydride precipitates have a tendency to group or stack with discrete separating distances to form the comparatively large hydride features often observed through optical microscopy [21,26–28].
This chaining effect is attributed to the development of a hydrogen atmosphere at the core of dislocations generated locally by hydride precipitation, as encouraged by the resulting lattice dilation in these defects [29].
The morphology, distribution and orientation of these ‘macro’-hydrides are produced as a function of the thermal history, residual and applied stresses, texture, and microstructure of a component [24].
For example, where there are high cooling rates or large matrix grains, a predominance of intra-granular hydrides are observed, while lower cooling rates or small grain sizes produce more inter-granular or grain boundary hydrides [21,28,30].
Similarly, macro-hydride orientation is heavily influenced by applied stresses, where compressive forces reorient hydrides with their normal perpendicular to the loading direction [21].
An applied tensile load, however, will align macro-hydrides with their normal parallel to the loading direction [15,21].
This macroscopic reorientation of hydrides is actually a process wherein the existing micro-hydrides within a macro-precipitate will dissolve and re-precipitate with a new macro-distribution, possessing an apparent normal parallel with the tensile component of an applied three-dimensional stress.
In practice, the thermomechanical history of the alloy will control the natural orientations of macro-hydrides precipitated in a component with no load [31].
However, should a fuel rod be exposed to an applied stress above a certain threshold, the reorientation phenomena occurs, where the threshold value is determined by processing and microstructure [31].
Despite differences in macro-morphology and orientation, all micro-hydrides are found to possess the same orientation relationship relative to the matrix of (0001)α∥(111)δ [24,28,30], irrespective of reorientation; although there are some examples of {101¯7}α∥{111}δ found in radially oriented inter-granular hydrides [32].
Phase field and finite element models have gone on to simulate and explain this orientation relationship in works like [33,34].
Hydrides, once precipitated in zirconium, degrade the mechanical properties of a component, leading to reductions in tensile strength, ductility and fracture toughness [35–40].
These changes can ultimately compromise the integrity of cladding during normal operating life , accident conditions and fuel storage [13].
As well as the degradation of mechanical properties, the presence of hydrides can also affect phenomena like pellet cladding mechanical interaction (PCMI); or introduce mechanisms for failure, such as delayed hydride cracking (DHC).
The former mechanism is the product of thermal expansion in fuel pellets introducing stresses into the cladding, which may then lead to the formation of cracks in areas made brittle by large hydride concentrations [13].
The latter mechanism, DHC, is a sub-critical, time dependent cracking phenomenon that requires long range hydrogen diffusion for repeated local hydride growth and fracture at a hydrostatic tensile stress raiser [5,41,42].
The process occurs over an extended period of time under a continuously applied load that is below the yield stress of the material [5,41,42].
Experimental techniques employed to investigate the general morphology and distribution of hydrides during precipitation/dissolution, as well as during the related mechanisms of failure, are limited because microscopy can only provide information for a single point in time.
Moreover, investigating components that have been irradiated during service life is both costly and difficult, making evaluations of real end of life microstructures problematic [43].
Synchrotron X-ray diffraction, on the other hand, provides an excellent tool for studying bulk processes taking place within materials, giving the ability to sample relatively large volumes of material while performing in-situ experiments [27].
The propensity of a synchrotron to produce a highly collimated, high-flux beam yields a significant degree of angular resolution, allows for rapid diffraction pattern acquisition, and generates very well defined peaks with minimal artificial broadening.
This is especially valuable when studying the precipitation process that takes place when hydrogen saturates zirconium, as measurements of very weak reflections, that may otherwise be overlapped by dominant zirconium peaks, can be made during thermal [23,27,44] or mechanical [22,24,31] experiments.
In tandem with the experimental work undertaken to study the zirconium-hydrogen system, modelling has also been performed to understand better the precise mechanisms involved in the nucleation, growth, mechanical properties of these precipitates and their effect on zirconium cladding.
Following on from the work undertaken by Carpenter to define the stress-free transformation strains in δ- and γ-hydrides, Singh et al.
were able to extend the methodology to describe volumetric misfit as a function of temperature [15,45].
This work serves to illustrate the temperature dependence of the misfit in the hydride system, where an additional ≈11% misfit arises from a 275°C increase in temperature, demonstrating the significant impact of thermal changes on hydride properties [45].
Barrow et al., in their evaluation of the impact of chemical and strain energy on hydride nucleation, model both the chemical driving force and strain energy associated with hydride nucleation, as a function of terminal solid solubility on precipitation, or TSSP, temperature (and thus bulk hydrogen content) [46].
From a comparison of the magnitude of these values, this work demonstrates the dominance of the chemical free energy contribution to driving force as the primary factor influencing the nucleation of hydrides [46].
Further to this, it is stated that the energy barrier to nucleation is dominated by the surface (or interfacial) energy, as opposed to strain energy, given that large elastic strains tend to be plastically relaxed through the formation of dislocations [46].
Interestingly, in other work, Barrow puts forward a precipitation process where γ-hydride nucleates and grows, before transforming into δ-hydride, yielding a precipitate with a core consisting of δ- and tips of γ-hydride [25].
Given that the γ-hydride has a lower accommodation energy than the δ-phase, it may be physically reasonable to suppose it acts as a precursor, before transforming during the process of coarsening [25].
If δ were considered the stable phase and γ metastable, this would imply there was some other factor preventing the stable phase from forming initially.
Another interpretation states that the γ-phase is stable and δ metastable, which would account for the initial formation of γ, the transformation to δ would then be the product of a destabilisation of γ, in favour of δ.
More recently, advances in phase-field modelling have allowed for the simulation of hydride precipitation with defined time and length scales, both with and without applied load [43,47,48].
The evolution of elastic stresses within the surrounding matrix can then be predicted for a range of hydride distributions, and the work presented in [48] shows a good agreement between simulated morphologies and those seen experimentally.
While these simulations have only been performed on γ-hydride thus far, the methodology employed can easily be extended to model δ-precipitates.
The phase-field methodology has also been used to demonstrate the effect of uniaxially applied load on hydride precipitation, where precipitates whose normals are aligned close to the tensile force are those predicted to show the most growth [49].
This work, stemming from a set of kinetic synchrotron X-ray diffraction experiments, seeks to quantify the rapid isothermal precipitation that takes place where significant undercooling is in effect.
To support these observations, a simple model describing nucleation rate as a function of the experimental temperatures is described.
This model and associated calculations are designed to build upon the work of other authors whom have produced expressions that describe the chemical, strain and interfacial energies of either a misfitting precipitate or the hydride system itself [50–52].
A single specimen of Zircaloy-4 measuring 50mm×5mm×0.4mm was used in this work, which was provided by the Institut de Radioprotection et de Sûreté Nucléaire, France.
As with the work presented by Zanellato et al., the material was rolled to 400μm and recrystallised, yielding an average grain diameter of 10μm and the strong basal texture associated with recrystallisation within rolled plate with a hexagonal close packed crystal structure [27].
The Kearns factors (details in [53]) for this material, as calculated from EBSD analysis, were ƒRD=0.107, ƒTD=0.219 and ƒND=0.673.
Following thermo-mechanical processing, the sample was charged with hydrogen to a specified average concentration using the methodology for charging and verification described by Zanellato et al.
[27].
Hot extraction was used to verify the concentration, and the result of charging was found to be within ±15ppmwt.
of the desired content, with gradients across the sample of 3ppmwt.mm−1 on average [27].
At the time of the experiment, the hydrogen concentration within the sample was measured to be approximately 430ppmwt., corresponding to the approximate hydrogen content of cladding material that has had a burnup of 49GWd/tU; close that listed for current fuel assemblies [54,55].
The experimental work presented herein was carried out at the European Synchrotron Radiation Facility (France), on high energy beamline ID15B; a full technical description of the beamline can be found in [56,57].
Diffraction was undertaken using transmission geometry with the beam normal to the surface of the specimen, allowing for a large volume of material to be sampled through the thickness of the sheet.
Debye–Scherrer rings were recorded using a Trixell Pixium 4700 detector.
An acquisition time of 5s and an average disk write time of 4–5s was achieved, yielding a temporal resolution of 9–10s per recorded diffraction pattern.
Throughout this experiment a monochromatic beam with a consistent energy of E=87.17±0.01keV, and corresponding wavelength of λ=0.14223Å, was used to illuminate the specimen, in conjunction with a beam geometry of 300×300μm2.
The material was mounted in the water-cooled and electrically conductive grips of an Instron ElectroThermal Mechanical Tester (ETMT8800), which was used to drive the thermal cycles using resistive (or Joule) heating.
As zirconium alloys display a strong tendency to oxidise at elevated temperatures, a slow flowing atmosphere of inert argon gas was supplied into the sealed and oxygen purged chamber in which the sample was mounted.
The temperature of the sample was set and maintained by an automatic feedback loop, which measured the temperature of the sample using an S-Type thermocouple spot-welded to the axial centre of the sample, close to the location of the incident beam.
The readings of the thermocouple were fed back to the control unit in order to adjust the heating current accordingly, to produce the desired temperature.
As a check of the thermocouple, Laboratory X-ray Diffraction (LXRD) was undertaken on the same sample, and a comparison of thermal expansion coefficients measured through SXRD and LXRD was made.
This secondary check of thermal expansion is employed as flaws in the spot welding of the thermocouple used in the ETMT can lead to erroneous temperature readings [58].
Both techniques showed good agreement in measured thermal expansion, and so the thermocouple was deemed a reliable measure of temperature.
The thermal profile for the experiment was comprised of one ramped transient, followed by seven quench and dwell cycles.
The initial cycle (C1), designed to measure solubility curves, heated at a rate of 1°Cs−1 from a base temperature (Tfloor) of 40°C up to a peak (Tmax) of 570°C, above the expected eutectoid temperature [59,60].
Holds were implemented in this cycle at Tmax and at 300°C as part of both heating and cooling ramps, during which lateral scans along the axial length of the sample were made to evaluate hydride distributions across the sample at elevated temperature.
During cooling in this cycle, a rate of 1°Cs−1 was employed to mirror the heating operation.
The terminal dissolution solubility for the sample was measured from C1 to be 439°C, which is lower than the 513°C predicted by McMinn for this concentration, and so a Tmax of 500°C was set for all following transients [60].
Each of the subsequent quench-based cycles (C2–C8) involved raising the temperature to Tmax and holding for 15min in order to fully dissolve all hydrides.
Given that a hydride precipitation memory effect has been discussed in the literature, [19,61], this process would impact on the precipitation solvus and kinetics recorded during all cycles.
By dwelling at elevated temperatures for a period after all hydrides have been dissolved, some of the dislocations left behind by previous hydride structures or cold work were given the chance to recover.
Along with this, residual stresses may also have been allowed to relax, the combination of which could reduce the possibility of heterogeneous nucleation at preferential sites, supporting an assumption of homogeneous nucleation throughout the experiment.
Following each dissolution hold, the sample was then ‘quenched’ by reducing the current used to heat the sample to achieve the desired dwell temperature for kinetics observations.
In practice, heat loss through conduction into the grips and radiation into the surrounding atmosphere did not allow instantaneous quenching, and a maximum cooling rate of approximately 30°Cs−1 was observed.
In the case of the largest change in temperature seen in this experiment (ΔT=400°C), this would mean it took 13.3s to reach the target temperature, however, observations were made from the onset of the cooling operation.
Following each quench, the temperature was held steady in the isothermal region that forms the basis of the analytical work presented, and from which all kinetics observations are made.
Finally, the sample was cooled back to Tfloor, where axial hydride distribution checks were made after each thermal cycle.
The desired dwell temperatures started at 100°C and rose by 50°C in 7cycles up to 400°C.
No measurable hydride peak was detected during the final 400°C cycle, despite the hold temperature being below the TSSP predicted by McMinn (445°C) [60].
From C1, the TSSP measured for the sample was found to be 361°C, potentially explaining why no hydride signal was detected during C8 and indicating that only minimal precipitation would be expected during C7 at 350°C.
Additionally, a 1°Cs−1 heating ramp was added at the beginning of the 350°C cycle to investigate long term changes to solubility towards the end of the experimental run.
Only a minimal change in the TSSD curve was observed, which was well within the expected experimental error of ±15.5ppmwt.. A schematic diagram, presented as Fig.
1, shows the thermal regime implemented throughout this experiment.
The hydrogen distribution checks made at Tfloor between each cycle and at elevated temperatures during C1 were performed as resistive heating produces a thermal gradient across the axial length of the material, emphasised by the loss of heat from the edges of the specimen due to water cooling in the grips [58].
This is particularly significant in this work as it is well documented that hydrogen has a strong tendency to diffuse towards cooler regions, potentially leading to hydrogen depletion in the significantly hotter axial centre of the sample, where measurements were taken [13,62,63].
The result of these checks confirmed that the thermal gradient did result in regions of marked depletion and enrichment, however, the most significant of these were primarily localised to material within 5mm of the grips (the total length between grips being 20mm).
The central region, from where measurements were taken, showed only a gradual depletion of hydrogen available for precipitation, where the difference at the point of diffraction between the initial state and the final state was no more than 30ppmwt..
The depletion measured during each cycle was accounted for in all calculations involving hydrogen concentration, and is reflected in both solubility plots that follow.
Additionally, a mass spectrometer was used to monitor the atmosphere exhaust from the test chamber, to detect any significant loss of hydrogen from the sample through desorption.
For hydride phase analysis, the recorded Debye–Scherrer ring patterns were integrated using Fit2D over the entire azimuthal range, rather than using partial integrations representing the transverse and rolling directions of the material.
This method was chosen as it allows for the sampling of all diffracting hydride crystals, rather than those with planes aligned to diffract close to the two principle directions.
This also allows for greater counting statistics to be incorporated, giving more accurate definition to low intensity peaks, along with significantly reduced background noise relative to the magnitude of the diffraction peaks of interest, thus improving the signal to noise ratio.
To investigate the impact of this choice of integration, an analysis of full-width-half-maximum (FWHM) was performed on hydride peaks from a full integration (360°) and 15° azimuthally integrated data (ψ±7.5°), taken from the two principle directions.
The results showed an average of 9% peak broadening in hydride reflections from fully integrated data, which was considered acceptable.
The XY format diffractogram files generated by Fit2D were normalised against the incident beam intensity to remove synchrotron energy decay and then batch processed using the command line operator for TOPAS-Academic V5, in conjunction with an in-house developed Matlab R2014a function.
In order to account for instrument broadening, peak shape parameters were defined from modelling a standard reference material and fixed throughout the analysis.
As peak shape is a function of beam, detector and sample, defining this with a calibrant allows the isolation of the effect of sample on peak profile [64,65].
This then allows for any peak broadening or asymmetry to be defined using parameters related to the material being studied.
Initially the diffraction patterns were simulated using a Rietveld structural model [66,67] for accurate measurements of phase weight percentage (wt%).
However, it was quickly noted that the software was not correctly emulating hydride reflections where intensities were near to extinction.
Instead, an alternate method was employed, where the diffracting weight percentage of hydride from the sample in its initial condition was measured using the Rietveld method, to provide a boundary condition.
This boundary condition was established as the diffracting weight fraction for the essentially-complete precipitation of 430ppmwt.
at Tfloor, where the solubility of hydrogen was predicted by JMatPro to be 0.19ppmwt.
and was considered negligible [60].
A second boundary condition comes from the assumption that when all hydrogen was dissolved into solution, the diffracting volume was at zero and no peak existed.
Using these two conditions, in conjunction with the assumption of a linear relationship linking volume fraction with integrated intensity, makes it possible to calculate the volume fraction of δ-hydride from the measured area of each phase within the modelled pattern [27].
Fig.
2 illustrates the diffractograms generated from fully integrated data at a number of key temperatures from throughout the experiment, over the interplanar spacing range considered.
The chosen reflections for volume fraction analysis are the {311}δ and {022}δ, with lattice spacings of ≈1.43Å and ≈1.68Å, respectively; decided for their high multiplicities (24 and 12) and good degree of separation from nearby matrix peaks.
The 40°C data set shows the hydride peaks at their maximum value, representing approximately 100% precipitation.
Conversely, the 500°C line shows total peak extinction where all hydrogen is in solution.
The final pattern, from the end of the 300°C isothermal hold, represents the diminished peak intensity representative of the lower volume of diffracting hydride associated with an increased solubility at this temperature.
Diffractogram modelling during batch processing was performed using the Le Bail method of the hkl function within TOPAS, described in detail in [68].
As with the Rietveld method, peak positions were defined by the space group and lattice parameters of the phase, while the intensities of individual reflections were allowed to vary independently of texture parameters and one another [27].
By inputting values of cell mass into the hkl structure of 371.616 for the hydride and 182.448 for matrix, taken from the fully refined Rietveld model, TOPAS was then able to calculate a weight percentage for each phase.
Similarly to the Rietveld method, this was calculated as a function of peak area for all peaks in both phases over the given 2θ or interplanar spacing range, illustrated as the x-axis in Fig.
2.
As the scale parameters used in each phase were allowed to refine freely to give the best possible fit to the data while accounting for the differences in peak profile between the two phases, the initial software calculated weight percentage values deviated significantly from the known boundary conditions.
However, by using the aforementioned relationship it became possible to calibrate these values to show the true phase distribution for any given acquisition.
In addition, JMatPro 7.0 and the ZRDATA database were used both for experimental planning and to calculate parts of the thermodynamic data underpinning the model presented.
From the calculated volume fraction of hydride, it becomes possible to derive the completion percentile for the precipitation process that takes place during the isothermal hold in each cycle, seen in Fig.
3.
The first point on this plot, at t=0, is taken as the last acquisition before the quench is performed.
Given that the quench is not truly instantaneous, the second point in each series tends to lie during a period of rapid thermal change.
This may artificially increase the time taken to reach completion thresholds by a small amount.
For the 100–250°C hold cycles (C2–C5), more than 95% of hydrogen has precipitated by t=15s; recorded from the second acquisition after the quench commences.
This is likely to have occurred partially during the rapid quench and partially during the first seconds of the isothermal hold.
The temporal resolution derived from the method employed for diffraction pattern acquisition is insufficient to deconvolute these two effects.
For the final two cycles, at temperatures greater than 250°C, the rate of precipitation is lower and reaching maximum completion takes significantly longer than at lower temperatures.
For cycle C7, a cycle that was only half the duration of others, there appears still to be some upwards slope towards the end of the data series, potentially indicating incomplete precipitation at the cessation of the cycle.
Unfortunately, time constraints cut this and the precipitation-free cycle (C8) short, and so 100% completion was taken as the weight percentage value reached at the end of this dwell.
This may artificially reduce the completion time for precipitation recorded at this temperature.
For this reason, the final hold two hold temperatures (cycles C7 and C8) are excluded from Fig.
4.
Taking all other percentage completion data sets, it becomes possible to plot a transformation map for the precipitation process, Fig.
4.
Unlike a conventional Time–Temperature–Transformation (TTT) diagram, a linear scale is used as the temporal resolution of 9–10seconds per acquisition would make a logarithmic scale unclear in the low time region.
Additionally, due to the rapid rate of precipitation that is observed, the percentage completion thresholds chosen for this analysis are limited to being close to 100%.
This map demonstrates the time taken to reach a threshold completion state in the precipitation process, as a function of the temperature at which each isothermal hold takes place.
The curvature towards longer times seen in the elevated temperature region reflects the significant reduction in precipitation rate seen in later cycles.
It should be noted, however, that while the highest temperature dwells do show significantly slower precipitation than the those at lower temperatures, 90% precipitation still occurs in just 58s during the 300°C hold.
In all series, it is clear that completion times are at their lowest at some temperature below 200°C.
For the 90–98% curves, the temporal resolution of the diffraction imaging setup acts as a limiting factor, preventing an accurate determination of the peak precipitation temperature beyond this accuracy from these curves.
In the 99% and 100% completion plots, however, curvature continues to the lowest recorded hold temperature, possibly indicating a maximum rate to precipitate hydrides close to, or lower than, the minimum experimental dwell temperature of 100°C.
It should be noted, however, that for all hold temperatures scatter in the data of between ±1% and ±3% (depending on cycle) means accurate judgement of 100% completion is difficult, and so the trend in this final series should be considered carefully.
Values for hydrogen solubility during dissolution are calculated from the recorded weight percentage of δ-hydride during the heating ramp in cycle C1, and from the identical ramp at the beginning of the final quench cycle (C7).
The diffracting weight percentage can be converted into a ppmwt.
value, and this can subsequently be subtracted from the boundary condition of 430ppmwt.
to give the amount of hydrogen in solution.
The dissolution solvi taken from C1 and C7 were compared as a cursory investigation into changes in solubility resulting from repeatedly cycling thermal transients.
As previously mentioned, the curve measured from the final cycle showed good agreement with the initial cycle, within the bounds of expected experimental error and known depletion, indicating that the solubility is stable throughout the experimental programme.
The curve for dissolved hydrogen, determined from precipitation occurring during cycle C1, is presented in Fig.
5 alongside data for slow heating and cooling rates taken from the literature.
Both the TSSP and TSSD from the literature are plotted, but the current experimental data does not show agreement with the expected TSSP from McMinn.
It should be noted that the cluster of points seen at 300°C is those recorded during a short dwell at this temperature.
The data produced by McMinn are representative of Zircaloy-2 and -4 with a number of processing states and were put forward as evidence that microstructure and chemical composition have little effect on solubility [60].
Additionally, the range of thermal transient rates used in his work, considered to be slow rates when compared with the present study, were also said to have little effect on hydrogen solubility [60].
Later work, undertaken by Zanellato, put forward evidence that significantly increasing the heating or cooling rate will lower the terminal solid solubility temperature observed during each thermal operation.
The greatest magnitude of shift seen in that work is of the order of 20°C, where the cooling rate was increased by an order of magnitude from 10°Cs−1 to 100°Cs−1 [27].
Similarly, the curve for solute hydrogen concentration recorded during rapid continuous cooling in the present study shows little agreement with the slower examples published by McMinn.
However, such a deviation can arise from a kinetic effect due to insufficiently fast hydride precipitation rather than a true change in solubility.
To investigate this in more detail, the evolution of solute hydrogen was investigated during the series of isothermal dwells that followed C1.
Fig.
6 again shows the TSSP and TSSD curves calculated by McMinn from slow cooling studies.
Also plotted are the measured quantities of hydrogen in solution at the beginning and end of each isothermal dwell region (C2–C8), as illustrated by the subplot schematic.
In this figure, the black dashed and solid lines represent data from literature continuous cooling transients, while the dotted lines with cross and circle markers are those from quench and hold transients during the present study.
It should be noted for clarity that no TSSD data from the present experiment is included in this figure, and that the literature TSSP has been plotted for clarity.
The final points in each quench series, at 400°C, are both set at a concentration of 400ppmwt., given that no precipitation was seen during that cycle and a depletion of 30ppmwt.
was observed by this point in the experiment.
As with the solute hydrogen measurements from continuous thermal transients, C1 and C7, the initial recorded solute hydrogen concentration during each isothermal hold has a significant positive systematic bias when compared with the literature TSSP.
Instead, these points, indicated as blue crosses, show reasonable agreement with the dissolved hydrogen level recorded from continuous rapid cooling cycle, Fig.
5.
When quenching at high cooling rates to elevated hold temperatures (those above 250°C), the process of hydride precipitation continues during the hold and hydrogen leaves solution until a final concentration is recorded.
The final value is well below the initial quenched solubility and that of rapid continuous cooling cycles, instead being in very close agreement with the TSSP predicted by McMinn.
Given this information, it seems possible that the TSSP curves put forward from experiments where rapid continuous cooling operations are used, do not truly represent the equilibrium solubility.
In the present work, this is reflected by the process of hydride precipitation clearly being unable to occur fast enough to keep pace with the rate of temperature change.
This substantiates the findings of Root, whom suggested that significant incubation times are required for the quantity of hydrogen in solution to reach true equilibrium [69].
The ramifications of this mean that proposed shifts in precipitation solubility temperature, as a function of heating/cooling rate, seen in [27] are not likely to be changes to the equilibrium solubility of the material.
Instead, they may be the mark of significant supersaturation developing, as the result of the precipitation kinetics being rate limited by a reduced driving force at these temperatures.
This possibility is further evidenced by the fact that in that work the dissolution solubility shifts towards lower temperatures with increasing heating rate, rather than towards higher temperatures [27].
With a high heating rate, one would expect the resulting dissolution solubility curve to either show little/no change from equilibrium or be shifted to higher temperatures, where the kinetics of the process are not fast enough to keep pace with the change of temperature.
While the true equilibrium solvus temperature is defined entirely by free energy curves (and is thus free of the influence of heating and cooling rates), a shift in the apparent TSS temperature may possibly occur with high heating or cooling rates.
This would be the product of a non-equilibrium microstructure being obtained at the cessation of cooling or heating.
Lastly, those values for final hydrogen in solution recorded at lower temperatures show little difference from the initial solubility points, being only a small amount above the literature TSSP line.
This is because hydride precipitation is essentially complete by the first diffraction acquisition, and little or no supersaturation remains owing to the rapid kinetics at these temperatures, brought about by a high driving force for precipitation.
While within expected experimental error, the positive bias at these lower temperatures (when comparing the final dissolved hydrogen concentration against the literature) may be the product of a transformation between γ- and δ-hydride, thought to occur below 260°C [32].
While no diffraction from γ was recorded, its existence in low quantities or very fine precipitates would potentially not generate any recordable diffraction signature, and other authors have said that hydride peaks are undetectable below 20ppmwt.
[46].
This could then tie up hydrogen and thus reflect in an artificially increased reading of hydrogen in solution when calculated solely from δ-hydride reflections.
The observation of γ-hydride tips on δ-hydrides, made by Barrow, may go some way towards justifying this possibility, as they could be in small enough volumes and quantities to minimise diffraction [25].
The experimental results show that hydride precipitation kinetics become increasingly rapid with decreasing temperature over the range studied in this work.
Hydrides in zirconium precipitate through a diffusion controlled process of nucleation and growth phase transformation [70,71], and in such transformations, the TTT diagram typically follows a C-shaped curve.
The present results from this set of experimental data, Fig.
4, follow a profile that would correspond to the upper part of this C-shape only.
In this regime, transformation kinetics are essentially limited by the thermodynamic driving force available, and as the temperature increases, the transformation is slower, as the product of a lower driving force.
The lower part of a typical C-curve, which is not observed in this study, corresponds to a regime where diffusion becomes rate limiting.
The present results therefore suggest that over the investigated temperature range it is the availability of sufficient driving force to overcome the significant energy barrier to hydride nucleation that dominates, and hydrogen diffusion never becomes rate limiting.
This energy barrier arises from the elastic strain that is generated as a product of the misfit that forms from the large volume expansion that takes place during precipitation, as well as the energy required to form new interfaces.
To explore whether this hypothesis is physically reasonable, simple classical models for nucleation and diffusion have been used to estimate the undercooling required to produce the peak nucleation rate and the diffusion distance of hydrogen.
An equation that describes nucleation rate (Iv) as a function of temperature is presented as Eq.
(1).
(1)Iv=NokThexp-(G∗+Q)kT Here, No represents the number of nucleation sites (zirconium atoms per unit volume for homogeneous nucleation), Q is the activation energy for diffusion, h and k are the Planck and Boltzmann constants, respectively, T is the temperature in Kelvin and G∗ is the energy barrier for hydride nucleation, described by Eq.
(2) [52].
(2)G∗=92πγfγe2gn-2 This expression is comprised of three primary components, where two terms describe interfacial energy (γf and γe), and the term gn representing nucleation energy density.
As δ-hydrides are considered to take the form of a disc or oblate spheroid, two distinct surface types can be identified; the large, relatively flat surfaces possess a lower interfacial energy of γf=0.065Jm−2, while the edge has an interfacial energy taken to be γe=0.28Jm−2 [52].
The final term in this expression can be calculated from Eq.
(3) [52].
(3)gn=kBTCxln(Cs/Ceq)-g∊+gex Here, kB is Boltzmann’s Constant, T is absolute temperature, Cx signifies hydrogen concentration in hydride precipitates per unit volume, calculated as a function of temperature using equilibrium values taken from JMatPro, Cs is the amount of hydrogen dissolved in the matrix and Ceq denotes the equilibrium solubility of hydrogen in the matrix, also from JMatPro [52].
The final two terms, gε and gex represent contributions from the strain energy associated with the volumetric misfit and that from externally applied forces, respectively [52].
As the system being considered has no externally applied load, this final term will be excluded from the model.
From Eqs.
(2) and (3), there are three energies influencing the nucleation of hydrides, one acting to promote nucleation and precipitation, and two acting to hinder it.
The chemical free energy that drives precipitation is described by the term kBTCxln(Cs/Ceq) in Eq.
(3), and arises from an excess of hydrogen in solution over the equilibrium concentration.
The two converse terms are those of the energy required to form new interfaces during nucleation and precipitation, described in Eq.
(2), and the energy required to form a precipitate that misfits with the encompassing matrix and must generate strain to exist.
Eq.
(4) calculates the strain energy of any shape of precipitate, given uniform dilatation, through the Eshelby approach, while assuming no plastic relaxation takes place [72].
This technique describes a precipitate with an associated isotropic misfit and isotropic elastic properties, within an infinite and elastically isotropic parent matrix, and simulates equal deformation in both the parent and precipitate [50].
(4)g∊=29·(1+ν)(1-ν)μ(ΔT)2vβ In this expression, ν is Poisson’s ratio, μ is the shear modulus for the parent matrix, ΔT is the cubic dilatation in an unconstrained transformation, and vβ is the specific atomic volume of the precipitate phase [72].
For the purpose of this work, the expression derived by Sing et al.
for volumetric expansion as a function of absolute temperature will be used, where ΔT=0.1506+7.38×10−5T [45].
When utilising the Eshelby method to calculate strain energy, it is then assumed that this overall volume expansion is split evenly between all directions, rather than occurring anisotropically.
The value for vβ in this expression is taken as the volume of a stable nucleus, calculated for an oblate spheroid morphology with the two major axes being equal to the critical radius (r∗), derived for each temperature considered within the model.
It should be noted that from the work of Barrow et al., elastic strains in the matrix are up to 4× greater than those seen in the hydride, and as the Eshelby model considers uniform deformation in both, values predicted through this method are an approximation rather than being absolute.
In previous attempts to model the hydride precipitation, a single temperature independent value of 1×108Jm−3 for strain energy has been used [52].
Accounting for the temperature dependence of the misfit, the values produced by the Eshelby model range from 1.21×107Jm−3 at 0°C to 5.79×109Jm−3 at 500°C.
When compared with the isothermal value given by Massih et al., this would correspond to a temperature of approximately 265°C in the present system.
From the work of Puls, and later used by Barrow et al., a value of 1.66×107Jm−3 is calculated, which corresponds to the strain energy calculated through the Eshelby method at 50°C [46,73].
The present results suggest that if the misfit varies as reported in [45], then the range of misfit strains with temperature is large and cannot be ignored in attempts to simulate hydride precipitation.
In reality the situation is further complicated because plastic relaxation may take place, and the degree of which will also be temperature dependent, where softening will occur at elevated temperatures.
Nevertheless, in the nucleation stage, it is expected that the larger misfit at elevated temperature exacerbates the effect of decreased chemical driving force, further slowing the nucleation kinetics.
Fig.
7 contains the result of the nucleation model for three considered scenarios; with no strain energy contribution, where the rate and peak temperature are highest, and including strain energy, derived from the Eshelby model, for both a disc and an oblate spheroid precipitate.
From this, it can be seen that the peak nucleation rate is predicted between 113°C and 149°C, depending on the chosen system, which is reasonably consistent with the transformation map, Fig.
4.
Strain energy is shown to have a significant effect on both the magnitude and position of the nucleation rate peak, reducing the optimum temperature for nucleation by 17°C and 36°C for an oblate spheroid and disc morphology, respectively.
However, the main factor that is responsible for the high degree of undercooling required to reach the peak nucleation rate is the relatively high interfacial energy reported for the hydride phase, an observation also made in previous studies [46].
Below the peak nucleation temperature, the process of hydrogen diffusion becomes the controlling influence within the model, where the local jumping of hydrogen over the interface regulates the rate of the process.
It is noted that at high temperatures (300°C and greater), where experiments show continued precipitation, this figure suggests a nucleation rate close to zero.
This is untrue, as substantial nucleation is still predicted by the model at these elevated temperatures in the strain-free system (of the order of 1027m−3s−1 at 300°C, for example), although the introduction of strain energy does reduce this considerably.
Clearly, the present model is very simple, and in practice, nucleation of hydrides is heterogeneous, rather than homogeneous.
However, heterogeneous nucleation of hydrides at features such as dislocations or sympathetically in the strain field created by other hydrides would serve only to reduce the strain energy component of the nucleation energy barrier.
As Fig.
7 shows, even if there is no strain energy required (through complete, instantaneous relaxation), a large undercooling for nucleation is still required, owing to the interfacial energy terms.
To understand why the lower portion of the expected C-curve was not observed, even at precipitation temperatures as low as 100°C, simple diffusion distance calculations have been performed for hydrogen dissolved in zirconium.
Taking the diffusion coefficient determined by Kammenzind et al.
for hydrogen in Zircaloy-4 (288–482°C), and extrapolating to cover a range relevant to the present work, it becomes possible to estimate the diffusion length for hydrogen [74].
This is done using a simple d=Dt approximation, where D is the diffusion coefficient and t is time.
Fig.
8 shows the result of these diffusion distance calculations at two length scales, where (a) is scaled to half the axial length of the test piece, giving an indication of how far from the centre of the sample, where diffraction measurements are taken, hydrogen is able to diffuse.
Both subplots consider diffusion to be isotropic for simplicity, however, given the highly textured nature of zirconium cladding this may not be true of all directions within a component.
The data contained in (a) serve to illustrate that with elevated temperatures and long hold times, hydrogen is able to diffuse moderate distances through the zirconium matrix.
Given an hour at 500°C, the diffusion distance is calculated as 1.2mm, which is 12% of the distance from the point of beam incidence to the ETMT grips, demonstrating why axial hydrogen distribution checks are important.
Fig.
8 – (b) looks specifically at distances relevant to both the inter-hydride spacing and overall precipitate length related to the sample being studied.
The average zirconium grain diameter for the sample measures 10μm, micro-hydride lengths typically range from 10nm to 200nm (although examples of large 1μm precipitates exist) and the inter-hydride spacing taken from transmission electron micrographs is no more than 2μm.
These values were recorded for the specific sample being examined in this work, and as hydride geometry and distribution will change with hydrogen concentration, thermal history and mechanical processing, this assessment is intended to be representative, rather than definitive.
Taking the curve for 1s, it is evident that hydrogen is capable of rapidly diffusing the average measured inter-hydride spacing at temperatures close to 150°C.
Similarly, it is also able to diffuse half of the width of a matrix grain in just ten seconds at this temperature.
The overall rate of transformation is limited either by the energy barrier or diffusion rate at upper and lower temperatures, respectively, giving the customary C-shaped curve to TTT diagrams.
The values for local diffusion distances above would suggest that while the diffusion of hydrogen controls the process of precipitation at temperatures below 0°C, the presence of rapid diffusion at temperatures relevant to fuel assemblies would mean that diffusion is never rate limiting.
This explains the extremely fast precipitation observed below 150°C, where 98% of precipitation is complete almost immediately.
As temperatures increase above 150°C, the drop in precipitation rate observed in Fig.
4 then occurs through the activation energy becoming the rate limiting process.
It should be noted, however, that while hydrogen diffusion is considered not to be rate limiting above 0°C the process of nucleation is still dependent on short range diffusion, where the local jumping of hydrogen across the interface regulates the speed of nucleation.
Where hydrogen concentrations are significantly lower, such as in the case of CANDU pressure tubes where the life design allowance is 100ppmwt.
of hydrogen, the inter-hydride spacing would in-turn be larger [75].
Such increases in the required distance for hydrogen to travel could then lead to diffusion becoming a rate-limiting factor.
From the diffusion data above this would seemingly not be until inter-hydride spacing was in the order of tens of microns, at least.
For the purpose of this work, however, the above assessment of diffusion distance seems sufficient to explain the lack of the lower half of the C curve expected in a conventional TTT diagram.
Synchrotron X-ray diffraction was able to investigate successfully the isothermal precipitation of hydrides within Zircaloy-4.
From completion percentage data, a peak in precipitation rate was measured close to, or below, 100°C.
The measured concentration of hydrogen in solution while precipitating, recorded during a 1°Cs−1 cooling transient, was shown to be in poor agreement with the literature.
During quench and hold cycles, the initial solubility showed a similar agreement with the continuous cooling values, but then continued to drift towards the position of the literature solubility.
This was explained as being the product of significant residual supersaturation, where the process of precipitation was unable to keep pace with the rate of temperature change.
This effect was primarily seen at temperatures above 200°C, while those below this threshold showed little or no recordable residual supersaturation after the quench, within the bounds of experimental error.
A simple model was used to predict the peak temperature for hydride nucleation using the Eshelby method for estimating misfit strain energy in two geometries of precipitate, as well as performing a strain energy-free calculation.
This represents a lower and upper bound to nucleation rate behaviour, respectively.
The temperature dependence of the misfit was included in this model and shown to be significant in the calculation of strain energy.
While only a simple approximation, this model is reasonably consistent with the experimental observations, predicting a peak in nucleation rate at between 113°C and 149°C.
Strain energy has a modest effect, reducing the temperature of peak nucleation rate by up to 36°C, depending on the precipitate geometry considered.
The reason a large undercooling is required to reach the peak nucleation rate is mainly attributed to the relatively high interfacial energy reported in the literature for the hydrides.
Diffusion distance calculations showed that hydrogen diffusion was not significantly rate limiting given the diffusion distances required at temperatures relevant to the life of zirconium clad fuel assemblies.
Ultimately, these simulations act to justify the experimental observations made through synchrotron X-ray diffraction.
However, it should be noted that while precipitation at these higher temperatures is considered relatively slow when compared with the lower thermal region, the overall process of hydride precipitation in zirconium alloys is very rapid, owing to the high mobility of hydrogen and the large driving force that develops with increased undercooling.
The author would like to thank the Institut de Radioprotection et de Sûreté Nucléaire for funding the synchrotron experiment and providing the samples utilised in this experiment.
Rolls-Royce and the Engineering and Physical Sciences Research Council are thanked for providing sponsorship funding, as well as the Hong Kong Polytechnic University for the help and support given with numerical modelling and providing funding during international secondment.
Further acknowledgements should also be given to the infrastructural support of the Materials Performance Centre at the University of Manchester, and the University as a whole.
Thanks are also given to J. Blomqvist and T. Maimaitiyili of the University of Malmö for fruitful exchanges on synchrotron analysis technique and best practice.
Finally, thanks should be given to A.T.W.
Barrow and S.C. Connolly for advice and technical guidance towards readying this work for publishing.
SAFT-γ force field for the simulation of molecular fluids 6: Binary and ternary mixtures comprising water, carbon dioxide, and n-alkanes The SAFT-γ coarse graining methodology (Avendaño et al., 2011) is used to develop force fields for the fluid-phase behaviour of binary and ternary mixtures comprising water, carbon dioxide, and n-alkanes.
The effective intermolecular interactions between the coarse grained (CG) segments are directly related to macroscopic thermodynamic properties by means of the SAFT-γ equation of state for molecular segments represented with the Mie (generalised Lennard–Jones) intermolecular potential (Papaioannou et al., 2014).
The unlike attractive interactions between the components of the mixtures are represented with a single adjustable parameter, which is shown to be transferable over a wide range of conditions.
The SAFT-γ Mie CG force fields are used in molecular-dynamics simulations to predict the challenging (vapour+liquid) and (liquid+liquid) fluid-phase equilibria characterising these mixtures, and to study phenomena that are not accessible directly from the equation of state, such as the interfacial properties.
The description of the fluid-phase equilibria and interfacial properties predicted with the SAFT-γ Mie force fields is in excellent agreement with the corresponding experimental data, and of comparable if not superior quality to that reported for the more sophisticated atomistic and united-atom models.
Mixtures comprising water (H2O), carbon dioxide (CO2) and n-alkanes (CH3(CH2)iCH3) are of key importance due to their central role in the petrochemical industry.
The systems are relevant in range of applications including enhanced oil recovery, CO2 storage, natural gas transportation, crude-oil processing, and water purification.
An accurate description of the phase behaviour of these mixtures is crucial for the optimal design of the corresponding chemical processes.
From the modelling perspective, mixtures of water, n-alkanes, and carbon dioxide are challenging due to the disparate nature of interactions between the components which results in highly non-ideal behaviour.
While n-alkanes are non-polar, CO2 possesses a significant quadrupole moment, while H2O has a large dipole moment and also exhibits hydrogen bonding.
The resulting mixtures are therefore highly asymmetric, both in terms of the molecular size and the energetic interactions.
The asymmetry of the interactions is responsible for different types of global fluid-phase behaviour exhibited by mixtures of these species; binary mixtures of (CO2+H2O) [1,2] and (n-alkanes+H2O) [3,4] are both characterised by type III behaviour (according to the classification of Scott and van Konynenburg [5,6]), corresponding to extensive regions of (liquid+liquid) immiscibility and a discontinuous (vapour+liquid) critical line.
In type III mixtures, the higher-temperature branch of the (vapour+liquid) critical line that starts at the critical point of the less volatile component merges with the (liquid+liquid) critical line at higher pressures.
The lower-temperature branch of the (vapour+liquid) critical line, which is usually much shorter, starts at the critical point of the more volatile component and merges with the (vapour+liquid+liquid) three-phase line at an upper critical end point (UCEP).
Mixtures of carbon dioxide with long n-alkanes also display type III behaviour, whereas type II behaviour is observed with short n-alkanes [7,8].
Type II behaviour is characterised by a continuous (vapour+liquid) critical line connecting the critical points of both pure components; the (liquid+liquid) critical line does not extend beyond the critical temperatures of either pure components, and merges with the three-phase line at an UCEP at lower presures.
The experimental phase diagrams of the aforementioned mixtures have been discussed by numerous authors.
In particular, Schneider and co-workers [7,8] have undertaken comprehensive studies of binary mixtures of (n-alkane+CO2), while (n-alkane+H2O) systems have been the focus of the work of Brunner [3].
A large body of experimental data is now available on the fluid-phase behaviour of the (CO2+H2O) binary mixture, which has been summarised in reviews by Diamond and Akinfiev [9] and by Spycher et al.
[10].
The most exhaustive studies at high temperatures and pressures were conducted by Todheide and Franck [1] and Takenouchi and Kennedy [2], however these data sets exhibit considerable inconsistency.
Subsequent work has not completely resolved the discrepancies, although the data by Todheide and Franck [1] has been found to exhibit errors in the composition of up to 1mol % [10].
The absence of consistent experimental data introduces a degree of uncertainty in the characterisation of these systems.
In terms of modelling, the use of cubic equations of state (EOSs) remains common practice in petrochemical engineering applications [11].
However, because cubic EOSs are fundamentally appropriate only for mixtures of near-spherical non-associating components, an accurate description of the fluid-phase equilibria is only possible if a number of empirical adjustable parameters are introduced, thereby curtailing the predictive capability of the approach.
As an example, Cismondi et al.
[12] have accurately described the fluid-phase behaviour of n-alkanes and carbon dioxide with a cubic EOS over a wide range of thermodynamic conditions by employing forty adjustable parameters.
In studies of associating mixtures such as aqueous mixtures of n-alkanes, it is highly advantageous to use an equation of state that can directly account for association.
The statistical associating fluid theory (SAFT) [13,14], which is based on the Wertheim formalism [15–20], allows one to take both the directional nature of the hydrogen-bonding interactions and the non-sphericity (chain characteristics) of the molecules into account explicitly.
A number of variants of this molecular-based EOS have been developed over the last twenty-five years including: SAFT for variable range interactions (SAFT-VR) [21–24]; SAFT for Lennard–Jones (LJ) based segments (soft-SAFT) [25]; perturbed-chain SAFT based on the structure of the hard-sphere chain (PC-SAFT) [26]; and the group contribution versions GC-SAFT [27,28], SAFT-γ [29–31]), and GC-SAFT-VR [32].
In this context we should also mention that the Wertheim theory of association can also been coupled with a cubic EOS, as in the cubic plus association (CPA) approach [33]; this extends the applicability of traditional cubic EOS to associating fluids.
The various incarnations of the SAFT EOSs have been successfully applied to study mixtures of relevance to our current work comprising water, carbon dioxide, and n-alkanes.
The main distinctive features of the fluid-phase equilibria of these systems can be accurately reproduced including: the transition from type II to type III phase behaviour exhibited by binary mixtures of (n-alkanes+CO2) with increasing carbon number [34–36]; the barotropic density inversion and solubility minima of water in carbon dioxide seen for (CO2+H2O) [37–46]; and the solubility minima of the hydrocarbon in the water-rich phase [47,48] and the extreme immiscibility found in (n-alkane+H2O) mixtures [49–57].
There has been some controversy as to whether an unlike “association” interaction is required between carbon dioxide and water to account for the change of the slope in solubility near the critical temperature of CO2 [37,58] or whether this behaviour can be reproduced without the need for additional association interactions [40–42].
Explicit dipole and quadrupole moments have also been incorporated into a SAFT description of the intermolecular models for water and carbon dioxide [38,39,59–64], with the advantage of providing a more physically detailed representation of the unlike interactions for the description of mixtures.
Although algebraic equations of state are still the preferred tool in the petrochemical community due to its computational efficiency, molecular-dynamics (MD) or Monte Carlo (MC) computer simulation techniques provide an insight at the microscopic level and allow one to study properties that cannot by accessed directly via an equation of state, such as interfacial and transport phenomena and the behaviour of the confined fluids [65,66].
Molecular simulation essentially provides an exact numerical description of a system once the force field has been specified [67], and as a consequence the onus is on the development of accurate intermolecular potentials.
The use of direct molecular simulation is becoming ever more prevalent as a consequence of the steady increase of computational power [68] and advances in modelling techniques [69].
Mixtures containing water, carbon dioxide, and n-alkanes have received considerable attention from the perspective of molecular simulation.
The quality of the description of the thermophysical properties obtained by molecular simulation depends critically on the underlaying intermolecular potential that is employed, particularly the unlike energetic interactions between the various components of the mixture.
A variety of all-atom (AA) and united-atom (UA) classical (non-polarisable) force fields have been employed to study these mixtures with varying degrees of success; owing to the large body of research we mention only some representative examples of the more recent studies with a focus on fluid-phase equilibria to set our current work in the appropriate context.
Cui et al.
[70] have used the Gibbs ensemble Monte Carlo (GEMC) simulation technique to determine the (vapour+liquid) equilibria (VLE) of binary mixtures of (n-hexane+CO2) using the elementary physical model (EPM2) of Harris and Yung [71] to represent carbon dioxide and the model developed by Siepmann, Karaborni, and Smit [72] (often referred to as SKS) for n-alkanes; the application of the common Lorentz–Berthelot (LB) geometric-mean combining rule for the unlike attractive dispersion interactions between the hydrocarbon groups and carbon dioxide was shown to provide a reasonable description of the mutual solubilities of the components in the coexisting phases for the thermodynamic states studies, with the largest deviations (∼10%) seen for the liquid phase.
In a grand canonical MC study of the global phase fluid-phase behaviour of (n-hexadecane+CO2) using a simplified LJ united atom model, Virnau et al.
[73] have shown that the unlike interaction has to be reduced to ∼90% of the LB value in order to represent the type III critical behaviour exhibited by the system.
The fluid-phase equilibria of a variety of mixtures comprising polar and non-polar species including carbon dioxide, n-alkanes, and water represented with UA exponential-6 (EXP-6) models have been simulated by Potoff et al.
[74] using grand canonical histogram-reweighting MC; the LB combining rules employed for the unlike molecular interactions were found to be inappropriate in the case of mixtures comprising components with large differences in polarity.
The focus of the subsequent GEMC study of Vorholtz et al.
[75] was specifically on binary mixtures of carbon dioxide (represented with the EPM2 model) and water (represented with the three-site simple point charge SPC [76] and SPC/E [77] models, or with the four-site transferable intermolecular potential TIP4P [78] model); the use of LB combining rules allowed for a reasonable description of the experimental fluid-phase equilibria only over a limited range of temperatures for supercritical conditions of carbon dioxide.
One would not expect the LB combining rule to provide a good description of the thermodynamic properties of this non-ideal system [79], and a significant improvement can be achieved by adjusting the unlike (CO2+H2O) interaction (corresponding to a modification of the geometric-mean rule) [80].
Potoff and Siepmann [81] have proposed a model for carbon dioxide within the transferable potentials for phase equilibria (TraPPE) framework to determine the VLE of mixtures containing nitrogen, n-alkanes, and carbon dioxide, using both grand canonical MC and GEMC simulation.
Two-centre LJ models with point quadrupoles (2CLJQ) have also been developed by Vrabec and co-workers [82,83] to describe a broad variety of mixtures (including mixtures of carbon dioxide with nitrogen or light hydrocarbons) to a high level of accuracy by again making use of empirically adjusted unlike interactions.
The more challenging determination of the solubility of water in n-alkanes of increasing chain length (ranging from n-hexane to n-hexadecane and to longer polyethylene polymers) has been investigated by Johansson et al.
[84] by GEMC simulation with the TraPPE model for n-alkanes [85] and the SPC model for water, using an empirical adjustment of unlike interaction to provide appropriate agreement with the experimental data; the inadequacy of the LB combining rule for these systems has also been noted in the recent work of Ballal et al.
[86].
Motivated by earlier studies [87–90], Ferguson et al.
[91] determined the complementary solubility and molecular conformation of n-alkanes (ranging from ethane to n-docosane, C22) in water using the TraPPE model for n-alkanes and the SPC/E model for water with the usual LB combining rule for the unlike interactions; the aqueous solubilities of the hydrocarbons determined using the replica-exchange molecular-dynamics (REMD) technique together with an incremental Widom insertion scheme were found to be in excellent agreement with experiment up to n-decane, but did not support the break in the trend at n-undecane reported in the literature which the authors attributed to possible experimental artifacts owing to the very low solubilities of the longer hydrocarbons.
It therefore appears that the LB recipe for the unlike (water+alkane) attractive interactions is sufficient to describe the solubility of n-alkanes in the water-rich phase but not of water in the hydrocarbon-rich phase.
A comprehensive study of the (CO2+H2O) binary mixture has been reported recently in a series of papers by Panagiotopoulos and co-workers for both the fluid-phase equilibria [92,93] and transport properties [94,95]; the performance of the various models for water (SPC, SPC/E, TIP4P, TIP4P2005, TIP4P/∊, and EXP-6) and CO2 (EPM2, TraPPE, and EXP-6) were assessed with the overall conclusion that none of the force fields are capable of adequately reproducing the experimental fluid-phase equilibria over a broad temperature and pressure range.
This is a good example of the fact that in spite of the relatively high-level of molecular detail, atomistic (non-polarisable) models are limited in terms of their transferability to different thermodynamic states and/or the representability of different properties to the same level of accuracy.
An appropriate choice of the unlike dispersion interaction between water and carbon dioxide applicable over a range of conditions is of central importance in this regard.
Within the last two decades, the use of a less-detailed coarse-grained (CG) description of intermolecular interactions is gaining popularity with the main purpose of enhancing the time and length scales computationally accessible for molecular systems of increasing size or complexity.
The key to the success of a CG approach is the availability of reliable force fields.
The most important requirements of CG models are the representability (of the various thermophysical properties), transferability (to broad thermodynamic conditions), and robustness (in the accuracy of the description of the target properties).
The most common procedure for the development of CG force fields is to follow a “bottom-up” approach, where the unwanted degrees of freedom of a more detailed atomistic/molecular model are integrated out.
For this purpose several techniques have been employed including iterative Boltzmann inversion [96], force matching [97,98], and inverse Monte Carlo [99].
Unfortunately, one generally does not known a priori which degrees of freedom are important for the description of a given target physical property, and the CG parameter sets obtained in this manner are typically state, property, and system dependent.
By contrast, the SAFT-γ Mie “top-down” methodology [100] is followed in our current work, whereby an accurate molecular-based EOS, namely the SAFT-γ Mie EOS [31], is employed to develop reliable Mie (generalised LJ) CG force-field parameters directly from macroscopic experimental thermodynamic properties.
The use of a high-fidelity EOS enables the simultaneous exploration of a wide parameter space for various target properties in order to estimate the set of parameters that provide the optimal description of the experimental data.
The first use of the SAFT EOS within a top-down approach involved the parameterisation of a model of water using a Wertheim TPT1 description complemented with a contribution to account for the dipolar interactions [59].
A generic top-down approach of this type has also been employed by Vrabec and co-workers [101–103].
In this case a semi-empirical EOS was developed by correlating a large set of volumetric and phase-equilibrium simulation data for two-center LJ (2CLJ) models which incorporate a dipole (2CLJD) or a quadrupole (2CLJQ) moment; the EOS was then used to develop reliable force fields for 78 pure components and 267 binary mixtures, including carbon dioxide and n-alkanes [83].
In a series of papers Elliot and co-workers have combined MD with high-temperature thermodynamic perturbation theory [104–106] within SAFT/SPEEDMD (for models based on the discontinuous square-well potential) [107–111] and, more recently, the SAFT-γ platform (for models based on the Mie potential) [112,113] to successfully parameterise CG intermolecular potentials for a broad range of fluids including paraffins, olefins, aromatics, ethers, alcohols, and carboxylic acids.
The PC-SAFT EOS has also been used by van Westen et al.
[114] to obtain force-field parameters for n-alkanes represented as chains of LJ segments; however, because the direct link between the intermolecular potential and the PC-SAFT description is partly lost, the final parameter set had to be obtained in an iterative manner by performing a number of additional simulations.
Gross and co-workers [115,116] have now refined the methodology for the development of transferable force fields for n-alkanes, olefins, and ethers based on the Mie potential by employing the PC-SAFT EOS to guide grand canonical MC simulations for the determination of the size and energy parameters.
The main advantage of using a framework based on the SAFT-γ Mie EOS [31] is that the link between the underlying intermolecular potential and the thermodynamic properties (through the Helmholtz free energy) is explicitly retained [24], thereby allowing one to estimate the CG force-field parameters reliably from the macroscopic properties of the system by direct parameter estimation with the analytical EOS.
The SAFT-γ Mie top-down approach [100] has now been applied successfully to develop CG force fields for broad classes of molecular fluids including carbon dioxide [117], greenhouse gases [118], hydrocarbons [118,119], aromatics [120], water [121], and aqueous solutions of nonionic [122], light-switching [123] and superspreading surfactants [124].
A corresponding states correlation can also be employed to parameterize models for molecules represented as chains of Mie segments in cases where only limited experimental data is available [125], though the procedure is not generally appropriate to determine the intermolecular parameters between the different species of the mixtures.
Some of the SAFT-γ CG Mie force-fields previously developed for pure components have already been used successfully to simulate selected thermodynamic, interfacial and transport properties of numerous mixtures [126–130].
In our current paper we apply the SAFT-γ Mie methodology to obtain accurate estimates of the unlike dispersion interactions for binary mixtures containing water, carbon dioxide, and n-alkanes, thereby allowing for the global fluid-phase equilibria to be determined by direct MD simulation.
The quality of the description of the mixtures is very sensitive to the specific interactions employed between the CG beads of the molecules, particularly in the case of the aqueous systems.
Comparisons are made where possible with the corresponding results obtained using the more detailed atomistic and united-atom models including explicit electrostatic interactions.
The SAFT-γ Mie EOS [31] provides a direct and robust link between the macroscopic and microscopic properties of the fluids.
Our current paper is a part of a series of contributions on the development of the SAFT-γ CG force fields.
These are based on the Mie intermolecular potential, which can be represented as(1)ϕMie(r)=C∊σrλr-σrλa,where(2)C(λa,λr)=λrλr-λaλrλaλaλr-λa.Here, λr and λa are repulsive and attractive exponents, respectively, controlling the softness/hardness and the range of the attraction of the potential, σ is the length scale, related to the average diameter of the spherical segment, and ∊ is the energy parameter.
The force field characterising the interactions between the molecular segments in the theory and the molecular simulations are based on the same Mie potential.
The intermolecular parameters are determined by using the “top-down” approach, estimated by matching the experimental macroscopic bulk properties of the fluid.
The focus of of our work is the description of binary and ternary mixtures of water, carbon dioxide, and n-alkanes.
For the three pure components, the saturated-liquid density and the vapour pressure are chosen as the target properties in the parameter estimation procedure [117,121,122].
For the sake of convenience, the parameter set is summarised in table 1.
The CO2 model is represented by a single-site CG sphere based on the Mie (23–6.66) potential, without any additional electrostatic interactions [117].
Despite the simplicity of the force field, with it one can accurately reproduce the entire phase envelope using a single parameter set.
The prediction of properties which are not used in the parameterisation procedure such as enthalpy of vaporisation, interfacial tension, compressed liquid density, as well as the second-derivative thermodynamic properties (thermal expansion coefficient, isobaric heat capacity, isothermal compressibility, speed of sound, and Joule–Thomson coefficient) were found to be in a very good agreement with experimental data [117].
In a comprehensive study, which compared the thermodynamic properties of seven different force fields for carbon dioxide, Aimoli et al.
[131] found that the SAFT-γ Mie force field provides a comparable accuracy to the higher-resolution force fields, including rigid and flexible atomistic three-site models.
The SAFT-γ CG Mie model of water is also based on the mapping of a single molecule to a single CG bead [121].
A spherical isotropic potential alone is not appropriate to provide an accurate physical representation of the interactions over a broad range of thermodynamic conditions as a consequence of averaging out the directional and electrostatic interactions of a highly polar and associating fluid such as water.
Since the effective CG interactions between water molecules are found to vary significantly with temperature, temperature-dependent size and energy parameters are introduced.
The saturation densities and vapour pressures can be accurately reproduced over a wide range of conditions using this set of temperature-dependent parameters.
An alternative parameterisation is necessary when the interfacial properties of water are of interest owing to the low level of resolution of the single-site model of water and the related issues of transferability with a CG procedure of this type [121].
The homologous series of n-alkanes are represented here as homonuclear chains of tangent Mie spherical CG segments.
The development of CG models for long n-alkanes such as n-decane (n-C10H22) and n-eicosane (n-C20H42) has already been successfully demonstrated using the SAFT-γ Mie formalism [118].
The n-decane molecule was represented by chains of three and n-eicosane chains of six fully flexible tangentially bonded Mie segments.
A certain degree of parameter degeneracy in terms of overall performance is expected as a consequence of the conformal nature of the EOS description [132].
In our current work, we use an alternative CG mapping for n-alkanes developed in reference [122], where each segment was taken to represent three alkyl carbon backbone atoms and their corresponding hydrogen atoms.
By applying this mapping, n-alkanes chains containing multiples of three carbon units can be represented directly: n-C6H14, n-C9H20, n-C12H26, n-C15H32, n-C18H38, etc.
A good description of the thermodynamic properties of these alkanes is found to be provided with CG alkyl beads characterised by the Mie (15–6) potential.
For convenience, the exponent pair (15–6) is also used to represent the interactions between the CG beads of the intervening alkanes considered here; the number of segments m is taken to be the nearest integer of the division of the carbon number C by three.
The size σ and energy ∊ parameters are then estimated from the experimental saturated-liquid density and vapour pressure of the individual alkanes following the usual SAFT-γ Mie procedure.
The chosen mapping is by no means unique, as one can postulate parameter sets that fulfil other requisites, such as being “universal” across the entire homologous series [119] or correlated to the critical properties [125].
It is important to treat the degree of flexibility of chain molecules in an appropriate manner.
In the case of n-alkanes it has been shown that a fully flexible model provides an accurate description the (vapour+liquid) equilibria and interfacial properties of the pure component system [118].
The effect of the chain flexibility has been examined recently in the context of the representation of n-alkanes with the SAFT-γ CG Mie force field [119]; a realistic description of the rigidity of the carbon backbone is necessary to accurately represent the structural and transport properties of the fluid.
A physically realistic treatment of the semi-flexible nature of n-alkanes is also very important in the case of aqueous mixtures, where the solubility and structure of the hydrocarbon in water is found to be very sensitive to the degree of flexibility of chain molecules [122].
As with the majority of versions of the SAFT EOS, the SAFT-γ Mie EOS [31] used in our current work is based on the first-order thermodynamic perturbation theory (TPT1) of Wertheim [133,20].
The specific geometry of the chain molecules is not predetermined at this level of the theory, although the approximations inherent in the TPT1 approach are expected to provide a more accurate description for elongated chains.
This is beneficial when one considers a relatively rigid alkyl backbone, described with realistic harmonic bond stretching and bond-angle bending intramolecular contributions:(3)Uintra=∑bondkbond(r-r0)2+∑anglekangle(θ-θ0)2,where kbond and kangle are the bond and the bending harmonic spring constants, respectively, and r0 and θ0 are the corresponding distance and angle at the potential minimum.
The parameters for the intramolecular bonding interactions are also given in table 1, and a detailed discussion of their determination is given in reference [119].
Simple combining rules are applied to describe the unlike interactions between the various components in the mixture [24].
The unlike size parameter σij is obtained using the Lorentz arithmetic-mean combining rule:(4)σij=σii+σjj2.The unlike repulsive λr,ij and unlike attractive λa,ij exponents are both determined using the following relationship:(5)λr/a,ij-3=(λr/a,ii-3)(λr/a,jj-3),which is consitent with the geometric mean of the van der Waals attractive constant [24].
The unlike energy parameter ∊ij is represented as(6)∊ij=(1-kij)σii3σjj3σij3∊ii∊jj,where kij is an adjustable binary-interaction parameter, with kij=0 taken as default to represent the commonly employed Berthelot geometric-mean combining rule (of the van der Waals attractive constant).
A proper prescription of the unlike energetic parameter is critical to adequately describe the interactions between different components [79].
Here we employ the SAFT-γ Mie EOS to estimate the kij parameter from appropriate experimental data of the given mixture in order to account for the differences in the attractive interactions between components.
Although the binary kij parameter of the mixtures is adjusted to experimental fluid-phase equilibrium data at only one temperature, the choice is transferable to various thermodynamic conditions and is therefore applied to represent the entire phase diagram.
A temperature dependence is already incorporated in the like interactions between water molecules, which allows one to capture the change in the interactions between the components of the aqueous mixtures at different temperatures.
The final set of molecular parameters (cf.
table 1), informed by the SAFT-γ Mie EOS, are used without modification in the MD simulations.
Two/three adjacent slabs of pure components located in a rectangular simulation box are taken as the initial configuration of the binary/ternary mixture, respectively, with standard periodic boundary conditions [67].
The fluid-phase equilibria is determined using MD simulation in the canonical ensemble (NVT), with constant number of particles N, volume V, and temperature T, using a temperature-quench algorithm [134].
Simulations of the (liquid+liquid) coexistence are initially carried out in the (isobaric+isothermal) NPT ensemble, with constant number of particles, temperature, and pressure P, for the sole purpose of allowing the system to attain densities close to the equilibrium values, followed by simulations in the NVT ensemble at the established equilibrium box dimensions to determine the average coexistence properties.
The temperature of the system is fixed using a Nosé–Hoover thermostat [135,136] with a coupling constant of 1.0ps, and the pressure is maintained by means of a Parrinello–Rahman barostat with a relaxation constant of 10.0ps.
The equations of motion are integrated using the leap-frog algorithm [67] with a time step of 5fs using the GROMACS 4.5.5 package [137].
A cutoff radius of 3.0nm is used throughout for all of the interactions between the various CG Mie beads; a relatively large cutoff is necessary in order to retain the close correspondence with the theory [121].
As an initial configuration for the simulation of (liquid+liquid) equilibria (LLE), a phase separated system is assembled in the rectangular simulation box, with dimensions constrained to a minimum of 6nm in x and y direction and four times larger in the z direction.
The total number of particles is chosen according to the box dimensions.
For the simulations of the VLE in the NVT ensemble, the z dimension is chosen to be approximately three times larger than for the LLE in order to cater for the low density of the vapour.
After equilibrium has been established, the configurations are sampled for a further 20ns to determine the configurational average of the various properties.
The composition of the fluid phases is estimated from the corresponding number density profiles of each component.
For each of the binary mixtures studied, the unlike interaction parameter is adjusted to the fluid-phase equilibria at one thermodynamic state (isothermal slice) by matching the value predicted with the SAFT-γ Mie EOS to the corresponding experimental data.
It is important to note that only a single binary adjustable parameter is required to account for the unlike energetic interactions between the components of the mixture over a wide range of conditions.
The parameters determined in this way are used to investigate the entire fluid-phase behaviour (including VLE, LLE, critical lines, and three-phase lines) predictably using the SAFT-γ Mie EOS.
Three-dimensional phase diagrams are produced to visualise the global phase behaviour.
For the mixtures with n-alkanes, the same binary kij parameter is used to describe the unlike interactions between water or carbon dioxide and the alkyl beads for components of different chain length.
MD simulation of the fluid-phase equilibria are then carried out for the mixtures described with the CG force fields developed with the SAFT-γ Mie EOS and the corresponding simulation data are compared critically with the experimental and theoretical results.
The experimental data of Takenouchi and Kennedy [2] for the (CO2+H2O) mixture at a temperature of T=473K are employed to obtain the unlike energetic interaction parameter between the two components; correlated pure-component data from the NIST database are also used in the analysis [138].
A value of kij=-0.07 estimated with the SAFT-γ Mie EOS is found to provide the best possible agreement with experimental data; this finding is consistent with the study of the (CO2+H2O) mixture carried out by Forte [44] with the SAFT-VR EOS, and by Niño-Amézquita et al.
[45] with the PCP-SAFT EOS.
The quality of representation with the SAFT-γ Mie EOS is illustrated in figure 1.
The solubilities in both phases are reasonably well reproduced, including the solubility minimum of water in the CO2-rich phase.
The transferability of the force field can be validated by employing the same parameter set at temperatures which are not considered in the parameterisation procedure.
The adjustable parameter kij is treated as temperature-independent and appears to be transferable to different conditions (e.g., at T=383K as seen in figure 1).
For this temperature the phase compositions and the solubility maximum are accurately captured at lower pressures.
At high pressures, the deviations become more apparent.
Overall, the corresponding data obtained by MD simulations performed at the same thermodynamic conditions agree well with the theoretical predictions.
The entire fluid-phase behaviour for the system with the unlike interaction parameter kij=-0.07 can be determined easily with the SAFT-γ Mie EOS.
The (CO2+H2O) mixture exhibits type III phase behaviour according to the classification of Scott and van Konynenburg [5,6], which is characteristic of highly immiscible fluids.
The global features of the fluid-phase behaviour, which is displayed in figure 2, is indeed dominated by extensive regions of fluid immiscibility.
The discontinuous (vapour+liquid) critical line can also be seen in the PT projection of the PTx surface shown in figure 3.
The critical line at intermediate pressures is in good agreement with experimental data but there are larger deviations at higher pressures.
The lower branch of the (vapour+liquid) critical line merges with the three-phase line at the UCEP; the three-phase line is at pressures slightly lower than the (vapour+pressure) curve of pure carbon dioxide.
The critical point of the CO2 is overestimated by the SAFT-γ CG Mie model [117] due to the mean-field nature of the theory, which in turn leads to a slight overestimation of the pressures of three-phase coexistence with the SAFT-γ Mie EOS.
A clearer representation of the three-phase line and the lower-pressure branch of the critical line is illustrated in the inset of figure 3.
The three-phase line separates the (liquid+liquid) equilibrium at higher pressures from the (vapour+liquid) equilibrium at lower pressures; at the three-phase line, two immiscible liquid phases coexist with the vapour phase.
In figure 4, the isothermal pressure-composition (Px) slice at a temperature of T=298K predicted with the SAFT-γ Mie EOS is compared to the corresponding experimental data [139–144].
The (vapour+liquid) equilibrium is in very good agreement with experiment; at the (liquid+liquid) equilibrium conditions, the concentration of carbon dioxide is slightly underestimated in both phases.
The three-phase pressure is reproduced within a deviation of 0.45MPa compared to the experimental value.
A small (vapour+liquid) region with a CO2-rich vapour in coexistence with a CO2-rich liquid phase is observed above the three-phase line.
The theory is able to capture the main features of the (CO2+H2O) mixture at this temperature, providing an explanation to the solubility minimum of water in the CO2-rich phase at higher temperatures: the solubility minimum is clearly ascribed to the proximity of the three-phase line.
At low temperatures, the solubility curve exhibits a sharp discontinuity at the three-phase coexistence pressure, and as the temperature increases above the UCEP, the discontinuity disappears thereby causing a minimum at pressures close the UCEP.
This behaviour is described in greater detail in reference [40].
The MD simulations reproduce the theoretical predictions very accurately (cf.
figure 4).
The three-phases – the H2O-rich liquid phase (L1), the CO2-rich liquid phase (L2), and the gaseous phase (G) – can be clearly distinguished in the corresponding density profile and the snapshot, presented in figure 5.
The adsorption of CO2 at the interface between the L1 and L2 phases is distinctly featured in the density profile.
The CG model of H2O employed in our study represents accurately the volumetric properties at the expense of only providing qualitative agreement for the interfacial tensions.
For an accurate representation of the interfacial properties, a different parameter set should be employed [121].
The predictive capabilities of the aformentioned SAFT-γ CG Mie models are accurate enough in aiding to detect errors and discern between conflicting experimental data sets [145].
The adsorption phenomena at the (liquid+liquid) interface can be studied via MD simulations at different temperatures and pressures.
The density profiles at temperatures of T=(383 and 423)K are presented in figure 6 at pressures of P=(8.18, 31.87, 62.30, and 102.10)MPa.
The density profile of H2O is not very sensitive to pressure and is therefore shown only at one pressure for clarity: CO2 exhibits a positive surface activity (dρCO2/dz=0,d2ρCO2/dz2<0), whereas H2O does not.
The surface activity of CO2 increases as the pressure increases until the limiting bulk liquid density is reached and saturation is established.
A lower surface activity and the widening of the interface produced by a decreased in the slope of the CO2 density curve is observed with increasing temperature.
Apart from the interfacial behaviour, the pressure dependence of the bulk densities of the (CO2+H2O) mixture are of scientific interest as the mixture exhibits mass barotropy (density inversion).
In this phenomenon, the supercritical CO2 phase becomes denser than the H2O phase at high pressures, so that the two phases change their positions in a gravitational field.
The experimental densities of the mixture [146] are compared to the predictions with the SAFT-γ Mie EOS and MD simulations at temperatures of T=(298, 304, 383, 423, and 473)K in figure 7.
The theoretically calculated densities of the CO2-rich phase are in excellent agreement with experiment, both in the sub- and the super-critical region, whereas the densities in the H2O-rich phase are slightly underestimated.
The pressures at which the barotropic density inversion occurs, at a given temperature, are compared to the experimental values [1] in table 2.
Taking into account that these are predictions with very simple models using only one adjustable unlike interaction parameter, the agreement between theoretical predictions and experimental data is quite encouraging.
The MD simulation data accurately reproduce the values predicted with the SAFT-γ Mie EOS (and thereby the experimental data).
The description of the fluid-phase equilibria for the (CO2+H2O) mixture obtained with our SAFT-γ CG Mie models are now compared to the corresponding results of the atomistic simulations determined by Liu et al.
[92] for some of the popular force fields for H2O (SPC, TIP4P, TIP4P2005, and EXP-6) and CO2 (EPM2, TraPPE, and EXP-6) using LB combining rules to describe the unlike pair interactions.
The simulated data can also be used to assess the contradictory experimental data reported by Todheide and Franck [1] and by Takenouchi and Kennedy [2].
Two isothermal slices of the PTx fluid-phase equilibrium surface are examined in figure 8: one at T=423K corresponds to the region well below the critical point of water and the other to the near-critical region (T=548K).
Due to considerable disagreement between the experimental data sets, it is difficult to judge which model delivers the best performance.
The representation with our SAFT-γ CG Mie force field is in closer agreement with the experimental data of Takenouchi and Kennedy [2], clearly exhibiting a solubility minimum at low pressures, which appears due to the proximity of the three-phase line at low temperatures, as discussed earlier.
The solubility minimum is not reproduced by any of the atomistic models.
When compared with the experimental data of Todheide and Franck [1], the best performance is achieved by the EXP-6 models for the CO2-rich phase and TraPPE/TIP4P2005 model for the H2O-rich phase at T=423K.
At high temperatures, the EXP-6 models perform better than the other atomistic models: the lower part of the phase envelope can be accurately reproduced, whereas the critical pressure is largely underestimated compared to both sets of experimental data.
The SAFT-γ Mie CG models exhibit a similar behaviour, with good agreement with experiment at low pressures but larger deviations and an underestimation of the critical point at high pressures.
Despite the high level of molecular resolution of the atomistic models one cannot adequately reproduce the fluid-phase behaviour over a wide temperature and pressure range: at low temperatures, the solubility in the both phases cannot be reproduced simultaneously; at high temperatures, only a qualitative description can be achieved with the atomistic models.
The performance of the SAFT-γ Mie CG models can be considered at least as good if not better than the atomistic models.
The main issue with the atomistic force fields is the use of conventional LB combining rules for the dispersion interactions between unlike components of the mixture; an improved description could certainly be obtained by refining the unlike attractive interaction, though this typically requires an iterative simulation procedure.
Having direct robust simultaneous links between theory, experiment, and simulation enables one to assess the adequacy of the force-fields in describing the fluid-phase behaviour over a wide range of conditions with greater confidence.
The approach described in the previous section is applied to study aqueous mixtures of n-alkanes which are characterised by broad regions of extreme (liquid+liquid) immiscibility.
A binary interaction parameter of kij=0.36 for the CG energetic interaction between the H2O and alkyl beads is estimated from the solubility of H2O in the n-hexane-rich coexisting liquid phase at a temperature of T=473K.
The quality of the theoretical representation of the experimental VLE and LLE reported in references [147–149] is illustrated in figure 9.
The (n-hexane+water) mixture exhibits a heteroazeotrope, where the vapour phase coexists with two immiscible liquid phases.
The azeotropic point is accurately captured by the SAFT-γ Mie EOS both in terms of composition and pressure at the given temperature.
MD simulations of our CG force field are seen to correctly reproduce the solubility of water in hexane-rich liquid phase.
The compositions of both phases in (vapour+liquid) equilibria are also accurately captured by the simulations.
The simulated values for the phase composition at the azeotropic point display remarkable agreement with the theoretical prediction and the experimental data.
The corresponding density profile of the mixture at the temperature of T=473K and pressure of P=3.33MPa is illustrated in figure 10.
Three phases – a H2O-rich liquid phase L1, an n-hexane-rich liquid phase L2, and a gas phase G – are clearly distinguishable from the densities of the bulk regions, which can also be seen in the snapshot of a representative configuration from the MD simulation.
The solubility of water in the n-hexane-rich liquid phase at LLE is accurately captured by the SAFT-γ Mie CG parameter set developed in our current work, both by the equation of state and by molecular simulation.
The concentration of n-hexane in the H2O-rich liquid phase is however very low and represents a challenge for any modelling technique.
Whereas the experimental mole fractions are of the order of 10-6 (xC6H14=1.5·10-6) [147], the prediction with the SAFT-γ Mie EOS is xC6H14=4.8·10-9.
Gratifyingly, the MD simulation of an equilibrium drop of n-hexane in bulk water described with our SAFT-γ Mie CG force field yields a solubility of xC6H14=3.3·10-5; the reader is referred to reference [122] for details of the procedure employed.
The modelling techniques may require refinement and a more sophisticated analysis, however it is encouraging that the order of magnitude of the experimental values is reasonably well reproduced.
Using a value of kij=0.36 for the unlike CG interactions between the H2O and alkyl beads, we extend the study to predict the rest of the fluid-phase diagram.
The family of (n-alkanes+H2O) mixtures is known to exhibit type III behaviour [3].
The extent of the (liquid+liquid) immiscibility gap can be visualised in the three-dimensional PTx phase diagram in figure 11.
The discontinuity of the (vapour+liquid) critical line is clearly seen in the corresponding PT projection shown in figure 12.
The description of the UCEP with the SAFT-γ Mie EOS terminating the low-temperature branch of the (vapour+liquid) critical line is slightly overestimated compared to experimental data due to the overprediction of the critical point for n-alkanes, which is apparent from the inset of figure 12.
In heteroazeotropic systems of this type, the three-phase line is located at higher pressures than the vapour pressures of either of the pure components.
As was already evident from figure 9, the prediction of the three-phase line is in excellent agreement with the experimental data.
The upper branch of the (vapour+liquid) critical line should exhibit a minimum typical of systems with (gas+gas) immiscibility of the second kind [150].
However, our calculations do not capture the change of the slope of the critical line thereby suggesting a (gas+gas) immiscibility of the first kind, characteristic of systems with even greater disparity in the intermolecular forces.
The interfacial profiles can be studied by MD simulation at pressures corresponding to (vapour+liquid) and (liquid+liquid) equilibria.
The interfacial region of the (n-hexane+H2O) mixture is shown in figure 13 at a temperature of T=473K and pressures below (P=2.16 and 2.87MPa) and above (P=6.25MPa) the three-phase line.
As for the mixture with CO2, H2O does not exhibit any surface activity in mixtures with n-alkanes, i.e., the average density profile has a monotonic hyperbolic shape.
By contrast, at pressures below the three-phase line, n-hexane exhibits a positive surface activity which increases as the pressure is increased.
Above the three-phase line pressure, n-hexane reaches the bulk liquid density and does not show any activity at the interface.
One can clearly see the progressive adsorption of n-hexane at the water surface in figure 13.
At low pressures a small surface excess is present, which grows into a liquid-like film that eventually becomes a bulk liquid phase at sufficiently high pressures.
Due to the lack of appropriate experimental data for the densities of the coexisting phases of the (n-hexane+H2O) mixture at the conditions of interest, we perform measurements in our current work.
The mass densities of the mixture are measured at a temperature of T=373.15K over the pressure range P=(0.6 to 20)MPa by using a stainless steel high-pressure LLE cell and a DMA HP densimeter (Anton Paar GmbH, Austria) with an accuracy of 5·10-3kgm-3.
Details of the experimental procedure are given in the Appendix, and the densities are summarised in table 3.
From the modelling perspective, the bulk densities can be obtained both by using the SAFT-γ Mie EOS or by direct molecular-dynamics simulation.
The calculated densities of (n-hexane+H2O) mixture are compared to the experimental data in figure 14.
A barotropic density inversion for (n-hexane+H2O) is expected only for long-chain alkanes, starting from n-octacosane (n-C28H58) [3], and therefore is not observed in the case of n-hexane.
Very good agreement between the densities predicted by the SAFT-γ Mie EOS, the MD simulations, and the experiment data is observed at T=373K; the MD simulations are also seen to reproduce the theoretical results with high accuracy at T=473K.
The wealth of data available for aqueous alkane mixtures provides a testing ground for the transferability of the model.
If the force field is transferable, the unlike interaction parameter which has been adjusted to reproduce the fluid-phase equilibria of the (n-hexane+H2O) mixture should also provide quantitative agreement with experiment for mixtures involving the other n-alkanes.
The experimental data for the (n-dodecane+H2O) mixture is assessed at a temperature of T=603K, which is above the UCEP.
The SAFT-γ Mie EOS captures both the low-pressure region of VLE and the high-pressure region of LLE (cf.
figure 15).
The MD simulations of the system represented with the SAFT-γ CG Mie force field given in table 1 are found to reproduce the theoretical values with reasonable accuracy for both the VLE and LLE phase boundaries.
The three-dimensional PTx fluid-phase diagram obtained with the SAFT-γ Mie EOS is presented in figure 16.
The discontinuity of the (vapour+liquid) critical line can be visualised as a PT-projection in figure 17.
As for the (n-hexane+H2O) mixture, the three-phase line is in excellent agreement with experiment.
Due to the overprediction of the critical point of the n-alkanes, the critical pressure close to the critical point of n-dodecane is overpredicted and the minimum in the upper branch of the critical line is not reproduced.
Nevertheless, the parameter set is capable of representing the main features of the fluid-phase equilibria and is transferable not only to different conditions but also to others members of the homologous series of the n-alkanes.
Binary mixtures of n-alkane and CO2 are particularly interesting because the fluid-phase equilibria exhibits a change from type II to type III behaviour on increasing the chain length of the n-alkane.
Capturing this feature with only one adjustable parameter for the entire homologous series is a challenging task for any equation of state [34].
A value of the binary parameter of kij=0.08 between the unlike interaction between the CO2 and alkyl CG beads is found to provide a good match with experimental data of the VLE data of the (n-hexane+CO2) mixture at a temperature of T=313K.
The corresponding parameter set (cf.
table 1) is found to provide a good representation of the fluid-phase equilibria at temperatures of T=(353 and 393)K for pressures removed from the critical point, whereas the critical point is overestimated by the theory as can be seen from figure 18a).
The results of MD simulations performed at the given temperatures compare well with the theory.
The same force-field parameters can be used to study the fluid-phase behaviour of other members of the n-alkane homologous series: an isothermal pressure-composition (Px) phase diagram for the long-chained (n-pentadecane+CO2) mixture is displayed in figure 18b).
The change of fluid-phase behaviour compared to that of the shorter-chain alkanes is evident with the appearance of the LLE at low temperatures (here at T∼292K).
In the theoretical prediction, the (liquid+liquid) immiscibility persists at high pressures, whereas the experimental results indicate that total miscibility is acheived for pressures above 20MPa.
The VLE is correctly described with the SAFT-γ Mie EOS [31] using the parameters given in table 1.
At the temperatures above the critical point of pure CO2, the fluid-phase equilibrium is accurately reproduced by the equation of state at low and intermediate pressures, the critical point of the mixture is however overestimated.
MD simulations performed based on the same force field reproduce the theoretical and experimental results to high accuracy.
The change from type II behaviour for the short chain alkanes to type III for the long chain alkanes is evident from the three-dimensional representation of the fluid-phase equilibria of mixtures of CO2 with n-hexane and n-pentadecane in figure 19 and the corresponding PT projection in figure 20.
The phase behaviour of the (n-hexane+CO2) mixture is qualitatively reproduced, displaying the continuous (vapour+liquid) critical line, which connects the critical points of the both components, and the (liquid+liquid) critical line, which merges with the three-phase line in UCEP.
Quantitatively, the (vapour+liquid) critical line is overestimated compared to the experimental data [151].
By contrast, the discontinuous (vapour+liquid) critical line can be identified in the PT representation of the (n-pentadecane+CO2) mixture, illustrating type III behaviour.
Starting from the critical point of n-pentadecane, the PT projection of the (vapour+liquid) critical line exhibits a maximum and then a minimum with decreasing temperature until it finally merges with the (liquid+liquid) critical line.
Again, only qualitative agreement with experiment is achieved: the experimental data [151] indicate that the critical line is characterised by a maximum at a temperature of T∼385K and a pressure of P∼23MPa and a minimum at ∼301K and ∼12MPa, while the critical line predicted with the SAFT-γ Mie EOS has a maximum at ∼450K and ∼35MPa and a minimum at ∼350K and ∼30MPa.
An analysis of the behaviour of the density of the (n-pentadecane+CO2) mixture predicted with the SAFT-γ Mie EOS is also made by comparison with the corresponding experimental data at different temperatures and pressures.
Mass barotropy has been observed experimentally for mixtures of CO2 with n-alkane homologues that exhibit type III phase behaviour [152].
The shortest alkane that exhibits a barotropic density inversion in mixtures with CO2 is n-tetradecane.
The theoretical studies of Segura and co-workers [153,154] have suggested that (n-alkane+CO2) mixtures characterised by type III behaviour exhibit both mass and molar barotropy, whereas type IV mixtures (e.g., n-tridecane+CO2) only display molar barotropy.
In our current work, the experimental densities reported for the (n-pentadecane+CO2) mixture by Tanaka et al.
[155] at a temperature of T=313K are compared to the predictions of the SAFT-γ Mie EOS, thereby revealing excellent agreement (figure 21).
Further isotherms at temperatures of T=(292, 353, and 393)K are calculated with the EOS and are found to be in good correspondence with the results of MD simulation.
Mass barotropy is observed at low temperatures: inversion is predicted at T∼292K and P∼9.5MPa for the MD simulation data, and at T∼313K and P∼22MPa with the EOS.
The adsorption phenomena in the interfacial region of the (n-pentadecane+CO2) mixture is studied at subcritical (T=292K) and supercritical (T=353K) temperatures of CO2 (cf.
figure 22).
At these conditions, CO2 is found to adsorb at the interface, whereas n-pentadecane does not exhibit any surface activity.
The extraordinary surface adsorption of CO2 in mixtures of long alkanes has already been examined in detail with a combination of square-gradient theory and SAFT [156,128].
At a pressure of P=5.2MPa, the maximum adsorption seen in the density profile at the temperature of T=292K is approximately three times higher than that observed at T=353K.
The large excess adsorption exhibited at the lower temperatures is an indicator of the proximity to (liquid+liquid) equilibrium (figure 18).
An increase in the density of CO2 in both phases and in the interfacial region is observed with increasing pressure.
A recent and comprehensive experimental study of the (n-decane+CO2+H2O) ternary mixture has been undertaken by Forte et al.
[157].
As the authors point out, a large part of the observable fluid-phase diagram is characterised by a three-phase region due to the immiscibility of the corresponding binary systems.
In our current work, we compare the experimental data with the description obtained with the SAFT-γ Mie EOS and from MD simulations of the CG models.
No additional parameters need to be determined for the ternary mixture, as all of the binary unlike interaction parameters have been specified for the binary systems.
In table 4 and figure 23 we compare the experimental [157] coexistence compositions of the ternary mixture with the predictions of the SAFT-γ Mie EOS and the MD simulations at T=393K and four different pressures removed from critical region (P=6.02, 7.58, 9.59, and 11.81MPa).
The fluid-phase behaviour is clearly dominated by a region of three-phase equilibria, the extent of which decreases with increasing pressure.
The SAFT-γ Mie EOS can be seen to provide an accurate description of the experimental measurements of the three-phase compositions for this range of thermodynamic conditions.
Additionally the extent of the two-phase coexistence is well represented with the theory.
Only very small single-phase regions can be detected at the margins of the ternary phase diagram.
The compositions of the CO2- and H2O-rich phases obtained by MD simulation of the three-phase region with the corresponding SAFT-γ CG Mie force fields are in good agreement with theoretical predictions; the solubility of CO2 in the n-alkane-rich phase is slightly overestimated for all pressures.
The overall trend of increasing solubility with pressure is correctly captured by simulations.
The density profiles of the (n-decane+CO2+H2O) ternary mixture at P=6.02MPa are displayed in figure 24.
Three phases (H2O-rich phase L1,n-decane-rich phase L2, and CO2-rich phase G) can clearly be distinguished from the magnitude of the total density, and are also apparent from the snapshot of the MD simulation.
From the figure one finds that CO2 exhibits a large adsorption peak at the interface between the H2O-rich and n-decane-rich phases and a small adsorption peak at the interface between the CO2-rich and n-decane-rich phases.
The density and the width of the interfaces are also found to increase with increasing pressure.
In our work we employ the SAFT-γ CG Mie force field to study binary and ternary mixtures comprising of water, carbon dioxide, and n-alkanes.
Based on the models developed for pure components and binary-interaction parameters informed by the SAFT-γ Mie equation of state, we obtain a quantitative description of multicomponent, multiphase equilibria.
The phase compositions, densities, critical lines, and three-phase lines predicted by the theory are found to accurately match the experimental data.
In addition, the same intermolecular parameter set is employed directly in molecular-dynamics simulations, essentially reproducing the aforementioned results but also allowing the study of the interfacial properties providing a microscopic molecular treatment of the system.
The key to the success of the SAFT-γ CG Mie procedure is that the EOS is based on a well defined Hamiltonian and thus can be used to extract the molecular force-field parameters directly from the macroscopic properties of the system.
This top-down coarse-graining approach offers a direct link between theory, simulation, and experiment, thereby allowing multiscale modelling of the system.
An advantage of the theory over simulation or experiment is that it allows one to gain an insight of the entire phase diagram in a very efficient manner.
On the other hand, molecular simulation provides microscopic information about the structural and interfacial properties, which are not directly accessible from the equation of state and difficult to probe experimentally.
Olga Lobanova is very grateful to Imperial College London for an Excellence Award in support of her PhD studies.
Additional support to the Molecular Systems Engineering Group from the the Engineering and Physical Sciences Research Council (EPSRC) of the UK (Grants EP/E016340 and EP/J014958), the Joint Research Equipment Initiative (JREI) (GR/M94426), the Royal Society-Wolfson Foundation refurbishment scheme (RSWF/SUC11/RND3/EEP), and the Thomas Young Centre (TYC-101) is gratefully acknowledged.
The mass densities of the (n-hexane+H2O) binary mixture are measured at a temperature of T=373.15K over a pressure range of P=(0.6 to 20)MPa by using a stainless steel high-pressure (liquid+liquid) equilibrium cell and a DMA HP densimeter (Anton Paar GmbH, Austria) with an accuracy of 5·10-3kgm-3.
The experimental procedure for determining densities is as follows.
First the densimeter is calibrated by using two reference fluids, ultra-high purity nitrogen (UHP=99.995%) and ultra-pure water.
After degasification in an ultrasonic bath (model 1210, Branson, Inc.), approximately, 20cm3 of each pure fluid is introduced in the high-pressure cell.
The chamber has two orifices (one at the top and the other at the bottom) and it is equipped with appropriately sealed borosilicate glass windows, which allow visualisation of the inner space during operation.
The cell is heated to the desired temperature by means of electric band heaters operated by a Watlow temperature controller model TC-211-K-989 (USA).
The temperature of the sample in the vessel is measured by means of a K-type thermocouple, and maintained constant to within ±0.1K.
At the specified temperature, the mixture is well stirred with a magnetic stirrer for several hours (7 to 10h).
The desired experimental pressure is then fixed by means a positive displacement pump (ELDEX HP, B- 100-S-2 CE, USA), and the experimental mixture is allowed to rest for at least 12h in order to guarantee phase separation.
After this equilibration step, each bulk phase (organic and aqueous) is transported to the high-pressure densimeter through a heated stainless steel tube.
The determination of the mass density is based on measuring the period of oscillation of a vibrating U-shaped tube filled with the fluid mixture sample.
During the operation, the temperature of the apparatus is maintained constant to within ±0.01K.
The pressure is measured by means a Swagelok type S pressure transducer connected to the densimeter.
The density measurements are repeated 25 times for each thermodynamic condition, and the average reported.
n-Hexane and ultra pure H2O purchased from Merck, are used without further purification.
The purity and source of the components is reported in table 5 as determined by gas chromatography (GC).
The mass density (ρ) of the pure fluids at a temperature of T=298.15K and atmospheric pressure are reported in table 6.
The reported values are also compared with those previously reported in the NIST-REFPROP [158] database.
Exact solution of the van der Waals model in the critical region The celebrated van der Waals model describes simple fluids in the thermodynamic limit and predicts the existence of a critical point associated to the gas–liquid phase transition.
However the behavior of critical isotherms according to the equation of state, where a gas–liquid phase transition occurs, significantly departs from experimental observations.
The correct critical isotherms are heuristically re-established via the Maxwell equal areas rule.
A long standing open problem in mean field theory is concerned with the analytic description of van der Waals isotherms for a finite size system that is consistent, in the thermodynamic limit, with the Maxwell prescription.
Inspired by the theory of nonlinear conservation laws, we propose a novel mean field approach, based on statistical mechanics, that allows to calculate the van der Waals partition function for a system of large but finite number of particles N. Our partition function naturally extends to the whole space of thermodynamic variables, reproduces, in the thermodynamic limit N→∞, the classical results outside the critical region and automatically encodes Maxwell’s prescription.
We show that isothermal curves evolve in the space of thermodynamic variables like nonlinear breaking waves and the criticality is explained as the mechanism of formation of a classical hydrodynamic shock.
In late 19th century, due to the outstanding contribution of its founding fathers, Boltzmann, Maxwell and Gibbs, Statistical Thermodynamics has been successfully introduced as the general conceptual framework for understanding equilibrium thermodynamic phenomena by means of statistical mechanics  [1,2].
The early success of the kinetic theory of gases and the discovery of the mean field approach for the derivation of the classical van der Waals equation  [3] nurtured the hope that an equally neat and clear description of critical phenomena would be as effective as it was away from the critical region.
Although second order phase transitions, as for example the ergodicity breakdown of real gases at the triple point, have nowadays been completely framed into a rigorous scaffold  [4]–also partially guided by techniques from the near field theory  [5,6]–first order phase transitions, such as gas–liquid phase transitions, turned out to be more elusive.
Indeed, in spite of the accuracy of the celebrated van der Waals equation for the description of real gases, the behavior predicted within the critical region, where a real gas turns into a liquid, significantly departs from the experimental observations.
Fig.
1 shows the typical isothermal curves of a real gas: above the critical temperature (for T>Tc) the pressure decreases as a strictly monotonic function of the volume; the critical point corresponds to the temperature T=Tc where the critical isotherm develops an inflection point; below the critical temperature (at T<Tc) real isotherms are constant within a certain volume interval in spite of the oscillating behavior predicted by the classical van der Waals equation.
Interestingly, the behavior of real isothermal curves within the critical region turns out to be intimately connected to the theoretical one via the celebrated Maxwell rule stating that the constant pressure plateau is placed in such a way it cuts lobes of equal areas on the associated van der Waals isotherm.
As it is well known, the Maxwell rule corresponds to the condition of thermodynamic equilibrium such that, below the critical temperature, the Gibbs free energy develops two minima of equal value  [7].
The remarkable validity, although heuristic, of Maxwell’s approach stimulated countless studies aimed at a rigorous statistical mechanical description of first order phase transitions as for instance in the works of Lebowitz and Penrose  [8] and van Kampen  [9], where large classes of pairwise interaction potentials for particles (continuous and hard-sphere-like respectively) are considered or the work by Griffiths  [10] that focuses on the study of analyticity properties of thermodynamic functions.
Alternative methods to analyze phase transitions have also been developed based on macroscopic approaches to thermodynamics.
For instance, the Landau theory allows to construct suitable asymptotic expansions of the free energy in the order parameters to obtain information of the critical exponents in the vicinity of the critical point (see e.g.
[11]); the Widom approach relies on the construction of effective free energy functions based on the analysis of their scaling properties  [12].
Further recent developments in this direction led to the formulation of the thermodynamic limit as the semiclassical limit of nonlinear conservation laws where phase transitions are associated to shock solutions of a hyperbolic nonlinear PDE in the class of conservation laws  [13–16].
Such nonlinear PDEs can be also derived in mean field theories from the analysis of differential identities of the free energy as showed in  [13,14,17–19] for the Curie–Weiss and the Sherrington–Kirkpatrick models, or from the analysis of thermodynamic Maxwell relations as showed in  [20,15] for the van der Waals model.
Both the microscopic statistical mechanical approach, via the study of correlation functions asymptotics, and the macroscopic thermodynamic approach, based on the expansion of the free energy in the vicinity of the critical point, show the intimate connection with the singularity and catastrophe theory–since the very first pioneering contributions by Arnold–and the Hopf bifurcation theory (see e.g.
[21]).
Despite the numerous progresses made in understanding phase transitions in a variety of contexts, from thermodynamics to classical and quantum field theory  [22,23], or complex and biological systems  [24,25], and the discovery of their intrinsic universality, a global analytical description of phase transitions for the van der Waals gas is still missing.
In this work, inspired by the theory of nonlinear PDEs, in the class of nonlinear conservation laws, we propose a novel method such that given an equation of state assumed to be accurate outside the critical region allows to construct a partition function for a finite number of particles N that is valid in the whole space of thermodynamic variables including the critical region.
This partition function automatically encodes the Maxwell equal areas rule.
Based on the mean field assumption that configurations of equal volume are equally weighted, we obtain the general functional form of the partition function.
For finite N, the partition function is, as expected, analytic in the space of thermodynamic variables but it develops a singularity in the thermodynamic limit N→∞.
We use the Laplace method for the asymptotic evaluation of the partition function for large N with the constraint that above the critical point, where the Laplace integral admits one single critical point, the leading asymptotics of the volume expectation value satisfies the classical van der Waals equation.
Remarkably, this condition allows to fix uniquely the functional form of the partition function in such a way that the logarithm of the probability density gives the correct Gibbs free energy of the van der Waals model above the critical point.
Finally, we prove that in the critical region, defined as the region in the space of thermodynamic variables where the Laplace integral admits multiple critical points, the leading asymptotics for the volume develops a discontinuous behavior, providing the exact analytical description of the first order phase transition.
Let us consider a fluid of N identical particles of mass m, whose center of mass is fixed at the origin of the reference frame, described by the Hamiltonian of the form (1)HN=∑l=1Npl22m−12∑l,m=1Nψ(rl,rm)+Pv(r1,…,rN), where pl is the momentum of the particles, ψ(rl,rm) is a two-body interaction potential, and the last term models the interaction with an external field where P>0 is a real positive mean field coupling constant and the volume v(r1,…,rN) is defined as the minimum convex hull associated to the configuration {r1,…,rN}.
The partition function is given by the standard formula for a canonical ensemble Z=∫dNpidNrie−βHN where β=(KBT)−1 with KB the Boltzmann constant and T the temperature.
Let us observe that fixing the center of mass breaks the translational invariance of the Hamiltonian (1) that otherwise would lead to a divergent partition function Z.
Integration over the moment variables pl returns Z=(2πmKBT)3N/2Z where (2)Z=∫dNriexp[N(t2∑l,mψ(rl,rm)+xv)].
The rescaled variables t=1/(NKBT) and x=−P/(NKBT) introduced above will allow us to define the thermodynamic limit and the choice of the notation emphasizes the formal analogy between the Gibbs free energy and the Hamilton–Jacobi function of the associated mechanical problem (see e.g.
[14]).
Let us introduce the density of free energy αN=−N−1logZ.
This quantity is sometimes also referred to as mathematical pressure, see e.g.
[26].
The expectation value of a given observable O is defined in the usual manner, i.e.
〈O〉=Z−1∫dNrOe−βHN.
In particular, let us observe that 〈v〉=−∂αN/∂x.
The thermodynamic regime is defined as the large particles limit N∼NA where NA≃6.022×1023 is Avogadro’s number.
In particular, for n moles of a gas of molecules of hard core volume b0 we have N=nNANKB=nRNb0=nbm where R=NAKB≃8.31J/mol K is the gas constant and bm=NAb0 is the molar hard core volume.
Hence, the gas constant R defines the typical scale for the variables x and t. We now assume that for fixed values of the variables x and t, configurations of equal volume occur with the same probability density, so that there exists a probability measure μ(v) such that the partition function (2) is of the form (3)Z=∫b∞dμ(v) where b=nbm is the total hard core volume.
This assumption gives a nonlinear generalization of the standard mean field approximation introduced for the statistical mechanical derivation of the van der Waals equation of state (see e.g.
[3]).
We also note that, from a formal perspective, this ansatz is equivalent to the request that the moments 〈vn〉 for the model (1) are such that the measure μ(v) is the solution to the Stieltjes moments problem   [27], that is 〈vn〉=∫b+∞vndμ(v).
Expressing the differential as dμ(v)=μ′(v)dv, the function μ′(v) gives the weight associated to a given volume configuration that, for fixed values of x and t, is the same for all configurations of equal volume.
As for the canonical ensemble the logarithm of the probability density in (2) is linear in the variables x and t (this ensures entropy maximization at equilibrium), we have that the probability density μ′(v) is such that logμ′(v)=N(xv+12tϕ(v)+σ(v)) for certain functions ϕ(v) and σ(v).
Hence, the partition function takes the form (4)Z=∫b+∞dμ(v)=∫b+∞eN(xv+t2ϕ(v)+σ(v))dv.
In the following we will prove that the functions ϕ(v) and σ(v) can be uniquely determined by the request that the expectation value 〈v〉 evaluated, away from the critical region, according to the partition function (4) satisfies, in the thermodynamic limit, the celebrated van der Waals equation (5)(P+av2)(v−b)=nRT.
We should stress that, as discussed in details below, the assumption about the existence of the measure in (3) is a strong enough information to fix uniquely the functional form of φ(v) and σ(v) with no further specifications on the two-body potential ψ(rl,rm).
However, we find instructive to present an heuristic phenomenological construction for a class of two-body nearest neighborhood potential depending on the distance of the form ψ(rl,rm)=ψ(rlm), where rlm=|rl−rm|.
In the thermodynamic limit, we can assume that for a given equilibrium configuration particles are on average approximately equidistant, i.e.
rlm≃r̄ where r̄ is the mean distance, then ∑l,mψ(rlm)≃∑l,mψ(r̄)∼Nψ(r̄).
We used the fact that the number of nearest neighborhood pairs is of order N. Let us consider for example an effective electric potential energy ψ(r̄)≃1Nβq24πϵ0r̄α≃q24πϵ0vα/3 where we observed that the mean distance for a given volume configuration is related to the volume per particle by the relation r̄≃(v/N)1/3 and the exponent β=α/3 is chosen to ensure the linear extensivity of the potential term in (1).
More in general we assume that in the thermodynamic limit we can write ∑l,mψ(rl,rm)≃ϕ(v), that is the potential energy can be expressed as a function of the volume.
Under this assumption, we observe that the partition function (2) can be equivalently written as Z=∫b∞dvDN(v)eN(t2ϕ(v)+xv) where DN(v)=∫dNriδ(v−v(r1,…,rN)) gives the number of configurations of the prescribed volume v. A direct comparison with the formula (4) leads us to the natural interpretation, for large N, of the function σ(v) as the configurational entropy of the system i.e.
σ(v)≃1NlogDN(v).
Let us now proceed by evaluating the leading order asymptotics, for N→∞, of the partition function (4) in the region of thermodynamic variables x and t where logμ′(v) admits one single critical point.
Laplace’s formula gives (6)Z≃2πNα″(v⋆)e−Nα(v⋆),N→∞ where α(v)=−xv−tϕ(v)/2−σ(v) and v⋆(x,t) is a stationary point for the potential α(v) such that α′(v⋆)=0, i.e.
(7)x+t2ϕ′(v⋆)+σ′(v⋆)=0.
In particular, formula (6) implies that α=limN→∞αN.
Identifying the external field constant P in the Hamiltonian (1) with the physical pressure in Eq.
(5) and choosing (8a)ϕ(v)=2a/〈v〉(8b)σ(v)=log(〈v〉−b).
Eq.
(7) coincides with the van der Waals equation (5).
We also note that the asymptotic matching condition (8b) allows us to evaluate the function DN(v) for large N that as expected is DN(v)≃(〈v〉−b)N. The prescriptions (8) are, according our procedure, the necessary matching conditions that uniquely fix the partition function (4) consistently with the van der Waals equation of state, which is assumed to be accurate above the critical region.
We note that α(v)=G/T, where G is the Gibbs free energy density of the van der Waals model.
A direct calculation shows that the partition function so obtained (9)Z=∫b∞eN(xv+tav+log(v−b))dv satisfies the Klein–Gordon equation in the light-cone variables (10)∂2Z∂x∂t=N2aZ.
Let us also observe that the integral expression (9) can be explicitly evaluated at finite N for t=0 and gives 〈v〉(x,0)=b−1/x−1/Nx that coincides with the equation of state (5) in the limit T→∞.
Using the self-consistency equation 〈v〉=N−1Z−1∂Z/∂x the Klein–Gordon equation (10) implies that the volume density satisfies the nonlinear viscous conservation law (11)∂〈v〉∂t=∂∂x(a〈v〉+1N∂log〈v〉∂t) of the type studied in  [28] and that is related to the viscous analog of the Camassa–Holm equation.
In the thermodynamic limit, above the critical temperature where the gradient of 〈v〉 is bounded, the term of order O(N−1) in (11) is negligible and the volume density satisfies the Riemann–Hopf type equation ∂t〈v〉=∂x(a/〈v〉) whose solution develops a gradient catastrophe in finite “time” t. As illustrated in Fig.
2(a), the volume 〈v〉 evolves in the space of thermodynamic parameters just like a nonlinear hyperbolic wave and the gradient catastrophe is associated to the critical point xc=−1/8b, tc=27b/8a, vc=3b.
Beyond the critical time tc, the physical solution develops a shock discontinuity, corresponding to a first order phase transition, whose position at fixed t>tc is determined by the equal area rule and its speed U is given the Rankine–Hugoniot condition U=−(a/vl−a/vr)/(vl−vr)=a/(vlvr)>0, where vl and vr are the limiting values of 〈v〉 respectively to the left and to the right of the jump.
It was observed in  [20] that the Rankine–Hugoniot condition is equivalent to the Clausius–Clapeyron equation implying that the shock speed is proportional to the latent heat associated to the first order phase transition and the trajectory of the shock is interpreted as the coexistence curve of the gas–liquid phase as shown in Fig.
2(b).
Such connection between phase transitions and scalar shock waves was first observed in the context of magnetic models (see e.g.
[16] and reference therein), and in the classical thermodynamic setting in  [15,20] where the notion of universality has been also discussed.
It is interesting to compare the mean field partition function (9) associated to the model Hamiltonian (1) and the equation of state obtained according to the standard canonical ensemble formalism.
For the sake of simplicity let us consider an ideal gas of non-interacting particles of hard core volume b.
In this case the ϕ(v)=0 and the coupling constant P models the interaction with the external environment.
We should stress that in the present formalism the gas does not occupy a prescribed volume, but the mean field partition function (9) accounts for all possible gas configurations over the whole space.
Evaluating explicitly the formula (9) we obtain Z=(N−1)!NN(−1x)N+1eNbx.
The equation of state for the gas of N particles is given, in full analogy with the case of mean field spin systems (see e.g.
[14]) via the self-consistency equation 〈v〉=1N1Z∂Z∂x=b−N+1Nx or equivalently (〈v〉−b)P=(N+1)KBT.
In the thermodynamic regime N≃nNA we obtain the well known ideal gas equation of state (12)(〈v〉−b)P=nRT, which allows us to identify the coupling constant P with the physical pressure that plays the role of the external magnetic field in spin systems.
Within the canonical formalism the ideal gas constituted by a fixed number of particles N of Hamiltonian H=∑l=1Npl22m is assumed to be in equilibrium with an external reservoir and occupy a prescribed volume, say V. The partition function is given by Z=∫dNpidNrie−H/KBT=(2πmKBT)3N/2(∫dr)N=(2πmKBT)3N/2(V−b)N. The pressure P=−∂F∂V is then defined in terms of the Helmholtz free energy F(V,T)=−KBTlogZ gives the equation of state in the form (12).
Unlike the canonical formalism the mean field Hamiltonian (1) encodes the boundary condition weighing different volume configurations that are intrinsically defined via the minimum convex hull.
As a consequence the number N associated to the spatial scale of the system takes into account of finite size effects, which are important near the criticality, in a consistent manner with the Maxwell rule.
RemarkWe observe that the procedure presently described, that allows us to extend the van der Waals equation of state to the critical region, can straightforwardly be generalized to the class of equations of state obtained from (large volumes) virial expansions of the form (see e.g.
[29]) P=nRTv(1+B1(T)v+B2(T)v2…) with Bi(T)=αi+12nRT+βi+1i=1,2,3,… where αi and βi are real constants given by the large volume asymptotic expansion of the functions σ(v) and ϕ(v) of the form ϕ(v)=−∑k=1∞αk+1kvkσ(v)=logV−∑k=1∞βk+1kvk.
The subset of the space of thermodynamic variables x and t where the free energy α(v) admits multiple critical (stationary) points defines the critical region associated to the gas–liquid phase transition.
In this case, the leading asymptotics at large N of the partition function (9) is given by the formula (13)Z≃∑i2πNα″(vi)e−Nα(vi),N→∞ where the sum runs over the local minima vi(x,t) of the free energy α(v).
Hence, consistently with the classical description of the van der Waals phase transition, below the critical temperature the Gibbs free energy develops three stationary points, two of which are local minima.
In the limit N→∞ the leading contribution to the partition function is given by the point of local minimum vm such that α(vm)≤α(vi), for all i≠m.
Hence, within the critical region, the solution is given by 〈v〉=limN→∞N−1∂xlogZ=vm where vm(x,t) is a root of the equation of state α′(vm)=0 such that the Gibbs free energy has the lowest local minimum.
The subset of the (x,t)-plane, such that α takes two equal minima α(vi(x,t))=α(vj(x,t)), represents the curve of resonance of the exponential contributions in (6) and identifies the shock line shown in Fig.
2(b).
As already known from the theory of classical shocks for the viscous Burgers equation, such resonance condition is equivalent to the equal areas rule  [30].
In Fig.
3 we plot the isothermal curves evaluated using the partition function (9).
As N increases the exact isothermal curves develop an inflection point and rapidly converge to the asymptotic behavior predicted by the Laplace formula.
We should emphasize that the partition function (9) provides a global description of isothermal curves in the space of thermodynamic variables and the description of the phase transition is apparently accurate already for relatively small N≃104 if compared with Avogadro’s number.
The formula (9), provides an explicit description of how finite size effects play the role of a singularity resolution mechanism.
It also gives a statistical mechanical based interpretation of the results obtained in  [20] that allow to identify the multi-scale regime characterizing the universal local form of the equation of state v=vc+N−1/4u(x−xc+a(t−tc)/vc2N−3/4,t−tcN−1/2) where u(ξ,τ)=−2∂logΛ∂ξ(ξ,τ),Λ(ξ,τ) is the Pearcey integral Λ(ξ,τ)=∫−∞∞e−18(z4−2τz2+4ξz)dz and (xc,tc,vc) are the coordinates of the critical point as evaluated above.
This work shows how our approach based on the combination of Statistical Mechanics and nonlinear PDEs theory provides us with a novel and powerful tool to tackle phase transitions.
This method leads to solution of perhaps the most known test-case that exhibits a first order phase transition (semi-heuristically described) such as the van der Waals model.
In particular we have obtained the first global mean field partition function (Eq.
(9)), for a system of finite number of particles.
The partition function is a solution to the Klein–Gordon equation, reproduces the van der Waals isotherms away from the critical region and, in the thermodynamic limit N→∞ automatically encodes the Maxwell equal areas rule.
The approach hereby presented is of remarkable simplicity, has been successfully applied to spin  [17–19,14,16] and macroscopic thermodynamic systems  [20,15] and can be further extended to include the larger class of models admitting partition functions of the form (4) to be used to extend to the critical region general equations of state of the form (7) including a class virial expansions.
AB has been partially supported by Progetto giovani GNFM-INdAM 2014 “Calcolo parallelo molecolare” and Laboratories for Information Food and Energy through the project INNOVA-MATCH.
AM has been partially supported by Progetto giovani GNFM-INdAM 2014 “Aspetti geometrici e analitici dei sistemi integrabili”.
On variational and symplectic time integrators for Hamiltonian systems Various systems in nature have a Hamiltonian structure and therefore accurate time integrators for those systems are of great practical use.
In this paper, a finite element method will be explored to derive symplectic time stepping schemes for (non-)autonomous systems in a systematic way.
The technique used is a variational discontinuous Galerkin finite element method in time.
This approach provides a unified framework to derive known and new symplectic time integrators.
An extended analysis for the new time integrators will be provided.
The analysis shows that a novel third order time integrator presented in this paper has excellent dispersion properties.
These new time stepping schemes are necessary to get accurate and stable simulations of (forced) water waves and other non-autonomous variational systems, which we illustrate in our numerical results.
The dynamics of various physical phenomena, such as the movement of pendulums, planets, or water waves can be described in a variational framework.
The development of variational principles for classical mechanics traces back to Euler, Lagrange, and Hamilton; an overview of this history can be found in [1,19].
This approach allows to express all the dynamics of a system in a single functional – the Lagrangian – which is an action integral.
Hamiltonian mechanics is a reformulation of Lagrangian mechanics which provides a convenient framework to study the symmetry properties of a system.
This is expressed by Noether's theorem which establishes the direct connection between the symmetry properties of Hamiltonian systems and conservation laws.
When one approximates the system numerically, it is advantageous to preserve the Hamiltonian structure also at the discrete level.
Given that Hamiltonian systems are abundant in nature, their numerical approximation is therefore a topic of significant relevance.
The topic of time integration of Hamiltonian systems has seen a rigorous development during the past decades.
This has resulted in symplectic time integrators that have been designed to preserve the Hamiltonian structure for an approximated system.
More information on symplectic integration of Hamiltonian systems can be found in [12] and [16].
According to the review in [20], variational time integrators can be proven to be symplectic by construction under certain conditions.
In the variational approach, one can derive well-known schemes or new schemes, as reported in [10,17,22,26].
One of the most actively developed numerical methods is the finite element method with time discontinuous basis functions.
Such a discontinuous Galerkin method in time was studied in [5–7].
In particular, the variational Galerkin finite element method in time was implemented in [11,20,22].
Integrators for variational principles including forcing were developed among others in [20].
The study of the variational time integrators that are discussed in this paper was motivated by our research into numerical discretizations of the equations describing nonlinear water waves, which have a Hamiltonian structure.
The Hamiltonian structure of nonlinear gravity free surface water waves was shown by Zakharov [29].
In [9] (and [8]), we constructed a numerical water wave tank based on a discrete Miles' variational principle with forcing.
The computational method had to fulfill a number of requirements in order to perform the simulations of the laboratory experiments presented in [14].
First, the experiments with which we compared numerical results run over a long period of time, covering also an extensive spatial domain, which meant that conservation of mass and bounded fluctuations of energy at the discrete level are important.
Second, in one of the test cases in [9] (also [8]) a focussing wave was created with a wave height five times exceeding the ambient, incoming wave height.
Therefore numerical stability had to be ensured.
Third, a piston wave maker was used in the experiments, which gives external forcing to the system and results in a non-autonomous Hamiltonian system.
It is non-trivial to meet all these requirements, in particular in combination with a spatial discretization of the nonlinear potential flow water wave equations.
We therefore investigate variational time integrators in detail in that paper.
In the construction of variational time integrators we choose to work with a discontinuous Galerkin finite element method in time, because it allows additional freedom to develop new symplectic integrators, also for non-autonomous Hamiltonian systems.
In this method the time domain is split into elements, and in each element the variables are approximated with polynomial expansions.
The discrete variational principle is then obtained by substitution of the approximations in the continuum variational principle, provided one calculates the contributions of the delta functions that arise.
After evaluation of the variational principle by calculating its variations, one obtains the discrete system of equations for the variational time integration method.
A crucial part of the discontinuous finite element method is the numerical flux arising from the before-mentioned delta functions.
It has to be introduced to establish a connection between the time elements since in each element only a local polynomial approximation is used.
We study a specially chosen numerical flux, somewhat inspired by the one explored in [24] and [25], which in turn was based on the theory of non-conservative products developed in [4] and [28].
The flux is also implemented in the numerical discretization for the nonlinear potential flow water wave equations, discussed in [8,9] and gives excellent results.
Combining the discrete variational principle with the derived numerical flux, we obtain a unified approach to construct time integrators for (non-)autonomous Hamiltonian systems.
We have derived both well-known and novel symplectic time stepping schemes of first, second and third order accuracy.
An extensive analysis for the discretizations is provided, including a linear stability analysis and an investigation of the symplectic nature of the schemes.
The novel third order scheme we have developed is shown to have improved dispersion properties.
Furthermore, within this approach we derive and test time stepping schemes for non-autonomous Hamiltonian systems, such as forced and damped oscillators.
We study the approximation of forcing and damping terms considering the discrete versions of certain integrals.
In [10], a unified discontinuous Galerkin framework for time integration is given using weighted residuals for general nonlinear ordinary differential equations.
New examples are given of two (implicit) symplectic Runge–Kutta methods, satisfying the symplectic condition by construction.
Our work is complementary as we derive discrete variational principles by staying within the variational framework, which variation subsequently yields the nonlinear algebraic set of equations to be solved.
While our schemes are always variational by construction, the symplectic and stability conditions need to be verified.
These conditions can be met depending on the choice of quadrature we use and the choice of the free parameter(s) in the jump conditions.
This complements Zhao and Wei's work [10], also because we developed our stable, variational and symplectic integrators for applications to water wave problems, since these can then be expressed as space and time discrete variational principles [9,27].
Extensions of the research presented here are found in Bokhove and Kalogirou [3].
The outline of this paper is as follows.
In Section 2 we introduce the dynamics of a Hamiltonian system with one degree of freedom.
Next, Section 3 introduces the variational discontinuous Galerkin method.
Here we obtain second order time stepping schemes, which can be formulated in the form of well-known symplectic schemes.
In Section 4 a novel time integrator of third order accuracy in time is derived and analyzed.
We analyze its linear stability and symplectic conditions.
In Section 5 we present numerical results for non-autonomous systems by extending the well-known symplectic schemes we derived in this complementary variational framework.
Our novel third order time integrator is used to simulate nonlinear water waves in Section 6.
Conclusions are drawn in Sec.
7.
More details on the analysis of the time integration methods are presented in two appendices.
The dynamics of a Hamiltonian system with one degree of freedom is embedded in the Lagrangian functional(1)L(p,q):=∫0T(pdqdt−H(p,q))dt, where p(t) is the momentum, q(t) the coordinate, and H(p,q) the energy or Hamiltonian.
The variational principle for the system is δL:=δpL+δqL=0, where δpL and δqL are the functional derivatives with respect to p and q.
The functional derivatives are defined as(2)δL=limϵ→0⁡1ϵ(L(p+ϵδp,q+ϵδq)−L(p,q)).
Consider the variation of the Lagrangian functional (1)(3)δL(p,q)=∫0T(dqdtδp+pdδqdt−∂H∂pδp−∂H∂qδq)dt=0.
After integration by parts of (3), using the end point conditions δq(0)=δq(T)=0, and the arbitrariness of the variations, the dynamics of a Hamiltonian system emerges as follows:(4)δp:dqdt−∂H∂p=0 and δq:dpdt+∂H∂q=0, with initial conditions p(t=0)=p0 and q(t=0)=q0.
The accurate numerical approximation of equations (4) is a well-developed subject of research.
Geometric integrators target the preservation of system properties at the discrete level and symplectic integrators ensure that the approximated system is also Hamiltonian [12].
We start to approximate the Lagrangian (1) rather than approximating equations (4).
To approximate the functional (1) for a single-degree-of-freedom system in time, we first divide the time domain [0,T] into N finite time intervals In=(tn,tn+1), n=0,…,N−1, with length Δt=tn+1−tn.
Each time interval In has a constant length and is related to a reference domain Iˆ=(−1,1) through the mapping Fn, defined as(5)Fn:Iˆ→In:τ↦t=12(tn(1−τ)+tn+1(1+τ)).
Second, functions (p,q) are approximated element-wise with a polynomial expansion.
In every element basis functions are continuous, but could be discontinuous across element boundaries.
The finite element space is defined as(6)Vτ:={vτ|vτ∈L2([0,T]),vτ|In∈Ps(In),∀In,n=0,⋯,N−1}, where Ps is the space of polynomials of degree s. Each variable in finite element In is now approximated as pτ∈Vτ, qτ∈Vτ and represented in a time slab (tn,tn+1) as(7)pτ=piφi,qτ=qiψi, with basis functions φi, ψi, which do not necessarily have to coincide, and expansion coefficients pi, qi.
We will in particular consider Lagrange and Bernstein polynomials.
Also, the Einstein summation convention will be used, which implies summation over repeating indices.
In order to account for the discontinuities at the element boundary we introduce the right and left traces of a variable d(t) at time tn as(8)dn,+:=limϵ↓0⁡d(tn+ϵ)anddn,−:=limϵ↓0⁡d(tn−ϵ), respectively.
At time level tn the following jump [[⋅]] and average {{⋅}}αβ operators can then be defined(9)[[d]]|tn=dn,−−dn,+ and {{d}}αβ|tn=αdn,−+βdn,+, where α,β are arbitrary real numbers with α+β=1 and α,β≥0.
The discrete functional for the Hamiltonian system (1) is now obtained after introducing polynomial approximations of p and q into the functional and splitting of the time integral into a summation over time intervals(10)Lτ(pτ,qτ):=∑n=0N−1∫tntn+1(pτdqτdt−H(pτ,qτ))dt−∑n=−1N−1[[qτ]]{{pτ}}αβ|tn+1.
The last term in (10) comes as a numerical flux.11For alternative derivations and further background on this type of numerical flux, see Appendix C in [9] and [3,24,25].
The flux is required to establish a connection between the time element boundaries, as we use discontinuous basis functions.
There is still some freedom in the definition of the discrete Lagrangian (10).
In the first place we need to specify the polynomial order of the basis functions.
As in general it is not possible or desirable to compute the integral of the Hamiltonian analytically, we also have to choose a quadrature rule [20].
In [13] it is proven that the order of the quadrature rule will give an upper bound for the attainable order of the time integration scheme.
Finally, the choice of the weight α in the flux term also leaves some freedom to derive and optimize different time integration methods.
The derivation of a time integration scheme is now performed as follows.
We choose a polynomial approximation of order s. To obtain a scheme of order s+1 we should take a quadrature rule with an order of accuracy that is at least s+1 [13].
After the scheme is derived, a linear stability analysis will be performed to find appropriate weights α in the flux term.
For our novel scheme further analysis is done to show that the scheme is symplectic.
We have rederived and reinterpreted the symplectic Euler scheme as a genuine discontinuous Galerkin scheme with piecewise constant polynomials for p and q, in each time slab, as well as the (modified) midpoint scheme using a discontinuous Galerkin scheme in which p and q are expanded using quadratic polynomials and both evaluated at the mid-points pn+1/2 and qn+1/2 of the integration interval only (see, [8]).
Next we show how this can be done for a scheme of second order accuracy because we use an extension of the resulting Störmer–Verlet scheme for the non-autonomous systems considered in §5.
To obtain a second order time integrator we need to choose linear basis functions in the polynomial expansions.
Various options for the approximation of the variables and integrals are possible.
Since we want to derive the known Störmer–Verlet scheme in our new approach, we take the trapezoidal rule for p and the midpoint rule for q, yielding(11)∫tntn+1H(pτ,qτ)dt=Δt2(H(pn,+,qn+1/2)+H(pn+1,−,qn+1/2)), where the expansion coefficient qn+1/2 is the value in the middle of the time interval.
We approximate the variables in each time element In as(12a)pτ=tn+1−tΔtpn,++t−tnΔtpn+1,−,(12b)qτ=2(t−tn)Δtqn+1/2+tn+tn+1−2tΔtqn,+, see Fig.
1.
Introducing (11)–(12) in the discrete functional (10), we obtain(13)Lτ(pτ,qτ,t)=∑n=0N−1((pn,++pn+1,−)(qn+1/2−qn,+)−Δt2(H(pn,+,qn+1/2)+H(pn+1,−,qn+1/2)))−∑n=−1N−1(2qn+1/2−qn,+−qn+1,+)(αpn+1,−+βpn+1,+).
Applying the variational principle δLτ=0, using the arbitrariness of the variations and end-point conditions δ(2q−1/2−q−1,+):=δq0,−=0,δp0,−=0, δqN,+=0,δpN,+=0, we obtain the following variational time integrator(14a)δqn,+:(β−1)pn,++(α−1)pn+1,−+βpn+1,++αpn,−=0,(14b)δpn,+:qn+1/2=qn,++β(2qn−1/2−qn−1,+−qn,+)+Δt2∂H(pn,+,qn+1/2)∂pn,+,(14c)δqn+1/2:2βpn+1,++(2α−1)pn+1,−=pn,+−Δt2(∂H(pn,+,qn+1/2)∂qn+1/2+∂H(pn+1,−,qn+1/2)∂qn+1/2),(14d)δpn+1,−:qn+1/2−α(2qn+1/2−qn,+−qn+1,+)=qn,++Δt2∂H(pn+1,−,qn+1/2)∂pn+1,−.
In Appendix B we show that this scheme is stable for weights α∈[0.5,1] and coincides under this choice with the well-known second order Störmer–Verlet time integrator.
It is interesting to note that we start with two discontinuous variables, but at the end there is no jump in p at the time levels tn, n=0,…,N anymore.
Moreover, the presence of a jump in the q variable does not result in a scheme different from the classical one (see Appendix B).
An adjoint Störmer–Verlet scheme can be obtained via a swapped linear approximation of the variables in each element In(15a)qτ=tn+1−tΔtqn,++t−tnΔtqn+1,−,(15b)pτ=2(t−tn)Δtpn+1/2+tn+tn+1−2tΔtpn,+, see Fig.
2, and a trapezoidal rule for q and a midpoint rule for p(16)∫tntn+1H(pτ,qτ)dt=Δt2(H(pn+1/2,qn,+)+H(pn+1/2,qn+1,−)).
Analogously to the previous case, it can be shown that this weighted scheme is stable for α∈[0,0.5] and a reformulation allows to recover a well known formulation of the scheme, as given in [12].
A natural question is whether it is possible to find new time integrators following the variational approach described in the previous section.
In this section we will construct a new time integration scheme with third order accuracy using the variational approach.
We will investigate convergence, symplecticity, linear stability and dispersion properties.
To obtain a third order scheme we choose in each time element In the following quadratic Lagrange basis functions(17a)pτ=(tn+tn+1−2t)(tn+1−t)Δt2pn,++4(t−tn)(tn+1−t)Δt2pn+1/2−(tn+tn+1−2t)(t−tn)Δt2pn+1,−,(17b)qτ=(tn+tn+1−2t)(tn+1−t)Δt2qn,++4(t−tn)(tn+1−t)Δt2qn+1/2−(tn+tn+1−2t)(t−tn)Δt2qn+1,−, as can be seen in Fig.
3.
There are three independent expansion coefficients present in the polynomial representation: pn,+ at the left boundary of the interval, pn+1/2 at the middle of the interval and pn+1,− at the right boundary of the interval and similarly for q.
We approximate the integral over the Hamiltonian with Simpson's third order quadrature rule as(18)∫tntn+1H(pτ,qτ)dt≅Δt6(H(pn,+,qn,+)+4H(pn+1/2,qn+1/2)+H(pn+1,−,qn+1,−)).
The discrete functional (10) then becomes(19)Lτ(pτ,qτ)=∑n=0N−1∫tntn+1pτdqτdtdt−Δt6(H(pn,+,qn,+)+4H(pn+1/2,qn+1/2)+H(pn+1,−,qn+1,−))−∑n=−1N−1(qn+1,−−qn+1,+)(αpn+1,−+βpn+1,+).
This choice of quadrature rule and basis functions corresponds to the choices made in [22] for the scheme called “P2N3Q4Lob” in their paper (keeping this acronym).
Nevertheless, the resulting scheme is different due to the choice of the numerical flux.
Applying the variational principle δLτ=0, using the arbitrariness of the variations and end-point conditions δq0,−=0, δp0,−=0, δqN,+=0,δpN,+=0, we get a new third order scheme(20a)qn+1/2=(1−32β)qn,++32βqn,−+Δt4(∂H(pn,+,qn,+)∂pn,++∂H(pn+1/2,qn+1/2)∂pn+1/2),(20b)pn+1/2=(1−32α)pn,++32αpn,−−Δt4(∂H(pn,+,qn,+)∂qn,++∂H(pn+1/2,qn+1/2)∂qn+1/2),(20c)qn+1,−=qn,++Δt∂H(pn+1/2,qn+1/2)∂pn+1/2,(20d)pn+1,−=pn,+−Δt∂H(pn+1/2,qn+1/2)∂qn+1/2,(20e)αqn+1,+=−16qn,++23qn+1/2+(α−12)qn+1,−+Δt6∂H(pn+1,−,qn+1,−)∂pn+1,−,(20f)βpn+1,+=−16pn,++23pn+1/2+(β−12)pn+1,−−Δt6∂H(pn+1,−,qn+1,−)∂qn+1,−.
Equations (20a) and (20b) have to be solved together as an implicit system.
The other four equations are explicit.
To perform the linear stability analysis we introduce the Hamiltonian of the harmonic oscillator H(p,q)=12ω2q2+12p2 into scheme (20).
With the prescribed Hamiltonian we can formulate scheme (20) in a form(21)(pn+1,−,qn+1,−,pn+1,+,qn+1,+)T=A(pn,−,qn,−,pn,+,qn,+)T. To ensure that the numerical solution is bounded, it is required for the eigenvalues λi of the matrix A to be less than or equal to one in modulus [12].
The characteristic polynomial of the matrix A is a quartic function in λ.
Since, the explicit form of the eigenvalues is complicated we therefore compute the stability criteria numerically in Matlab.
We take a range of weights α, a range of frequencies multiplied by time step ω△t, substitute them into the expressions for the exact solutions of a quartic function, and show the domains where the maximum in modulus eigenvalue is smaller than one maxi=1,…,4|λi|≤1.
One can see in Fig.
4 that the only stability region located near ω△t=0 is the one corresponding to α=0.5.
For α=0.5 and the Hamiltonian of the harmonic oscillator the transformation matrix becomes equal to(22)A=(−35g−12g5Δt1−g−ω2Δt1−g412g5Δtω2−35gΔt1−g41−g1−g−ω2Δt1−g4115g(Δt2ω2−9)4g15Δt(Δt2ω2−9)Δt1−g41−g−4g15Δtω2(Δt2ω2−9)115g(Δt2ω2−9)), where g=5Δt2ω2Δt2ω2+16.
The eigenvalues are computed numerically, see Fig.
5, which is a two-dimensional slice of the plot in Fig.
4 with a fixed weight α=0.5.
Within the (grey) stability domain, all the eigenvalues are equal to one in modulus, which leads to an absence of dissipation in the numerical scheme.
The numerical calculation of the eigenvalues provides the linear stability condition |ωΔt|≤1.757, which is slightly more restrictive than the criteria for the Störmer–Verlet scheme.
Compared with the scheme P2N3Q4Lob in [22], we see that bumps are present in the linear stability domain for both cases, but the stability restriction for P2N3Q4Lob is less restrictive and equals |ωΔt|≤22.
As the authors mention in [22], the scheme P2N3Q4Lob is implicit and all the equations have to be solved simultaneously, which may pose a more difficult numerical challenge.
The third order scheme proposed here does not possess this shortcoming, as will be demonstrated further.
Since the value α=0.5 gives the best stability results, the time integration scheme (20) becomes(23a)qn+1/2=34qn,−+14qn,++Δt4(∂H(pn,+,qn,+)∂pn,++∂H(pn+1/2,qn+1/2)∂pn+1/2),(23b)pn+1/2=34pn,−+14pn,+−Δt4(∂H(pn,+,qn,+)∂qn,++∂H(pn+1/2,qn+1/2)∂qn+1/2),(23c)qn+1,−=qn,++Δt∂H(pn+1/2,qn+1/2)∂pn+1/2,(23d)pn+1,−=pn,+−Δt∂H(pn+1/2,qn+1/2)∂qn+1/2,(23e)qn+1,+=43qn+1/2−13qn,++Δt3∂H(pn+1,−,qn+1,−)∂pn+1,−,(23f)pn+1,+=43pn+1/2−13pn,+−Δt3∂H(pn+1,−,qn+1,−)∂qn+1,−.
The accuracy of the scheme (23) is verified for a linear system with Hamiltonian H=12p2+12ω2q2.
The results in Table 1 show that the scheme (23) is third order accurate.
We define the L2 error as Lerr2=∑n=1N∫tn−1tn(qτ−qex(t))2dt for the polynomial approximation qτ defined in (17b) and the exact solution qex of the autonomous system derived in (A.8).
The final results of the scheme are the values qn+1,+, pn+1,+; therefore the L∞–error and ΔH–error are defined as(24a)Lerr∞=max1≤n≤N⁡|qn,+−qex(tn)|,(24b)ΔHerr=max1≤n≤N⁡(H(pn,+,qn,+))−min1≤n≤N⁡(H(pn,+,qn,+)).
The dispersion analysis is performed following [15].
In order to compute the dispersion properties of the new variational time integration scheme (23), we take the Fourier modes (i.e., we now take λ=e−iωˆΔt such that |λ|=1)(25)(pn,−qn,−pn,+qn,+)=(ae−iωˆnΔtbe−iωˆnΔtce−iωˆnΔtde−iωˆnΔt) and substitute these into the system (23), which gives a linear system(26)e−iωˆ△t(a,b,c,d)T=A(a,b,c,d)T. In general, we are not interested in the results for the variables pn+1,−, qn+1,−, as those are intermediate data.
What matters is a relation for the variables pn+1,+, qn+1,+.
In the continuum case, as shown in Appendix A, the exact dispersion relation is c=±idω, cf.
(A.7).
Nevertheless, as there are four equations, all of them have to be included in the analysis.
We manipulate system of equations (26) symbolically with Matlab to find the relations between a, b, c and d; and, to obtain the numerical frequency ωˆ.
For the dispersion analysis we introduce the variables(27)x=1−Δt6ω636(4−Δt2ω2)2,y=Δt3ω36(4−Δt2ω2).
Notice, that x→1,y→0, when Δt→0.
The first family of solutions of (26) is(28a)a=−ibω,c=−idω,b=−(x+iy)d,(28b)ωˆ=1Δtarccos⁡−4(4−Δt2ω2)x+Δt2ω2(Δt2ω26−3)Δt2ω2+16.
We see, that the second relation in (28) is the exact one.
The numerical frequency is, however, wrong as can be verified by taking the limit Δt→0 in (28b).
The second family of solutions is(29a)a=−ibω,c=−idω,b=(x−iy)d,(29b)ωˆ=1Δtarccos⁡4(4−Δt2ω2)x+Δt2ω2(Δt2ω26−3)Δt2ω2+16.
The relation c=−idω is the exact dispersion relation, and the numerical frequency converges to the exact one ωˆ→ω if Δt→0 in (29b), see Fig.
6.
The third family of solutions is(30a)a=ibω,c=idω,b=(x+iy)d,(30b)ωˆ=−1Δtarccos⁡4(4−Δt2ω2)x+Δt2ω2(Δt2ω26−3)Δt2ω2+16.
In this case the numerical frequency converges to the negative value, ωˆ→−ω when Δt→0.
The fourth family of solutions is(31a)a=ibω,c=idω,b=(−x+iy)d,(31b)ωˆ=−1Δtarccos⁡−4(4−Δt2ω2)x+Δt2ω2(Δt2ω26−3)Δt2ω2+16.
Here the numerical frequency does not converge to the correct value when Δt→0.
Since there are two parasitic modes we need to be careful that they do not pollute the solution.
For the parasitic modes the relation b→−d holds.
One important step is to choose a=c and b=d, i.e.
take p0,−=p0,+ and q0,−=q0,+ as initial conditions.
In this case the parasitic modes are eliminated.
For the nonlinear water wave case we verified the consequence of this initial filtering numerically in §6, which appears to be sufficient to ensure stability in the nonlinear case.
This absence of an initial jump leads to satisfactory results.
If a one-degree-of-freedom Hamiltonian system is approximated with two variables pn and qn, the symplecticity condition is formulated as(32)MTJ−1M=J−1, where the Jacobian of the transformation is defined as(33)M=∂(qn+1,pn+1)∂(qn,pn).
The structure matrix J, equal to(34)J=(0I−I0), where I is the identity matrix with the dimension equal to the degrees of freedom of the Hamiltonian system, relates to the Hamiltonian system as(35)(qp)t=J∇H=(01−10)(HqHp).
Here the subscript (⋅)t refers to taking a time derivative and Hq≡∂H/∂q, etc.
For details we refer to [12].
Since we are working with an extended system, i.e.
a one-degree-of-freedom Hamiltonian system is approximated with four variables pn,−,pn,+ and qn,−,qn,+, the matrices in the symplecticity condition (32) are also extended.
We take(36)M=∂(qn+1,−,qn+1,+,pn+1,−,pn+1,+)∂(qn,−,qn,+,pn,−,pn,+), and the structure matrix J, defined as in (34) for a two-dimensional system.
The Hamiltonian system can then be written as(37)(q−q+p−p+)t=(J)∇H=(000100100−100−1000)(Hq−Hq+Hp−Hp+).
We verified in Matlab using symbolic manipulation that the condition (32) is valid for the new system (23).
The details of the matrix M are not given explicitly here since the expressions are too large to present.
For a Hamiltonian system with one degree of freedom the symplecticity condition means preservation of an area in phase space [12].
A phase–space plane is shown in Fig.
7.
A point on the plane is given as (pn,+,qn,+), as our prime interest lies in these variables.
We define four initial conditions in the plane, that constitute a quadrilateral shape.
The numerical approximation of a harmonic oscillator system develops according to the scheme (23).
The system cycles over time through the same states, which are defined by the initial conditions.
The area of the rectangular shape oscillates around the initial state, see Fig.
8.
From a geometrical point of view, the symplecticity condition (32) means in the general case with d>1 degrees of freedom that the sum of the oriented areas of the projections of a shape in 2d space onto the planes (pi,qi) is conserved [12].
Therefore Figs.
7 and 8 do not prove symplecticity, but illustrate the property of the plus-variables pn,+,qn,+ to cycle through the same states.
The conclusions on the novel third order scheme (23) are as follows.
The scheme is symplectic, but the linear stability condition is more restrictive than the condition for the Störmer–Verlet scheme.
The scheme is third order accurate, which is verified both for linear and nonlinear problems (see §6 and also Bokhove and Kalogirou [3]).
In the following section, we develop time integration of non-autonomous Hamiltonian systems.
Consider a damped one-degree-of-freedom Hamiltonian system with the following non-autonomous variational principle, (see [23] and [27] for related examples):(38)0=δL˜(p,q)=δ∫0T(pdqdt−H(p,q))eγtdt=∫0T(dqdt−∂H∂p)δ(peγt)−(d(peγt)dt+∂H∂qeγt)δqdt+peγtδq|0T, with γ>0 the coefficient of the linear momentum damping and end-point conditions δq(0)=δq(T)=0.
Variation of (38) with respect to the variables p and q yields Hamilton's equations for the damped oscillator when the Hamiltonian is H(p,q)=12p2+12q2 (and unit frequency).
We find(39a)δ(peγt):dqdt=∂H∂p=p,(39b)δq:dpdt+γp=−∂H∂q=−q.
With the coordinate transformation q=Qexp⁡(−γt/2) and p=Pexp⁡(−γt/2), the variational principle then becomes(40)0=δ∫0T(PdQdt−H˜(P,Q))dt, with modified Hamiltonian(41)H˜(P,Q)=12P2+12Q2+12γPQ.
Hence, the system (40) is reformulated as an autonomous system, viz.
the modified Hamiltonian (41) is time independent in the new variables P and Q.
We can now monitor this modified Hamiltonian at the discrete level.
We start with a first order approximation of the functional (38) in time.
We consider the discrete variational principle(42)L˜τ(pτ,qτ):=∑n=0N−1∫tntn+1(pτdqτdt−H(pτ,qτ))eγtdt−∑n=−1N−1[[qτ]]{{pτeγt}}αβ|tn+1, where the variables (p,q) are approximated as piecewise-constant per time interval (tn,tn+1).
As in [8], to obtain the well-known formulation of the symplectic Euler scheme, we rename the approximations as qτ=qn,+,pτ=pn+1,− and take α=1,β=0.
The exponential time-dependent term responsible for the damping is new and will be approximated using the value at the right boundary of the time element eγtn+1.
Substituting these approximations into (42), we obtain(43)L˜τ(pτ,qτ):=∑n=0N−1−ΔtH(pn+1,−,qn,+)eγtn+1−∑n=−1N−1(qn,+−qn+1,+)(pn+1,−eγtn+1).
We take variations δL˜τ=0 of (43) with respect to the variables pn+1,−,qn,+, use the arbitrariness of variations and end-point conditions δq0,−=0, δp0,−=0, δqN,+=0, to obtain(44a)pn+1,−=pn,−−Δt∂H(pn+1,−,qn,+)∂qn,+−(1−eγ(tn−tn+1))pn,−,(44b)qn+1,+=qn,++Δt∂H(pn+1,−,qn,+)∂pn+1,−, after division by the exponential term eγtn+1.
We see that the first step is implicit and the second step is explicit.
The exponential term in (44a) needs some clarification.
Using(45)1−eγ(tn−tn+1)=1−e−γΔt≈γΔt, shows that the discrete equation (44a) is an approximation to the damped equation(46)p˙+∂H∂q+γp=0.
This ensures that the modified total energy (41) will be preserved as will be demonstrated next.
Consider the Hamiltonian H(p,q)=12p2+12q2.
We compare three cases for the damped variational principle, viz.
approximation (44a) and the more straightforward (forward and backward Euler) approximations for the damping term:(47)pn+1=pn−Δt∂H(pn+1,qn)∂qn−Δtγpn, or(48)pn+1=pn−Δt∂H(pn+1,qn)∂qn−Δtγpn+1, while the second equation (44b) in the system remains unchanged.
We verify the absence of drift in a discrete version of the modified energy (41) at each time level tn, n=0,…,N,(49)Hex(pn,−,qn,+,tn)=12((pn,−)2+(qn,+)2+γpn,−qn,+)eγtn for the different approximations.
The results of the discrete modified energy are presented in Fig.
9 for the different approximations versus time.
We see, that the modified energy for approximation (44a) oscillates around the initial state, while the other two approximations reveal a drift in energy.
The latter was expected for the forward Euler approximation of the forcing term in (47).
Nevertheless, the result is unexpected for the implicit scheme (48), where the approximation of the damping term is done analogously to the symplectic Euler scheme.
Therefore our variational approach allows also the derivation of time integrators that result in a bounded fluctuation of special integrals.
A Hamiltonian system is studied that has a Lagrangian (1) with a forcing f(p,q,t) added(50)L(p,q):=∫0T(pdqdt−H(p,q)−f(p,q,t))dt.
We consider the simple case of a forced oscillator.
The Hamiltonian is H(p,q)=12p2+12q2 with forcing function f(p,q,t)=−aγtq, where aγ is the amplitude.
The corresponding system reads(51)dqdt=p and dpdt=−q+aγt.
This system is non-autonomous, but we can introduce a coordinate transformation to make it autonomous, as follows(52)q=Q+aγt,p=P.
The Lagrangian (50) for a forced harmonic oscillator is then reformulated as(53)L(p,q)=∫0T(PdQdt−(12P2+12Q2−aγP))dt+16a2γ2T3.
An important quantity to monitor is the Hamiltonian in the new coordinates(54)Hˆ=12P2+12Q2−aγP, which will be used to compute the order of accuracy of the forced scheme.
The exact solution to the problem (51) is(55)qex=q(0)cos⁡(t)+(p(0)−aγ)sin⁡(t)+aγt.
Next, we will consider the symplectic Störmer–Verlet scheme to compute the evolution of this forced system.
We consider the second order approximation with added forcing, and investigate the accuracy for different discretizations of the forcing term.
We construct a Störmer–Verlet time stepping method adjoint to the one discussed in detail in §3.2.
We take appropriate polynomial approximations (15), the mixed quadrature rule (16) and α=0.
Then we obtain a discrete approximation of the Lagrangian (50) formulated as follows(56)Lτ(pτ,qτ)=∑n=0N−1(pn+1/2(qn+1,−−qn,+)−Δt2(H(pn+1/2,qn,+)+H(pn+1/2,qn+1,−)))−Δt2(f(pn+1/2,qn,+,t⁎)+f(pn+1/2,qn+1,−,t⁎⁎)))−∑n=−1N−1(qn+1,−−qn+1,+)pn+1,+.
Computing the variations, we obtain the following system of equations(57a)δpn,+:qn+1,+=qn+1,−,(57b)δqn,+:pn+1/2=pn,+−Δt2∂H(pn+1/2,qn,+)∂qn,+−Δt2∂f(pn+1/2,qn,+,t⁎)∂qn,+,(57c)δpn+1/2:qn+1,−=qn,++Δt2(∂H(pn+1/2,qn,+)∂pn+1/2+∂H(pn+1/2,qn+1,−)∂pn+1/2),+Δt2(∂f(pn+1/2,qn,+,t⁎)∂pn+1/2+∂f(pn+1/2,qn+1,−,t⁎⁎)∂pn+1/2)(57d)δqn+1,−:pn+1,+=pn+1/2−Δt2∂H(pn+1/2,qn+1,−)∂qn+1,−−Δt2∂f(pn+1/2,qn+1,−,t⁎⁎)∂qn+1,−, where we decide on the choice of the time levels t⁎ and t⁎⁎ later.
Convergence tests will be performed to compare different forcing term approximations.
We use the following approximations of the integral on the forcing term in (57)(58)∫tntn+1f(pτ,qτ,t)dt=Δt2(f(pn+1/2,qn,+,tn)+f(pn+1/2,qn+1,−,tn+1))or(59)=Δt2(f(pn+1/2,qn,+,tn+1/2)+f(pn+1/2,qn+1,−,tn+1/2)).
The convergence tests are based on the L∞-norm, see Table 2, and the L2-norm, see Table 3.
Estimates of the extended energy error (54) is given in Table 4.
The exact solution qex in (55) is used.
The set up of the numerical test is as follows.
We start with Δt=1, the number of time steps is N=40.
The final time, where we compare the results, is T=40.
The amplitude of the forcing is a=0.1 with γ=0.1.
The initial coordinate q0=0.1 and the initial momentum p0=−0.1.
The errors are defined as(60a)Lerr∞=max1≤n≤N⁡|qn−qex(tn)|,(60b)Lerr2=∑n=1N∫tn−1tn(qτ−qex(t))2dt,and(60c)ΔHˆerr=max1≤n≤N⁡(Hˆ(pn,qn,tn))−min1≤n≤N⁡(Hˆ(pn,qn,tn)), where qτ is an approximation of q in the time interval (tn−1,tn).
For the second order approximation considered here, we have qτ defined in (15).
These schemes are all second order accurate and the extended energy Hˆ is bounded and reveals no drift in time.
One of these approaches was successfully used for the variational water wave problem with a wave maker in [9].
We investigate the performance of the third order symplectic time integrator (23) to compute nonlinear potential flow water waves in a water basin.
The dynamics of the water waves can be embedded in Miles' variational principle [21].
For a wave basin with solid boundaries this variational principle is(61)0=δL(ϕ,η,ϕs):=δ∫0T(∫0L(ϕs∂η∂t−12g(η2−b2))dx−∫0L∫−b(x)η(x,t)12|∇ϕ|2dzdx)dt.
The domain Ω={0<x<L,−b(x)<z<η(x,t)}⊂R2 is time dependent due to the movement of the free surface boundary SF:z=η(x,t).
The bottom is fixed SB:z=−b(x), and also the solid walls to the left and right of the basin: SL:x=0,x=L are fixed.
The velocity potential is denoted by ϕ(x,z,t), and ϕs(x,t)=ϕ(x,z=η,t) is the velocity potential at the free surface; ∇ is the gradient operator.
The total energy or Hamiltonian of the system is the sum of potential plus kinetic energies(62)H(ϕ,η,ϕs):=∫0L12g(η2−b2)dx+∫0L∫−b(x)η(x,t)12|∇ϕ|2dzdx.
By taking the variational derivatives with respect to ϕ, η, ϕs and the velocity potential at the fixed boundaries ϕ(0,z,t), ϕ(L,z,t), ϕ(x,−b,t), in (61) we obtain the potential flow equation with nonlinear boundary conditions [18,21,9].
When we substitute finite element approximations at the free surface η≈ηh(x,t)=ηl(t)φl(x),ϕs≈ϕsh(x,t)=ϕk(t)φk(x) and ϕ≈ϕh(x,z,t)=ϕi(t)φ˜i(x,z) into (61) the space discrete variational principle takes the form(63)0=δ∫0TL(ϕi′,ϕk,ηl)dt=δ∫0T(Mklϕkdηldt−12gMklηkηl−12Aijϕiϕj)dt, with Mkl the symmetric mass matrix and Aij the symmetric matrix associated with the kinetic energy and Laplace operator, given standard Lagrange basis functions φl(x) and φ˜i(x,z).
Summation over repeated indices is used.
Indices k and l concern free surface nodes and degrees of freedom, indices i and j concern all nodes and degrees of freedom, while indices i′ and j′ concern the interior nodes and degrees of freedom (so excluding the free surface ones).
For detailed definitions we refer the reader to [8,9].
The basis functions in the interior remain time dependent as they follow the free surface movement.
The Hamiltonian (62) is approximated as(64)H(ϕ,η)=12gMklηkηl+12Aij(η)ϕiϕj, where the boldface variables denote the vectors of unknown coefficients.
Further, as in [9], we need to approximate the variables in time.
We choose within each time interval t∈(tn,tn+1), with tn+1=tn+△t, a quadratic expansion in time analogous to the expansion (17).
Then the free surface η and potential ϕ are approximated as:(65a)ηl=(tn+tn+1−2t)(tn+1−t)Δt2ηln,++4(t−tn)(tn+1−t)Δt2ηln+1/2−(tn+tn+1−2t)(t−tn)Δt2ηln+1,−(65b)ϕi=(tn+tn+1−2t)(tn+1−t)Δt2ϕin,++4(t−tn)(tn+1−t)Δt2ϕin+1/2−(tn+tn+1−2t)(t−tn)Δt2ϕin+1,−.
Using approximations (65) we obtain an approximation of the continuous variational principle (61) similar to (19), resulting in the discrete minimization problem(66)0=δ∑n=0N−1L(ϕin,+,ηln,+,ϕin+1/2,ηln+1/2,ϕin+1,−,ηln+1,−)=δ∑n=0N−1Mkl(16ϕkn,+(−3ηln,++4ηln+1/2−ηln+1,−)+23ϕkn+1/2(−ηln,++ηln+1,−)+16ϕkn+1,−(ηln,+−4ηln+1/2+3ηln+1,−))−Δt6(H(ϕin,+,ηln,+)+4H(ϕin+1/2,ηln+1/2)+H(ϕin+1,−,ηln+1,−))−δ∑n=−1N−112Mkl(ηln+1,−−ηln+1,+)(ϕkn+1,−+ϕkn+1,+).
Variation of (66) with respect to the independent variables ϕkn,+, ηln,+, ϕkn+1/2, ϕj′n+1/2, ηln+1/2, ϕkn+1,−, ϕj′n+1,−, ηln+1,− gives the following variational discretization(67){Mklηln+1/2=34Mklηln,−+14Mklηln,++Δt4(∂H(ϕin,+,ηln,+)∂ϕkn,++∂H(ϕin+1/2,ηln+1/2)∂ϕkn+1/2),Mklϕkn+1/2=34Mklϕkn,−+14Mklϕkn,+−Δt4(∂H(ϕin,+,ηln,+)∂ηln,++∂H(ϕin+1/2,ηln+1/2)∂ηln+1/2),Ai′j′(ηn+1/2)ϕi′n+1/2=−Akj′(ηn+1/2)ϕkn+1/2,Mklηln+1,−=Mklηln,++Δt∂H(ϕin+1/2,ηln+1/2)∂ϕkn+1/2,Mklϕkn+1,−=Mklϕkn,+−Δt∂H(ϕin+1/2,ηln+1/2)∂ηln+1/2,Ai′j′(ηn+1,−)ϕi′n+1,−=−Akj′(ηn+1,−)ϕkn+1,−,Mklηln+1,+=43Mklηln+1/2−13Mklηln,++Δt3∂H(ϕin+1,−,ηln+1,−)∂ϕkn+1,−,Mklϕkn+1,+=43Mklϕkn+1/2−13Mklϕkn,+−Δt3∂H(ϕin+1,−,ηln+1,−)∂ηln+1,−,Ai′j′(ηn+1,+)ϕi′n+1,+=−Akj′(ηn+1,+)ϕkn+1,+.
As verification test, we consider standing waves in a rectangular basin with a unit depth H=1 and the basin length L=10.
A linear solution is used to set the initial condition at t=0(68)η(x,t)=Aωgcos⁡(kx)sin⁡(ωt)cosh⁡(kH);ϕ(x,z,t)=Acos⁡(kx)cos⁡(ωt)cosh⁡(kz), with amplitude A=0.075, wave number k=2πNw/L, with integer Nw=3, and the dispersion relation ω2=gktanh⁡(kH).
The waves computed with the variational discretization (67) show no decay in discrete energy over one hundred periods of time, as can be seen in Fig.
10.
The error in the discrete energy is used to compute the order of accuracy.
As we can see in Table 5, the scheme is third order accurate in time.
The time integration method (67) poses a conceptual advantage compared to the second order Störmer–Verlet scheme, presented in this section, see also [9].
The Störmer–Verlet scheme requires a trapezoidal-midpoint quadrature rule, which makes it more difficult to determine the times at which the interior potential has to be evaluated, because the discrete Neumann operator (the inverse of Ai′,j′) has to be applied first.
The interior velocity potential depends on the free surface elevation and the free surface velocity potential.
It results in an unsymmetrical approximation of the variables, which is not optimal.
The surface elevation is approximated in time via ηsn,+ and ηsn+1,−, while the free surface potential is approximated via ϕsn,+ and ϕsn+1/2.
The resulting numerical discretization, which is discussed in [9], provided excellent results, but it also required a lot of additional analysis.
In contrast, the method (67) is symmetrical and therefore there is no question of how to compute the velocity potential in the interior.
Moreover, method (67) solves one implicit system with three equations, then six explicit steps and gives third order accuracy, while the Störmer–Verlet scheme solves an implicit system for ϕi′,ϕs first, then an implicit system for ηs,ϕi′ and one explicit step for ϕs.
Therefore the second order scheme requires the solution of two uncoupled implicit systems of equations versus one coupled set of equations for the third order scheme.
We have constructed novel symplectic time integrators using a variational approach.
The time integration method is based on a discontinuous Galerkin discretization in time, using a novel numerical flux, which connects the adjacent time slabs.
We first demonstrated how several well-known time integrators, such as the symplectic Euler and Störmer–Verlet time integrators, can be derived using this variational approach.
Subsequently, a new symplectic time integrator was derived third order accurate in time.
The third order time integrator was shown to have excellent dispersion properties and was also successfully applied to compute nonlinear water waves in a basin.
Finally, the variational approach to derive time stepping schemes was also developed and tested for non-autonomous Hamiltonian systems, such as forced and damped oscillators as well as driven water wave problems (see also [3]).
We acknowledge financial support of the Technology Foundation STW and The Netherlands Organization for Scientific Research (NWO) for the projects “Complex wave current interactions in a numerical wave tank”, STW grant no.
08130, and “Compatible Mathematical Models for Coastal Hydrodynamics”, NWO grant no.
613.000.803, as well as Bokhove's EPSRC grant no.
EP/L025388/1 “FastFem: Behaviour of Fast ships in waves”.
In this section we will show the essential steps in the analysis of the harmonic oscillator following work in [2].
The dynamics of a harmonic oscillator is embedded in the functional(A.1)L(p,q):=∫0T(pdqdt−H(p,q))dt, with p(t) the momentum, q(t) the coordinate and H(p,q) the energy (or Hamiltonian) of the oscillator, which is defined as(A.2)H(p,q):=12p2+12ω2q2, with the frequency ω.
The variational derivative of this Hamiltonian is(A.3)δH(p,q):=∂H∂pδp+∂H∂qδq=pδp+ω2qδq.
From the variational principle δL(p,q)=0 with end-point conditions δq(0)=0, δq(T)=0, the dynamics of the harmonic oscillator emerges:(A.4)(pq)t=(0−ω210)(pq)=A(pq), with initial conditions p(t=0)=p0 and q(t=0)=q0.
Subscript t refers to differentiation in time.
The characteristic polynomial for system matrix A is(A.5)λ2+ω2=0 with eigenvalues λ=±iω.
Following, e.g.
[15], we can find the dispersion relation by taking a Fourier mode(A.6)(pq)=(ae−iωˆtbe−iωˆt).
Solving it, we arrive at(A.7)ωˆ=ω,a=−iωborωˆ=−ω,a=iωb.
Finally, the exact solution of this system used in convergence tests is(A.8)(pq)=(cos⁡(ωt)−ωsin⁡(ωt)1ωsin⁡(ωt)cos⁡(ωt))(p0q0).
In this section we provide the details of the analysis of the scheme given by (14) and introduced in Section 3.2.
We demonstrate that the scheme coincides with the Störmer–Verlet time stepping scheme.
In the system of equations (14) we have three variables from the time slab (tn−1,tn): qn−1,+, qn−1/2, pn,−; four variables from the time slab (tn,tn+1): pn,+, qn,+, qn+1/2, pn+1,−, and two variables from the time slab (tn+1,tn+2): pn+1,+, qn+1,+.
One can notice that variables qn−1/2,qn−1,+ appear only in combination.
To make the system solvable, we reformulate (14) with the variables [[q]]|tn+1,qn+1/2 and [[p]]|tn+1 and pn+1,− as unknowns:(B.1a)δqn,+:β[[p]]|tn+1=α[[p]]|tn,(B.1b)δpn,+:qn+1/2=qn,++β[[q]]|tn+Δt2∂H(pn,+,qn+1/2)∂pn,+,(B.1c)δqn+1/2:pn+1,−=pn,++2β[[p]]|tn+1−Δt2(∂H(pn,+,qn+1/2)∂qn+1/2+∂H(pn+1,−,qn+1/2)∂qn+1/2),(B.1d)δpn+1,−:α[[q]]|tn+1=qn+1/2−qn,+−Δt2∂H(pn+1,−,qn+1/2)∂pn+1,−.
In addition, we have the relations: qn+1,+=qn+1,−−[[q]]|tn+1=2qn+1/2−qn,+−[[q]]|tn+1, and pn+1,+=pn+1,−−[[p]]|tn+1.
Hence, there are enough equations to close the system.
To study the linear stability of the system (B.1), we take H=12p2+12ω2q2.
For this Hamiltonian, system (B.1) is rewritten in matrix form as(B.2)([[p]]|tn+1pn+1,+[[q]]|tn+1qn+1,+)≡An([[p]]|tnpn,+[[q]]|tnqn,+)=(αβ0002α−αβ1−Δt2ω22−Δtω2β−Δtω2−ΔtΔt3ω24αβα(1+Δt2ω22)Δt2ω22αΔtΔt−Δt3ω24α2β−βα−Δt2ω2β2α1−Δt2ω22α)([[p]]|tnpn,+[[q]]|tnqn,+).
The characteristic polynomial of matrix An has the form(B.3)(αβ−λ)(βα−λ)(λ2+(ω2(Δt)2−2)λ+1)=0, from which we can see that the necessary condition to satisfy the linear stability requirement |λ|≤1 is to take α=β=0.5.
Nevertheless, this restriction can be alleviated by taking [[p]]|tn≡0.
In this case the first row of the transformation matrix An in (B.2), will be removed and the characteristic polynomial (B.3) reduces to(B.4)(βα−λ)(λ2+(ω2(Δt)2−2)λ+1)=0, which gives two linear stability requirements:(B.5)α≥0.5and|ωΔt|≤2.
Therefore, if [[p]]|tn≡0, a range of weights α∈[0.5,1] leads to a stable system under the usual Störmer–Verlet stability condition.
Considering the above mentioned requirement α∈[0.5,1], [[p]]|tn≡0, we can recover the well-known form of the unforced symplectic Störmer–Verlet scheme.
First, we rewrite (B.1) as(B.6a)δqn,+:pn,+=pn,−,(B.6b)δpn,+:qn+1/2=βqn,−+αqn,++Δt2∂H(pn,+,qn+1/2)∂pn,+,(B.6c)δqn+1/2:pn+1,−=pn,+−Δt2(∂H(pn,+,qn+1/2)∂qn+1/2+∂H(pn+1,−,qn+1/2)∂qn+1/2),(B.6d)δpn+1,−:βqn+1,−+αqn+1,+=qn+1/2+Δt2∂H(pn+1,−,qn+1/2)∂pn+1,−.
We see from (B.6a), that we can denote pn:=pn,+=pn,−.
Also, one can introduce a variable qn:=βqn,−+αqn,+, and recover the known form of the system, where the first and second steps are implicit and the third step is explicit(B.7a)qn+1/2=qn+Δt2∂H(pn,qn+1/2)∂pn,(B.7b)pn+1=pn−Δt2(∂H(pn,qn+1/2)∂qn+1/2+∂H(pn+1,qn+1/2)∂qn+1/2),(B.7c)qn+1=qn+1/2+Δt2∂H(pn+1,qn+1/2)∂pn+1.
The resulting scheme is the symplectic Störmer–Verlet integrator [12].
The characteristic polynomial for the transformation matrix of this scheme has the form(B.8)λ2+(ω2(Δt)2−2)λ+1=0.
The eigenvalues λ therefore satisfy the condition |λ|≤1 if |Δtω|≤2, which is the well-known stability condition for the Störmer–Verlet scheme.
The dispersion relation, as discussed, for example, in [2], has the form(B.9)a=−iωb1−ω2Δt24,ωˆ=arccos⁡(1−ω2(Δt)22)Δt.
A numerical modelling of gas exchange mechanisms between air and turbulent water with an aquarium chemical reaction This paper proposes a new numerical modelling to examine environmental chemodynamics of a gaseous material exchanged between the air and turbulent water phases across a gas–liquid interface, followed by an aquarium chemical reaction.
This study uses an extended concept of a two-compartment model, and assumes two physicochemical substeps to approximate the gas exchange processes.
The first substep is the gas–liquid equilibrium between the air and water phases, A(g)⇌A(aq), with Henryʼs law constant H. The second is a first-order irreversible chemical reaction in turbulent water, A(aq)+H2O→B(aq)+H+ with a chemical reaction rate κA.
A direct numerical simulation (DNS) technique has been employed to obtain details of the gas exchange mechanisms and the chemical reaction in the water compartment, while zero velocity and uniform concentration of A is considered in the air compartment.
The study uses the different Schmidt numbers between 1 and 8, and six nondimensional chemical reaction rates between 10−∞(≈0) to 101 at a fixed Reynolds number.
It focuses on the effects of the Schmidt number and the chemical reaction rate on fundamental mechanisms of the gas exchange processes across the interface.
•This article provides a new concept and modelling strategy for a gas exchange process at a gas–liquid interface, followed by an aquarium chemical reaction.
This article considers the gas exchange processes across the interface by separating this phenomenon into two physicochemical substeps; the first is a gas–liquid equilibrium of the gas at the interface, and second an aquarium chemical reaction.
The modelling strategy is useful to evaluate the gas exchange rate of a highly reactive gas in water, with a lot of applicabilities in the fields of environmental sciences, atmospheric physics, chemical and mechanical engineering, and limnology.
This article examines the effects of the Schmidt number of the gas, and the chemical reaction rate, on the gas exchange rate at the interface.
The numerical data and results provided in this article are new and comprehensive, and show significant role of the chemical reaction in water on the gas exchange at the interface.
Chemodynamics of a gaseous material exchanged between the air and turbulent water phases is one of the commonly observed physicochemical processes in the environment.
One of the most well-known examples of the gas exchange is an uptake of carbon dioxide (CO2) from the atmosphere into the ocean, and vice versa, across the ocean surfaces [1–4].
The gas exchange has been considered as one of the important processes of the global carbon cycle, and many field measurements of the gas exchange rate have been attempted to quantify the role of the ocean storage of CO2 on the global budgets of carbon [5].
The gas exchange mechanisms between the atmosphere and river water have also been under scrutiny to clarify microbiological mass balances of CO2 and oxygen (O2) in the field of limnology [6,7].
Several reports have expressed concerns that CO2 uptaken across the interface causes acidification of the seawater, since it is believed to have potential environmental and ecological impacts [8].
A series of chemical reactions of CO2 in the seawater are expressed as [9],(1a)CO2(g)⇌CO2(aq)(1b)CO2(aq)+H2O⇌H2CO3(1c)H2CO3⇌H++HCO3−(1d)HCO3−⇌H++CO32− The first substep indicates the gas–liquid equilibrium of CO2 between the two phases, and the following three substeps are the chemical reactions of CO2 into the seawater.
The effect of the chemical reaction rates of CO2 in the seawater on the gas exchange rate should be investigated carefully to clarify details and future development of the ocean acidification, and to assess possible impacts on the ecosystem in the ocean, as well as the details of the global carbon cycle.
Another example of the environmental chemodynamics of a gas exchanged between air and water with the chemical reaction is production of trifluoroacetic acid, CF3COOH, which is known as TFA [10,11].
This substance is one of the degradation products of 1,1,1,2-tetrafluoroethane (CH2FCF3), or, HFC-134a, which has been used as a refrigerant of a mobile fleet [12].
A few studies on the atmospheric chemistry have pointed out that HFC-134a is broken down by a series of stratospheric photochemical reactions after it is emitted into the atmosphere [13,14], and trifluoroacetyl fluoride, CF3COF, is one of the degradation products of the photochemical reactions [10,11].
CF3COF is further broken down to TFA by hydrolysis in cloud moistures following the physicochemical processes with the aid of ultraviolet rays [13,14],(2a)CF3COF(g)⇌CF3COF(aq)(2b)CF3COF(aq)+H2O→CF3COOH+HF TFA is one of the materials having strong organic acidity, and can accumulate in a closed surface water such as a seasonal wetland after wet depositions because of its low decomposition potential in the environment [15].
Both modelling of the formation processes of TFA, and predictions of its concentration in rainwater are desired to assess the details of accumulation in closed surface waters, and the biochemical effect of TFA on aquarium ecosystem [16].
Physicochemical processes of formation of the acid rain in cloud droplets caused by, for example, sulphur dioxide (SO2) released into the atmosphere also involve hydrolysis shown by [17](3a)SO2(g)+H2O⇌SO2⋅H2O(3b)SO2⋅H2O⇌H++HSO3−(3c)HSO3−⇌H++SO3− The physicochemical processes explained above suggest that the gas exchange between air and water with an aquarium chemical reaction play an important role in determining chemodynamics of the gaseous substances in the environment.
Quantification of the gas exchange processes between air and water has been examined extensively in the fields of oceanography, and environmental sciences for the reasons expressed above.
References which discuss the effect of the chemical reaction on gas transport processes into turbulent water are, however, extremely sparse.
For example, many field measurements of the gas exchange rates at the atmosphere–ocean interfaces have been reported without considering the chemical reactions of CO2 [18,19].
Also, many numerical studies on the gas exchange processes based on a direct numerical simulation (DNS) [20–24], and a large-eddy simulation technique (LES) [25–27], have not considered the effect of the aquarium chemical reactions.
One of the reasons for the sparsity of references is that field and laboratory measurements on the gas exchange mechanisms with the aquarium chemical reactions are difficult, especially if the chemical reactions are fast.
One of the good examples of the difficulties of measuring chemical reaction processes of the gases across the interface are indicated in a report by De Bruyn et al.
[28], in which both Henryʼs law constants and the chemical reaction rates for several halides are obtained by several studies.
Table 1 in their report [28] suggests that very large uncertainties exist in the measurements of Henryʼs law constant, and the chemical reaction rates of halides, resulting in considerable errors of predictions of the gas exchange rates.
Another reason for the neglected effect of the aquarium chemical reactions on the gas exchange processes is that many researchers and scientist have not been aware of the critical roles of the aquarium chemical reactions on the gas exchange between air and water.
It should also be pointed out here that field and laboratory measurements of the gas exchange processes require observations of very fine-scale concentration and velocity fluctuations particularly in the near-interface turbulent boundary layer, because of a large Schmidt number of O(103).
The current hardware of a high-performance computing system, on the other hand, will not allow us to perform DNS of turbulent flows with the gas exchange of such a large Schmidt number.
While LES methodology is available to compute the gas exchange at the interface for Sc∼O(102) [25–27], accuracy and reliability of the numerical solutions depend strongly on the subgrid-scale eddy diffusivity.
It seems to be recognized under the present circumstances that the subgrid-scale eddy diffusivity modelling of large Schmidt number gas exchange problems need more sophistication, especially in the case of large Reynolds number turbulence.
Introducing the effect of the chemical reactions in water on the gas exchange processes may require additional efforts to modify the eddy diffusivity modelling of the reactive gases.
We need careful examination to model the gas exchange processes at the interface with the aquarium chemical reactions in more comprehensive manner.
This study proposes a new framework of a numerical modelling of the gas exchange between air and water across their interface, and subsequent chemical reaction in water based on an extended two-compartment model.
The major purpose of this study is to provide a fundamental concept for modelling physicochemical processes of the gas exchange, followed by the chemical reaction in water.
Demonstrating fundamental data and knowledge on the important environmental transport phenomena, especially the effects of the Schmidt number and the chemical reaction rate on the gas exchange mechanisms across the interface have also been attempted.
The gas exchange processes are separated into two physicochemical substeps, the first is the gas–liquid equilibrium between the two phases, and the second is the chemical reaction in the water phase.
A first-order, irreversible chemical reaction of the gaseous material after its uptake into the water phase is assumed here to simplify interactions of the chemical reactions and turbulent transport phenomena in water.
While a traditional two-compartment model assumes uniform concentration of a material in each compartment, the present two-compartment model uses a computational fluid dynamics (CFD) technique in the water compartment to evaluate temporal development of three-dimensional profiles of the velocity and concentration fields.
A direct numerical simulation (DNS) approach is used to evaluate profiles of fluid velocities and concentrations in water, and several important turbulence statistics have been evaluated without using turbulent closures, and subgrid-scale models.
We assume that a fluid flow in the water phase is a well-developed turbulent water layer of a low Reynolds number, and the Schmidt number is varied from 1 to 8 to observe the effects of the molecular diffusion of the gas in sub-interface water on the gas exchange rate at the interface.
Six degrees of the nondimensional chemical reaction rate are used to find the effect of the chemical reaction rate on the gas exchange mechanisms.
Extrapolations of the gas exchange rates and the related transport phenomena toward larger Schmidt number and the faster chemical reaction rate will also be examined to predict the gas exchange processes of the actual gases of Sc∼O(102) based on results from the present numerical experiments.
This study introduces an extended version of a two-compartment concept to model the environmental gas exchange processes between the air and water phases.
Fig.
1(a) shows a schematic representation of the two compartments for considering the gas exchange between the air and water phases.
The compartment of the air phase is filled with the gas A with a constant concentration CAG and temperature T, and the law of the ideal gas is satisfied in it.
The partial pressure of A in the compartment is expressed by pA=CAGRT, using the gas constant R. The gas in the compartment does not have any flow velocities, and they are assumed zero everywhere in it.
Fig.
1(b) demonstrates outlook of the structure in the compartment of the water phase.
While a traditional two-compartment model assumes uniform concentration of a material in each compartment, this study considers three-dimensional unsteady distributions of the fluid velocity and concentrations only in the water compartment.
We assume that a well-developed turbulent water layer bounded by a solid bottom and a flat gas–liquid interface is established in the compartment [20,22,24].
Sizes of the compartment are Lx in length, Ly in width, and δ in height, and the two boundaries of the compartment at z=0 and δ correspond to a gas–liquid interface, and a solid bottom, respectively.
This study separates the gas exchange processes into the two physicochemical substeps.
The separation of the physicochemical processes is illustrated in Fig.
1(a) schematically, and the mathematical formulation is given below.
The first substep is the gas–liquid equilibrium of A at the gas–liquid interface,(4)A(g)⇌HA(aq) where H is Henryʼs law constant.
Assuming the gas–liquid equilibrium of A at the interface, the following relation of the equilibrium state of A is obtained,(5)CAint=pAH where CAint is the equilibrium concentration of A at the interface.
Inserting the state of an ideal gas into Eq.
(5) leads to the partition coefficient of A between the air and the water phases, K [29],(6)K=CAGCAint=HRT The equilibrium concentration of A at the interface, CAint, can be evaluated by giving the values of H, CAG (or, pA), and T. For example, the partition coefficient of CO2 between the gas and liquid is estimated as K≈1.2 at 298.2 K [30], and CAint is given by CAint=CAG/K=pA/(KRT).
By assuming 0.03% volume fraction of CO2 in the standard pressure of the atmosphere, pA=30Pa, at T=293.2K, the equilibrium concentration at the interface is predicted as CAint≈0.01molm−3 by using Eqs.
(5) and (6).
The second substep is the gas exchange between the two compartments with an irreversible chemical reaction of A in turbulent water,(7)A(aq)+H2O→κAB(aq)+H+ where κA is the chemical reaction rate.
In a well-mixed water compartment without the effect of the molecular diffusion and advection of A and B, the chemical reaction process of Eq.
(7) can be described as(8a)ddtCA=−κCA(8b)ddtCB=κCA where κ=κACH2O, and CH2O is concentration of water.
Since the water phase is considered as a dilute solution of the gas A, and the variation of the total molar of H2O by the chemical reaction is sufficiently small compared with that of A, the chemical reaction rate is approximated as a first order of CA.
Using the assumption of the first-order chemical reaction, the chemical reaction rate κ has a unit [T−1].
A well-developed low-Reynolds-number turbulent water layer with the aquarium chemical reaction is assumed in the directions parallel to the interface, as shown in Fig.
1.
The turbulent water flow has a mean flow in the x direction driven by a well-controlled pressure gradient, and is bounded by a gas–liquid interface at z=0, and a solid bottom at z=δ, respectively.
Periodicity is assumed for all the variables in the direction parallel to the interface, therefore, statistical properties of turbulent fluctuations depend only on the distance from the interface, z.
The gas–liquid interface is assumed to be flat and shear-free in this study, and the free-slip boundary condition at the interface, ∂u/∂z=∂v/∂z=w=0 is used.
A non-slip boundary condition, u=v=w=0, is applied to the bottom boundary.
The concentration of the gas A at the interface is considered uniform, and constant as determined by Eq.
(5) based on the gas–liquid equilibrium, while CA=CA∞ is imposed at the bottom boundary.
This study assumes CA∞=0 for simplicity of the problem.
Zero concentrations at the interface and the bottom are used for the concentration of B.
It is assumed that fluid in the water phase is incompressible and Newtonian, with constant density, ρ.
The physical properties of water, e.g., kinematic viscosity of water, ν, are also assumed to be constant.
The molecular diffusivities of A and B in water are considered identical, hence, DA=DB=D is used in the present study.
The governing equations of the viscous fluid flow and gas transport are consequently expressed by(9a)∂ui+∂xi⁎=0(9b)∂∂t⁎ui+=∂∂xj⁎(1Reτ∂∂xj⁎ui+−uj+ui+)−∂p+∂xi⁎(9c)∂∂t⁎CA⁎=∂∂xj⁎(1Reτ⋅Sc∂∂xj⁎CA⁎−uj+CA⁎)−K⋅CA⁎(9d)∂∂t⁎CB⁎=∂∂xj⁎(1Reτ⋅Sc∂∂xj⁎CB⁎−uj+CB⁎)+K⋅CA⁎ with the nondimensionalization(10)ui+=uiuτ,p+=pρuτ2,CA⁎=CAΔCA,CB⁎=CBΔCA,xi⁎=xiδ,t⁎=tδ/uτ where ui(i=1,2,3) is the velocity, p is the pressure, CA and CB are the concentrations of A and B, t is time, and xi is three-dimensional coordinate, respectively.
These governing equations are normalized by using the shear velocity at the bottom, uτ, kinematic viscosity of water, ν, height of the water layer, δ, and the concentration difference of A between the interface and the bottom of the water compartment, ΔCA=CAint−CA∞, as shown in Eq.
(10).
The subscripts 1,2, and 3 denote the streamwise, spanwise, and interface-normal directions, respectively, corresponding to the x, y, and z directions, and u1=u, u2=v, u3=w are used to signify the velocity component in the discussions below.
These governing equations involve three nondimensional parameters,(11a)Reτ=uτν/δ(11b)Sc=νD(11c)K=κuτ/δ The first is the Reynolds number, the second is the Schmidt number, and the last is the nondimensional chemical reaction rate.
The Reynolds number, Reτ, is set to be 150, which corresponds to the Reynolds number defined by the bulk-mean velocity of water, Um≡(1/δ)∫0δ〈u〉dz, and Rem≡Umδ/ν≈2300.
Here, 〈.〉 denotes the ensemble average, as introduced in Eq.
(15) later.
The Schmidt number is varied between Sc=20(=1) and 23(=8) to observe the effect of the molecular diffusivities of A and B in water on the gas exchange processes at the interface.
The nondimensional chemical reaction rate this study deals with is varied between 10−∞⩽K⩽101, choosing six values of K=10−∞(≈0), and 10n with n=−3 to 1, to discuss the effect of the chemical reaction rate on the gas exchange processes.
Hence, 24 cases of the combinations of Sc and K have been considered in the present study to examine the effects of the Schmidt number and the chemical reaction rate on the gas exchange mechanisms across the interface.
The present numerical experiments employ the same numerical procedures used in the previous study to solve the governing equations (9a)–(9d) [24].
The governing equations are discretized by a finite difference method on a Cartesian staggered grid using a second-order central difference in space for the all terms.
The discretized governing equations are advanced by a third-order Runge–Kutta method for the nonlinear and the source/sink terms and a second-order Crank–Nicolson method for the linear terms [31], combined with the four-substep fractional time step strategy [32].
A Helmholtz-type partially differential equation of the velocity, concentrations and pressure are solved by a combination of the fast Fourier transforms (FFT) and the Gaussian elimination for a tridiagonal linear equations [33].
A total of 7.15×106(=256×288×97) grid points is used to discretize the governing equations in the water compartment whose sizes are Lx×Ly×δ=5πδ×2.5πδ×δ, or, Lx+×Ly+×δ+=2356×1178×150, where superscript + denotes that the variable is nondimensionalized by uτ and ν.
The sizes of the water compartment are confirmed large enough in the x and y directions to impose the periodic boundary conditions for the all variables [24], however, suitability of the sizes of the water compartment is verified later in this study.
The grid spacings in both the x and y directions are equidistant, and Δx+≈9.20, and Δy+≈4.09.
The grid spacing in the z direction is determined by a hyperbolic tangent function to concentrate the grid points in both the regions near the bottom and the interface.
The minimum and maximum spacings are Δz+≈0.183 and 3.49, respectively.
Suitability of the grid resolution of numerical data for Sc=1 is confirmed by the previous study [24].
Suitability of the grid resolution for the other Schmidt numbers is verified by observing nonphysical profiles of spectra of the fluctuations of the concentration fields in large wavenumber space, as shown later.
It should be mentioned here that the Schmidt number of a gaseous material in water is generally in the order of 102–103.
For example, the molecular diffusivity of CO2 has been reported as about 2.02×10−9m2s−1 at 298.2K by Zeeby [34] based on his molecular dynamics simulations.
Considering the kinematic viscosity of water at the same temperature of 8.90×10−7m2s−1 [35], the Schmidt number is approximately 4.4×102.
Also, Sc≈3.6×102 is obtained in the case of the oxygen exchange across the interface at 298.2K [36].
The Schmidt numbers this study considers, 1⩽Sc⩽8, appear to be two to three orders of magnitude smaller than the actual Schmidt numbers of the actual gases.
The use of one-digit Schmidt numbers is inevitable in this study, since employing the actual Schmidt numbers of the order of 102 or larger requires an enormous number of grid points, and computational resources to resolve all the essential scales of the concentration fluctuations.
The effect of the Schmidt number on physics of the gas exchange is observed carefully to extrapolate the results from the present DNS toward the larger Schmidt numbers.
This effort will offset the major limitation of this study, in which Sc∼O(100) is used to avoid huge computational resources for the present numerical experiments.
The concept of the gas transfer velocity is often introduced to quantify the gas exchange rate at the gas–liquid interface.
The gas transfer velocity, kL, is defined by(12)QA=kL(CAint−CA∞)=kL(CAGK−CA∞) where QA is the gas flux of A at a unit area of the interface and time.
The dimension of kL is [LT−1], therefore, this quantity has the same dimension of the velocity, and is referred to as the “gas transfer velocity.” By applying Fickʼs law of the molecular diffusion at the interface, the gas transfer velocity is expressed as(13)kL=DΔCA|∂∂z〈CA〉|z=0 where 〈CA〉 is the ensemble average of CA based on the definition shown in Eq.
(15).
This equation indicates that the gas transfer velocity is proportional to the concentration gradient of A at the interface.
Nondimensionalization of Eq.
(13) using the nondimensional concentration of A and z coordinate, CA⁎ and z⁎, leads to a definition of the Sherwood number,(14)Sh≡kLδD=|∂∂z⁎〈CA⁎〉|z⁎=0 Eq.
(14) shows that the Sherwood number is equivalent to the nondimensional concentration gradient at the interface.
Introducing the nondimensional parameter is beneficial to compare the gas transfer velocity in a different experimental and numerical conditions [22,24].
In the following discussions, a variable f is decomposed into its time-space average between t=0 to TA over the x–y plain, and fluctuation around the average like f=〈f〉+f′, where 〈f〉 is defined by(15)〈f〉(z)=1TA∫0TA(1LxLy∫0Lx∫0Lyf(x,y,z,t)dxdy)dt This study uses sufficiently long time duration for computing turbulence statistics, TA+≡TAuτ2/ν⩾4500, which is more than 30 times that of the characteristic time scale of the surface-renewal eddies at the interface [24] to obtain fully-converged turbulence statistics.
This section discusses the effect of the Schmidt number on the gas exchange mechanisms in the case of zero chemical reaction rate, K=10−∞.
The discussions are helpful to understand fundamental physicochemical processes of the gas exchange across the interface.
In addition, the statistical analyses of the concentration fluctuations, and the turbulent contribution of the gas flux without the chemical reaction can also be used for validations of the grid resolution and suitability of the compartment sizes of the water phase.
Figs.
2(a)–2(d) show instantaneous concentration profiles of A in the y–z plane, which is perpendicular to the mean flow in water for Sc=1–8 in the case of zero chemical reaction rate (K=10−∞) to shed light on the effect of the Schmidt number on turbulent transport of A in water.
The four plots show the concentration profiles of A in the whole of the computational domain, hence, the sizes of the snapshots are W+=1178 in width, and H+=150 in height.
The comparison of the concentration profiles demonstrates that turbulent mixing of the gas A in water is accelerated as the Schmidt number increases.
The contrast of these black-and-white mappings becomes more uniform in grey except in the region close to the interface (upper boundaries of these snapshots) and the bottom (lower boundaries).
It is also very clear from these mappings that the presence of the small-scale concentration fluctuations is more visible as the Schmidt number increases.
The emphasis of the fine-scale fluctuations of the concentration of CA′ by increasing the Schmidt number is demonstrated more clearly in Fig.
3.
These snapshots are enlarged images of the concentration profiles depicted in Figs.
2(a)–2(d), whose sizes are W+=400 in width and H+=150 in height, exhibiting intensification of the fine-scale concentration fluctuations by increasing the Schmidt number.
One-dimensional spectra for the concentration fluctuations, and co-spectra of w′ and CA′, are computed in both the x and y directions to elucidate the effect of the Schmidt number on the concentration fluctuation in a more statistical manner.
Here, the spectra and the co-spectra are defined by(16a)EAA(kα)=12π〈Fα(CA′)Fα⁎(CA′)〉(16b)CowA(kα)=12πR〈Fα(w′)Fα⁎(CA′)〉 where Fα(.)
denotes the Fourier transform in the α(=1,2) direction, R(.)
shows the real part of a complex number, and the superscript ⁎ signifies the complex conjugate.
The spectra satisfy the following relation,(17)〈CA′CA′〉=∫0∞EAA(k1)dk1=∫0∞k1EAA(k1)d(lnk1)=∫0∞EAA(k2)dk2=∫0∞k2EAA(k2)d(lnk2) Similarly, the co-spectra, CowA satisfy(18)〈w′CA′〉=∫0∞CowA(k1)dk1=∫0∞k1CowA(k1)d(lnk1)=∫0∞CowA(k2)dk2=∫0∞k2CowA(k2)d(lnk2) where w′CA′ is the turbulent gas flux of A in the direction perpendicular to the interface.
This quantity appears in the time-space averaged gas transfer equation in Eq.
(22) as introduced later, and is one of the important turbulence statistics necessary to understand the gas exchange processes.
Figs.
4(a) and 4(b) show the one-dimensional spectra of the concentration fluctuations at z+=15,30, and 75 (z⁎=0.1, 0.2, and 0.5) in both the x and y directions.
These spectra are normalized by 〈CA′CA′〉 at the individual locations.
This figure illustrates the plots of the relation between (kαδ)EAA(kα) and log(kα) (α=1,2) to compare the effect of the Schmidt number on a semi-logarithmic charts, based on the relation shown in Eq.
(17).
Fig.
4 shows clearly that the contribution of the small-scale concentration fluctuations becomes significant as the Schmidt number increases.
For example, (k1δ)EAA(k1) evaluated at z+=15 reaches a maximum at k1δ=2.8 for Sc=1, k1δ=6.0 for Sc=2, and k1δ≈8 for Sc=4 and 8.
Also, it is observed easily from Fig.
4(b) that (k2δ)EAA(k2) at z+=15 decays sufficiently small in the high wavenumber region, k2δ>40 for Sc=1, and the value is in the order of 10−6.
On the other hand, (k2δ)EAA(k2) at k2δ=40 for Sc=8 is not small enough compared with that for Sc=1, and the value is 0.055.
Although the decay of the spectra seems to be slower as the Schmidt number becomes larger, the spectra of CA′ fall to zero at the large wavenumber space for the all Schmidt number cases.
The spectra profiles also suggest that the grid resolution needs to be fine enough as the Schmidt number increases, especially in the y direction to resolve fine-scale concentration fluctuations, and the present computations resolve the fluctuations appropriately.
Figs.
5(a) and 5(b) show the co-spectra CowA evaluated at z+=15,30 and 75, in both the x and y directions.
These co-spectra are normalized by 〈w′CA′〉 at the individual location, based on the relation shown in Eq.
(18).
These co-spectra, contrary to the profiles of the spectra of the concentration fluctuations, demonstrate that the effect of the Schmidt number is marginal, indicating very similar profiles for the all Schmidt number cases.
The marginal effect of the Schmidt number on the co-spectra profiles suggest that the turbulent gas flux is dominated mainly by the fluid flow structures.
These profiles further indicate that the drop-off of these co-spectra is sufficient in the high wavenumber space k1 and k2 increase, and nonphysical profiles of the co-spectra are not observed in these profiles.
The two-point correlations of CA′ in the x and y are defined by(19a)CAA(x)=〈CA′(x0+x,y0,z)CA′(x0,y0,z)〉(CArms)2(19b)CAA(y)=〈CA′(x0,y0+y,z)CA′(x0,y0,z)〉(CArms)2 where 0⩽x0<Lx, and 0⩽y0<Ly, and CArms is the root-mean-square of CA′.
The two-point correlations are computed from the spectra of CA′ shown in Fig.
5 using the Weiner–Khinchin theorem [37].
Figs.
6(a)–6(d) show the profiles of the two-point correlation of the concentration fluctuations CA′ at z+=15, 30, and 75 in the x and y directions, respectively.
The profiles exhibit that these two-point correlations fall to zero as x and y increase toward the largest separations, x+=2.5πδ+≈1178, and y+=1.25πδ+≈589.
Also, it is found from these plots that the effect of the Schmidt number on the structures of the concentration fields appears in the length scales of about x+<100, and y+<200.
The length scales of x+≈100 and y+≈200 correspond to the wavenumbers k1δ=k2δ≈11.8, and the spectra of CA′ at these wavenumbers are altered drastically by the Schmidt number, as already shown in Figs.
4(a) and 4(b).
The drop-off of the two-point correlation to zero Fig.
6 indicates is satisfactory, and interactions of the concentration fluctuations between two points of the largest separation do not exist.
The sizes of the compartment in the water phase this study uses are considered large enough in both the x and y directions to cover the largest flow structures.
Together with the profiles of the spectra and co-spectra, the current grid points resolve all the essential scales of the concentration fluctuations, while the largest flow structures are not minced by an inappropriateness of the sizes of the computational compartment.
Figs.
7(a)–7(d) show the effect of the chemical reaction rate K on the mean concentration profiles of the gas A in turbulent water for Sc=1, 2, 4, and 8, respectively.
Comparison of the concentration profiles between K=10−∞ and K=10−3 suggests that the effect of the chemical reaction on the gas exchange at the interface is considered extremely small.
On the other hand, the concentrations of A in turbulent bulk water at about z⁎>0.2 for K=101 fall to zero for the all Schmidt number cases, exhibiting extreme difference of the profiles for K=10−∞ and 10−3.
These concentration profiles of A for K=101 suggest that the time scale of the chemical reaction is short enough, compared with that of the turbulent gas exchange by the surface-renewal motions from the interface toward turbulent bulk.
The fast chemical reaction consumes a considerable portion of the gas A during its transport processes within the turbulent boundary layer below the interface, and the concentration of A become very small outside of the interfacial turbulent boundary layer.
We need a quantitative discussion of the time scales of the surface-renewal motions, TS, and that of the chemical reaction, TC, to examine the effect of the chemical reaction on the gas exchange processes across the interface.
The two time scales are defined by the following equations(20a)TS=V/Λ(20b)TC=1/κ where V and Λ are the characteristic velocity and length scales of the surface-renewal eddies [38], respectively.
The details of the estimation of the time scale TS have been discussed in the previous study [24].
The results of the previous statistical analysis demonstrate that the time scale of the surface-renewal motions is evaluated as TS¯≡TSuτ/δ≈0.664.
We understand easily from Eqs.
(20a) and (20b) that the two time scale is approximately the same for K=100, because of TC¯≡1/K=uτ/κδ=1.0.
It is also evident that the time scale of the chemical reaction overwhelmingly faster that of the surface-renewal in the case of K=101 (i.e., TC¯=0.1).
A significantly large part of the gas A is consumed very quickly by the fast chemical reaction of K=101, during its exchange processes very near the interface.
The time scale of the chemical reaction is comparable, or longer than that of the surface-renewal for the other cases of K, the gas A is not consumed so quickly in its exchange process, and the concentration profiles of A is non-zero in turbulent bulk.
Figs.
8(a)–8(e) illustrate an example of the effect of the chemical reaction rate on the instantaneous profiles of A for Sc=4 in the y–z plane, which is perpendicular to the mean flow of water, to show the trend found in Fig.
7 in a more visible manner.
These plots of the concentration profiles cover the domain sizes of W+=589 in width, and H+=150 in height.
These snapshots of the instantaneous concentration profiles are easy to compare with each other, since the computations have been performed using the same fluid velocity profiles.
The result for K=10−∞ is omitted from this figure, since the profile of A is almost identical to the case of K=10−3, in which zero-concentration profile of B is achieved in the water compartment.
The statistical observations discussed above are confirmed qualitatively from the instantaneous profiles of the concentration of A in turbulent water.
The disappearance of A in turbulent water is significant as the chemical reaction rate increases, and the substance A survives only in the region very close to the interface for K=101.
We observe essentially the same instantaneous profiles of A in turbulent water for the other Schmidt number cases.
Fig.
7 also demonstrates that the gas exchange rate at the interface, which is proportional to the nondimensional concentration gradient at the interface as shown in Eq.
(14), increases with the increase the chemical reaction rate for the all Schmidt number case.
Fig.
9 illustrates the relation between the chemical reaction rate K and the nondimensional gas exchange rate, Sh, to clarify the effect of the chemical reaction on the gas exchange rate.
The dashed lines in this figure signify the Sherwood number in the case of K=10−∞.
This graph shows that the effect of the chemical reaction enhances the gas exchange across the interface, as indicated in Fig.
7, and increases the gas exchange rate for Sc=1–8 span by approximately one order by increasing the chemical reaction rate 104 times from K=10−3 to 101.
The relation also indicates that the effect of the chemical reaction in water should be considered exactly for evaluating the gas exchange rate at the interface, especially as the gas is highly reactive in water, since the exchange rate is very sensitive to the chemical reaction rate.
The surface-renewal model for approximating the gas exchange processes at the interface predicts that the gas exchange rate is proportional to the molecular diffusivity of a gas, kL∝(D/TS)1/2 [38].
Nondimensionalizing this equation leads to(21)ShSc−n=α where α is a constant, and n=1/2 has been predicted based on the model [24,38].
This relation is often used to estimate the gas exchange rate of a gas between air and water using a reference value of the exchange rate Sh0 at Sc0, as Sh=Sh0(Sc/Sc0)1/2 [7].
The validation has not been covered in the previous study [24], and it should be performed here to extrapolate the value of the gas exchange rate from Sc∼O(100) toward O(102) accurately, in particular under presence of the chemical reaction.
Fig.
10 illustrates the relation between the Schmidt number and the nondimensional gas exchange rate for the six chemical reaction rates.
The lines in this figure indicate the best-fit results of the present numerical experiments using the relation of Eq.
(21).
Table 1 summarizes the values of α and n for the individual chemical reaction rates obtained by the least-square method.
The same values of n are also indicated in Fig.
10.
Table 1 indicates that the exponent of Eq.
(21) is n=0.472 for K=10−∞, which agrees well with n=1/2 predicted by the model developed by Dankwerts [38].
The exponent predicted by the present numerical study is very close to the results of the DNS study of gas transport at Reτ=300 (Rem≈5090) and 1⩽Sc⩽8 by Kermani et al.
[23].
Wang and Lu [26], and Calmet and Magnaudet [25,27] employed a large-eddy simulation (LES) technique for a turbulent water layer, and have discussed the relation between the gas exchange rate and the molecular diffusivity of a scalar across the interface without considering an aquarium chemical reaction.
Their results exhibit suitability of n≈1/2 at a high Schmidt number up to Sc=200.
Table 1 also shows that the exponent n is very close to 1/2 in the cases of K=100, and 101, in which the time scale of the aquarium chemical reaction is equivalent, or faster than that of the surface-renewal eddies.
A deviation from the surface-renewal model is observed for the gas exchange processes across the interface in the cases of K=10−2 and 10−1, in which the effect of the chemical reaction is not negligible, but the time scale of the chemical reaction is not fast enough to overwhelm that of the surface-renewal eddies.
The results in Fig.
10 show that the surface-renewal model is expected appropriate to extrapolate the effect of the Schmidt number on the gas exchange rate from Sc∼O(100) toward O(102), using Eq.
(21) with n=1/2, in both the limits of the zero and infinite chemical reaction rate.
It is also demonstrated that careful extrapolation of the gas exchange rate using the relation Sh∝Sc1/2 is required in a case of moderate chemical reaction rate.
Here, it is revealed successfully that the present predictions by DNS can extrapolate the Sherwood number to the criteria of the larger Schmidt number and the chemical reaction limits, using the results in Figs.
9 and 10.
The extrapolations of the effect of the Schmidt number to larger values, e.g., Sc>O(102), can be applied by using Eq.
(21) with the values in Table 1.
The exponent n in the cases of the zero and infinite chemical reaction limits is very close to 1/2, and the surface-renewal model provides a good approximation for predicting the gas exchange rate in these cases.
Taking time-space average of the transport equation of the gas A with a decomposition of the concentration and velocities into their mean and fluctuating parts, the following equation is obtained(22)〈w+′CA⁎′〉︸Turb−1ReτSc(∂∂z⁎〈CA⁎〉)︸Vis=−{K∫0z⁎〈CA⁎〉dz⁎+1ReτSc(∂∂z⁎〈CA⁎〉)z⁎=0}︸Total It is obvious from this equation that the total gas flux is constant in the case K=10−∞ [22], and the constant should be kL+≡kL/uτ=Sh/(ReτSc) using the definition of the Sherwood number in Eq.
(14).
The total flux is not constant, and decreases with increasing the distance z by the first term of the right-hand side of Eq.
(22) if the chemical reaction rate is not zero.
Profiles of the turbulent gas flux in Eq.
(22) are important in establishing a simple, and predictive turbulent closures for estimating the gas exchange rate at the interface at the very high Schmidt numbers [39].
Figs.
11(a)–11(d) illustrate the effect of the chemical reaction rate on the profiles of turbulent and viscous contributions of the gas fluxes in the region close to the interface 0⩽z+=zuτ/ν⩽50, for Sc=1–8, respectively.
It is easily observed from these graphs that increasing the chemical reaction rate enhances the viscous contribution, especially in the layer below the interface, z+<1 (or, z⁎<1/150).
Fig.
11 also shows that the enhancement of the turbulent contribution is rather inconspicuous, compared with the enhancement of the viscous contribution.
It is clear that the role of the molecular diffusion on the gas A exchange at the interface is important, especially under presence of the fast chemical reaction.
The profiles of the turbulent and viscous contributions of the gas flux also suggest that the turbulent closures for the gas exchange without the chemical reaction are applicable to the cases of non-zero chemical reaction rates, without drastic modification of their mathematical structures.
One of the reasons for this speculation is that the turbulent gas flux is proportional to z+2 at the interface for the all chemical reaction rate.
This asymptotic behaviour of the turbulent and viscous contributions of the gas flux indicates that the eddy diffusivity is proportional to z+2 [26], since it is defined by(23)Dτ+≡Dτν=−Reτ(〈w′CA′〉∂〈CA〉/∂z) Figs.
12(a)–12(d) depict the profiles of the eddy diffusivity of the gas A against z+ for the Schmidt number cases of Sc=1–8, respectively.
This figure confirms that Dτ+ is proportional to z+2 at the near-interface region z+⩾10 for the all Schmidt number cases.
This figure also indicates that the asymptotic behaviour is not altered by the presence of the chemical reaction.
The same asymptotic behaviour has been reported by Wang and Lu [26] in turbulent water flows with/without thermal stratification at high Prandtl numbers up to Pr=200 using the LES technique.
Also, Suga and Kubo [39] obtain the same asymptotic behaviour of the eddy diffusivity at the interface by their theoretical consideration in a turbulent open-channel flow.
The predictions of the gas exchange rates across the interface based on the turbulent closure developed by Suga and Kubo agree well with the laboratory experiments by Komori et al.
[40,41], and Rashidi et al.
[42] at the high Schmidt numbers up to 103.
It could be speculated from the results of the present numerical study, together with the results in Refs.
[26] and [39], that the essential mechanisms of the gas exchange across the interface is not varied drastically by the presence of the chemical reaction.
Predicting the concentration of the degradation product of B is an important issue on chemodynamics in the environment, especially if it has ecotoxicity, or serious impacts on the ecosystem and human being.
Mechanisms of the chemical reaction processes in water, combined with turbulent gas exchange and mixing near the interface, should be modelled appropriately in environmental transport problems.
The numerical modelling proposed in this study can predict three-dimensional profiles of B in the water compartment, and the effect of the turbulent mixing in water on the production of B is examined based on the numerical data represented in this study.
Figs.
13(a)–13(d) illustrate the effect of the chemical reaction rate on the mean concentration profiles of B for Sc=1–8, respectively.
The profiles of 〈CB⁎〉 for K=10−3 are very small in the entire region, which stay in the order of approximately 10−3 or less for Sc=1, and 10−2 or less for Sc=8.
It is also revealed that the profiles of 〈CB⁎〉 for K=101 are different with those for the other chemical reaction rates.
The profiles of 〈CB⁎〉 for K=101 have clear peak in the near-interface region, z⁎≈0.080 for Sc=1, 0.059 for Sc=2, 0.043 for Sc=4, and 0.030 for Sc=8, however, the other profiles are rather flat in turbulent bulk at 0.1<z⁎<0.9 for the all Schmidt number cases.
The difference is caused by presence of very fast chemical reaction of K=101, whose time scale of the chemical reaction is overwhelmingly shorter than that of the surface-renewal motions, and the effect of the chemical reaction on the production of B is very active in the layer below the interface.
Figs.
14(a)–14(e) illustrate an example of the effect of the chemical reaction rate on the instantaneous profiles of B for Sc=4 in the y–z plane to show the trend found in Fig.
13 visually.
These plots of the concentration profiles cover the domain sizes of W+=589 in width, and H+=150 in height.
These snapshots of the instantaneous concentration profiles are easy to compare with each other and those exhibited in Fig.
8, since the computations have been performed using the same fluid velocity profiles.
The snapshots show that the production of B in turbulent water is activated by the increase of the chemical reaction rate, and confirm the same trend depicted in Fig.
13.
Also, it is clearly observed that the concentration of B in turbulent water is smaller than the equilibrium concentration at the interface, even if the chemical reaction rate is K=101.
We observe high-concentration spots of B in the region close to the interface in the case of K=101 as shown in Fig.
14(e), where the concentration of B is near the equilibrium concentration of A at the interface.
The comparison of these snapshots with Fig.
8 suggests that the gas A transferred inside turbulent water reacts very quickly, producing intermittent high-concentration spots of B in the region close to the interface.
It is interesting to quantify the effects of the Schmidt number and the chemical reaction rate on the bulk-mean concentration of B in water.
The data could present important information on evaluating the environmental impacts of the degradation product of B, as well as acidification of water by the chemical reaction.
Here, the bulk-mean concentration of B is defined by(24)CB⁎¯=∫01〈CB⁎〉(z⁎)dz⁎ Fig.
15 depicts the effect of the Schmidt and the chemical reaction rate on the bulk-mean concentration CB⁎¯.
It is worth to mention here that the bulk-mean concentration of B reaches approximately 0.6 as the chemical reaction rate and the Schmidt number increase to infinite, and the concentration is smaller than the equilibrium concentration of A at the interface.
This figure indicates that progress of the chemical reaction is somewhat interfered by turbulent mixing in water, and the efficiency of the chemical reaction is up to approximately 60%.
The efficiency of the chemical reaction in water will be a function of the Reynolds number of the water flow, and the efficiency could increase as the Reynolds number increases.
We need an extensive investigation on the efficiency of the aquarium chemical reaction in the near future to extend the results of this study further to establish practical modelling for the gas exchange between air and water.
This paper proposed a new numerical modelling to assess the environmental chemodynamics of a gaseous material exchanged between air and turbulent water with an aquarium chemical reaction.
The study employed an extended concept of a two-compartment model, and separated the gas exchange processes into two physicochemical substeps to approximate mechanisms in the gas exchange between the two compartments in a simple manner.
The first was the gas–liquid equilibrium between the air and water phases, A(g)⇌A(aq), and the second a first-order irreversible chemical reaction in turbulent water, A(aq)+H2O→B(aq)+H+.
While zero velocity and uniform concentration of A was considered in the air compartment, a direct numerical simulation (DNS) technique was applied to the non-uniform water compartment to obtain unsteady three-dimensional turbulent velocity and concentration fluctuations in turbulent water.
The gas exchange rate of A, and the bulk-mean concentration of B produced by the chemical reaction were evaluated from the results of the present numerical data, as well as several important turbulence statistics.
The simulations were performed using different Schmidt numbers between 1 and 8, and six degrees of nondimensional chemical reaction rates between 10−∞(≈0) to 101 at a fixed Reynolds number, Reτ=150(Rem≈2300).
This study examined the effects of the Schmidt number and the chemical reaction rate on fundamental mechanisms of the gas exchange processes across the interface.
Major findings through a series of numerical experiments performed in this study are summarized below.
This study examined the effect of the Schmidt number on the turbulence statistics of the concentration fluctuations and the turbulent contribution of the gas flux of A in the case of zero chemical reaction.
The examination showed that fine-scale concentration fluctuations are intensified by increasing the Schmidt number, however, the co-spectra of the turbulent contribution of the gas flux are not altered by the Schmidt number.
The examination also validated soundness of the numerical data of the gas exchange processes obtained by the present numerical experiments, confirming that the sufficiently fine grid resolution is applied in a sufficiently large-scale computational compartment of turbulent water.
The effects of the chemical reaction rate and the Schmidt number on the gas exchange rate of the gas A between air and water exhibited that increasing both K and Sc enhance the gas flux at the interface.
The effect of the chemical reaction rate on the gas exchange rate was monotonic, and its effect alters the exchange rate drastically, increasing by approximately one order by increasing the chemical reaction rate 104 times from K=10−3 to 101 at any Schmidt number this study considered.
It was also observed that the gas exchange rate can be correlated by Eq.
(21), with the exponent of n≈1/2 in the zero and infinite chemical reaction limits.
The correlations between the gas exchange rate and the Schmidt number or the chemical reaction rate this study obtained are expected applicable to extrapolate the exchange rate to outside of the criteria of the two parameters this study examined.
This study considered a decomposition of the gas flux of A into the turbulent and viscous contributions, and a comparison of the two contributions were conducted.
The examination showed that the viscous contribution to the total gas flux of A is strengthened by the chemical reaction in water especially in the region very close to the interface, while the turbulent contribution is not enhanced drastically compared with the viscous contribution.
It is also revealed that the eddy viscosity is proportional to the square of the distance from the interface, and the asymptotic behaviour is not changed drastically by the chemical reaction rate.
This study further examined the effect of the Schmidt number and the chemical reaction rate on the production mechanisms of B by the aquarium chemical reaction.
The bulk-mean concentration of B in water is shown to increase with increasing both Sc and K, however, the bulk-mean concentration is shown to approach toward a saturated value, CB⁎¯≈0.6, in the limits of infinite Sc and K. The data is considered useful to assess the impact of the substance of B if it has ecotoxicity.
Acidification of water by the chemical reaction is also possible to assess by using the data this study presents.
The following is a list of subjects not covered by the present numerical experiments.
(1)This study did not consider the effect of the Reynolds number on the gas exchange mechanisms with an aquarium chemical reaction, since our attention is rather focused on the effect of the Schmidt number and the chemical reaction rate.
We accept the importance of numerical experiments using a different Reynolds numbers to understand fundamental mechanisms of the gas exchange across the interface in a more comprehensive manner.
Also, the Schmidt numbers considered were in the extent of one digit, and the effect on the gas exchange rate of A and the bulk-mean concentration of B are extrapolated.
A numerical study using a DNS technique for a large Schmidt number case, e.g., Sc≈100, is possible to conduct, using finer grid resolution of the order of 10243 by applying a high-performance computing system.
Such a super-large-scale computation should be performed to confirm suitability of Sh∝Scn, especially n≈1/2 for K→0 and K→∞.
While the chemical reaction this study assumed was irreversible, a reversible chemical reaction, A(aq)+H2O⇌B(aq)+H+, should also be considered for developing further applications of the results from the present numerical works.
The gas exchange mechanisms and the chemical reaction processes in turbulent water will be altered if the chemical reaction is reversible, especially in turbulent bulk where the difference between the concentrations of A and B is small.
This topic should also be covered by an extension of this study in the near future.
This study considers a well-developed turbulent water layer, and the effect of the air flow above the interface is ignored.
Many geophysical and environmental flows with the gas–liquid interface are coupled, with very intense interactions with each other.
The effect of a turbulent boundary layer in the air flow cannot be neglected in those cases, and the air–water coupled turbulence with the chemical reactions should also be considered.
This study has been conducted without obtaining financial support from outside of the National Institute of Advanced Industrial Science and Technology (AIST).
The author does not have any conflicts of interest on the topics covered in this report, and this has been drafted as a scientifically neutral publication, without any intentional bias.
The author thanks Professor Clive Langham at the Department of Dentistry, Nihon University for his careful reading of the manuscript and many useful comments for improvement of the manuscript.
Static and dynamic modifications to photon absorption: The effects of surrounding chromophores This Letter investigates the influence, on the molecular absorption of light, of surrounding chromophores.
Two novel rate contributions are identified – one vanishing for a medium with no static dipole moment.
The other, dynamic term is used to model a system of primary absorbers and secondary chromophores distributed in a host medium.
Further modification provides a basis for modelling a case where the medium is, itself, marginally absorptive, thus accounting for optical losses as the input propagates through the surrounding host.
The results facilitate tailoring of secondary chromophore and host effects in the pursuit of materials with specific absorption features.
It is well-known that the optical properties of atoms and molecules can be influenced by their electronic environment.
Local field effects on spontaneous emission rates within nanostructured photonic materials for example are familiar, and have been well summarized [1].
Optical processes, including resonance energy transfer are similarly dependent on the local environment of molecular chromophores [2–4].
Many biological systems are known to contain complex organizations of molecules with absorption bands shifted due to the electronic influence of other, nearby optical centres.
For instance, in widely studied light-harvesting complexes, there are two identifiable forms of the photosynthetic antenna molecule bacteriochlorophyll, with absorption bands centred on 800 and 850nm; it has been shown that the most efficient forms of energy transfer between the two occurs when there is a neighbouring carotenoid species 5–7.
Until now, research on the broader influence of a neighbouring, off-resonant, molecule on photon absorption has mostly centred on the phenomenon of induced circular dichroism, where both quantum electrodynamic (QED) calculations [8–10] and experimental procedures [11–13] predict and verify that a chiral mediator confers the capacity for an achiral acceptor to exhibit circular differential absorption.
In this Letter, we investigate the influence of one or more secondary chromophores, to be labelled M, on the absorption of light by a primary absorber molecule, A.
The secondary species is assumed to have an electronic energy level that is slightly above the input photon energy – i.e.
its optical absorption is blue-shifted compared to the primary absorber – to rule out M as a competing acceptor.
It emerges that there is a dynamic contribution to the absorption rate that can be extended by integrating over all possible positions and orientations of the mediators, thereby modelling a continuous medium in which both absorbers and secondary chromophores are embedded.
Further refinement enables this model to account for a wider range of materials in which, like the biological materials mentioned above, the primary absorbers and the secondary species are distributed within a marginally absorptive host material with its own optical characteristics.
Developing such a theory is shown to provide wider links with both the molecular and bulk properties of materials.
Molecular QED is the analytical tool of choice for analysis of the interactions of light with molecules, and their electromagnetic interactions with each other [10,14].
Quantizing the whole system under consideration, particles and fields alike, this formulation of theory introduces the virtual photon to describe the couplings between particles of matter [15,16].
Where molecules are not in direct contact, all intermolecular interactions must be mediated by virtual photon exchange; ensuring a fully retarded, causal framework.
In such a framework, the non-relativistic Hamiltonian is promoted to operator form and, for a system comprised of interacting molecules, indexed by ξ, is exactly expressible as:(1)H=Hradiation+∑ξHmatter(ξ)+∑ξHinteraction(ξ),where the sum over the discrete index, ξ, denotes the individual optical centres.
Furthermore, the rate, Γfi, of an identified transition process is given by the Fermi ‘Golden Rule’.
For a system proceeding from initial state i to final state f:(2)Γfi=2πℏ-1ρf|Mfi|2where ℏ is the reduced Planck’s constant, ρf is the density of states, and Mfi is the quantum amplitude for the event.
A process consisting of N interactions is described by Nth-order perturbation theory, such that its quantum amplitude Mfi is given by the Nth term of the perturbation expansion [17]:(3)Mfi=〈f|Hint|i〉+∑r〈f|Hint|r〉〈r|Hint|i〉(Ei-Er)+∑r,s〈f|Hint|s〉〈s|Hint|r〉〈r|Hint|i〉(Ei-Er)(Ei-Es)+...
Elementary absorption by individual chromophores generally entails the annihilation of single photons, and is accordingly represented by the first order term in Eq.
(3).
The analysis of optical processes involving two or more coupled centres – electronically distinct in the sense of being separated beyond significant wavefunction overlap – invokes higher order terms; it is these that formally require a QED treatment cast in terms of virtual photon coupling.
Since every discrete molecular transition is a local matter-radiation interaction event, for each exchange of a virtual photon there has to be one photon creation and one corresponding photon annihilation event.
In the following, we first develop in precise QED terms, the mathematical modelling of photon absorption, and then extend this analysis to a medium-modified case.
In every case the initial and final system states are given by:(4)|i〉=|ψ0(A);ψ0(M)〉|n(k,η)〉;(5)|f〉=|ψα(A);ψ0(M)〉|(n-1)(k,η)〉;where ψ designates the wavefunction of either the acceptor, A, or inert mediator, M. Moreover, the subscript of ψ corresponds to either: the electronic ground state 0, or the excited state α (in the case of A).
The radiation is modelled as a number state of wave-vector k and polarization label η, with photon population given by n. Moreover, the photon energy is necessarily such that Eα-E0≡Eα0≈ℏck.
The probability amplitude for the process of photon absorption, modified by the presence of a secondary chromophore is given by the sum of three terms:(6)Mfi=Mfi(A)+Mfi(MA)+Mfi(AM),where Mfi(A) is the amplitude for absorption by the acceptor molecule, A, alone; the second term, Mfi(MA), corresponds to the mediator molecule absorbing a photon and then transferring the energy to the acceptor molecule, and Mfi(AM) denotes the absorption of a photon by A, which then interacts with M. Each of the three possible configurations is represented diagrammatically in Figure 1.
According to the Feynman prescription, the contributions to the matrix element are terms corresponding to all topologically distinct Feynman diagrams, examples of which are displayed in Figure 2 [18].
We determine the rate from the Fermi rule, equation (2), which now depends on the square modulus of Eq.
(6):(7)|Mfi|2=Mfi(A)2︸(1)+Mfi(MA)2︸(2)+Mfi(AM)2︸(3)+2ReMfi(A)Mfi(MA)‾︸(4)+Mfi(A)Mfi(AM)‾︸(5)+Mfi(MA)Mfi(AM)‾︸(6),in which numbering has been introduced so that terms may be tackled individually.
The leading order term is term (1), which corresponds to absorption in the absence of the mediator.
The terms (2) and (3) are obtained from third-order perturbation theory, and are therefore small in comparison to term (1), which, implies that term (6) is also small.
Thus, the first correction terms to the absorption rate are terms (4) and (5).
Firstly, we calculate the leading order term, where no other molecule is involved.
In the electric dipole approximation, the interaction Hamiltonian is given by Hint=-ε0-1μ·d⊥, with the transverse electric field given by:(8)d⊥(r)=i∑k,ηℏckε02V12e(η)(k)a(η)(k)eik·r-e¯(η)(k)a†(η)(k)e-ik·r.where V is the quantization volume, while e(η)(k)and a(η)(k) are the polarization vector and photon annihilation operator respectively for a mode with polarization η and wave-vector k. The right-most term in Eq.
(8) is the Hermitian conjugate of the term on the left, with a†(η)(k)defined as the photon creation operator.
Thus, we have:(9)Mfi(A)=〈f|Hint|i〉=-inℏck2Vε012μ(A)α0·e(η)(k)eik·rA,where rA is the position vector of the acceptor molecule.
We assume the wavefunctions are real.
The square modulus of the above – term (1) from Eq.
(7) – is:(10)Mfi(A)2=nℏck2Vε0μ(A)α0·e(η)(k)2,which, by substituting into the Fermi rule and performing a three-dimensional isotropic rotational average [10,19,20], yields:(11)〈Γ(A)〉=πnckρf3Vε0μ(A)α02.This well-known result [10] is presented for later comparison with the modifications to the rate introduced by secondary (mediator) chromophores.
To begin calculation of the correction terms, we compute Mfi(AM)from third-order perturbation theory.
The third term of Eq.
(3) and a programme of contour integration delivers a contribution to the quantum amplitude as dependent on a molecular interaction tensor:(12)Mfi(AM)=-inℏck2Vε012ej(η)αjkA(α0)Vkl(k,RMA)μlM(00),where RMA is the distance between the absorber and secondary chromophore and Vij(k,RMA) is the general form of the fully retarded dipole–dipole interaction tensor given as [21,22]:(13)Vij(k,R)=eikR4πε0R3(1-ikR)(δij-3RˆiRˆj)-k2R2(δij-RˆiRˆj).
In Eq.
(12), the polarizability tensor for the acceptor model is labelled αijA(α0) which is duly presented in the following general form:(14)αijξ(α0)=∑rμiξ(αr)μjξ(r0)(Eξ(r0)-ℏck)+μjξ(αr)μiξ(r0)(Eξ(rα)+ℏck).
The static interference term of the probability – term (5) of Eq.
(7) – follows as:(15)2ReMfi(A)Mfi(AM)‾=Re-nℏckVε0ei(η)e‾j(ή)μiA(α0)μ‾iM(00)α‾jkA(α0)V¯kl(k,RMA) Depending explicitly on μ¯lM(00), we now assume that the secondary chromophores have no static dipole moment, causing this term to vanish.
To begin calculation of the remaining correction term, we compute Mfi(MA), as before, from third-order perturbation theory, which delivers this term of the quantum amplitude as:(16)Mfi(MA)=-inℏck2Vε012ej(η)αjkM(00)Vkl(k,RMA)μlA(α0),where αijM(00)is the polarizability tensor for the molecule M. Thus, term (4) from Eq.
(7) becomes:(17)2ReMfi(A)Mfi(MA)‾=Re-nℏckVε0ei(η)e¯j(η′)μi(A)α0μ¯lA(α0)α¯jkM(00)V¯kl(k,RMA), It is worth noting that in the multipolar formulation of QED the interaction tensor can be generalized to couplings between electric and magnetic multipoles of any order [23–25].
Therefore, the form of Eq.
(17) can be modified to permit calculation of modifications to absorption in a medium with strong magnetic dipole or electric quadrupole transition moments.
In fact, it is through involvement of the magnetic transition dipole moments that an achiral molecule may display induced circular dichroism in the presence of a neighboring chiral molecule [9].
We now turn attention to the case of a material in which there is a distribution of secondary chromophores, variously located at different distances from, and relative orientations with respect to, each primary absorber.
Represented by Figure 3, it is to be assumed that both species are held within a host matrix that is essentially transparent in the wavelength region of interest, and which can to that extent be represented as a continuous medium characterized by a refractive index nω with a non-zero but essentially negligible imaginary component (the assumption is to be revised in Section 4.1.).
The presence of this host medium introduces a modification to the equations given in the previous section.
First, assuming for simplicity that the group velocity is equal to the phase velocity c/nω, the electric displacement field expansion of Eq.
(8) is adapted to the following form [17,26,27]:(18)d̃⊥(r)=i∑k,ηℏckε02V12nω2+23nωe(η)(k)a(η)(k)eik·r-e¯(η)(k)a†(η)(k)e-ik·r.
The matrix element for absorption independent of M is accordingly also modified as:(19)M̃fi(A)=-inℏck2Vε012nω2+23nωμ(A)α0·e(η)(k)eik·rA, Furthermore, the transfer tensor given in Eq.
(13), which is the form appropriate for species interacting in a vacuum, also now accommodates the effects of the continuous surrounding medium:(20)Ṽij(nωk,RMA)=1nω2nω2+232Vij(nωk,RMA).
Therefore the host-influenced equivalent of Eq.
(16) is duly:(21)M̃fi(MA)=-inℏck2Vε012nω2+23nωej(η)αjkM(00)Ṽkl(nωk,RMA)μlA(α0), To be clear, we are assuming that the surroundings, characterized by nω, are comprised of neither A nor M. Now, the effects of every secondary chromophore in the system must be taken into account.
To effect an analytically tractable calculation, and to avoid unnecessarily complicated results, we assume that the secondary species have random orientations; this justifies performing a three-dimensional isotropic rotational average with respect to the orientation of M. Substituting Eq.
(13) and (20) into (17) and enacting the rotational average, enables the expression to be written as:(22)2ReM̃fi(A)M̃fi(MA)‾=Re-Inω12cπε02∑Me-in¯ωkRMARMA3nω2+23nωn¯ω2+23n¯ω3α¯λλM(00)×(e·μA(α0))[(1+in¯ωkRMA)(e¯·μ¯A(α0))-3e¯·RˆMA(μ¯A(α0)·RˆMA)-n¯ω2k2RMA2{(e¯·μ¯A(α0))-(e¯·RˆMA)(μ¯A(α0)·RˆMA)}],where Greek subscripts denote laboratory-frame co-ordinates.
The pre-factor has been cast in terms of mean irradiance I, where I=nℏc2knωV.
To aid interpretation of the mathematical result, let us assume that the input radiation propagates at an angle γ to the dipole moment of the acceptor molecule.
Working in spherical coordinates with r, ϕ and θ being the radial, azimuthal and polar coordinates respectively, we model a continuous medium by re-expressing the sum over all mediators as the product of the mediator concentration, CM and an integral over all positions of M.(23)Re-InωCM12cπε02∫∫∫e-in¯ωkRMARMA3nω2+23nωn¯ω2+23n¯ω3α¯λλM(00).|μA(α0)|2×cosγ1+(in¯ωkRMA)(cosγ-3cos(θ-γ)cosθ)-n¯ω2k2RMA2(cosγ-cos(θ-γ)cosθ)RMA2sinθ∂θ∂ϕ∂RMAwhere the integration is performed over every point in R3 by a triple integral with standard limits, and RMA2sin(θ)dθdϕdRMA is the volume element [28].
Implementing the θ integral, the R-independent terms and others that are dependent on R−1 all vanish, and further integration over the azimuthal angular coordinate introduces a factor of 2π.
By imposing a minimum distance, Rmin, between the acceptor molecule and the mediator molecules in the continuum we can use the following identity:(24)∫Rmin∞e-in¯ωkRMARMA∂RMA=e-in¯ωkRmin(in¯ωkRmin+1)n¯ω2k2.
Given that the wavenumber is real and the overbar denotes complex conjugation, this identity is valid for Im(nω)>0, i.e.
when the medium has at least a marginal absorption over the wavelength range of concern – which in practice will always be the case.
Before developing this further, we first take the first two terms of a power series expansion of the exponential e-in¯ωkR, such that Eq.
(24) is re-expressed as:(25)∫Rmin∞e-in¯ωkRMARMA∂RMA=Rmin2+1n¯ω2k2.
We then obtain the following medium-induced correction to the absorption rate:(26)2ReInωCM(1+n¯ω2k2Rmin2)cos2γ9cε02nω2+23nωn¯ω2+23n¯ω3α¯λλM(00)|μA(α0)|2.
The zeros of Eq.
(26) can be readily identified as γ=mπ-π/2, where m represents any integer value.
Now, we can take the limit when Rmin tends to zero.
This assumption is readily shown to have minimal impact on the quantitative values delivered by the above expression – based on an optical input where k=107m-1, the value of Rmin would realistically need to exceed 100nm before delivering a 1% additive contribution to the overall result of Eq.
(26).
Finally, by rotational averaging with respect to the propagation angle of the input radiation or, equivalently, the orientation of the absorber molecule A, we obtain:(27)2ReM̃FI(A)M̃FI(MA)‾=Re2InωCM27cε02nω2+23nωn¯ω2+23n¯ω3α¯λλM(00)|μA(α0)|2.
It is now possible to compare the free-field term and the dynamic correction.
It is clear that for the modification of absorption to become significant, the medium requires a polarizability tensor with large diagonal components.
The transition dipole of the acceptor molecule does not affect the ratio of these two terms, since the square modulus of this vector appears in both the free-space expression, Eq.
(10), and the dynamic correction, Eq.
(27).
In the previous section, photon absorption at a primary chromophore has been shown to be modified by the proximity of a secondary chromophore.
Assuming the secondary mediators to be non-polar, the lead correction to the absorption process was expressed as either Eqs.
(17), (27), representing isolated systems or those embedded within a host material, respectively.
The theory in the previous section is consistent with the concept that radiative loss as light propagates through the host medium is minimal, i.e.
the host is essentially transparent for photons of energy ℏω.
By revisiting this assumption, equations of wider application are now to be derived for a system in which the primary and secondary chromophores are held within a host material that exhibits more significant absorptive loss.
The lead matrix element contribution – term 4 in Eq.
(7) – is now developed from Eq.
(23) upon the substitution of nω=nω′-inω″ for all cases of nω.
Here, nω′ is the real part of the refractive index and nω″ is a small but non-zero constant, physically representing a host with low optical density.
Hence, we obtain:(28)2ReIk2CMnω′-inω″nω′+inω″2cos2γ9cε02nω′-inω″2+23nω′-inω″nω′+inω″2+23nω′+inω″3∫Rmin∞e-inω′+inω″kRMARMA∂RMAα¯λλM(00)|μA(α0)|2 In the above expression, separate integrations over θ and ϕ have already been implemented, again delivering a result dependent on R−1, however the integration over R now requires an alternative identity to that employed in Eq.
(24), namely:(29)∫Rmin∞e-i(nω′+inω″)kRMARMA∂RMA=e-i(nω′+inω″)kRmin(i(nω′+inω″)kRmin+1)k2(nω′2+2inω′nω″-nω″2).Again using the first two terms of a power series expansion of the exponential e-i(nω′+inω″)kR, equation (29) can be re-expressed as:(30)∫Rmin∞e-i(nω′+inω″)kRMARMA∂RMA=Rmin2+1k2(nω′2+2inω′nω″-nω″2).
Substituting the right hand side of Eq.
(30) into (28), and performing the rotational average with respect to the propagation angle of the input radiation, the emerging expression is equivalent to:(31)2ReICMnω′2+2337cε02nω′6+4inω′5nω″nω′3nω′2+21+nω′2k2Rmin2+inω′2nω″(5nω′2+2)+nω′2k2Rmin2(7nω′2+6)α¯λλM(00)|μA(α0)|2.Since nω″ is small, all terms that are non-linear in nω″ in the above and any subsequent expression are negligible, and can thus be discarded.
Following a geometric series expansion, Eq.
(31) is re-expressible as:(32)2ReICMnω′2+2337cε02nω′4nω′nω′2+21+nω′2k2Rmin2+inω″nω′2-6+nω′2k2Rmin23nω′2-2α¯λλM(00)|μA(α0)|2, Continuing with the procedure established in the previous section, as Rmin tends to zero, the final expression for the correction to the absorption rate – in an absorptive host medium, follows as:(33)2ReM̃fi(A)M̃fi(MA)‾=2ICM|μA(α0)|2(nω′2+2)337cε02nω′4[nω′(nω′2+2)αλλ′M(00)+nω″(nω′2-6)αλλ″M(00)], The preceding result is expressed as a sum of both real and imaginary contributions, having implemented a re-expression of the molecular polarizability tensor such that α¯λλM(00)=αλλ′M(00)-iαλλ″M(00).
All non-zero, real and imaginary terms in Eq.
(32) have been collated with αλλ′M(00) and αλλ″M(00), which respectively represent the real and imaginary parts of α¯λλM(00).
It is rewarding to note that in the limit where αλλ″M(00) and n″ are zero, Eq.
(33) reduces to the earlier result (27).
Notably, the extent to which the difference is significant depends on a product of terms representing low-level absorption by both the host and the secondary chromophore species M. Lastly, it is interesting to note that while the presented results are analytically tractable, the broadly analogous modification of resonance energy transfer through interaction with a secondary chromophore is not as tractable, requiring the use of numerical methods [29,30].
Using a fully quantized radiation formalism it has been shown that, in a host material containing two or more types of molecular chromophore, the rate of single photon absorption by a primary chromophore is significantly influenced by others that absorb at a shorter wavelength.
Corrections to the absorption rate are predicted – and subsequently characterized – as a result of intermolecular coupling between the primary and secondary centres.
Upon the basis that such coupling is mediated by virtual photons, the lead correction term to the photon absorption rate emerges as a quantum interference contribution, dependent on the molecular polarizability of the molecular mediators.
In considering the more experimentally feasible conditions where such a system of primary and secondary chromophores is incorporated within a host matrix or molecular scaffold, the dependence on the refractive index of the surrounding molecular environment is duly identified.
Finally, the effect of partial absorption by the host itself is accommodated, to give a result of wider applicability.
In application to the most complex types of optical media, determining the optimum criteria for modified optical absorption will require account to be taken of other, possibly competing acceptors; it can be anticipated that the emerging result will exhibit a sensitive dependence on the ratio of acceptors to mediators.
To further extend the analysis, it may also be desirable to model the set of mediators as having some alignment preference.
Such a situation would require use of weighted rotational averaging [31], with explicit calculation of the static correction terms if the chromophores are polar; this may demand the implementation of computational techniques.
The prospect represents scope for future work.
The authors would like to thank EPSRC for funding this research.
Localised distributions and criteria for correctness in complex Langevin dynamics Complex Langevin dynamics can solve the sign problem appearing in numerical simulations of theories with a complex action.
In order to justify the procedure, it is important to understand the properties of the real and positive distribution, which is effectively sampled during the stochastic process.
In the context of a simple model, we study this distribution by solving the Fokker–Planck equation as well as by brute force and relate the results to the recently derived criteria for correctness.
We demonstrate analytically that it is possible that the distribution has support in a strip in the complexified configuration space only, in which case correct results are expected.
Complex Langevin (CL) dynamics  [1,2] provides an approach to circumvent the sign problem in numerical simulations of lattice field theories with a complex Boltzmann weight, since it does not rely on importance sampling.
In recent years a number of stimulating results has been obtained in the context of nonzero chemical potential, in both lower and four-dimensional field theories with a severe sign problem in the thermodynamic limit  [3–8] (for two recent reviews, see e.g.
Refs.
[9,10]).
However, as has been known since shortly after its inception, correct results are not guaranteed  [11–16].
This calls for an improved understanding, relying on the combination of analytical and numerical insight.
In the recent past, the important role played by the properties of the real and positive probability distribution in the complexified configuration space, which is effectively sampled during the Langevin process, has been clarified  [17,18].
An important conclusion was that this distribution should be sufficiently localised in order for CL to yield valid results.
Importantly, this insight has recently also led to promising results in nonabelian gauge theories, with the implementation of SL(N,C) gauge cooling  [8,10].
The distribution in the complexified configuration space is a solution of the Fokker–Planck equation (FPE) associated with the CL process.
However, in contrast to the case of real Langevin dynamics, no generic solutions of this FPE are known (see e.g.
Ref.
[19]).
In fact, even in special cases only a few results are available  [11,20,17,21].
In Refs.
[17,18] this problem was addressed in a constructive manner by deriving a set of criteria for correctness, which have to be satisfied in order for CL to be reliable.
These criteria reflect properties of the distribution and, importantly, can easily be measured numerically during a CL simulation, also in the case of multi-dimensional models and field theories  [6].
A widely used toy model to understand CL is the simple integral (1.1)Z=∫−∞∞dxe−S,S=12σx2+14λx4, where the parameters in the action are complex-valued.
This model had been studied shortly after CL was introduced  [22,11,23], but no complete solution was given.
As we will see below, its structure, with complex σ, is relevant for the relativistic Bose gas at nonzero chemical potential  [4,20].
Recently, a variant of this model (with σ=0 and λ complex) was studied by Duncan and Niedermaier  [21]: in particular they constructed the solution of the FPE, using an expansion in terms of Hermite functions.
They considered the case of “complex noise”, in which both the real and imaginary parts of the complexified variables are subject to stochastic kicks.
Unfortunately, it has been shown in the past that generically complex noise may not be a good idea, since it leads to broad distributions in the imaginary direction and hence incorrect results  [17,18].
This was indeed confirmed in Ref.
[21].
In this paper we aim to combine the insights that can be distilled from the criteria for correctness discussed above with the explicit solution of the FPE, adapting the method employed in Ref.
[21] to the model (1.1).
The paper is organised as follows.
In Section  2 we discuss CL and the criteria for correctness.
To keep the paper sufficiently accessible, we first briefly review how to arrive at the criteria for correctness and subsequently present numerical results, for both real and complex noises.
In Section  3 we study the probability distribution in the complexified configuration space, by solving the FPE directly as well as by a brute-force construction using the CL simulation, again for complex and real noises (the latter was not considered in Ref.
[21]).
In Section  4 we combine our findings concerning the distribution and the criteria for correctness, and provide a complete characterisation of the dynamics.
Section  5 contains the conclusion.
Finally, in order to see whether the structure found numerically can be understood analytically, a perturbative analysis of the FPE is given in the Appendix.
We consider the partition function (1.1).
We take λ real and positive, so that the integral exists, while σ is taken complex.
Analytical results are available: a direct evaluation of the integral yields (2.1)Z=4ξσeξK−14(ξ), where ξ=σ2/(8λ) and Kp(ξ) is the modified Bessel function of the second kind.
Moments 〈xn〉 can be obtained by differentiating with respect to σ.
Odd moments vanish.
The aim is to evaluate expectation values numerically, by solving a CL process.
We start from the Langevin equation, (2.2)ż=−∂zS(z)+η, where the dot denotes differentiating with respect to the Langevin time t and the (Gaussian) noise satisfies (2.3)〈η(t)η(t′)〉=2δ(t−t′).
After complexification, (2.4)z=x+iy,η=ηR+iηI,σ=A+iB, the CL equations read (2.5)ẋ=Kx(x,y)+ηR,ẏ=Ky(x,y)+ηI, with the drift terms (2.6)Kx≡−Re∂zS(z)=−Ax+By−λx(x2−3y2),(2.7)Ky≡−Im∂zS(z)=−Ay−Bx−λy(3x2−y2).
The form of the drift terms is similar as in the Bose gas, after a reduction to a single momentum mode  [20].
The normalisation of the real and imaginary noise components follows from Eq.
(2.3) and is given by 〈ηR(t)ηR(t′)〉=2NRδ(t−t′),〈ηI(t)ηI(t′)〉=2NIδ(t−t′),(2.8)〈ηR(t)ηI(t′)〉=0, with NR−NI=1.
Here NI≥0 is a free parameter, which can be varied.
In principle, expectation values should be independent of the choice of NI, but in practice they are not.
Real noise amounts to NI=0.
Expectation values are obtained by averaging over the noise.
After this averaging, holomorphic observables evolve according to (2.9)〈O〉P(t)=∫dxdyP(x,y;t)O(x+iy), where the distribution P(x,y;t) satisfies the FPE (2.10)Ṗ(x,y;t)=LTP(x,y;t), with the FP operator (2.11)LT=∂x(NR∂x−Kx)+∂y(NI∂y−Ky).
In order to justify the approach, we also consider expectation values with respect to a complex weight ρ(x,t), (2.12)〈O〉ρ(t)=∫dxρ(x,t)O(x), which satisfies its (complex) FPE (2.13)ρ̇(x,t)=L0Tρ(x,t),L0T=∂x[∂x+(∂xS(x))].
This equation has a simple stationary solution, ρ(x)∼e−S(x), which is the desired weight.
The task is now to show that the two expectation values 〈O〉P(t) and 〈O〉ρ(t) are equal, (2.14)〈O〉P(t)=〈O〉ρ(t), at least in the limit of large t, making use of the respective FPEs and the Cauchy–Riemann (CR) equations  [17,18].
Here it is essential that only holomorphic observables are considered, which evolve according to(2.15)∂tO(z,t)=L̃O(z,t), with the Langevin operator (2.16)L̃=[∂z−(∂zS(z))]∂z.
We note that for holomorphic observables, L̃=L, where L is the transpose of LT introduced above.
The equivalence (2.14) can indeed be shown, as discussed in detail in Refs.
[17,18], provided that integration by parts in y is allowed, without the presence of boundary terms at infinity.
This construction involves the products P(x,y;t)O(x+iy) for ‘all’ observables O(x), and hence it puts severe constraints on the decay of the distribution at infinity.
This will indeed be shown to be crucial below.
From now on we consider only the equilibrium distribution P(x,y), assuming that it exists, and hence drop the t dependence.
In the large t limit, the equivalence (2.14) can then be expressed in terms of the criteria for correctness   [17,18](2.17)CO≡〈L̃O(z)〉=0, which in principle need to be satisfied for a complete set of observables O(z).
Here the expectation value is taken with respect to the equilibrium distribution P(x,y), or equivalently, a noise average.
After separating real and imaginary parts, the criteria take the form (2.18)ReL̃O=ReO″+KxReO′−KyImO′,(2.19)ImL̃O=ImO″+KxImO′+KyReO′, where the primes denote differentiation with respect to z.
We consider as observables (2.20)On(z)=1nzn, with n even (the odd powers vanish by symmetry).
The associated consistency conditions, (2.21)Cn≡1n〈L̃zn〉=0, then take the explicit form (2.22)C2=1−〈σz2+λz4〉,(2.23)C4=〈3z2−σz4−λz6〉,(2.24)C6=〈5z4−σz6−λz8〉,⋯ which are of course nothing but the standard Schwinger–Dyson (SD) relations between n-point functions, which should be satisfied in order for the theory to be solved correctly.
We now turn to the numerical solution of the CL process, using the simplest lowest-order discretisation with an adaptive stepsize  [24].
For the results shown here, the total combined Langevin time for each parameter set is 2×106 Langevin time units and the maximal stepsize is 5×10−5.
We have verified that finite stepsize corrections are negligible.
We have studied various combinations of σ and  λ, keeping Reσ=A>0.
Here we focus on σ=1+i and λ=1.
In Fig.
1 CL results are shown for the real and imaginary parts of the observables 1n〈zn〉 and for the criteria for correctness Cn=1n〈L̃zn〉, for n=2,4,6,8.
The figure shows the result for real noise, NI=0: all expectation values agree with the exact result, denoted with the horizontal lines, and the criteria for correctness are all consistent with 0, as it should be.
In Fig.
2 we show how the observables and the criteria for correctness depend on the amount of complex noise.
In the top figures we see that for small NI the observables with n=2,4 appear to be consistent with the exact result, while for larger NI they start to deviate.
Perhaps surprisingly, the lowest-order criterion C2 is consistent with 0 for all NI shown.
This implies that even though 〈z2〉 and 〈z4〉 have converged to the wrong result at larger NI, this occurs in such a way that the condition (2.22), i.e.
the corresponding SD equation, is still satisfied.
The possibility of multiple solutions to the SD equations when solving CL has been observed earlier in Ref.
[13] (see also Refs.
[25,26]).
In order to detect problems, it is necessary to consider higher moments.
In Fig.
2(below), we observe that for small NI the observables (with n≥6) and the criteria (with n≥4) are only marginally consistent with the expected results, while for larger NI they suffer from large fluctuations and can no longer be sensibly determined.
According to the analytical justification  [17,18], this implies that the results from CL cannot be trusted in the presence of complex noise.
Below we give an interpretation of this in terms of the properties of the probability distribution.
For now we tentatively conclude that, if we assume that the large fluctuations reflect the slow decay of the distribution in the imaginary direction, P(x,y) should decay as 1/|y|α, with 5≲α≲7, which will indeed be confirmed below.
A crucial role in the justification of the method is played by the equilibrium distribution P(x,y) in the complexified space.
In Refs.
[17,18] it was shown in detail that for CL to give correct results, it is necessary that the product of the distribution and a suitable basis of observables drops off fast enough in the imaginary direction.
This condition can be translated into the criteria for correctness, as discussed above.
Unfortunately the Fokker–Planck equation, satisfied by the distribution, cannot be solved easily, except in the case of a noninteracting model (λ=0); see the Appendix.
In this section we study the distribution following two approaches.
Firstly, it is possible to collect histograms of the (partially integrated) distribution during the CL evolution.
Note that very long runs are required, in order to sample the configuration space properly.
Here we will in particular be interested in the partially integrated distributions (3.1)Px(x)=∫−∞∞dyP(x,y),Py(y)=∫−∞∞dxP(x,y).
We note that this approach can easily be extended to multi-dimensional integrals and field theories.
We refer to this as the brute force method.
Secondly, for the zero-dimensional model we consider here, it is possible to expand the distribution in terms of a truncated set of basis functions and solve the resulting matrix problem numerically, following Duncan and Niedermaier  [21].
We discuss this approach in the next subsection.
We consider the eigenvalue problem (3.2)−LTPκ(x,y)=κPκ(x,y), where the FP operator LT was given in Eq.
(2.11) and takes the explicit form (3.3)LT=NR∂x2+(Ax−By)∂x+NI∂y2+(Ay+Bx)∂y+2A+λ(x3−3xy2)∂x+λ(3x2y−y3)∂y+6λ(x2−y2).
We denote the eigenvalues of −LT with κ and the eigenfunctions with Pκ(x,y).
If there is a unique ground state P0 with eigenvalue κ=0, and for all other eigenvalues Reκ>0, the time-dependent distribution can be written as (3.4)P(x,y;t)=P0(x,y)+∑κ≠0e−κtPκ(x,y), and the equilibrium distribution is given by P0(x,y).
In the CL simulations we observe convergence to well-defined expectation values (at least for the low moments, n=2,4) and hence we are certain that an equilibrium distribution exists.
In order to solve the eigenvalue problem, we follow closely Ref.
[21].
The FP operator is invariant under x→−x,y→−y, which implies that eigenfunctions have a definite parity, Pκ(x,y)=±Pκ(−x,−y).
The ground state is expected to satisfy P0(x,y)=P0(−x,−y), such that observables of the type 〈(x+iy)n〉, with n odd, vanish.
If Pκ is an eigenfunction of LT with eigenvalue κ, then so is Pκ∗ with eigenvalue κ∗.
It is expected that P0 is real.
In Ref.
[21]P(x,y) was doubly expanded in a basis of Hermite functions, i.e.
(3.5)P(x,y)=∑k=0NH−1∑l=0NH−1cklHk(wx)Hl(wy), where ω is a variational parameter appearing in the harmonic oscillator eigenfunctions, and NH indicates the number of Hermite functions included in the truncated basis.
The coefficients ckl have to be determined.
In order to do so, we introduce creation and annihilation operators, satisfying (3.6)[a,a†]=[b,b†]=1, and write (3.7)x=12ω(a+a†),px=−i∂x=iω2(a†−a),(3.8)y=12ω(b+b†),py=−i∂y=iω2(b†−b).
In terms of these, −LT reads (3.9)−LT=NRpx2+NIpy2−i(Ax−By)px−i(Ay+Bx)py−2A−6λ(x2−y2)+λ4ω[X(x,y)−X(y,x)], with the quartic terms (3.10)X(x,y)=−4iω(x3−3xy2)px,X(y,x)=−4iω(y3−3x2y)py.
Note that X is independent of ω.
Finally, in terms of the creation/annihilation operators, the FP operator reads (3.11)−2ωLT=−NR(a†+a2−2a†a−1)−NI(b†+b2−2b†b−1)+Ā(a†2−a2+b†2−b2+2)+2B̄(b†a−a†b)−4Ā−λ̄[(a†2+a2+2a†a)−(b†2+b2+2b†b)]+λ̄12[X(a,b)−X(b,a)], where (3.12)X(a,b)=(a+a†)3(a†−a)−3(a†+a)(a†−a)(b†+b)2, and we introduced the rescaled parameters, (3.13)Ā=Aω,B̄=Bω,λ̄=6λω2.
In Ref.
[21], where A=B=0, ω was chosen to be proportional to λ, and no adjustable parameters were left on the RHS of Eq.
(3.11).
As we see below, there is a great advantage in keeping ω arbitrary.
We can now compute the matrix elements with respect to the Hermite functions, using the notation (3.14)|mn〉=1m!n!a†mb†n|0〉,a|0〉=b|0〉=0, where (3.15)Hm(ωx)=〈x|m〉,Hn(ωy)=〈y|n〉.
The matrix elements are (3.16)−2ω〈kl|LT|mn〉=[(NR−λ̄)(2m+1)+(NI+λ̄)(2n+1)−2Ā]δk,mδl,n−[(NR+λ̄−Ā)fkmδk,m+2+(NR+λ̄+Ā)fmkδk,m−2]δl,n−[(NI−λ̄−Ā)flnδl,n+2+(NI−λ̄+Ā)fnlδl,n−2]δk,m+2B̄(mlδk,m−1δl,n+1−knδk,m+1δl,n−1)+λ̄12[Xkl,mn−Xlk,nm], with (3.17)Xkl,mn=[fkmδk,m+4+(2m+3−6n)fkmδk,m+2+6(m−n)δk,m−(2m−7−6n)fmkδk,m−2−fmkδk,m−4]δl,n−3[fkmδk,m+2−fmkδk,m−2+δk,m][flnδl,n+2+fnlδl,n−2], and (3.18)fkm=k!m!.
Following Ref.
[21], the double indices k,l and m,n (all taking values from 0 to NH−1) are converted into single ones, via (3.19)i=kNH+l+1,j=mNH+n+1, and the inverse (3.20)ki=(i−1−mod(i−1,NH))/NH,li=mod(i−1,NH),(3.21)mj=(j−1−mod(j−1,NH))/NH,nj=mod(j−1,NH), with i,j=1,…,NH2.
The matrix elements are denoted as LijT=〈kl|LT|mn〉, and the eigenvalue problem is written as (3.22)−LijTvj(κ)=κvi(κ).
We have solved this matrix problem with a FORTRAN90 code using subroutines provided by the LAPACK library  [27].
Since the matrix size is NH2×NH2, there is an upper limit of what is practically feasible.
For the maximal number of Hermite functions we have considered, NH=150, the numerical computation takes around 36 h on a standard work station.
Convergence can be tested by increasing NH and varying ω (see the detailed discussion below).
Considering the eigenvalue at (or closest to) 0, the distribution P0(x,y) can be reconstructed from the corresponding eigenvector, as (3.23)P0(x,y)=∑i=1NH2vi(0)Hki(wx)Hli(wy).
Below we drop the subscript ‘0’.
We start with the case of complex noise.
The parameters in the action are taken as σ=1+i and λ=1, and we consider a basis with 30≤NH≤150 Hermite functions.
The values of ω we used are listed in Table 1.
In the limit of large NH the results are expected to be independent of the value of ω.
In practice however, we find that for finite NH the parameter ω plays the role of a tuning parameter: in particular, when ω is too small, there are eigenvalues with a negative real part.
This becomes more prominent as NI is reduced; see below.
Obviously, in this application this would mean that the FP evolution would not thermalise and display runaway behaviour.
Since the CL evolution thermalises (and is obviously independent of the choice of ω), we expect the real parts of all eigenvalues to be nonnegative.
When the value of ω is increased, we observe that the eigenvalues with a real negative part move into the positive half-plane and the spectrum around the origin converges.
Convergence can also be seen by studying the reconstructed probability distribution P(x,y), using Eq.
(3.23).
Interestingly, we always find an eigenvalue consistent with 0.
When ω is increased even more, convergence properties worsen again.
We find therefore that there is an ω interval for which: 1.there is an eigenvalue consistent with 0; the other eigenvalues are in the right half-plane; the reconstructed ground state distribution is stable under variation of NH and ω.
We first consider NI=1, as in Ref.
[21].
The smallest 15 eigenvalues are shown in Fig.
3(left), for several values of ω.
For the ω values shown here, all eigenvalues are in the right half-plane and the spectrum around the origin is to a good extent independent of ω.
The reconstructed distribution P(x,y), obtained using the eigenvector corresponding to the eigenvalue at (or closest to) the origin, is shown in Fig.
4(top).
We find a smooth distribution with a double peak structure, similar as in Ref.
[21].
Next we reduce the amount of complex noise and consider NI=0.01.
The spectrum is shown in Fig.
3(right) and the reconstructed distribution in Fig.
4(below).
The findings are similar as with NI=1, but ω has to be increased more in order to find convergence and even then the larger eigenvalues are hard to establish.
The distribution has again two peaks, which are now more pronounced and rotated in the xy-plane.
We note the symmetry P(−x,−y)=P(x,y).
Importantly, the distribution is more squeezed in the y direction and the main features are contained in the interval −0.45<y<0.45.
In order to clarify the relevance of these findings, we show in Fig.
5 the partially integrated distributions Px(x) and Py(y), see Eq.
(3.1), on a logarithmic scale, for the case of NI=1.
Besides presenting results for various ω values, we also show the histogram obtained during a CL simulation.
We observe an acceptable agreement between the CL results and the solution of the FPE for ω∼1.5,2, down to a relative size of 10−6, after which the FP solution can no longer cope.
We interpret this as a manifestation of the truncation.
When ω is taken too large, the disagreement occurs for smaller values of x and y.
The distributions do not go to zero rapidly but decay as a power, which is clearly visible on a log–log plot.
In Fig.
6 we show the distributions multiplied by xk and yk respectively, for k=4.8,5, and 5.2, using the CL data.
At large |x| and |y|, we observe a power decay with power 5, i.e.
(3.24)Px(x)∼1|x|5,Py(y)∼1|y|5.
This suggests that the distribution decays as (3.25)P(x,y)∼1(x2+y2)3, which we have verified by studying the decay of (3.26)Pr(r)=∫02πdϕrP(rcosϕ,rsinϕ), which indeed decays as 1/r5.
We note that this power decay is in agreement with the conclusions from the moments above: 〈z2〉 and 〈z4〉 are well-defined and can be numerically determined without any problems, while the higher moments diverge, which in the CL simulation is reflected in large fluctuations.
We now turn to the case where CL appears to work well, i.e.
with real noise (NI=0).
The eigenvalues are shown in Fig.
7 for a number of ω values.
For ω<4 eigenvalues with negative real part are present (not shown in the figure).
We note that in all cases there is an eigenvalue at (or close to) the origin, but in general convergence is much harder to establish from a study of the eigenvalues alone.
In order to have a handle on this we also analyse the partially integrated distributions Px and Py under variation of NH and ω, and also compare those with the histograms obtained with CL.
The results are shown in Fig.
8 for Py(y) (top) and Px(x) (bottom).
In the case of Py, convergence as NH is increased is clearly visible (top, left).
We note that for the largest NH values the distribution agrees with the result obtained by direct Langevin simulation, indicated with the black line.
The distribution is very well localised and appears to drop to 0 around y=0.28.
We come back to this below.
Convergence as ω is increased is demonstrated in Fig.
8(top, right) and we observe that a large value of ω is required, ω∼50.
It is of course expected that the chosen value of ω eventually becomes irrelevant, but for finite NH keeping ω as a tuning parameter is essential.
The distribution Px(x) is shown in Fig.
8(below) as a function of x (left) and x4 (right), on a logarithmic scale.
In contrast to the case of complex noise, we now find an exponential rather than a power decay.
Results from the FPE agree with the CL histogram, independently of the value of ω in this case, but only down to a relative size of 10−4; varying ω does not help in this case (increasing NH probably will).
From the CL result, we see that the distribution falls off as (3.27)Px(x)∼e−ax4,a∼0.295.
Naively this behaviour can be expected, since for large |x| the original weight behaves as ∼exp(−λx4/4).
We note that the prefactor is 0.295, which is slightly larger than λ/4=0.25.
Interestingly this seems to be understandable from a perturbative analysis; see the Appendix.
The reconstructed distribution is shown in Fig.
9.
This distribution has similar characteristics as at NI=0.01, except that the two peaks are now very pronounced and the saddle around the origin is much deeper.
The peaks lie mostly in the y direction and they are therefore clearly visible in Py(y).
The distribution is squeezed even more than before and its main support is in the region −0.3<y<0.3.
The ripples visible for larger y values are an artefact of the truncation.
In fact, in the next section we will demonstrate that the distribution is strictly 0 when |y|>0.3029.
We conclude that for this choice of parameters (σ=1+i and λ=1) the decay in the case of real noise is manifestly different compared to complex noise.
In the latter we found a power decay, resulting in ill-defined moments 〈zn〉 when n>4, while here we find exponential decay in the x direction and, as we will see below, in the y direction support only inside a strip.
As a result there is no problem in computing higher moments, since they are all well-defined.
From the solution of the FPE and the CL process, we conclude tentatively that for real noise the distribution is localised in the y direction and has support in a strip around the origin only, with −0.3≲y≲0.3.
This conclusion can be made more precise by studying the classical flow diagram and properties of the FPE.
This analysis can also be used to find parameter values for which CL breaks down for real noise (see Section  4.3).
The classical flow diagram is shown in Fig.
10, for σ=1+i and λ=1.
We show the direction of the classical force by an arrow pointing in the direction (Kx(x,y),Ky(x,y)).
The arrows are normalised to have the same length.
The classical force is of course independent of NI.
There are three fixed points, where Kx=Ky=0: an attractive point at the origin and two repulsive fixed points, determined by σ+λz2=0, or (4.1)x2−y2=−A2,xy=−B2λ, yielding (x,y)=(±0.455,∓1.10) in this case.
The flow is directed towards the origin, provided that |y| is not too large.
This can be made more precise by studying where Ky(x,y) changes sign.
We find that Ky(x,y)=0 at (4.2)yp(x)=2(B3λ+x2)12cos(α+pπ3),p=1,3,5, where (4.3)α=−arctan(2λAx[(B3λ+x2)3−(Ax2λ)2]12)+πΘ(x), with Θ(x) the step function.
These lines are indicated in the classical flow diagram with full lines.
For the parameter values we consider here, the upper and lower curves have extrema at x=±0.1749, y=∓0.9530, while the curve in the centre has its extrema at x=±0.5502, y=∓0.3029.
We now realise that along the horizontal dashed lines, which are determined by the extrema of the centre curve where Ky=0 (y=±0.3029 in this case), the flow is always pointing inwards, i.e.
towards the real axis.
In the absence of a noise component in the vertical direction, this creates a barrier for the Langevin evolution beyond which it cannot drift.
Note that the repulsive fixed points actually help to establish this.
Hence, provided that the process starts within this strip, it will never be able to leave (in the case of real noise and in the limit of zero stepsize).
We have verified that if the dynamics starts out outside of the strip, it quickly finds its way into it, due to the mostly restoring properties of the classical flow.
We conclude therefore that in the case of real noise the process takes place in the strip determined by (4.4)−0.3029<y<0.3029.
This is consistent with the conclusions drawn above from the histograms and the FPE solution of the distribution P(x,y).
In the presence of complex noise, this conclusion no longer holds and the entire xy-plane can be explored.
It is possible to make the argument based on classical flow presented above rigorous and show directly from the FPE that the equilibrium distribution P(x,y) is strictly zero in strips in the xy-plane, assuming sufficient decay, i.e.
(4.5)Kx,y(x,y)P(x,y)→0 as x and/or y→±∞.
To achieve this, we note that the FPE takes the form of a conservation law, i.e., (4.6)Ṗ(x,y;t)=∂xJx(x,y;t)+∂yJy(x,y;t), with (4.7)Jx=(NR∂x−Kx)P,Jy=(NI∂y−Ky)P, which allows us to consider the charge, (4.8)Q(y,t)=∫−∞∞dxJy(x,y;t).
Specialising now to the equilibrium distribution (and hence dropping the t dependence), we find that Q(y) is independent of y, provided that the product of the drift Kx(x,y) and the distribution P(x,y) drops to zero at large |x|, since (4.9)∂yQ(y)=∫−∞∞dx∂yJy(x,y)=−∫−∞∞dx∂xJx(x,y)=−Jx(x,y)|x=−∞∞=0.
We note that the required condition is always satisfied in our case, even in the case of the power decay.
Since Q(y) vanishes as y→±∞ (because Jy(x,y) does, again relying on the sufficient decay), we find that (4.10)Q(y)=∫−∞∞dx(NI∂y−Ky(x,y))P(x,y)=0.
For real noise, this yields therefore the condition (4.11)Q(y)=∫−∞∞dxKy(x,y)P(x,y)=0, for all y.
Since P(x,y) is nonnegative, this condition allows us to derive the following useful property: if Ky(x,y) has a definite sign as a function of x for given y, P(x,y) has to vanish for this y value.
As a function of x, Ky(x,y) is a parabola with an extremum at (4.12)x0=−B6λy and a curvature of 6λy.
The value at the extremum is given by (4.13)F(y)≡Ky(x0,y)=−λy[(y2−A2λ)2−3A2−B212λ2].
Consider now the case that y is positive (negative).
In that case, when F(y)>0(F(y)<0), Ky(x,y) is strictly positive (negative) and hence P(x,y) has to vanish.
The zeros of F(y) are given by (4.14)y±2=A2λ(1±1−B23A2), provided that 3A2−B2>0.
Inspection shows that F(y)>0 when y−<y<y+ and F(y)<0 when −y+<y<−y−: hence for these y values, P(x,y)=0.
When 3A2−B2<0, F(y) has no zeros and F(y) and y have opposite signs.
In that case, Ky(x,y) has no definite sign and the reasoning cannot be followed.
To summarise, we find the following: 1.when 3A2>B2, P(x,y)=0 when y−2<y2<y+2, as illustrated in Fig.
11; when B2>3A2, there are no restrictions on P(x,y).
In the first case the distribution can in principle be nonzero in the outer region, y2>y+2.
However, once the process is in the inner strip determined by y2<y−2, it will not be able to leave this strip, due to the nature of the drift terms.
Hence there is no objection to putting the distribution to zero also when y2>y+2.
We conclude therefore that the equilibrium distribution has support in the strip determined by y2<y−2 only, in agreement with the reasoning above.
Note that P(x,y) is therefore a nonanalytic function of y.
Of course the value of y− agrees with the boundary determined in the example in the previous section, i.e.
with the position of the dashed lines in Fig.
10, as it should be.
For vanishing B, the action is real and the distribution is (for real noise) strictly localised on the real axis, y=0.
For small B, the width of the allowed region around y=0 is nonzero and set by (4.15)y−2∼B212λA.
Hence increasing the amount of complexity by increasing B results in a broadening of the distribution with a width ∼2B.
The importance of this controlled increase has been emphasised earlier in Ref.
[28].
The argument presented above breaks down in the presence of complex noise.
In that case, the process is pushed out in the y direction and the repulsive fixed points come into play.
Once the repulsive fixed point is crossed, large excursions in the y direction take place and the distribution is no longer localised.
When the amount of complex noise is small, it takes time to notice this, but eventually it will happen.
There are therefore no strips for complex noise, which also follows from the formal derivation above.
This is demonstrated in Fig.
12(left), where Py(y) is shown for the values of NI considered above.
As shown above, this leads to power decay, Py(y)∼1/|y|5.
Interestingly, the derivation above demonstrates that strips are only present when 3A2>B2.
For larger B values, one may therefore expect a breakdown of CL with real noise, similar as with complex noise.
This is indeed what happens.
The distribution Py(y) as B is increased is shown in Fig.
12(right), for real noise.
Note the similarity with the figure on the left.
The delocalisation has a detrimental effect on the results of the CL process.
This is demonstrated in Fig.
13, where the moments minus the exact result are shown on the left and the criteria for correctness on the right.
We observe that increasing B has a similar effect as increasing NI; cf.
Fig.
2.
The distributions for the case that σ=1+3i and λ=1 are shown in Fig.
14.
The top figure shows P(x,y), obtained with the FPE.
We note that the distribution still appears to be mostly contained within a strip.
However, a closer look at the partially integrated distributions obtained with CL, see Fig.
14(bottom), shows that again power decay is present, with the same power as before.
This power decay sets in once the process has crossed the repulsive fixed points, which for this choice of parameters are located at x=±1.04 and y=∓1.44.
The weight of the power tails is clearly small, yet it is enough to give rise to fluctuations for the higher moments when solving the CL process.
We conclude that in the absence of strips a universal power law decay is present, which results in a breakdown of the formal justification  [17,18] and wrong or wildly fluctuating results in practice.
Finally we will show that it is possible to understand the universal decay directly from the FPE.
We start from the assumption that the distribution is of the form (4.16)P(x,y)=c(x2+y2)α at large x and y, where we found numerically that the power α is consistent with 3.
Substituting this Ansatz in the FPE (2.10), we find, after some algebra and the removal of common factors, that (4.17)αx2−y2+2α(NRx2+NIy2)(x2+y2)2+A(1−α)+λ(3−α)(x2−y2)=0.
At large x and/or y the final term dominates: requiring that this term vanishes yields indeed α=3.
This construction assumes that the behaviour at large distance is approximately rotationally invariant in the xy-plane and that there are no preferred directions, which would invalidate the Ansatz and the power counting above.
Based on our numerical evidence, this seems to be the case.
We note that the final term in Eq.
(4.17) is independent of σ=A+iB and NI; hence the decay at large distance is independent of the parameters in the action and of the amount of complex noise.
We also note that B has disappeared from Eq.
(4.17): the reason is that B breaks the invariance under x→−x and independently y→−y, while the Ansatz is invariant under those.
The conclusion is therefore that the decay at large x and y is universal.
Of course the presence of complex noise and/or a large value of B2>3A2 is essential in catalysing large excursions, which lead to the power decay.
Notably, the power decay appears to be unavoidable unless its appearance is strictly forbidden, as in the case of the strips for real noise and B2<3A2.
In order to justify the results obtained with complex Langevin dynamics, it is necessary that the probability distribution is sufficiently localised in the complexified configuration space.
Here we have studied the properties of this distribution via a number of methods, in the case of a simple model.
Using the insights gathered from classical flow, histograms obtained during the CL process, the criteria for correctness and the explicit solution of the FPE, a complete characterisation of the distribution can be given.
In the case of real noise and provided that B2<3A2, where σ=A+iB, we found that the distribution is strictly localised, i.e.
it has support in a strip in the configuration space only, with exponential decay in the real direction.
In this case all moments are well-defined and, relying on the analytical proof of the method, correct results are expected.
We also found that the criteria for correctness are satisfied.
In contrast, when the noise is complex or when B2>3A2, the entire configuration space is explored.
Large excursions are possible due to the presence of repulsive fixed points and the decay of the distribution changes dramatically.
We found strong indications that for large |x| and |y|, the distribution decays as a power, according to(5.1)P(x,y)∼1(x2+y2)3.
A consequence of this slow decay is that higher moments are no longer well-defined.
As a result, these and the criteria for correctness suffer from large fluctuations during the CL process, an important signal of failure.
Here it is important to emphasise that the inclusion of higher moments is essential to observe the breakdown.
In this model the FPE can be solved explicitly, via an expansion in a truncated set of basis functions.
However, it is still a nontrivial problem and perhaps the best way to find the distribution is by brute force, i.e.
during the CL simulation.
This also has the benefit of being applicable to higher dimensional models.
In the case of the localised distribution in the strip, the used basis set may not be the one that is best adapted to the problem and, in hindsight, once it has been demonstrated that the distribution has support in a strip only, a more suitable basis can be used.
This would however limit the generality of the approach.
As an outlook, we note that in the more realistic cases of multi-dimensional models and field theories, the luxury of solving the FPE is typically not available.
However, we have demonstrated that the essential insight can already be obtained from a combination of histograms of partially integrated distributions and the criteria for correctness, which gives a consistent picture of the dynamics.
These tools are readily available in field theory.
Finally, our conclusions are also immediately applicable to nonabelian SU(N) gauge theories, for which gauge cooling provides a means to control the distribution in SL(N,C), a possibility not present in simpler models.
We thank Denes Sexty and Ion-Olimpiu Stamatescu for discussion.
This work is supported by STFC, the Wolfson Foundation and the Royal Society.
In order to understand the numerical solution for the distribution P(x,y) found above further, we discuss in this appendix the perturbative solution of the FP equation (2.10) in the stationary limit.
Although it is only of limited use, it provides some insight, especially along the x axis.
We write the FP operator (2.11) as (A.1)LT=L0T+λL1T, with (A.2)L0T=NR∂x2+(Ax−By)∂x+NI∂y2+(Ay+Bx)∂y+2A, and (A.3)L1T=(x3−3xy2)∂x+(3x2y−y3)∂y+6(x2−y2).
The (normalisable) solution of the lowest-order equation, (A.4)L0TP(0)=0, is given by (A.5)P(0)(x,y)=N0exp[−αx2−βy2−2γxy], with (A.6)α=AD[(NR+NI)(A2+B2)−A2],(A.7)β=AD[(NR+NI)(A2+B2)+A2],(A.8)γ=A2BD, where (A.9)D=(NR+NI)2(A2+B2)−A2, and N0 is the normalisation constant, (A.10)1N0=∫dxdye−αx2−βy2−2γxy=παβ−γ2.
This solution is similar to the one found in the relativistic Bose gas at nonzero chemical potential  [20].
It is easy to see that it is the correct solution at leading order, by computing (recall that NR−NI=1 and σ=A+iB) (A.11)〈(x+iy)2〉P=∫dxdyP(0)(x,y)(x+iy)2=1σ.
More generally, one may equate the two expectation values (A.12)〈O(x)〉ρ=∫dxρ(x)O(x),(A.13)〈O(x+iy)〉P=∫dxdyP(x,y)O(x+iy), which, assuming that it is possible to shift x→x−iy, yields the relation  [29,30](A.14)ρ(x)=∫dyP(x−iy,y), where the LHS should be independent of NR,I.
Evaluating the y integral yields in this case (A.15)ρ(0)(x)=N0′e−S(x),S(x)=12σx2, with (A.16)N0′=σ2π, which is indeed the expected answer.
To compute higher-order corrections, we expand (A.17)P(x,y)=∑k=0∞λkP(k)(x,y).
Higher-order corrections are determined by the inhomogeneous partial differential equation, (A.18)L0TP(k)+L1TP(k−1)=0.
The homogeneous equation is solved by P(0).
To find the particular solution, we factor out the leading order solution, (A.19)P(k)=P(0)p(k), (with p(0)=1), and write (A.20)L0TP(k)=P(0)L0′Tp(k),L1TP(k)=P(0)L1′Tp(k), with (A.21)L0′T=NR[∂x−4(αx+γy)]∂x+(Ax−By)∂x+NI[∂y−4(βy+γx)]∂y+(Ay+Bx)∂y,(A.22)L1′T=(x3−3xy2)(−2αx−2γy+∂x)+(3yx2−y3)(−2βy−2γx+∂y)+6(x2−y2).
Higher-order corrections are then determined by (A.23)L0′Tp(k)=−L1′Tp(k−1).
For the first-order correction, this yields (A.24)L0′Tp(1)=2αx4+8γx3y−6(α−β)x2y2−8γxy3−2βy4−6(x2−y2).
The RHS of Eq.
(A.24) is a fourth-order polynomial with only even powers.
As a particular solution we may therefore attempt a polynomial of fourth degree, with only even terms appearing and containing 8 unknown coefficients, (A.25)p(1)(x,y)=c40x4+c31x3y+c22x2y2+c13xy3+c04y4+c20x2+c11xy+c02y2.
Inserting this Ansatz in Eq.
(A.24) yields a set of linear equations for the coefficients which can be solved.
Since the expressions become rather unwieldy, we give here the results for real noise only, since this is the case of interest.
For real noise (NR=1,NI=0), the parameters in the lowest-order solution (A.5) simplify, and (A.26)α=A,β=A(1+2A2B2),γ=A2B.
The coefficients of the first-order correction (A.25) are given by (A.27)c20=0,c02=12A(2A2−B2)B2(4A2+B2),(A.28)c11=−6(A2+B2)B(4A2+B2),c22=−9A2(4A2−B2)B2(4A2+B2),(A.29)c40=−3A22(4A2+B2),c04=−A2(36A2−5B2)2B4,(A.30)c31=−2A(5A2−B2)B(4A2+B2),c13=−2A(36A4−7A2B2−B4)B3(4A2+B2).
Hence, to first order, the (normalised) distribution is given by (A.31)P(x,y)=N1P(0)(x,y)[1+λp(1)(x,y)], with (A.32)1N1=1−(7A4+3A2B2+2B4)2(A2+B2)2(4A2+B2)λ.
This distribution satisfies the FP equation to order O(λ).
It can be checked that it yields the correct moments to this order, e.g.
(A.33)〈(x+iy)2〉P=∫dxdyP(x,y)(x+iy)2=1σ−3λσ3+O(λ2).
One may also verify that evaluating (A.34)ρ(x)=∫dyP(x−iy,y) yields in this case (A.35)ρ(x)=N1′e−12σx2(1−λ4x4)+O(λ2), with (A.36)N′=σ2π(1−3λ4σ2), as it should be.
It is clear that the perturbative distribution is not positive definite and strictly speaking only applies when the perturbative correction λp(1)(x,y) is small with respect to 1, i.e.
around the origin.
However, it can be made positive definite by a simple exponentiation, (A.37)P(x,y)=P(0)(x,y)exp[λp(1)(x,y)], which has the same leading order λ dependence.
This distribution is normalisable since the coefficients of the quartic terms are all negative.
An example is shown in Fig.
A.15.
We observe a double peak structure, as in the main text.
At large y values, the exponentiated construction cannot be correct, since it decays exponentially rather than be 0 outside the strip found above.
In the x direction, however, the perturbative solution gives a surprisingly good description of the decay.
Taking y=0, we find (A.38)P(x,0)∼exp(−Ax2+c40λx4), where, for A=B=1, c40=−3/10.
This result is compared with the solution of the FP equation in Fig.
A.16(left), and is seen to agree better than expected.
We note that the prefactor 0.3 is also close to what was observed for the integrated distribution Px(x).
In Fig.
A.16(right), we also show a comparison with the perturbative expression (A.39)P(0,y)∼exp[−(β−c02λ)y2+c04λy4], where β=3, c02=12/5 and c04=−31/2 (again for A=B=1).
Even though c02 is positive, it is not large enough to change the curvature.
Note that the oscillations visible in the solution of the FPE are due to the finite number of basis functions (NH=150).
Molecular structure-property correlations from optical nonlinearity and thermal-relaxation dynamics We apply ultrafast single beam Z-scan technique to measure saturation absorption coefficients and nonlinear-refraction coefficients of primary alcohols at 1560nm.
The nonlinear effects result from vibronic transitions and cubic nonlinear-refraction.
To measure the pure total third-order nonlinear susceptibility, we removed thermal effects with a frequency optimized optical-chopper.
Our measurements of thermal-relaxation dynamics of alcohols, from 1560nm thermal lens pump and 780nm probe experiments revealed faster and slower thermal-relaxation timescales, respectively, from conduction and convection.
The faster timescale accurately predicts thermal-diffusivity, which decreases linearly with alcohol chain-lengths since thermal-relaxation is slower in heavier molecules.
The relation between thermal-diffusivity and alcohol chain-length confirms structure-property relationship.
Infrared lasers are a powerful tool for investigating molecular vibrations.
In this Letter, we use pulsed IR lasers to study the nonlinear optical response in primary alcohols.
Our results show a correlation between the structure of the alcohol and their nonlinear optical coefficients.
Typically, optical nonlinearities are measured at very dilute concentrations, e.g., at ∼10−4M, under the assumption that the non-interacting solvents essentially do not contribute to the effective nonlinearity of the solute.
However, structural modifications of organic solvents can have profound effects on nonlinear properties.
An earlier example has been the sensitive acoustic experiments with primary alcohols [1,2] that measured the pressure–density nonlinearity [3,4] as the acoustic characteristics of the media, changed with the chain-length of the alcohols [1].
In our nonlinear measurements with femtosecond lasers, as well, the enhanced sensitivity when the thermal effects are managed has lead us to the useful correlation of molecular structure with the nonlinear properties of the primary alcohols.
There is considerable interest in the study of two-photon absorption coefficients [5,6], saturation absorption coefficients (β) [7–9], and nonlinear index of refraction (n2) in materials at 1560nm because of their potential applications in optoelectronics, photonics devices and material science.
We study β and n2 for a homologous series of primary alcohols as our model system.
This model system enables us to establish a structure-property relationship by monotonically increasing the chain-length of the alcohols and measuring their corresponding β and n2.
Several studies on optical nonlinearity in materials through Z-scan technique [6,10] have been reported.
Wavelength dependence of third-order susceptibilities have also been explored both theoretically [11] and experimentally [12].
Previous study of optical nonlinearity variations in alcohol molecules differing in size and structure were reported at 780nm wavelength [13] where the extremely small two-photon cross-sections essentially reflected changes in n2.
However, as we establish here, studies with high repetition-rate laser experiments without chopper had thermal corruption.
We use open and close aperture Z-scan experiments, in analogy to the saturation absorption work discussed earlier in water [8], to respectively measure the β and n2 for a series of primary alcohols with the help of 1560nm femtosecond laser pulses, however, with the important inclusion of an optical-chopper.
The vibrational combination states of the alcohols are coupled by the femtosecond laser pulses at 1560nm.
These couplings result in the absorption of 1560nm and the excited molecules undergo relaxation through non-radiative processes, which gives rise to transient thermal effects.
These transient thermal effects are related to the pure optical nonlinearity of the samples and can be measured as a change in their n2 values [14].
The transient thermal effects of individual pulses accumulate in case of high repetition-rate lasers to produce a cumulative thermal effect at longer timescales.
We measure this cumulative thermal effect with the mode-mismatched two-color pump–probe experiment.
We remove the cumulative thermal effect for high repetition-rate laser experiments with the help of an optical-chopper to recover pure optical nonlinearity.
When we use an optical-chopper, its off-time window provides sufficient time for thermal-relaxation, and we acquire all our measurements immediately with the chopper opening [15].
However, thermal-relaxation variation for different samples and their correlation with their molecular structures is yet to be attempted.
So, we explore this correlation by the thermal lens (TL) spectroscopy, which is highly attractive, nondestructive, and noninvasive.
In many cases, TL spectroscopy is the most sensitive optical technique for measuring the thermal properties of solids and liquids [16,17].
Thermal lens can be treated as an ideal thin concave lens of a particular focal length that causes distortion of light passing through it and this simple model has been studied in detail [18–20].
Many authors have used pump–probe experiments and two-color Z-scan techniques to measure TL in different samples [8,21–23].
In our specific experiments, we use two collinear femtosecond laser pulses at 1560nm as the pump and at 780nm as the probe, to explore the evolution of thermal-relaxation timescales in different samples.
The choice of pump–probe beam diameters is important for such experiments.
For example, while modeling the aberrant nature of TL through pump–probe scheme under steady state condition, Shen et al.
[24,25] used a higher probe beam-diameter to include larger area information, since thermal excitation perturbs not only the thermal nature within the excitation beam radius but also outside it.
However, the thermal-relaxation dynamics from this model would wrongly show that different points in the sample experience different thermal-relaxation rates due to the existence of different thermal gradients in their vicinity.
In order to circumvent such difficulties, we use smaller probe beam-diameter in our experiments, so that it is totally contained within the thermally excited region in the sample.
Our probe beam radius, in particular, was chosen to be half the size of that of the pump beam.
The variation in the timescales has also been used to get information about molecular structure-property correlation.
Our experimental setup (Fig.
1) involves a mode-locked femtosecond Er:doped fiber laser (Femtolite, IMRA Inc.), which generates pulses centered at fundamental wavelength 1560nm and its second harmonic 780nm collinearly as a single beam with 50MHz repetition-rate.
The 1560nm pulse is ∼300fs wide and has an average power of 40mW while the 780nm pulse is 90fs wide and has an average power of 20mW.
A collinear Mach–Zhendar interferometer is built with the help of two dichroic beam splitters: one separates the two wavelengths and the other recombines them.
For the single beam third-order optical nonlinear measurements at 1560nm, we block the 780nm arm in the interferometer and allow only the 1560nm pulses whose beam-diameter is 5mm.
We used 15mW of 1560nm laser power, which resulted in the maximum peak-power of 7.97×108W/cm2 at the focal point in the samples.
After passing through the sample, the transmitted beam is focused with another focusing lens into the Peltier cooled InGaAs photodiode (Acton Research), and the corresponding signal is measured with a 500MHz oscilloscope (LeCroy LT354M) interfaced to computer with a National Instruments GPIB card.
We measure β and n2 of alcohols from standard open and closed aperture Z-scans respectively.
We take the samples in 1mm thick quartz cuvette.
A motorized translation stage (model ESP-300), which can step with a minimum resolution of 0.0554078μm, moves the sample across the focal point of the first lens and data acquisition is performed using LabView programming.
HPLC-grade alcohols were obtained from Sigma–Aldrich and were used without further purification.
The requisite purity of these samples was confirmed through their individual UV–vis spectra.
The Z-scan transmittance for open aperture (saturation absorption) and closed aperture (nonlinear-refraction) are plotted using Matlab (Fig.
2a and b).
For measurements of the thermal-relaxation process in liquids, 1560nm and 780nm beams are simultaneously focused into the sample cell after passing through the collinear interferometer (Fig.
1).
The 1560nm beam, which acts as the thermal pump in our experiment, is chopped with an optical-chopper having 50% duty cycle.
A silicon photo-detector (Thorlab: DET210) is used to detect the 780nm probe beam through an 80% closed aperture to determine the thermal-relaxation.
The chopping of the 1560nm beam provides on–off modulation to the thermal pump.
Different chopper frequencies provide the correspondingly different thermal ‘on’ and ‘off’ times to the sample.
We measure the response to this thermal effect with the 780nm probe beam as the chopper frequencies are varied from 3Hz to 350Hz.
In the third-order optical nonlinearity measurements at 1560nm, we measure the normalized transmittance versus the sample position for both closed and open aperture cases and determine the values of n2 and β, respectively.
We have used the phenomenological model [6] for fitting our data though we are aware of the other molecular state fitting models [26,27] that are also possible.
Since we focus on the trends of nonlinear coefficients rather than the molecular energy state mechanisms, the phenomenological fits that we use are sufficiently reliable and are widely accepted in literature.
The β values are obtained by curve fitting the observed open aperture transmittance, T(z), which is measured as a function of sample position with respect to the focal point as per the following Eq.
(6):(1)T(z,S=1)=∑m=0∞[-q0(z,0)]m(m+1)32where,(2)q0(z,t)=βI0(t)Leff(1+z2z02) Here T(z, S=1) is the normalized transmittance, such that, S=1 denotes the open aperture case, β is the nonlinear absorption coefficient, I0(t) is the peak on-axis irradiance at the focus, Leff=(1−eαL)/α is used for the induced third-order nonlinearity, L is thickness of the cuvette, α is linear absorption coefficient and z0 is the Rayleigh range=πω0/λ.
The materials that exhibit absorption saturation show a decrease in their absorption coefficient when measured using high laser intensity.
The absorption coefficient α depends on the intensity I of the incident laser as:(3)α=α01+IISwhere α0 is the low-intensity absorption coefficient, and IS is a parameter known as the saturation intensity.
In the saturation absorption process, the third-order optical response is due to the population change induced by single photon absorption.
The resulting modified population interacts with the field and thus, the overall process results in no net photon absorption.
On the other hand, the n2 is determined from close aperture Z-scan traces using the following fitting Eq.
(6):(4)T(z,Δφ0)=1+4Δφ0x(x2+9)(x2+1)where, x=z/z0, Δφ0(t)=kΔn0(t)Leff, k=wavevector=2π/λ, λ is wavelength, Δn0(t)=n2I0(t), n2 is the coefficient of nonlinear-refraction in m2/W.
The β and n2 are respectively related to the imaginary and real parts of the third-order susceptibility as follows [6]:(5)Im(χ(3))=n0ε0c2βω(6)Re(χ(3))=2n02ε0cn2where n0, as before, is the linear refractive index, ε0 is the permittivity of space, c is the velocity and ω is the angular frequency of light.
The absolute value of χ(3), i.e.
|χ(3)|, for all the samples was calculated from the following relationship:(7)|χ(3)|=[(Re(χ(3)))2+(Im(χ(3)))2]1/2 Subsequent to such high repetition-rate femtosecond laser pulse excitation, the molecules undergo de-excitation through non-radiative pathways, which often induces long transient thermal lensing in the samples.
Measure of nonlinear optical parameters for the sample is thus corrupted by thermal effects.
To measure pure optical nonlinear parameters, the thermal effect should be removed.
We noticed in our experiments that the thermal effect can be effectively probed through close aperture Z-scan measurements.
Furthermore, we show that it is easier to remove thermal effect in open aperture Z-scan as compared to the close aperture experiments with the help of an optical-chopper.
We systematically explore the removal of thermal effect by using methanol as a representative case.
We used an optical-chopper and measured the peak-power of 1560nm in both the open and close aperture situations as the chopper-frequency was gradually changed from 5Hz to 500Hz with 50% duty cycle.
This corresponded to an on–off-time range of 100–1ms.
The use of the chopper does not change or modulate the repetition-rate of the laser and the number of pulses striking the sample per second remains same.
However, the number of pulses that actually strike the sample in a row before being turned off reduces by a factor of 100 (i.e., changes from 5×106pulses to 5×104pulses) as our chopper-frequency increases from 5Hz to 500Hz.
In addition, as the chopper-frequency increases, the chopper opening rise-time correspondingly reduces and quickly approaches the limit where it is remarkably smaller than the thermal-relaxation time of the sample, and since we only measure the maximum height of the 1560nm signal, the better the rise-time of the chopper opening, the better is the removal of the thermal effect.
For the open aperture experiments with 1560nm laser, we positioned our samples at the lens focus, which also corresponded to the minimum position (dip) of the Z-scan trace and changed the frequency of the chopper.
The chopper-frequency modulated waveforms were directly recorded from the oscilloscope and the maximum in the InGaAs photodiode voltage was monitored for each chopper-frequency.
These maxima values were then plotted with respect to the individual chopper frequencies (Fig.
3a).
In the closed aperture case, two situations arose (Fig.
3b).
In one case, we kept our sample at the minimum position (dip) while in the other case we kept our sample at the maximum position (peak) of the close aperture Z-scan traces.
We observed the variation in the maximum of the photodiode voltage with chopper-frequency in both cases, which resulted in the two plots in Fig.
3b.
From this study, we find that the peak-power becomes almost constant for open aperture case after 250Hz, indicating the removal of thermal effect, though it is not as obvious in the close aperture case.
However, the close aperture case also has important embedded information, which is revealed when we plot the peak to dip separation (ΔTPV) with respect to individual chopper frequencies as in Fig.
3c.
In fact, the maximum values measured at the peak and dip positions of the close aperture Z-scan (Fig.
3b) for different chopper frequencies behave oppositely at low chopper frequencies to effectively minimize thermal effect.
Since ΔTPV is proportional to the n2, it essentially reduces with increasing chopper-frequency to reach the value of pure n2 as the thermal effect becomes reduced at higher chopper frequencies.
The minimum position in Fig.
3c is also important, which we discuss later.
As discussed above for single beam experiments at 1560nm wavelength, all alcohols in the series show non-resonant saturation absorption and cubic nonlinear-refraction.
The saturation effect is expected due to the vibronic transition at this particular wavelength.
In both the cases, we get negative values of β and n2.
In Fig.
4a we show that with the increase of chain-length, i.e., with the increase in the number of carbon atoms in homologous primary alcohols, there is a gradual linear increase in n2, whereas the variation of the β values shows a parabolic increase with the maximum value at hexanol.
We also monitored the variation in the absolute value of total third-order susceptibility, i.e., |χ(3)| with increasing number of carbon atoms in the alcohol series (Fig.
4b), which shows that total |χ(3)| decreases linearly as the chain-length increases along the alcohol series.
Thermal excitation of samples induces cylindrical equi-temperature surfaces in the samples, which have the same axis as that of the excitation laser beam.
The innermost temperature surface has the highest temperature resulting in the lowest refractive index and vice versa, which in turn generates a graded index waveguide like condition in the sample that manages to misguide the beam from its path.
As a consequence, for a two-beam pump–probe experiment, a hole is burnt at the centre of the probe beam in its spatial profile after passing through a thermally excited medium.
The radius of the burnt hole depends on the temperature gradient of the probed region.
When the chopper blocks the pump beam, thermal-relaxation takes place and the temperature gradient in the probed region decreases, which is directly proportional to the thermal-relaxation.
This results in a reduction in the radius of hole in the probe beam.
Thus, a measurement of the increase in the probe beam power (in mV) through the close aperture Z-scan gives an effective estimate of this thermal-relaxation process.
In order to measure thermal-relaxation for alcohols, we use 1560nm and 780nm as the pump and the probe beams, respectively.
In presence of 1560nm pump, we perform close aperture Z-scans for each sample and detect only the probe beam (780nm) transmittance by a silicon photo-detector.
We keep the sample at the dip (minimum) position of the close aperture Z-scan traces and measure the variation of 780nm probe power with the modulation of the chopper-frequency in the pump beam.
In all our measurements, we take an average of at least 500 data points, so that, the standard deviation is within 1%.
We plot the variation of the 780nm probe power with respect to different chopper-frequency for every sample.
Next, we fit them with exponential decay models, which enable us to determine the timescales associated with their respective relaxation.
We find that, for all the samples, the probe power through the close aperture decreases bi-exponentially with increase in the chopper-frequency, implying that the thermal-relaxation is a bi-exponential process involving two timescales.
For such non-radiative thermal-relaxation processes, bi-exponential decay means that it cannot be explained with a pure conduction process only.
So, we attribute the faster decay time, τ1, to the thermal conduction process and the slower decay time, τ2, to thermal convection.
Both τ1 and τ2 were found to have an overall increasing trend that can be correlated with the variation of molecular structures of the alcohols along the series.
An oscillating behavior is also embedded in this overall increase, more drastic in case τ2, which we find has a correlation to the density variation across the alcohol series (Fig.
5a).
Thermal conduction timescale (τ1) can also be expressed as follows [28–33]:(8)τ1=(ρC/κ)·ωe2(9)orτ1=ωe2/Dwhere C, ρ and κ are the specific heat, density and thermal conductivity of the samples, respectively.
D is the sample’s thermal-diffusivity and ωe is the pump beam radius in the sample.
In our case, the beam waist of 1560nm at the focal point of the lens is found to be 27.7μm.
From the above equations (Eqs.
(8) and (9)), we calculate the thermal diffusivities for each of the alcohols using the time constants we determined experimentally.
The diffusivity values for all the alcohols are reported in Table 1.
The trend in the thermal-diffusivity values are shown in Fig.
5b, which shows that thermal-diffusivity values decrease gradually as the chain-length increases along the alcohol series.
As the chain-length increases, the size of the molecule becomes larger resulting in slower relaxation timescale for the higher members of the alcohols.
We find that the values of the thermal-diffusivity that we determined for the alcohol series are in agreement with the previously reported values for methanol and ethanol [34].
Finally, we would also like to point out that, for the case of methanol, the minimum position of ∼125Hz in Fig.
3c corresponds to the thermal conduction timescale of ∼8ms in Fig.
5a, which, in fact, provides us an alternate route to measure thermal conduction timescales from simple single beam Z-scan measurements.
Besides methanol, this could be generalized to other samples as well.
Removal of thermal effect as was discussed earlier (∼250Hz for methanol), therefore, represents a minimum chopper-frequency which is roughly twice the thermal-relaxation time of the sample concerned.
We have shown how molecular structures can be correlated to their optical nonlinear response by calculating their total third-order optical nonlinear susceptibility from a respective measure of β and n2 values of a series of primary alcohols with standard single beam open and close aperture Z-scan experiments.
All nonlinear measurements were performed using 1560nm femtosecond light pulses.
The molecular structure-property correlation shows that the β values follow a parabolic increase, whereas the n2 values increase almost linearly as the primary alcohol chain-length increases.
In contrast, |χ(3)| decreases linearly as the alcohol chain becomes longer along the series.
All these sensitive nonlinear studies have become possible as we managed to minimize the thermal effect with the use of an optical-chopper with optimized frequency.
Our investigations with thermal lens pump–probe technique resulted in measurements of thermal-relaxation timescales and thermal-diffusivity of the homologous primary alcohol series.
Two thermal-relaxation timescales were observed experimentally for such non-radiative process: a faster component, τ1 corresponding to thermal conduction and a slower, τ2, representing thermal convection, both of which show an increasing trend for longer chain alcohols.
We calculate thermal-diffusivity values from the experimental τ1 values, which, in general, decrease linearly with chain-length of the alcohols as expected, since thermal-relaxation is sluggish in heavier molecules.
The diffusivity values agree with the previously reported theoretical and experimental values in literature.
We also show an alternate measure of thermal conduction timescales from simple single beam Z-scan measurements instead of the mode-mismatched two-color pump–probe Z-scan experiments.
Our work, thus, provides the generalization that a minimum chopper-frequency of roughly twice the thermal-relaxation time of the sample concerned is sufficient for the removal of thermal effect.
D.G.
thanks the funding support of the Ministry of Communication and Information Technology, Govt.
of India, the Swarnajayanti Fellowship scheme under the Dept.
of Science and Technology, Govt.
of India, and the Wellcome Trust Senior Research Fellowship program (UK).
Thermal treatment of simulant plutonium contaminated materials from the Sellafield site by vitrification in a blast-furnace slag Four waste simulants, representative of Plutonium Contaminated Materials (PCMs) at the Sellafield site, were vitrified through additions of Ground Granulated Blast-furnace Slag (GGBS).
Ce (as a Pu surrogate) was effectively partitioned into the slag product, enriched in an amorphous CaO–Fe2O3–Al2O3–SiO2 phase when other crystalline phases were also present.
Ce L3 edge XANES data demonstrated Ce to be present as trivalent species in the slag fraction, irrespective of the waste type.
Estimated volume reductions of ca.
80–95% were demonstrated, against a baseline of uncompacted 200L PCM waste drums.
The dissolution behaviour of PCM slag wasteforms was investigated at 50°C in saturated Ca(OH)2 solution under N2 atmosphere, to simulate the hyperalkaline anoxic environment of a cementitious UK Geological Disposal Facility for Intermediate Level Waste (ILW).
These experiments demonstrated the performance of the slag wasteforms to be comparable to that of other vitrified ILW materials considered potentially suitable for geological disposal.
A significant proportion of the UK higher activity radioactive waste inventory comprises Plutonium Contaminated Material (PCM), the untreated packaged volume of such wastes is projected to be in excess of 20,000m3 on the Sellafield site alone [1].
PCM wastes are generally packaged in PVC bags and stored in 200L drums.
The current treatment option for these wastes is super-compaction of the 200L drums, with the resulting pucks stacked in 500L containers and encapsulated with a cement grout.
This approach is effective for the bulk of PCM waste arisings, however, there exists a significant quantity of non-compactable waste which requires additional treatment.
Ideally, the selected treatment method for non-compactable PCM wastes should seek to immobilise long-lived Pu isotopes, which are radiotoxic and potentially mobile in the subsurface, within a passively safe material prior to interim storage and disposal [2].
There is growing interest in the use of thermal processes, such as vitrification, to treat PCM materials and other Intermediate Level Wastes (ILW1The term ILW is used in the UK to describe those higher activity wastes (4GBq α-activity or 12 GBq β,γ-activity, per tonne) that are not sufficiently heat generating for this to be taken into account in the design of storage or disposal facilities.1) which are incompatible with cement encapsulation [3–12].
Key drivers for the application of thermal treatment processes include the reduced volume, improved passive safety, and superior long term stability, of the vitrified wasteform products.
These benefits are derived from oxidation of the metallic waste fraction, destruction of organic components, and evaporation of entrained water, combined with simultaneous immobilisation of radioactive and chemotoxic elements within a glass or slag (i.e.
a partially crystallised glass) material; a separate metal phase may also be produced, which may assist in scavenging less electropositive (and potentially volatile) metals, such as Tc and Ru, if present.
The objective is generally to achieve a product of low porosity, with the desired partitioning of radioactive and chemotoxic elements between glass and ceramic phases, and a homogeneous distribution of radionuclide host phases over the volume of the product wasteform.
A review of the development and application of thermal treatment technologies to higher activity wastes in the UK, including potential technology platforms, was recently published by Hyatt and James [12].
In this contribution we demonstrate that simulant UK PCM wastes can be effectively treated by vitrification using additions of ground granulated blast-furnace slag (itself a waste product of steel manufacture).
Four distinct PCM waste simulants were used in this study: PVC waste; metallic waste; masonry waste; and mixed waste (comprising all three aforementioned components).
These four simulants represent bounding cases for PCM wastes arising on the Sellafield site.
PCM wastes are contained in 200L sealed mild steel drums, each weighing 20kg and packed with 25–80kg of PCM [2], see Fig.
1.
The desired approach is to treat each drum in its entirety, using a common methodology, without repackaging of the content.
Therefore, all waste simulants contain a significant mild steel component representative of the containment drum.
The compositions of the four PCM waste simulants are given in Table 1, the compositions of the individual waste stream components are given in Table 2.
The upper PuO2 content of the PCM wastes is estimated to be 230g Pu per 80kg drum, equivalent to 0.32wt% PuO2 [2].
Since this study was concerned with demonstration of the proposed approach, CeO2 was used as a surrogate for PuO2, on grounds of safety, cost and expediency.
We acknowledge that such a simulant cannot fully reproduce the physical and chemical behaviour of another element.
However, Ce is amongst the most useful and representative inactive Pu simulants, albeit with limitations including different redox potentials [13,14].
Nevertheless, the solubility of trivalent Ce and Pu was found to be broadly similar in (boro)silicate glass compositions at temperature >1400°C, relevant to the process conditions of this investigation [14].
The upper estimated PuO2 content of PCM wastes is the molar equivalent of 0.207wt% CeO2.
In the present study, we elected to use ca.
five times the amount of CeO2 surrogate, allowing for both conservatism in the approach (effectively representing the highest conceivable PuO2 content) and to assist detection and quantification by analytical methods (particularly XRF and SEM/EDS).
Thus, all melts were doped with 1.043wt% CeO2 as a PuO2 surrogate.
From consideration of the four PCM waste types in Table 1, and the objective of treating all waste types using a single methodology, it was considered that the high metal content required a mixed metal and oxide slag wasteform, since complete oxidation of the reactive metal waste fraction was unrealistic in the absence of an oxidising gas sparge.
The presence of an oxide slag phase during processing was considered essential, in order to accommodate Pu/Ce which are largely insoluble in steels [15–17].
To maximise volume reduction, and assist partitioning of Pu/Ce into the slag fraction, both metal and slag phases were required to be fully molten during processing.
The temperature-limiting factor in this requirement was the need to ensure that all steel components were fully molten during processing.
The Fe–C phase diagram, the Fe–Cr–C phase diagram, and modified Ni-bearing versions thereof, all show that the liquidus temperature, Tliq, varies from ca.
1548°C for pure Fe to ca.
1150°C at 4.3wt% C, increasing with further increase in C content [18–21].
The effects of Cr and Ni upon these upper and lower values of Tliq are generally small by comparison.
Therefore, we selected 1560°C as the target melting temperature, which was indeed sufficient to melt the steel components.
In the case of PVC and metal wastes, no oxide or “slagging” materials were themselves available in the waste materials to immobilise the Pu/Ce surrogate inventory.
Since a primary requirement was that all waste types should be treated using the same methodology, it was necessary to define a suitable oxide “slagging” additive and the proportion of additive to waste feed.
The molten slag and metal fractions are required to be in equilibrium during the vitrification process, therefore, we selected Ground Granulated Blast-furnace Slag (GGBS) as the additive.
GGBS is essentially a CaO–Al2O3–SiO2 glass, produced as waste product from steel making [22,23].
GGBS is a low cost and commercially-available material, typically >99% amorphous by volume.
Glasses in the CaO–Al2O3–SiO2 system, have capacity for substantial incorporation of other elements present in the PCM wastes, particularly FeOx (up to ca.
40wt%) and PuO2 (up to ca.
20wt%), and are known to exhibit high chemical durability [24–26].
Furthermore, GGBS could be expected to be compatible with the aluminosilicate and calcium aluminosilicate building wastes present in the masonry and mixed PCM waste types, allowing all four waste types to be treated with the same additive.
Initial scoping experiments confirmed the suitability of using GGBS as an additive, with a 1:1 ratio of waste to additive by weight.
In an effort to minimise the amount of additive used, and hence maximise waste loading potential, a 3:1 ratio of waste to additive was used for the research reported here.
The metallic PCM waste components, given in Tables 1 and 2, suggest that this fraction of the vitrified PCM wastes should consist predominantly of a carbon steel, except in the case of metal waste feed, for which a high Ni Cr steel incorporating trace Al, Pb and Cu could be expected.
No additions to improve performance or capability of the metal wasteform fraction were considered necessary (for example, additions of Zr in an effort to incorporate Pu in intermetallics as noted by Keiser and Abraham [16,17]), since the aim was to effectively partition Pu/Ce into the slag fraction.
This could potentially allow the metallic fraction to be disposed of as LLW, licenced waste, or even reused, thus drastically reducing the volume of higher activity waste requiring storage or disposal, leading to substantial life cycle cost savings.
It should be noted, however, that in operando tapping of liquid slag and metal, to achieve separation of these components, has not yet been routinely demonstrated using currently available thermal treatment technologies.
Post process separation of slag and metal components would not be desirable, since the primary aim of the thermal treatment process is to produce a packaged product suitable for interim storage and disposal.
The four bounding PCM wastes, given in Table 1, were simulated using the most appropriate materials and geometries.
“Mock up” PCM drums were assembled using the following components: PCM drums were simulated using mild steel paint cans and lids (Fenton Packaging Ltd.); PVC bags were replicated using identical PVC sheeting (Romar Workwear Ltd.); the metallic waste was simulated using commercial grade 18/8 stainless steel, aluminium and copper (Avus Metals & Plastics Ltd.), and lead shot (Aldrich); the inorganic waste was simulated using waste Pyrex labware, crushed masonry, concrete and window glass; CeO2 (from Acros Organics, >99.9%; dried 15h at 600°C) was used as a PuO2 surrogate.
Commercially available ground, granulated blast-furnace slag “Calumite” was used as an additive [27].
The analysed chemical composition is given in Table 3.
Calumite is a powdered material, with a typical particle size distribution between limits of ca.
40 to ca.
400μm.
PCM waste simulants and additives were weighed according to the formulations shown in Table 4 (weight fractions of composite items are given in Table 2).
As-prepared simulants are shown in Fig.
2.
Can lids were pierced to allow PVC combustion and gas egress early in the thermal cycle, thereby avoiding potentially dangerous explosive releases.
Waste cans were crushed slightly at the can base to allow good ingress into the containment crucible (Fig.
2e).
The required quantity of GGBS was added down the sides and on top of each sealed PCM simulant can as shown in Fig.
2.
Samples were melted using a waste to GGBS additive ratio of 3:1 in alumina crucibles.
However, initial scoping studies indicated the metal waste feed to be incompatible with alumina crucibles, due to prohibitive corrosion, and “Salamander” Plumbago crucibles, formed from a clay/graphite composite, were preferred (supplied by Morgan Ceramics).
Crucibles, containing simulant waste and additive, were placed in an electric muffle furnace and heated overnight at 2°C/min to 1100°C for 8h; crucibles were subsequently transferred to a gas-fired furnace, which had been preheated to 1100±25°C, then ramped to 1560±10°C over a period of 90min.
The preheat step was necessitated by the requirement to avoid thermal shock of the alumina crucibles.
Temperature measurements were verified using an optical pyrometer throughout the melting procedure.
Crucibles were held at 1560±10°C for a total of 2.5h, then removed from the furnace and allowed to cool in air.
During the transfer of the preheated metal waste crucible, from the electric muffle furnace to the gas-fired furnace, it was noted that a small but significant amount of the slag phase had “foamed over” the top of the crucible likely due to partial oxidation of the clay/graphite crucible component (see Section 3.1).
The density of the each slag fraction was determined using He gas pycnometry on powdered materials in the size fraction between mesh sizes 100 (0.149mm) and 200 (0.074mm).
An AccuPyc 1340 II pycnometer was used with 200 purges of the chamber, 50 cycles, an equilibration rate of 0.005psimin−1 at 25°C in a 1cm3 chamber and a fill pressure of 12.5psi.
The estimated precision was ±0.01gcm−3.
The density of each metal fraction was determined using the Archimedes method in deionised water.
Three separate measurements were averaged in each case.
The estimated precision was ±0.02gcm−3.
Chemical analyses were performed by X-ray fluorescence (XRF) spectroscopy and Inductively Coupled Plasma Optical Emission Spectroscopy (ICP–OES); the chloride content in slag and metal products was determined by a standard addition method using samples dissolved in a mixture of hot HNO3 and HBF4.
ICP–OES analysis of Ce content at 100–1000ppm was associated with a relative uncertainty of 5% based on analysis of uniform reference materials.
At 10–100ppm of Ce, the relative uncertainty increased to 20%, and below 10ppm the relative uncertainty was 30%.
Errors associated with XRF analyses are given in the relevant tables.
Scanning electron microscopy (SEM) with energy-dispersive X-ray spectroscopy (EDS) and Secondary Electron Imaging (SEI) and/or Backscattered Electron Imaging (BEI) was carried out primarily with a JEOL JSM6400 SEM and Oxford Instruments EDS.
Specimens were prepared by mounting representative samples in epoxy resin, curing, grinding using successive grit SiC papers to 1200 grit, and polishing using diamond paste to 1μm grade.
Specimens were carbon coated prior to SEM analysis.
X-ray diffraction was performed on powdered (<75μm) samples of glass and pieces of metal, using a Siemens D5000 goniometer with Co K radiation and a diffracted beam monochromator.
Analyses of the resulting diffraction patterns were performed using the WinXPoW software package.
Dissolution experiments were based on a modification of ASTM C1285-02 (2008) Product Consistency Test (PCT) Test Method B [28], for a full description of the methodology the reader is referred to the work of Utton et al.
[11].
The slag wasteform fractions were prepared by grinding in a Tema mill, the fraction between mesh sizes 100 (0.149mm) and 200 (0.074mm) was collected, washed in ASTM Type I water and isopropanol (using an ultrasonic bath), and dried overnight at 90°C.
The test was carried out in triplicate for each sample and a blank (in duplicate) was carried throughout the procedure.
Experiments were carried out under dry nitrogen and all solutions were made in de-aerated ASTM Type I water at 50°C, to avoid carbonation and precipitation of calcium carbonate.
The wasteform samples were weighed into the HDPE vessels and saturated calcium hydroxide solution was added (to simulate the hyperalkaline conditions of a UK cementitious Geological Disposal Facility (GDF)).
The ratio of slag wasteform surface area to solution volume (SA/V) was 1200m−1.
The mean particle diameter was assumed to be 112μm [28], and the number of particles was calculated from pycnometry density measurements.
In an effort to maintain saturation of the calcium hydroxide solution, an ultrafiltration unit filled with calcium hydroxide slurry, was suspended in the solution.
The vessels were sealed and placed in an oven at 50°C.
The solutions were sampled after 3, 7, 14, 21 and 28days and the vessels were returned to the oven after sampling.
Aliquots were withdrawn from each vessel and passed through a 0.2μm syringe filter to remove particulates.
The solutions were then acidified with concentrated nitric acid (10μl per ml).
The total amount of solution removed was less than 10% of the original volume.
The leachate was analysed by ICP Mass Spectroscopy, except for Na and K which were determined by Atomic Emission Spectroscopy.
Ce L3 edge XAS data from all slag fractions, plus CeO2, and CePO4 (with the monazite structure) were acquired on beamline X23A2 of the National Synchrotron Light Source (NSLS), Brookhaven National Laboratory (BNL), USA.
This beamline is configured with a piezo-feedback stabilised Si (311) upwards reflecting monochromator and single bounce harmonic rejection mirror.
Data for CeO2 and CePO4 were acquired in transmission mode using finely ground specimens dispersed in BN to achieve a thickness of one absorption length.
For the slag fraction specimens, data were acquired in fluorescence mode.
Incident and transmitted beam intensities were measured using ionization chambers operated in a stable region of their I/V curve filled with mixtures of He and Ar or N2.
Fluorescence emission was detected using a four channel Si-drift detector, with appropriate dead time correction as described previously [29].
Data reduction and analysis was performed using the program Athena [30].
The observed melting behaviour generally showed that the last items to melt were, as expected, the steel components.
No violent reactions were observed between the waste simulants and glass additive.
However, the alumina and clay/graphite crucibles suffered obvious corrosion during melting.
In the case of samples melted in alumina crucibles, this resulted in the slag fraction of the wasteform having a content of Al2O3 greater than the nominal composition (see Section 3.2).
In the case of samples melted in the clay/graphite crucibles, both slag and metal fractions were found to contain carbon derived from crucible corrosion (see Section 3.4).
In addition, the clay/graphite crucibles exhibited significant weight loss due to oxidation at high temperature.
Sample appearances varied considerably as a function of the PCM waste type, as shown in Fig.
3.
A substantial metallic wasteform fraction resulted for the vitrified metal waste type, whereas only a small metal component was obtained in the mixed waste type.
In contrast, the vitrified PVC and masonry waste types appeared to be composed only of slag material with no appreciable fraction of metallic component.
The slag derived from the metal waste feed exhibited little obvious texturing or visible crystallinity.
In contrast, the slag fraction derived from the masonry waste consisted of an upper glassy layer and a lower crystalline layer, and the slag fractions derived from the PVC and mixed wastes were, respectively, partially and strongly crystalline throughout.
The XRF determined compositions of major elements in the slag and metallic wasteform fractions are reported in Tables 5 and 6, respectively, together with Ce determined by ICP–OES.
Ce was determined to be effectively partitioned to the slag fraction, as expected.
The overriding difference in slag fraction composition is the Fe2O3 content of the final oxide fraction of the wasteform; this is apparently dominated by the type of crucible used and therefore by the degree of oxidation available to the (waste plus additive) mixtures during the thermal treatment process.
The higher Fe2O3 content in the slag fraction of the PVC waste, compared to that derived from masonry and mixed waste types, is a consequence of the proportionately higher mild steel component.
The slag resulting from the metal waste type, melted in a clay/graphite crucible, has a very low Fe2O3 content consistent with partitioning of Fe into the substantial metal fraction under reducing conditions.
Chemical analysis determined no measurable retention of Cl within the slag fraction of the wasteforms.
It was therefore concluded that all Cl present in the PVC was volatilised by the high temperature treatment.
Analyses of the metallic fractions were consistent with the composition of the wastes from which they originated.
Table 7 reports the measured densities of the slag and (where formed) metallic fractions of the PCM wasteforms.
The densities of the measured metallic fractions are consistent with the compositions of steels and high iron alloys [31].
The slag fraction was found to be composed of a CaO–Fe2O3–Al2O3–SiO2 glass, plus crystalline Ca2(Mg2Fe4)(Al4Si2)O20 (dorrite), (Mg,Fe)(Cr,Fe,Al)2O4 (spinel), and at least one other unidentified phase; see Figs.
4 and 5.
EDS analyses showed Ce (measured as Ce2O3) to be concentrated primarily in the glass phase (4.04wt%) compared to the crystalline phases (0.43wt%).
The slag fraction of this sample was found to be largely composed of a CaO–Fe2O3–Al2O3–SiO2 glass, with small inclusions of graphite (resulting from crucible corrosion) and Fe (with trace Ni and Cr); see Figs.
6 and 7 (note: the volume fraction of the metallic inclusions was below the detection limit of X-ray diffraction).
SEM and EDS mapping showed the metallic fraction to be composed of a major Fe bearing phase containing Si and Ni, a minor Fe and Cr bearing phase, and trace amounts of and C (derived from crucible corrosion), see Fig.
8.
EDS analysis demonstrated Ce to be concentrated exclusively within the glass phase, within detection limits.
Powder XRD and SEM/EDS demonstrated the slag fraction to be composed of a CaO–Fe2O3–Al2O3–SiO2 glass and a crystalline (Mg,Fe)(Cr,Fe,Al)2O4 (spinel) phase; see Figs.
9 and 10.
The XRD reflections of the spinel phase exhibited noticeable broadening consistent with the small (<1μm) particle size evident in the SEM images.
EDS analyses showed Ce (measured as Ce2O3) to be concentrated primarily in the glass phase (0.90wt%) compared to the crystalline phases (0.21wt%).
The slag fraction of this sample was found to be a CaO–Fe2O3–Al2O3–SiO2 glass, plus crystalline Ca(Mg,Al)(Si,Al)2O6 (diopside), (Mg,Fe)(Cr,Fe,Al)2O4 (spinel), and at least one other unidentified phase; see Figs.
11 and 12.
Again, the XRD reflections of the spinel phase exhibited noticeable broadening consistent with the small (<1μm) particle size evident in the SEM images.
SEM and EDS mapping showed the metallic fraction to be composed of a major Fe bearing phase with minor Ni and Cr, the glass/metal interface was defined by a chromium oxide phase; see Fig.
13.
EDS analyses showed Ce to be concentrated exclusively in the glass phase, no Ce was found in the metallic fraction within detection limits.
The average pH of the blank and all test solutions was found to be constant at 12.2±0.1 units over the duration of the dissolution experiments, demonstrating the solution pH to be effectively buffered over the test period.
Fig.
14 shows the concentration of Ca in the blank and test solutions, as a function of time.
Within precision, the concentration of Ca in the blank solution remained effectively constant at 0.8±0.1gL−1 over the 28day test period.
In all test solutions, however, the Ca concentration showed a smooth decrease with time, reaching 0.4±0.1gL−1 after 28days.
The effectively constant Ca concentration in the blank solution is consistent with the exclusion of CO2 under the anoxic conditions of the experiment, which would otherwise lead to carbonation and precipitation of CaCO3.
Consequently, the depletion of Ca from solution in the case of the test experiments implies the precipitation of Ca bearing secondary alteration phases at a rate which exceeds the diffusion of Ca into solution from the filter unit.
A similar depletion of Ca in solution was reported from experiments on other simulant ILW glasses under similar conditions by Utton et al.
[11].
The compositions of PCM slag simulants did not contain any significant quantity of boron or lithium, which are normally used as a soluble marker of glass dissolution since these elements do not generally participate in the precipitation of secondary alteration products.
Therefore, the normalised mass loss of Na and K, shown in Fig.
15, were considered as a marker for slag dissolution.
The normalised mass loss of Na and K was similar for the slag materials derived from vitrification of the mixed, masonry and metal wastes, as shown in Fig.
15.
Measured values of NLNa and NLK were observed to increase rapidly over the first 14days of the experiment; thereafter, NLNa and NLK increased more slowly and appeared essentially constant after 28days.
However, experiments of longer duration are required to confirm this conclusion.
The normalised mass loss of Si was also similar for the slag materials derived from vitrification of the mixed, masonry and metal wastes, as shown in Fig.
15.
Within precision, the measured NLSi values were unchanged over the 28day test period, however, as shown in Fig.
15, a gradual upward trend in NLSi was apparent over the course of the experiments.
The concentration of Si in test solutions after 28days was 0.07–0.15mgL−1, significantly below, but approaching, the estimated silica saturation limit under the conditions of the experiment (ca.
0.3mgL−1, estimated using PHREEQC [32]).
These observations suggest that the apparent steady state conditions are controlled both by the formation of a calcium silicate phase (depleting Ca from solution) and the approach to silica saturation.
The normalised mass loss of Na, K and Si for the slag material derived from vitrification of the PVC waste, was evidently greater than that from that of the other slag materials, as shown in Fig.
15.
The rate of normalised mass loss of Na and Si decreased rapidly after the first 14days of the experiment, suggesting the approach to pseudo steady state conditions.
The concentration of Si in solution at 28days was close to the estimated saturation limit, suggesting that the observed decrease in the rate of glass dissolution is due to the approach to silica saturation.
In summary, the PVC derived slag was apparently less durable than the other slag materials, under the test conditions applied.
The durability of the slag materials derived from mixed, masonry and metal wastes, increased in this order, but were broadly similar.
The concentration of Ce in solution was below or close to the detection limit over the 28day test period and did not exceed 0.4μgL−1, consistent with the low solubility of trivalent lanthanides at high pH, as discussed in Section 4.3.
X-ray Absorption Spectroscopy has been shown to be a powerful tool for the investigation of element speciation and local co-ordination in (boro)silicate glasses for radioactive waste immobilisation [33–35].
Fig.
16 shows the Ce L3 edge X-ray Absorption Near Edge Structure (XANES) of the slag fractions, together with the data acquired for CeO2 and CePO4 (monazite) standards.
The Ce LIII edge XANES of Ce3+ (e.g.
CePO4) is characterised by a white line comprising a single intense feature attributed to the transition from an initial 2p64f15d0 state to a 2p54f15d1 final state, modified by local density of unoccupied states.
In contrast, the Ce L3 edge XANES of Ce4+ (e.g.
CeO2) is characterised by a white line comprising two features of lower relative intensity.
Following the notation convention employed by Bianconi et al.
[36], the consensus is that these two features, labelled A and B in Fig.
16, are due to transition from an initial 2p64f05d0 state to: (a) the 2p54f05d1 final state; and (b) the 2p54f15d1L1 final state (where Ln denotes a ligand hole) [36–40].
Other more subtle features are also observed.
Feature C is considered to be associated with transition to a 2p54f25d1L2 final state, whereas the weak feature D is assigned to quadrupole transitions [37–40].
The XANES spectra shown in Fig.
16 permit straightforward fingerprinting of Ce oxidation state.
From comparison of the spectra it is apparent that the XANES data of slag fraction closely resemble that of CePO4, demonstrating that Ce3+ is the dominant species.
From consideration of our measurement resolution (0.1eV) in comparison with the edge shift associated with Ce3+ and Ce4+ oxidation states (∼1.6eV), we estimate the detection limit of Ce4+ in our measurements to be ∼6%.
Cerium partitioning within the wasteform may be considered in terms of two key partitioning ratios:•Ce (slag fraction):Ce (metallic fraction) Ce (amorphous slag fraction):Ce (crystalline slag fraction/s) No relic or precipitated Ce, Ce2O3 or CeO2 were detected in any sample following vitrification, indicating that it was dissolved fully within the wasteform.
ICP–OES analyses of the slag and metallic fractions (where formed), confirmed that Ce was overwhelmingly incorporated in the slag fraction.
The ICP–OES method has the higher accuracy for total Ce, so these analyses are considered for partitioning between slag and metallic fractions.
For partitioning between glass and crystalline components of the slag fraction, spatially resolved EDS analyses were used.
It should be noted that the interaction volume of electrons with the sample, coupled with other sources of error, leads to significant uncertainties in the derived partitioning ratios within the slag component, which should therefore be considered only indicative at low concentration.
Cerium contents (as elemental Ce) of the slag fractions were in the range 0.67–1.16wt% whilst the metallic fractions contained 0.007–0.019wt% (Tables 5 and 6).
The slag to metal partitioning ratios, based on Ce ICP–OES analyses, were calculated as summarised in Table 8 and show that at least 97% of the analysed Ce was partitioned into the slag phase.
Although the proportion of Ce partitioned into the metal fraction is small, it is not negligible.
Assuming similar Pu distributions, the partitioning achieved would not be sufficient to permit disposal of the metal waste fraction as low level waste (LLW).
Nevertheless, the partitioning ratios obtained are encouraging and could potentially be optimised by extended high temperature reaction to assist Pu transfer from the metal to slag phase, and by judicious adjustment of the proportion of additive to waste and processing conditions.
Furthermore, the above calculations may be taken as conservative if we consider the possibility that small amounts of slag fraction may have been occluded in, or otherwise contaminated, the metal samples analysed.
This seems entirely plausible, given that clean separation of oxide and metallic components was neither attempted nor achievable with certainty.
The measured Ce content of the metallic fraction could be accounted for entirely, if this material contained approximately 1wt% of the slag material.
The slag fractions derived from the PVC, masonry and mixed waste types all exhibited a significant crystalline component.
Ce partitioning between the amorphous and crystalline phases was found to be dependent on the nature and quantity of the crystalline phase(s) present.
The slag derived from vitrification of PVC waste exhibited a substantial crystalline component; consequently, the amorphous phase was enriched to ca.
4.0wt% Ce2O3, but the accompanying dorrite and spinel phases incorporated low or trace levels of Ce2O3, ca.
1.0 and 0.2wt%, respectively.
The slag materials derived from vitrification of masonry and mixed wastes exhibited considerably smaller crystalline component, the amorphous phase contained ca.
1.2wt% Ce2O3 with little or no detectable level of Ce in the accompanying crystalline phases.
No crystalline phases were detected within the slag fraction derived from vitrification of the metal waste type.
The dissolved Ce appeared to be homogeneously distributed throughout the amorphous matrix and no concentration gradients or cerium “hot spots” were apparent in EDS maps.
The upper analysed Ce2O3 content of 4.0wt%, within the amorphous phase derived from the PVC waste, is equivalent to a Ce2O3 concentration of approximately 1.2mol%.
The solubility of Ce2O3 and Pu2O3 in (boro)silicate glasses has been shown to be broadly similar, with an upper limit of ca.
2mol% at 1450°C [14,41–43].
Therefore, it is considered feasible that the slag component of the wasteforms developed here could incorporate Pu at the concentrations expected.
The solubility of trivalent Ce and Pu in silicate glasses is known to be significantly higher than that of tetravalent Ce and Pu, particularly above 1100°C.
Therefore, it may be understood that the strongly reducing conditions achieved in PCM vitrification effectively assist in the partitioning of Ce within the glass phase of the slag wasteform.
The objectives of the preliminary dissolution experiments undertaken in this study were:•To investigate compatibility with the hyperalkaline conditions expected in a cementitious UK GDF.
To identify significant differences in the dissolution behaviour of the slag wasteforms in relation to their chemical composition in relation to the waste type.
To compare the relative performance of the slag products with other vitrified ILW products considered potentially suitable for geological disposal.
The data reported here are consistent with the conclusions of a recently published study of model and full scale simulant ILW glass dissolution in saturated Ca(OH)2 solution at 30–90°C [9,11].
The observed depletion of Ca from solution has been shown to be associated with the formation of calcium silicate hydrate (CSH) phases, resulting from reaction between the hydrated glass surface and dissolved Ca [10,11,44].
This CSH phase has been shown act as a passivating layer at pH 8.7, reducing the rate of glass dissolution [44].
Although Ca was depleted from test solutions, the hyperalkaline pH, key to minimising the solubility of long lived actinides in the cementitious GDF concept [45], was maintained over the duration of the experiments, as discussed below.
The slag materials derived from metal, masonry and mixed wastes were found to be considerably more durable, compared to that derived from PVC waste, based on the normalised mass loss of Na, K and Si.
In comparison to the other slag materials, the lower durability of PVC derived slag can be attributed to the low SiO2 fraction in the glass as shown in Table 5 (due to the correspondingly high Fe2O3 content, as discussed in Section 3.1).
Glass durability generally increases with increasing SiO2 content in silicate glasses, as a consequence of the increasing ratio of bridging to non-bridging oxygens, leading to increased glass network polymerisation and reduced interconnectivity of alkali channels which permit exchange of cations between leachate solution and glass [46].
Likewise, the durability of the slag materials derived from mixed, masonry and metal wastes, increased in this order with increasing SiO2 content, from 34 to 52wt%, as shown in Table 5.
The initial and residual dissolution rates based on the normalised release of Na (measured over the first and final 7days of the experiment, respectively) were 0.05gm−2d−1 and 0.002gm−2d−1 for the slags derived from metal, masonry and mixed wastes.
These dissolution rates are comparable with the previously reported for simulant inactive UK ILW glasses, with initial and residual dissolution rates of 0.02–0.04gm−2d−1 and 0.003–0.005gm−2d−1, respectively (based on NLNa under similar conditions) [10,11].
The initial dissolution rate of the slag derived from PVC waste was markedly higher, ca.
0.2gm−2d−1, as a consequence of the lower SiO2 content of this glass.
However, the residual rate of 0.01gm−2d−1 is comparable with that of other model and simulant full scale inactive UK ILW glasses.
Although Ca was depleted from test solutions, the hyperalkaline pH, key to minimising the solubility of long lived actinides in the cementitious GDF concept, was maintained over the duration of the experiments.
Consequently, the measured Ce concentration in test solutions was generally below or close to the detection limit in test solutions up to 21days and did not exceed 0.4μgL−1.
Given the trivalent speciation of Ce relevant to this study, our findings are consistent with the reported solubilities of trivalent Pu, Am, Nd, Sm and La, which are lower than 0.5μgL−1 at pH>9 under inert (CO2 free) atmospheres at 50–60°C [47].
Ewart et al.
and Berry et al.
studied the solubility of several actinides in a cementitious near-field environment using saturated Ca(OH)2 solutions under CO2 free conditions [48,49].
Concentrations in solution of Pu4+ and Am3+, the most relevant to this study, were reported to be below 0.02μgL−1 at pH 12.
The corrosion behaviour of the metallic fractions of the PCM wasteforms was not investigated in this study, since the aim was to partition Ce, as the Pu surrogate, into the slag component (as was demonstrably achieved).
However, a brief consideration is warranted as our results indicate that a small inventory of Pu may reside in the metallic fraction following thermal treatment.
A detailed European study of metallic materials in GDF conditions by Kursten et al.
[50] concluded that the current best estimates for corrosion rates of carbon steel in alkaline media are of the order of <0.12μmy−1 at 30°C, and <1μmy−1 at 80°C.
If Pu is assumed to partition identically to Ce, this would translate to an estimated upper available Pu release inventory of 4×10−4gm−2d−1.
In summary, it may be concluded that the slag materials produced from vitrification of the bounding PCM waste types considered, are broadly comparable, in terms of durability, to other simulant UK ILW glass products considered potentially suitable for geological disposal [10–12].
However, we cannot draw definitive conclusions concerning the compatibility of these wasteforms with a cementitious GDF concept, at the present time.
To do so would require understanding and modelling of glass dissolution and solution reaction mechanisms that influence the pH buffering capacity of the backfill fluid, which is beyond the scope of the current study.
Depletion of Ca from solution, as a consequence of reaction with the hydrated glass surface to form a calcium silicate hydrate (CSH) phase, may not necessarily be detrimental to the overall safety concept, since CSH materials are known to effectively sorb actinides from solution [51].
However, it is reasonable to conclude that the high durability of the PCM slags reported in this study, combined with the low solubility of actinides, under the hyperalkaline conditions of an ILW GDF, appear promising for application.
Volume reduction is a key requisite for treatment of PCM: the untreated packaged volume on the Sellafield site alone is projected to be in excess of 20,000m3 [1].
The driver for maximum volume reduction, to minimise interim storage and disposal costs, does not impose a limit on volume reduction a priori.
However, criticality concerns dictate that immobilised PCM requires a minimum level of dilution to ensure safe dispersion of Pu within the host matrix.
This provides a lower boundary for the volume of vitrified PCM material.
Further considerations of processing and GDF performance may provide additional constraints on volume reduction, but are out with the consideration of this study.
Relative to the untreated PCM waste, packaged in 200L drums, the slags derived from vitrification of PVC and metal wastes achieved an estimated volume reduction of 95%, whereas the slags derived from vitrification of masonry and mixed wastes achieved an estimated volume reduction of 80%.
These estimates, based on mass balance calculations, should be considered indicative rather than definitive, given the scale of the experiments and hence the sensitivity to small mass variations arising, for example, from: residual glass adhered to the crucible walls; corrosion of crucibles; foaming of glass melts; and, incomplete separation of slag and metal components.
As noted above, criticality considerations dictate lower boundaries for vitrified PCM volumes.
Projected PuO2 contents of PCM wastes are expected to have an average value in the region of 33g Pu per 200L drum, making an average of 0.165g/L [52].
Criticality considerations dictate that the maximum allowable concentration of Pu is 60g/L, and contingencies suggest that ideally a concentration of <6g/L is preferable.
If we assume that 6g/L is a reasonable target for Pu concentration in the slag fraction, with 60g/L an uppermost limit, it can be shown that, assuming one 200L drum holds 230g Pu, we require ca.
38L of the slag fraction to accommodate Pu at a concentration of 6g/L.
This decreases to just 3.8L of the slag fraction at the uppermost Pu concentration limit of 60g/L.
This lower boundary for vitrified waste volume implies a maximum volume reduction factor of 98%.
It may therefore be concluded that a 3:1 ratio of waste to GGBS, by weight, produces slag wasteform volumes commensurate with the preferred target concentration of Pu based on criticality considerations.
However, lower waste to GGBS ratios, with a consequent increase in overall wasteform volume, may be preferable (particularly in the case of the PVC and metallic waste types), where estimated volume reduction may achieve Pu concentrations near the upper limit.
Estimation of possible Pu release to the off gas system, based on the results of the vitrification melts carried out in this study, is made exceptionally difficult by the inaccuracy of the mass balance data concerning losses from the melt, especially given the relatively small size of the melts (<200g) and significant crucible corrosion.
Therefore, it is not possible to comment on the potential Pu release with confidence.
Owing to the compositions of the Sellafield PCM wastes being vitrified (Tables 1 and 2; see also Ref.
[2]), the greatest burden on the off-gas system in terms of volume will be provided by the combustion products of the PVC material.
Less than 100ppm of Cl were recorded in all samples, so it is likely that all of the Cl (as HCl) would be lost into the off-gas system.
An additional burden will arise from any water present in the waste; carry-over and evaporation from the (additive+waste) mixtures being vitrified; and smaller quantities of SOx, NOx and oxides of carbon.
Emission of toxic organic chemicals such as dioxins from PVC combustion has caused concern in other incineration scenarios, however, based on the available literature, it is considered that these compounds should be destroyed at the very high treatment temperatures relevant to this study (>1500°C) [53–58].
Further detailed study, outside the scope of this work, will be necessary to fully quantify off-gas burdens.
This study demonstrated proof of concept for thermal treatment of PCM wastes, representative of those present on the Sellafield site, by vitrification with the addition of ground granulated blast-furnace slag, with a PCM waste to additive ratio of 3:1 by weight.
The nature of the product wasteforms was influenced by the type of PCM waste and crucible containment.
High metal feeds necessitated the use of clay/graphite crucibles, imposing a reducing environment, leading to a combined slag/metal wasteform; the slag component was thus strongly depleted in Fe2O3.
Feeds with a lower fraction of metal could be processed in alumina crucibles, leading to a slag wasteform for the PVC and masonry wastes, and a combined slag/metal wasteform for the mixed waste type (with a higher metal fraction).
Overall, the approach developed here would be compatible with commercially available batch wise in container and plasma vitrification technologies, described elsewhere [3,12,59].
Ce, as Pu surrogate was found to partition overwhelmingly (>97%) within the slag component, where a metal component was also present.
Within the slag component, Ce was found to be incorporated primarily within the CaO–Fe2O3–Al2O3–SiO2 glass phase with trace incorporation in other crystalline phases.
The bulk speciation of Ce was determined to be Ce3+, comparable with the known speciation of Pu3+ silicate glasses produced at high temperature.
The upper analysed Ce2O3 concentration in the glass phase (1.2mol% for the PVC waste) was well below the reported solubility limit of Ce2O3 and Pu2O3 in (boro)silicate glasses (ca.
2mol%) processed at comparable temperature.
Therefore, it is reasonable to expect that the slag component of the wasteforms developed here could incorporate Pu at the concentrations expected from treatment of PCM wastes.
The estimated volume reduction factors of 80–95% demonstrated in this proof of concept investigation, utilising a 3:1 ratio of PCM waste to GGBS, by weight, produced slag wasteform volumes commensurate with the estimated maximum desirable volume reduction factor of 98%, based on criticality considerations (relative to untreated PCM waste, packaged in 200L drums).
The dissolution behaviour of the slag wasteforms in saturated Ca(OH)2 (to simulate the hyperalkaline conditions of a cementitious GDF), proved similar to that of UK ILW waste glasses considered potentially suitable for geological disposal.
Depletion of Ca from solution was inferred to be the consequence of reaction with the hydrated glass surface to form a calcium silicate hydrate (CSH) phase.
The volume reduction and high durability of the PCM slags reported in this study, combined with the low solubility of actinides, under the hyperalkaline conditions of a UK ILW GDF, appear promising for application.
This research was primarily sponsored by Sellafield Ltd. and the Engineering and Physical Sciences Research Council through The University of Sheffield Knowledge Transfer Account, grant number EP/H500170/1.
Use of the National Synchrotron Light Source, Brookhaven National Laboratory, was supported by the US Department of Energy, Office of Science, Office of Basic Energy Sciences, under Contract no.
DE-AC02-98CH10886.
NCH is grateful to The Royal Academy of Engineering and Nuclear Decommissioning Authority for funding.
NCH gratefully acknowledge part support from the Engineering and Physical Sciences Research Council under grant numbers EP/I012214/1 and EP/G037140/1.
CLC is grateful to The University of Sheffield for award of a Vice Chancellor’s Fellowship.
Analysis of implicit and explicit lattice sensitivities using DRAGON Deterministic lattice physics transport calculations are used extensively within the context of operational and safety analysis of nuclear power plants.
As such the sensitivity and uncertainty in the evaluated nuclear data used to predict neutronic interactions and other key transport phenomena are critical topics for research.
Sensitivity analysis of nuclear systems with respect to fundamental nuclear data using multi-energy-group discretization is complicated by the dilution dependency of multi-group macroscopic cross-sections as a result of resonance self-shielding.
It has become common to group sensitivities into implicit and explicit effects to aid in the understanding of the nature of the sensitivities involved in the calculations, however the overall sensitivity is an integral of these effects.
Explicit effects stem from perturbations performed for a specific nuclear data for a given isotope and at a specific energy, and their direct impact on the end figure of merit.
Implicit effects stem from resonance self-shielding effects and can change the nature of their own sensitivities at other energies, or that for other reactions or even other isotopes.
Quantification of the implicit sensitivity component involves some manner of treatment of resonance parameters in a way that is self-consistent with perturbations occurring in associated multi-group cross-sections.
A procedure for assessing these implicit effects is described in the context of the Bondarenko method of self-shielding and implemented using a WIMS-D4 multi-group nuclear library and the lattice solver DRAGON.
The resulting sensitivity results were compared to those calculated by TSUNAMI-1D, which computes implicit sensitivities using a different methodology consisting of a combination of linear perturbation theory and automatic differentiation.
Energy-dependent sensitivity profiles and integrated sensitivity coefficients are presented, as well as a comparison of calculated sensitivities for different energy group structures and geometry dimensionalities.
An essential part of nuclear reactor analysis is the prediction of the three-dimensional space-time kinetics of neutrons in a relatively large, finite, heterogeneous, three-dimensional reactor core.
In a majority of safety analyses the prediction of reactor physics responses is performed using neutron diffusion theory applied to three-dimensional systems, with inputs usually derived from deterministic neutron transport solutions of two-dimensional lattice geometries.
There has been increased activity related to uncertainty and sensitivity in reactor physics calculations, and the Organization for Economic Cooperation and Development – Nuclear Energy Agency (OECD-NEA) has sponsored an ongoing benchmark entitled “Uncertainty Analysis in Modelling” (UAM) related to these efforts.
The goal of this work is to offer a strategy for computing lattice sensitivities using the DRAGON lattice code and WIMS-D4 multi-group library.
Results are presented with comparison to those from TSUNAMI, developed by Oak Ridge National Laboratories.
In general, neutron transport theory allows for the prediction of the spatial distribution of neutrons given the reactor geometry, boundary conditions, and nuclear interaction data.
Thus the solution will depend on problem-specific information (geometries, materials, and boundary conditions) as well as generic inputs related to nuclide-specific interaction probabilities, fission energy distribution, and neutron and fission product yields.
These generic input data are contained in nuclear data libraries which provide the appropriate information as a function of neutron energy.
Most deterministic lattice neutron solvers predict the neutronic behavior as a function of discrete energy intervals, and hence they need the input data to be transformed into multiple and discrete energy groups (so-called multi-group data).
The code NJOY is typically applied to process continuous energy nuclear data into corresponding multi-group cross sections.
Once computed at the lattice level, the neutron flux and nuclear interaction rates are used to develop so call few-group nuclear data, which form the input for three-dimensional full-core diffusion calculations.
Within this work we have developed a sensitivity and uncertainty analysis tool with the capability of examining the explicit effects of a perturbation as well as the implicit effects which manifest themselves through changes in the spatial-temporal neutron flux which arise from self-shielding effects.
At the outset of this work, a fundamental objective is that the sensitivity and uncertainty tool must utilize well established transport methods and codes, and must be practical for engineering analyses in the sense that it can ultimately be applied to large scale nuclear reactor physics computations.
An additional goal was to develop a tool with the flexibility to examining sensitivities for a wide range of input parameters, such as those related to geometry, composition, temperatures, etc., which in many cases cannot be examined easily with adjoint-based methods.
The algorithm developed utilizes the DRAGON lattice physics code, which is an open-source transport solver already in use in the Canadian nuclear industry.
Where possible we have validated our responses against established sensitivity tools using alternative solution methods and algorithms.
This section provides a very brief overview of transport methods and the fundamental sensitivity components which are addressed in this work.
A continuous-energy formulation of the neutron transport equation is shown in Eq.
(1).
(1)Ω⋅∇ϕ→(r,E,Ω→)+∑T(r,E)ϕ→(r,E,Ω→)=∫4πdΩ′∫0∞∑S(r,E′−E,Ω→′→E,Ω→→Ω→)ϕ→(r,E′,Ω→′)dE′+χ(E)4πk∞∫4πdΩ′∫0∞v¯(r,E′)∑F(r,E′,Ω→′)ϕ→(r,E′,Ω→′)dE′While evaluated nuclear data such as ENDF (Herman and Trkov, 2009) provides continuous-energy neutronic interaction information, most deterministic lattice codes do not solve the transport equation using continuously-varying parameters for practical reasons.
Cross-section processing is first performed to produce an equivalent set of discretized multi-group neutron interaction parameters, so that the solution of the transport equation is done over discrete rather than continuous energy field (Tsvetkov and Walter, 2011; MacFarlane and Boicourt, 1982; Dunn and Greene, 2002).
Typically the neutron flux is used as a weighting factor in the transformation from continuous to multi-group nuclear data.
Once collapsed to multiple and discrete energy groups – usually numbering from a few dozen to a few hundred – the data is stored in multi-group nuclear data libraries, and often packaged with lattice physics codes, whose task is to determine neutron flux and reaction rates as a function of energy group using problem-specific materials and geometries.
The cross-section averaging in each energy group, g, using an estimate of the neutron flux, ϕ(E) as a weighting function is shown in Eq.
(2), where σ(E) is a microscopic cross-section continuously varying in energy, and σg is the average of σ(E) over some energy interval, g.(2)σg=∫gσ(E)ϕ(E)dE∫gϕ(E)dE Resonant cross-sections cause flux depressions to occur in an energy neighborhood near to the cross-section resonances.
The less dilute the nuclide, the larger its contribution to total cross-section and the larger the flux depression that results in the vicinity of the resonance.
A nuclide admixed at infinite dilution causes no perturbation and the flux remains smoothly varying, typically on the order 1/E at intermediate energies.
Since the dilution is a function of the fuel design and materials, it is necessarily problem specific.
Depressions in flux cause the multi-group constants that contain resonances to take on smaller values, since the flux, acting as an averaging weighting function, exhibits a local minimum that corresponds to the cross-section peak energy.
This effect is called resonance self-shielding (Hébert, 2009), on account of the flux being effectively prevented by the resonance from attaining a smoothly varying form when at low dilution.
The flux will be reduced (or shielded) as compared to that observed at high dilution, and in-turn such flux when used as a weighting function will tend to reduce said multi-group constant.
The process of correcting the multi-group cross-sections to new values that reflect the flux depressions at the problem-specific dilution is an initial step in lattice calculations (Duderstadt and Hamilton, 1976).
Complexity is added in that one discrete energy group may contain one, several, or partial resonances within its energy boundaries.
Multi-group libraries, such as the WIMS-D4 (Taubman, 1975; Leszczynksi et al., 2007) or AMPX master libraries (Bowman and Dunn, 2009), are designed to allow for self-shielding calculations, and include resonance parameters specifically to accommodate that task.
The quantity and style of resonance parameters are tailored to particular resonance self-shielding methods.
A common and straightforward strategy is Bondarenko's method (Bondarenko, 1964; MacFarlane, 2010; Greene, 2009).
The Bondarenko method observes that the continuously-varying flux in the neighborhood of the narrow resonances tends to behave as the product of two components: an underlying smooth 1/E component, and a component that varies as 1/∑T.
The Bondarenko formula of a self-shielding cross-section, σ¯g, in a particular energy group interval, g, is shown in Eq.
(3).
Eq.
(3): Bondarenko resonance self-shielding.
(3)σ¯g=∫g(σ(E)1/E)/(σt(E)+σ0)dE∫g(1/E)/(σt(E)+σ0)dEwhere, σ(E) is a continuously varying, resonant microscopic cross-section.
σt is the microscopic total cross-section of the resonant nuclide.
σ0 is the background cross-section in per-resonant atom units.
s¯g: is the discrete, self-shielded resonant cross-section in group g. In practice, many multi-group nuclear data libraries (including WIMS-D4 and AMPX-format libraries) include tables of Bondarenko self-shielded group constants that have been pre-computed for a variety of temperatures and background cross-sections, including an infinite background cross-section.
The calculation of problem-dependent self-shielding effects can be accomplished by interpolating the table to the problem dilution and temperature that corresponds to the specific nuclear lattice under investigation.
The method proposed in this paper to treat uncertainties within the existing computational framework utilizes these pre-computed background information in the propagation of the nuclear data sensitivities through the resonance self-shielding treatment.
Lattice physics sensitivity analysis, as applied in this paper, involves the determination of the magnitude of change of lattice output responses with respect to changes in input parameters.
The output responses of interest include the multiplication constant, k∞, and few-group, homogenized cross-sections as well as other important characteristics such as mean neutron lifetime, prompt and delayed fractions, and reactivity feedback coefficients.
While lattice input parameters include nuclear data (typically in multi-group format), mixture temperatures, material atom densities, physical geometry and so on, this paper will focus exclusively on nuclear data, such as microscopic cross-sections, to highlight the efficiency and benefits of the perturbation method using the pre-computed Bondarenko background data.
However, it should be noted that the sensitivity and uncertainty analysis tool discussed here is applicable to a much larger domain of sensitivities including those with respect to composition, geometry, temperature and density inputs.
The ways through which response sensitivities emerge with respect to microscopic cross-sections differ between the continuous-energy and multi-group formulations of neutron transport.
Explicit sensitivity, present in both formulations, exists through direct action, where parameters perturb responses via the operators of the transport equation.
However, in the discrete-energy case, there are additional, implicit sensitivities (Williams and Rearden, 2008) that are associated with the dependencies that exist between discrete group cross-sections that are absent from their continuous-energy counterparts.
The implicit effects are a product of resonance self-shielding effects, in that the cross-sections of a particular nuclide contribute to the background cross-section seen by all other admixed nuclides and therefore affects the flux, which affects the weightings used in the homogenization process.
Therefore, while not explicitly observable in the transport equation, self-shielding sensitivities are implied by the discretization of the parameter space in energy.
The distinction between explicit and implicit sensitivities can be alternately explained through the observation that the explicit component is the sensitivity of lattice output responses with respect to self-shielded cross-sections, and the implicit component is the sensitivity of self-shielded and discretized cross-sections with respect to their infinite-dilution counterparts.
It has been observed that implicit sensitivity components can be substantial in some cases, accounting for up to 40% of the total sensitivity in some energy groups (Rearden et al., 2005).
The proper accounting of both the explicit and implicit sensitivities is necessary for comprehensive sensitivity and uncertainty analysis of lattice physics calculations.
Strategies for estimating implicit sensitivities depend on the underlying method for performing resonance self-shielding calculations implemented by the particular lattice code, and the set of resonance parameters used by the code to facilitate those calculations.
This paper describes a strategy for the estimation of implicit sensitivities that is applicable to a Bondarenko-type self-shielding calculation using the resonance parameters included in a WIMS-D4 format nuclear data library.
While the implementation details are specific to the library and lattice code adopted, the overall method is generic and can be applied to a large body of lattice physics codes.
A methodology was developed and implemented in a new toolset, DINOSAUR (Ball, 2012), which calculates sensitivities and propagates uncertainties of both nuclear data and problem-dependent model data through lattice calculations.
DINOSAUR creates new multi-group nuclear data libraries whose data has been modified compared to a set of unperturbed reference data.
The solution of the lattice physics equations applied to each new library is performed using the lattice code DRAGON (Marleau et al., 2012).
By comparing the perturbed cases to the reference cases, a set of sensitivities can be computed.
Implicit sensitivities are quantified by DINOSAUR by correcting resonance integrals found in the nuclear data library based on perturbations of infinite-dilution cross-sections, as is shown in Section 3.3.
The methodology is capable of performing a variety of calculations including sensitivity analyses and Monte Carlo covariance propagation as part of uncertainty analysis.
Monte Carlo covariance propagation is performed by randomly simultaneously generating new samples of each uncertain parameter (i.e.
multi-group cross-sections) according to their joint probability distributions.
The best-estimate values of each variable are taken to be their statistical means, and their covariance is provided as input to the uncertainty propagation sequence.
Higher order statistical moments must be assumed, such as whether the distributions are Gaussian, uniformly-distributed, etc.
DINOSAUR writes each set of random samples to a new cross-section library, and calls on a best-estimate lattice solver to compute the corresponding output responses to each set of input, giving rise to a distribution of output responses whose covariance can be analytically computed.
DINOSAUR can also calculate sensitivities using a direct numerical perturbation (DNP) approach, which is the subject of this paper.
The DNP mode of DINOSAUR is similar to its Monte Carlo sampling mode in several respects: DINOSAUR determines new values for uncertain parameters; those values are written to new cross-section libraries; a best-estimate solver is called on by DINOSAUR to compute the output response associated with the newly written libraries; DINOSAUR analyze the output response to determine useful information.
Where the DNP and Monte Carlo mode differ is in the manner by which new variable values are determined.
DNP is a straightforward numerical method for estimating the partial derivatives of response functions by making small changes to an input parameter of interest and evaluating the response with each change, according to, for example, a method of finite difference.
In the context of the DINOSAUR tool, the small change, or perturbation, is made on a variable by multiplying it by a number that is, near unity.
When the perturbed value is calculated, it is written to a new cross-section library according to the procedure outlined above and in a way that satisfies a set of consistency requirements that are described in Section 3.3.
Therefore, to calculate the sensitivity of a response (e.g.
k∞) with respect to an n-group cross-section requires n+1 best-estimate calculations: one reference solution and n perturbed solutions that each correspond to one of the n perturbed energy groups of the cross-section.
The calculation of a partial derivative of k∞ with respect to cross-section of type, i, in energy group, g, σgi is straightforward as shown in Eq.
(4).
(4)∂k∞(σgi)∂σgi=k∞(σgi+δiσgi)−k∞(σgi)δiσgi An appropriate perturbation size is selected such that it achieves a balance of avoiding numerical truncation which occur when too small a perturbation is applied and too large a perturbation which may mask the local sensitivity.
A dimensionless sensitivity coefficient, Sgi of k∞ with respect to the same parameter is shown in Eq.
(5).
It is common to see sensitivity coefficients expressed in per-unit lethargy, as in Eq.
(6), where Eg+ and Eg− are the upper and lower energy bounds of group g, respectively.
This accounts for the fact that parameters in broad energy groups tend to show more sensitivity to output responses merely as a result of their occupying a larger fraction of the energy domain than parameters in narrower groups.
(5)Sgi=∂k∞(σgi)∂σgi×σgik∞(6)SLgi=Sgilog(Eg+/Eg−) The focus of work described in this paper is the determination of total sensitivities (implicit plus explicit) using DNP and to highlight the importance of their treatment in lattice physics calculations with comparisons made to solutions obtained using an alternate method based on linear perturbation theory.
A flowchart of the DNP sequence of DINOSAUR is shown in Fig.
1, and descriptions of its modules are listed in Table 1.
During the 1990s, the International Atomic Energy Agency (IAEA) identified the need to update the aging WIMS-D4 libraries with the latest nuclear data.
It subsequently launched the WIMS Library Update Project (WLUP) (IAEA, 2007) to produce new WIMS-D4 format libraries that would replace the outdated WIMS-D4 library, which was based on evaluated nuclear data from the early 1960s.
The results of the WLUP are 69 and 172 energy group libraries, “IAEA”, which contains data for 173 materials.
The WIMS-D4 format includes resonance parameters for performing resonance self-shielding and multi-group parameters that are required for solving the transport equation.
The list of multi-group parameters that are explicitly stored in the library is shown in Table 1.
Some parameters are also assigned identifier (ID) codes by DINOSAUR, which are also shown in the table, if applicable.
As shown in Table 2, the IAEA WIMS-D4 library contains scattering and absorption data that is “lumped”, that is, data that is an aggregate of several reaction cross-sections that are not individually recorded in the library explicitly.
Definitions of the lumped absorption and scattering data are shown in Eqs.
(7) and (8), respectively.
(7)σabsortlumped=σabsorb−σ2n−2σ3n=σγ+σf+σα⋯−σ2n−2σ3n(8)σsl=σscatterl+2σ2nl+3σ3nl=σslasticl+σinelasticl+2σ2nl+3σ3nlHowever, for most materials, only a transport-corrected, isotropic (l=0) lumped scattering matrix as shown in Eq.
(9) is included in the library, where the isotropic scattering matrix is corrected by a term that is a function of the first-order, linearly anisotropic matrix.
(9)σs,jtranc=σs,j0−f(σs,j1)In addition to the parameters listed in Table 2, other parameters, which are not explicitly stored on a WIMS-D4 format library, can at least be computed or approximated by those parameters and are shown in Table 2.
Such parameters, such as ν¯, are used internally by DINOSAUR and are given DINOSAUR identifier codes.
They are listed in Table 3.
The lumped one-dimensional scattering cross-section, XS, is a column-integrated lumped isotropic (l=0) scattering matrix.
Therefore, the i-th group of XS is as shown in Eq.
(10).
For nuclides with negligible inelastic scattering, α-particle production and neutron production, XS will approximate that nuclide's elastic scattering cross-section.
This is the case for some important moderating nuclides such as hydrogen and deuterium.
(10)XSj≡∑kXSMj→k0For many nuclides, a good approximation of the neutron capture cross-section, σγ, can be made by subtracting the fission cross-section, if present, from the lumped absorption cross-section, and then disregarding any negative values.
Negative absorption values may arise due to the negative lumped absorption contributions from σ2n and σ3n.
DINOSAUR defines a parameter, XG, as an approximation of σγ in this manner.
(11)XG≡MAX(XA−XF,0)The average number of neutrons per fission is implicitly stored in a WIMS-D4 format library and can be determined exactly by the ratio of the XF and XVF parameters as shown in Eq.
(12).
(12)NU≡XVFXFLastly, the total cross-section of non-transport correct nuclides can be computed merely from the sum of the lumped scattering and lumped absorption cross-section as shown in Eq.
(13).
For materials whose scattering matrix is transport-corrected, the total cross-section as defined in Eq.
(13) is only an approximation to the total cross-section, and is in fact identical to the transport cross-section, XTR.
(13)XT≡XS+XAThe DINOSAUR parameters XS, XG and NU are valuable for comparing DINOSAUR sensitivities to those calculated by other codes whose reference data includes those parameters explicitly.
The WIMS-D4 library contains a set of resonance parameters suitable for performing Bondarenko-type resonance self-shielding of microscopic cross-sections.
Indexed by background cross-section and temperature, tables of resonance integrals, Ia(T, σ0) and Iνf (T, σ0) exist for each resonance energy group, and correspond to absorption and fission-yield cross-sections, respectively, of selected nuclides.
The resonance integrals are defined in Eqs.
(14) and (15).
(14)Ia(T,σ0)≡σ¯a(T,σ0)σ0σ¯a(T,σ0)+σ0(15)Ivf(T,σ0)≡σ¯vf(T,σ0)σ0σ¯a(T,σ0)+σ0where, σ0 is the background cross-section.
σ¯a is the discrete, self-shielded absorption cross-section in group g. σ¯uf is the discrete, self-shielded fission-yield cross-section in group g. The self-shielding cross-sections that appear in Eqs.
(14) and (15) as part of the formulation of the IAEA library resonance integrals were calculated by the GROUPR module of the cross-section processing code NJOY when the library was compiled.
This section details a method for evaluating lattice sensitivities using DNP.
Implicit sensitivity components are treated in the analysis using a set of resonance parameter corrections derived from perturbations in infinitely dilute cross-sections.
While the calculation of explicit sensitivities is straightforward, determining the implicit effects – and hence the total sensitivity – requires more attention.
The determination of explicit sensitivities merely requires that the perturbations be applied after the resonance self-shielding step of the lattice calculation sequence.
The resulting sensitivities are therefore in reference to changes in the self-shielded cross-section, rather than to changes in the infinitely-dilute cross-section.
Recall that the total sensitivity, which includes both explicit and implicit sensitivity components, is the sensitivity of a response to changes in an infinitely-dilute cross-section rather than one that is self-shielded at a particular problem dilution.
When calculating the total sensitivity, perturbations must be applied prior to resonance self-shielding on the problem-independent, infinitely-dilute cross-sections, with appropriate changes to resonance parameters to ensure internal consistency between the resonance parameters and the newly perturbed infinitely-dilute cross-sections.
Consider a small perturbation factor, (1+δi), applied to an infinitely-dilute, discrete resonant cross-section, σi, of type i, in energy group g, as in Eq.
(16).
Eq.
(16): A perturbed, discrete cross-section in group, g.(16)σ¯ig′=(1+δi)σ¯ig=(1+δi)∫gσi(E)ϕ(E)dE∫gϕ(E)dETo develop a unique and useful treatment for reactor-scale applications which deterministically solve the multi-group neutron transport equation and which can be used within the self-shielding function, it can be assumed that the perturbation of the discrete cross-section is solely attributable to its continuous-energy counterpart, σ(E), and not, for example, the result of a perturbation in the fine structure of the weighting function, W(E), which takes a 1/E form at slowing-down energies.
A second assumption can be made that the discrete perturbation is uniformly attributable to the continuously-varying cross-section over the energy interval, g, as in Eq.
(17).(17)σ¯ig′=(1+δi)σ¯ig=∫g(1+δi)σi(E)ϕ(E)dE∫gϕ(E)dE=∫gσ′i(E)ϕ(E)dE∫gϕ(E)dE(18)σ′i(E)=(1+δi)σi(E)Eq.
(18) relates perturbations in a discrete, infinitely-dilute group cross-section to a uniform perturbation in the continuous-energy cross-section within the group energy boundaries.
Substituting the perturbed continuous-energy cross-section in Eq.
(18) into the Bondarenko equation for resonance self-shielding, Eq.
(3), yields Eq.
(19).
(19)σ¯ig′(T,σb)=∫gσ′i(T,μ)σ′i(T,μ)+σ0dμ1σ′i(T,μ)+σ0dμIn Eq.
(19), a change of variable has been performed from neutron energy, E, to neutron lethargy, μ in order to simplify notation, and in addition, the dependency of parameters on temperature have been explicitly referenced.
Note that the perturbed continuous-energy resonant cross-section, σ′i(T,μ), results in corresponding perturbations to the total cross-section of that nuclide, σ′t(T,μ) and the resulting self-shielded resonant cross-section, σ¯ig′.
By expanding the perturbed cross-section of Eq.
(19), the equation can be re-written as sown in Eq.
(20).
(20)σ¯ig′(T,σb)=∫g(1+δi)σi(T,μ)(1+δt)σt(T,μ)+σ0dμ1(1+δt)σt(T,μ)+σ0dμ Since the perturbation is constant over the energy group, the factor (1+δ) in the numerator of Eq.
(20) can be removed from the integration, and the equation reduces to,(21)σ¯ig′(T,σb)=(1+δi)∫gσi(T,μ)(1+δt)σt(T,μ)+σ0dμ∫g1(1+δt)σt(T,μ)+σ0dμ=(1+δi)∫gσi(T,μ)σt(T,μ)+σ0(1+δt)dμ∫g1σt(T,μ)+σ0(1+δt)dμ=(1+δi)σ¯igT,σ0(1+δt)=(1+δi)σ¯ig(T,σ′0)It can be seen in Eq.
(21) that when a perturbation factor (1+δ) is applied to an infinitely dilute cross-section, the resonance self-shielded cross-section is perturbed by the same factor, but must also evaluated at a modified background cross-section.
Note than in Eq.
(21) a new parameter σ′0 has been introduced which will be referred to as an effective perturbed background cross-section, which is defined in Eq.
(22).
(22)σ′0=σ01+δt Consequently, it can be seen from Eqs.
(21) and (22) that when a perturbation factor (1+δ) is applied to an infinitely dilute cross-section, the resonance self-shielded cross-section is perturbed by the same factor, but must also evaluated at a perturbed background cross-section, σ′0, which is the unperturbed background cross-section divided by the resulting perturbation of the total cross-section, (1+δt).
This technique has the advantage that it does not require the knowledge of any continuously-varying data and can be calculated using only multi-group parameters and hence can be directly adopted into the existing deterministic transport codes that are used in industrial applications.
In particular this has high value to the nuclear power industry where often multi-group input datasets for deterministic lattice calculations are fixed, since the data has been evaluated and adjusted with complimentary data specific to the design.
Hence, the methodology proposed here does not require operational specific nuclear data to be re-derived from raw continuous nuclear data libraries.
The assumption, that the group-wise perturbation can be characterized by a uniform scaling of the continuous-energy cross-section (rather than, for example, the lateral translation of resonances) has similarity to the calculation performed by the BONAMIST module (Greene, 2009; Rearden et al., 2009a) of the TSUNAMI-1D (Rearden, 2009b) code, which is that the continuous cross-section is fully correlated within each group.
Of the parameters used by DINOSAUR, some are independent and others are dependent.
The independent parameters are those that are perturbed by DINOSAUR by request of the user and for which sensitivity coefficients to k∞ are computed.
Conversely, the dependent parameters are only perturbed as a consequence of the perturbing of an independent parameter, in order to ensure the library remains self-consistent.
In DINOSAUR, a set of consistency rules are enforced to ensure that dependent quantities such as lumped cross-sections and resonance integrals are adjusted to reflect the change made to the perturbed independent quantities.
For example, it is quite intuitive that any perturbation applied to σf(XF) should result in a commensurate modification to σvf(XVF), but other parameter relationships are not so intuitive.
Such consistency rules are necessary to ensure that perturbed libraries do not violate physical restraints.
This section describes all the consistency rules currently implemented in DINOSAUR.
A table of independent and dependent parameters is shown in Table 4.
There are four sets of consistency rules, each corresponding to one of the DINOSAUR's four independent parameters, shown in Tables 5–8.
For each table, the rules are triggered when a perturbation of the form listed under the “Independent parameter perturbation” column occurs.
All “primed” quantities (i.e.
XTR′, XS′, etc.)
are perturbed quantities, whereas their non-primed counterparts (i.e.
XTR, XS, etc.)
are unperturbed reference quantities.
With the exception of σ’0, whose formulation was shown previously in Eq.
(22), the other perturbed parameters have a general form as shown in Eq.
(23).
(23)X′=X+δXX=(1+δX)X General form of perturbed parameter, X The rules in Table 5 describe the manner by which five dependent reference parameters, XTR, XSM, XP, RIa and RIvf are perturbed to XTR′, XSM′, XP′, RIa′ and RIvf′, to achieve mutual consistency with XS′, which is the scattering parameter XS perturbed by a factor of (1+δXS).
Rule #1.D and Rule #1.E account for the implicit sensitivity effect, and will be described in more detail below.
The consistency rules for XG, XF, and NU are found in Tables 6–8.
Several of the consistency rules involve recalculation of the resonance integrals, RIa and RIvf.
These integrals require such recalculation because it is clear from Section 3.2 that a perturbation of an infinitely-dilute cross-section implies a related perturbation on resonance self-shielded cross-sections according to Eq.
(21).
By substituting Eqs.
(21) and (22), into Eqs.
(14) and (15), equations describing perturbed resonance integrals can be determined, as shown in Eqs.
(24) and (25).
(24)I′a(T,σ0)=σ¯′a(T,σ0)σ0σ¯′a(T,σ0)+σ0=(1+δXA)σ¯a(T,σ′0)σ0(1+δXA)σ¯a(T,σ′0)+σ0(25)I′vf(T,σ0)=σ¯′vf(T,σ0)σ0σ¯′a(T,σ0)+σ0=(1+δXVF)σ′vf(T,σ′0)σ0(1+δXA)σ′a(T,σ′0)+σ0 The task associated with propagating infinite-dilution perturbations to resonance integrals involves re-computing the integral as a function of self-shielded cross-sections that have been evaluated at a perturbed background.
Given a perturbed infinite-dilution cross-section, the first step in this procedure is to re-arrange Eqs.
(14) and (15) to solve for the self-shielded cross-sections that correspond to each resonance integral in the table.
(26)σ¯a(T,σ0)=Ia(T,σ0)σ0σ0−Ia(T,σ0)(27)σ¯vf(T,σ0)=Ivf(T,σ0)σ0σ0−Ia(T,σ0) Therefore, from the table of resonance integrals within a multi-group library, there can be found a table of self-shielded cross-sections that correspond to indices of temperature and σ0.
For a given perturbation of (1+δ) applied to an infinitely-dilute cross-section, for example, σa, the table of self-shielded cross-sections, σ¯a(T,σ0) is interpolated linearly to determine a table of perturbed self-shielded cross-sections,(28)σ¯′a=(1+δXA)σ¯a T,σ′0=σ01+δXT The interpolated self-shielded cross-sections can be substituted into Eqs.
(24) and (25) to produce a table of perturbed resonance integrals that are self-consistent with the perturbed infinite-dilution cross-section.
An example diagram of this procedure applied to the absorption resonance integral table is shown in Figs.
2 and 3.
The methodology for calculating the combined implicit and explicit sensitivities to multi-group cross-sections that has been described in Section 3 was used by the DINOSAUR code to generate sensitivity profiles of a lattice multiplication constant, k∞, with respect to various multi-group neutron cross-sections of importance.
The sensitivity profiles were generated by DINOSAUR in both a 69 and 172-group energy structure.
The two IAEA multi-group libraries, each of the WIMS-D4 format, are freely available from WLUP.
For this work, the employed IAEA library was based on ENDF/B-VII nuclear data, and is hereafter referred to as IAEA-endf-7.
The DINOSAUR sensitivity profiles are compared against sensitivities calculated by TSUNAMI-1D (Rearden, 2009b), part of the SCALE 6.0 code package (Bowman, 2007) developed by Oak Ridge National Laboratory (ORNL).
TSUNAMI-1D uses an adjoint-based linear perturbation theory approach to compute sensitivities for one-dimensional lattice cells.
In this paper, TSUNAMI-1D sensitivities were calculated in 238 energy groups using the SCALE “xn238v7” data library based on the ENDF/B-VII evaluation.
Implicit sensitivities are calculated by TSUNAMI-1D using automatic differentiation applied to its self-shielding modules.
Note, however, that the comparison between results of DINOSAUR and TSUNAMI-1D is not perfect, because the perturbed parameters are not precisely the same.
Whereas DINOSAUR can compute sensitivities with respect to the XS, XG, XF and NU parameters of each nuclide, TSUNAMI-1D sensitivities are those of individual cross-sections precisely, such as elastic and inelastic scattering, (n,γ), (n, 2n), (n, α), v¯, and so on.
While XF and NU correspond exactly to (n, f) and v¯, XG is only an approximation of (n,γ) for most nuclides.
Likewise, the comparability of XS with elastic scattering depends greatly on the nuclide in question, being precise for hydrogen, but questionable for nuclides with large inelastic scattering cross-sections.
These discrepancies must be considered when doing a direct validation of DINASAUR to TSUNAMI in this regard.
The sensitivity comparisons reported in this paper are therefore carefully selected for input parameters that are highly comparable between the two codes: 1H elastic scattering (XS) and neutron capture (XG); 16O elastic scattering (XS); 235U neutron capture (XG), fission (XF) and v¯ (NU); and 238U neutron capture (XG), fission (XF) and v¯ (NU).
Plots of selected parameters from xn238v7 and DINOSAUR parameters derived from IAEA-endf-7 are shown in Figs.
4–10.
Additional comparisons for parameters that are not directly related between the codes were also performed and the differences observed were as expected.
Sensitivities of the lattice multiplication constant, k∞, with respect to multi-group neutron cross-sections has been performed by DINOSAUR and TSUNAMI-1D on an LWR pin cell model of the Peach-Bottom unit 2 (PB-2) General Electric Type 4 Boiling Water Reactor (BWR), whose lattice specifications are shown in Table 9.
Whereas TSUNAMI-1D is limited to considering only one-dimensional geometries, the analysis by DINOSAUR was performed on both a 1-D and 2-D model of the BWR lattice.
A Wigner–Seitz approximation was applied to the square-pitched PB-2 BWR lattice in order to establish one-dimensional equivalent geometries that were used by TSUNAMI-1D and the 1-D calculation by DINOSAUR.
The 2-D DINOSAUR geometry was modeled on the specifications in Table 9 exactly.
Sensitivity profiles (sensitivity per unit lethargy vs. energy) of some parameters of interest are shown in Figs.
11–17.
The cross-section sensitivities calculated by DINOSAUR using DNP closely resemble those of TSUNAMI-1D.
However, TSUNAMI-1D, due to its library having a finer multi-group energy structure in the resonance range – 61 groups to DINOSAUR's 27 in the energy interval between 0.1keV and 1.0keV – often finds series of tall, narrow peaks separated by troughs that are together consolidated into broader yet shorter peaks by DINOSAUR.
This peak-flattening can be seen in the DINOSAUR sensitivity profiles of UXG238, UXG235 and HXS1.
All display similar peak-flattening compared to TSUNAMI-1D due to lower energy resolution, especially at energies above 0.1keV.
The peak-flattening phenomena could be avoided, however, by employing a multi-group library with more energy groups for the DINOSAUR calculations.
The capability of DINOSAUR to calculate implicit sensitivity effects depends on the presence of resonance integrals for self-shielding in the WIMS-D4 library, and the method may be limited to only explicit effects for libraries that have limited data.
For example, the IAEA WIMS-D4 resonance integral tables are not extensive.
In addition to providing no scattering resonance integrals for any isotope, neutron absorption and fission-yield integrals are absent for several isotopes that are often admixed at low dilution and for which self-shielding effects may be important, such as Zirconium absorption and 238U fission at high energies.
Best-estimate predictions of k∞ as computed by both codes using ENDF/B-VII data is shown in Table 10.
The perturbation sizes are listed in Table 11.
More discussion on perturbation sizes is offered in Section 4.3.
Integrated sensitivities – the sum of the dimensionless sensitivity coefficients over all energy groups – of k∞ with respect to all the parameters in Table 11 are shown in Table 12.
The sensitivities in Table 12 are total sensitivities (implicit plus explicit) and were computed with TSUNAMI-1D using both ENDF/B-VI and ENDF/B-VII data, and also by DINOSAUR using only ENDF/B-VII data but applied to both one-dimensional and two-dimensional models of the lattice specified in Table 9.
As shown in Table 12, there is good agreement between DINOSAUR integrated sensitivities (for both 1D and 2D models) and those computed by TSUNAMI-1D.
Interestingly, sensitivities computed by DINOSAUR to the parameters UXF238, UNU238 and UNU235 using ENDF/B-VII data find better agreement when TSUNAMI-1D uses its ENDF/B-VI data rather than it own ENDFB-VII data.
This may indicate small inconsistencies in the ENDF/B data used by the IAEA and ORNL when processing their respective multi-group libraries.
A characteristic of DNP is that some difficulties can be encountered when perturbing input parameters to which the sensitivity of k∞ is especially small, and the selection of an appropriate perturbation, or step, size.
In such cases, perturbations of several hundred or thousand percent may be necessary to effect sufficient change in a response function to compute its partial derivative.
The response, k∞, is calculated by DRAGON to a (default) precision of six decimal places, which consequently demands a minimum perturbation on a cross-section such that the perturbed value of k∞ is distinguishable from the unperturbed value by at least two decimal places so that the difference can be reasonably quantified.
Applied to input variables with especially low k∞ sensitivity, the perturbation demanded by numerical considerations may be so large as to no longer result in a suitable estimation of the partial derivative about the unperturbed value.
This is referred to as a “step-size dilemma” (Martins et al., 2003).
The sensitivity of k∞ of the BWR pin cell to 56Fe XG is shown for perturbations of various magnitudes in Figs.
18–21, to illustrate the effect of perturbation size on the calculated sensitivity.
In general these figures demonstrate that even for cases where extremely low sensitivities exist, and large perturbations are applied, there remains good agreement between the DNP methodology developed here and the adjoint methods within TSUNAMI.
A methodology has been developed to compute lattice reactor physics output sensitivities to input parameters using a direct numerical perturbation approach as well as to propagate parameter covariance.
Perturbations are made to microscopic cross-sections stored in a multi-group library prior to the self-shielding calculations by the lattice solver.
A set of rules have been developed for correcting WIMS-D4 resonance integrals and other parameters, which are derived from perturbations to multi-group cross-sections in order to capture implicit sensitivity effects that arise through resonance self-shielding.
The unique aspect of this work is that it allows for DNP studies of lattice physics uncertainties including the effects of self-shielding, and allowing for the perturbation of all physics parameters as well as operational, composition and geometry Applied to a Boiling Water Reactor fuel cell, DINOSAUR integrated sensitivities and sensitivity profiles show good agreement to those predicted by TSUNAMI-1D for parameters that are well-suited to direct comparison.
This agreement is somewhat expected, as both DINOSAUR and TSUNAMI-1D feature essentially equivalent formalisms for assessing implicit sensitivities, and as both codes were referencing ENDF/B-VII data.
However, some non-trivial disagreement is seen in some integrated sensitivities, in particular for the (n, γ)/XG parameter of 238U.
As the best-estimate values of the multiplication constant differ by approximately 500pcm, some difference in parameter sensitivities must be expected, arising from the best-estimate solution methodology or reference data rather than from the methods for performing sensitivity analysis.
For example, integrated sensitivities suggest some discrepancy in some isotopes between the specific ENDF/B data used between the SCALE and IAEA multi-group libraries employed in this study.
Future work is also planned to include nonlinear interpolation schemes to evaluate the second-order effects apparent in the lookup tables which provide the relationship between the shielding factor and σ0.
Little difference was observed in the sensitivity results when comparing one-dimensional to two-dimensional BWR lattice models.
As the latice cell is almost axisymmetric in the first place, this result is also expected, since the one-dimensional model only features a different outer boundary shape and not changes to the fuel pin or clad shape.
Where one should expect to see value in fully two-dimensional sensitivity analysis is for lattice cells that are ill-suited to the Wigner–Seitz approximation, or in other words, lattice cells that cannot be easily represented by an axisymmetric system.
A CANDU® fuel bundle lattice is a good example of such a system.
From a practical perspective, in contrast to the perturbation theory approach used by TSUNAMI-1D, direct numerical perturbation encounters some difficulty in establishing the local gradient of output responses with respect to especially insensitive parameters, such a cross-sections belonging to cladding alloy materials in low abundance.
Care must be taken when selecting perturbation sizes, as a perturbation that is too small will not sufficiently perturb k∞, and a perturbation that is too large increases the effect of truncating higher-order terms during the computation of the gradient using a finite-difference.
To achieve a sufficiently large change in k∞ so that the difference is observable within the convergence criteria of the lattice solution, perturbations of input parameters of several thousand percent can be necessary.
In such cases, the finite difference approach may no longer lead to a reasonable approximation of the local gradient with respect to the input parameter about its best-estimate value.
For the reactions most relevant to criticality, reaction rates, and collapsed few-group nuclear data, the technique did provide measures of sensitivity in comparison to the adjoint-based alternatives.
Where the methodology differentiates itself, is in its ability to handle the integrated effects of nuclear data, operational variables, composition and geometry, and does so by accounting for the changes in self-shielded cross section as a result of any such changes.
In summary, the main objective of this work was to establish a methodology for computing total sensitivities (implicit plus explicit) of reactor lattice multi-group parameters utilizing existing libraries structures and industrial computational tools such that the result has maximum benefit for nuclear power reactor analysis.
A procedure for the accurate computation of sensitivities is necessary to perform covariance propagation, which is ultimately useful for assessing the confidence associated with practical engineering calculations such as the local power generated by each fuel bundle or fuel channel, reactivity coefficients, end-of-cycle isotope inventories, and so on.
Signatures of material and optical chirality: Origins and measures Chirality in materials and light is of abiding interest across a broad range of scientific disciplines.
This article discusses present and emerging issues in relation to molecular and optical chirality, also including some important developments in chiral metamaterials.
Quantifying the chirality of matter or light leads to issues concerning the most appropriate measures, such as a helicity parameter for specific chiral chromophores and technical measures of light chirality.
An optical helicity and chirality density depend on a difference between the numbers of left- and right-handed photons in a beam.
In connection with circularly polarised luminescence, adoption of the Stokes parameter to spontaneous emission from chiral molecules invites critical attention.
Modern spectroscopic techniques are often based on the different response arising from left-handed circularly polarised light compared to right-handed light.
This dissimilarity can be exploited as a foundation for the separation of chiral molecules, promising new avenues of application.
The properties of chiral molecules have interested scientists for over 150 years, ever since the discovery by Pasteur that there are two forms of tartaric acid [1].
Chiral molecules comprise non-superimposable mirror-image forms, each enantiomer readily identifiable by its rotation of linearly polarised light, in opposite directions for each conformer.
In the natural world, many bioactive molecules are homochiral, one enantiomer predominating for reasons whose origin remains debatable, and which some claim as the basis for life [2–5].
Typically the two enantiomers behave as if physically identical, for example in NMR and IR spectroscopy, but they interact differently with other biological material or synthetic compounds of chiral form.
Similar principles apply to materials that are structured on a mesoscopic scale, where a relatively new field of endeavour has arisen in the construction of metamaterials that can also exhibit chirality through larger, nano- or micro-scale architectures [6].
Since the receptors in human biology mostly consist of chiral molecules, drug action mostly involves a specified enantiomeric form.
This has spurred the development, especially in the pharmaceutical industry, of a host of techniques to secure enantiopure products.
Such methods, mostly multi-step and time-consuming, can typically be cast in one of two distinct categories: synthetic mechanisms designed to produce a single stereoisomer, or separation techniques to isolate distinct enantiomers from a racemic mixture.
A significant drawback, for either approach, is a dependence on a supply of enantiopure reagents or substrates – synthesis routes generally utilise chiral building blocks or enantioselective catalysts [7,8], while enantiomer separation techniques typically incorporate chiral selector molecules to form chemically distinct and distinguishable diastereomeric complexes [8,9].
A key requirement in aiming to achieve enantiopure products, irrespective of the synthetic method, is therefore a means to measure, and duly quantitate the enantiomeric excess – signifying the degree of chirality within molecular products.
Chiral discrimination through optical means is well-known to offer direct, non-contact ways to distinguish between molecules of different handedness, based on observations such as the subtle differences in absorption of left- and right-handed circularly polarised light, or indeed the twisting of polarisation in optical rotation.
Other optical methods, under more recent development, also show some promise to achieve enantiomer separation, as will be introduced later.
In the following sections we first discuss the various measures of chirality, both for matter and for optical radiation.
We then review the principles that underlie manifestations of chirality, both in conventional molecular and supramolecular materials, and in newer forms of metamaterials – some of which can, indeed, display chiral discrimination to an extent far exceeding what can be achieved with molecular substances.
Following this, we then exemplify the optical characterisation of chiral matter by considering the helicity of light that such matter can produce through spontaneous emission, and we go on to discuss the new optical methods of enantiomer separation, before a final summary.
To begin, we briefly review a variety of methods aiming to quantify material chirality.
Most such efforts, aiming to assist the design of asymmetric synthesis and chiral catalysis through the quantification of chirality, have been proposed by computational chemists and physicists, typically employing methods based on chirality functions – focusing on the mathematical properties of molecular geometry and associated symmetry point groups [10,11].
The objective of such assessments is typically to predict, and experimentally verify by optical measurement, both the value and sign of a helicity parameter that is uniquely associated with specific chiral chromophores.
In optical rotation and circular dichroism, the sign of the predicted value is of particular interest as each is recognised as a pseudoscalar measure of chirality, exhibiting identical absolute value, yet opposite sign for each member of a pair of enantiomers.
Methods of quantitating chirality have also been proposed using the principles of structure-activity-relationship (SAR) modelling, in which mathematical association is made between the structure of molecules and their resulting chemical or biological activity [12,13].
Nonetheless, even for any specific enantiomer, it has to be borne in mind that both optical rotation and circular dichroism exhibit dispersive behaviour, such measures of chirality varying with the wavelength of light.
As we have seen, for a system comprising just one chemical component to register its possible chirality, without involving secondary material, generally calls for optical methods.
Although these offer an advantage of immediacy, the extent of chiroptical discrimination is seldom large, because the underlying mechanisms engage not only the electric field E, but also the magnetic field B of the optical radiation.
For later reference, we note that E, B and k (the wave-vector) of light usually form a mutually orthogonal right-handed Cartesian vector set.
Although both electromagnetic fields are related to a vector potential A, through E=−∂A/∂t and B=▿×A, in the UV and visible region most molecules couple strongly only with the former, electric field.
It is therefore worth considering how the chirality of light itself can be quantified: building on original work by Lipkin [14], this is an issue that has attracted significant interest in the last few years [15].
The mechanisms for the engagement of optical chirality with matter will be discussed later, in Section 3.
In a quantum operator formalism, there are two terms that serve as specific technical definitions for the observable chirality of light.
One is the optical chirality density; the other, known as the optical helicity, is the scalar product of operators for the vector potential and the magnetic field, A·B, integrated over all space [16].
Specifically, both have been shown to depend on a difference between the numbers of left- and right-handed photons in the beam [17,18].
Indeed, Bliokh and Nori [19] have shown that the maximum and minimum values of the optical chirality correspond solely to the left- and right-handed circular polarisation light, respectively.
In the case of a unidirectional, monochromatic beam, all such measures prove to be proportional to the net spin, ultimately associated with the spin-1 character of each photon [20].
This establishes a rigorous connection between optical angular momentum and the capacity of light to engage with material chirality.
Indeed, the photon as an elementary particle (compared to massive particles) has a special capacity in this respect: its spin projection in the propagation direction is relativistically frame-invariant, and therefore it can represent an absolute basis for measures of chirality and helicity.
There are still, nonetheless, some unresolved issues connected with formulating suitable corrections for optical angular momentum as light travels through a dielectric material medium.
This problem owes its origin to the Abraham–Minkowski controversy concerning the linear momentum of light in such a medium [21–23], from which the Abraham formulation predicts a modification to the angular momentum of light on entry into a dielectric, while Minkowski does not.
In this debate, Padgett et al.
[24] supports the Abraham formulation, although they acknowledge that earlier work favours the Minkowski form, while Pfeifer et al.
[25] claim that both predictions are, in practice, identical.
Despite complications in the theoretical formalism, there are numerous well-established methods that exploit the differences in response of left- and right-handed enantiomers as polarised light interacts with chiral molecules.
A host of spectroscopic techniques including circular dichroism [26,27], optical rotation [28] and circular polarised luminescence [29] are based upon such disparity, as illustrated by Figure 1.
Complementary techniques have also been developed based on the observation that chiral molecules can confer their optical property onto achiral material, giving rise to induced circular dichroism – the measurement of which proves particularly useful in the study of protein interaction and binding [30,31].
Nonetheless, the quantification of chirality in optical fields cannot be regarded as a direct measure of chirality in the source.
Indeed there is no absolute measure of intrinsic chirality, for molecules – nor is there any conserved chiral property, in the totality of the light and matter system, in either photon absorption or emission.
It has been asserted that, for certain kinds of beam, the measure of helicity might exceed any value attributable to any conventional source of circularly polarised light – resulting in claims for the existence of superchiral light – but recent work has proven that this is not possible [17,18].
Superchirality has been offered as an explanation for the anomalously large signal sometimes reported in circular dichroism studies – as, for example, described by Hendry et al.
[32] in their observations of proteins adsorbed onto chiral metamaterials.
However, this can be explained in terms of the enhanced signal arising from the well-known surface plasmonic amplification effect in systems fabricated with a metal substrate [33,34].
Other claims that a standing wave can be generated with ‘superchiral’ nodes [35], by passing circular polarised light through a chiral film onto a mirror (so that the partially reflected wave interferes with the incident wave), have been shown to be consistent with a mismatch between the electric and magnetic fields of similarly superpositioned waves in the near-field region of a mirror [36].
Although the main interest of this article is molecular chirality, chiral effects have been observed in a range of other compositions such as thin film structures [37–39], with proposed applications including chiral motors [40] and memory effects in chiral domains [41].
However, it is arguably the development of metamaterials that has inspired the most recent resurgence of a broader interest in chirality.
Accordingly it is appropriate to briefly consider such structures, to establish a context for some recent developments.
Metamaterials differ from naturally-occurring materials in that their composite units – which determine the optical response – are not atomic or molecular, but are usually sub-wavelength metallic structures that permit localised plasmonic resonances [42–45].
The latter engage the electromagnetic field with behaviour determined primarily by the size and shape of the fundamental units [46], and in such interactions the arrays of plasmonic sub-units behave as a homogeneous material.
A common example is the split-ring resonator – a pair of sub-wavelength non-magnetic concentric split metallic rings – that is often the basis for a medium with negative refraction [47–51].
Materials with such properties, predicted by Pendry [52] and Tretyakov et al.
[53], have well-publicised potential applications including invisibility cloaking, a perfect superlens [54], and nanolevitators based on a repulsive Casimir force [55].
The description of negative index metamaterials as ‘left-handed’ is potentially misleading, and unrelated to its chirality meaning: in the metamaterial context it signifies that the field vectors E, B and k form a left-handed configuration [56].
Confusion may also arise on using the term ‘chirality’, since this could apply to multiple facets of experiments involving light and metamaterials.
In essence, a structure that is 2D chiral, but which lacks 3D chirality due to its intrinsic mirror symmetry, can acquire the attributes of 3D chirality when a directionality is imposed in the third dimension, as for example by a throughput of circularly polarised light.
This could arise due to the presence of a dielectric substrate in substances such as planar chiral metamaterials (PCMs) [57].
One recent study has demonstrated the chiroptical behaviour of a ‘windmill’-based system comprising gold layers separated by magnesium fluoride, for example [58].
Gammadions, more widely studied, also exhibit optical activity [59] since each circular polarisation state excites different plasmonic modes in the structure, with distinctive resonances and loss factors.
Similarly, achiral nanorods can be assembled on DNA scaffolding in ways to have collective plasmonic circular dichroic response [60]; examples of such structures are shown in Figure 2.
In fact, there are additional effects to be considered when beams of light with a helically structured wavefront are involved.
Such beams, often known as optical vortices, may convey orbital angular momentum [61–63], as well as any spin angular momentum related to circular polarisation – in other words, an angular momentum component that is independent of polarisation states.
Generally, the orbital and spin angular momentum are decoupled in the absence of matter.
With sub-wavelength metallic structures, however, it is possible to tune the spin-orbit coupling of transmitted light beams [64,65].
Similarly, the construction of nanoantenna arrays according to molecular point groups allows direct generation of optical vortices; here, the sub-units must have either chiral symmetry or form PCMs [66].
Furthermore, spiral nano-structures, for example, can excite surface plasmon fields endowed with orbital angular momentum.
Of course, these are evanescent waves, which decay exponentially in the direction of the surface normal; thus the angular momentum transfer is necessarily along the surface [67].
In the present connection with light endowed with orbital angular momentum, it is especially interesting that a vortex beam is shown to induce circular dichroism in non-chiral nanostructures [68].
The measurement of optical intensities of left- or right-handed emission is relatively routine, and chiral materials that exhibit efficient circularly polarised luminescence – a large difference in the two intensities – can provide useful probes of chirality in structures to which they bind [69,70].
Recent developments of theory, addressing differential spontaneous emission, focus upon individual chiral molecules promoted to an electronically excited state, which then decay by photon emission.
It emerges that the various optical measures that might be used to quantify circularly polarised emission require particularly careful application.
One such measure assesses polarisation information in terms of a Stokes vector [71], relating to a formalism in which matrix polarimetry is commonly deployed to determine information such as the structure and relative orientation of chromophores [72,73].
The Stokes vector comprises four distinct parameters relating to wave intensities measured in different polarisation states, and for the chiral systems presently discussed, the most significant of these is S3 – a measure of the difference in right- and left-circularly polarised field intensity.
While S3 represents a seemingly robust means to interpret chiral discrimination in the far-zone regime (where the wavelength of propagating light is much less than the distance between the source and point of detection), its adoption in the near-zone proves inconsistent with the cylindrical symmetry of the emitted radiation.
Results of identical form are determined from both classical and quantum based derivations, their agreement indicating that the non-physical nature of the near-zone result casts doubt upon the short-range validity of the Stokes parameter.
Alternative measures of optical helicity directly addressing the coupling between a molecular emitter and detector show features of the familiar near-zone electric dipole-dipole coupling interaction, consistent with an overall chiral dissymmetry in the coupled source-detector system.
Beyond the ability to simply quantify molecular chirality – even to detect specific enantiomers through microwave spectroscopy [74,75] – there exists a feasible means by which the interaction of an optical input with chiral chromophores can distinguish, and ultimately separate distinct enantiomers from a racemic molecular mixture [76,77].
Chiral molecular systems commonly exhibit selection rules that allow transitions to occur in which an excited state is accessible from the ground state through both electric and magnetic dipole transition moments.
For example, chiral differentiation in circular dichroism relates to a cross-term that contains the pseudoscalar product of the electric and magnetic transition dipoles.
Similarly, optical rotation is dependent on a tensor that also comprises a transition electric and magnetic dipole.
This type of feature can be exploited to secure, by means of a recently proposed mechanism, the optical separation of enantiomers – the basis for which is shown in Figure 3.
In particular, the forward – Rayleigh scattering process engages circularly polarised light, introducing physics that is connected to the optical trapping of chiral molecules.
Semiclassically, this might be described as a manifestation of a dynamic (ac) Stark effect: the optical trap produces an energy lowering of the molecule due to the presence of the irradiating electromagnetic beam.
In detail, it has been shown that circularly polarised light of a specified helicity will produce a different potential energy, and hence a different optical force, when interacting with a left-handed compared to a right-handed enantiomer.
This chiral discrimination provides a physical basis for the optical separation of enantiomers, in which one enantiomer is driven more than the other towards a local intensity maximum (or minimum) of the irradiating beam and, thus, produces a difference in the time-averaged concentrations of the two enantiomers.
Such a differential force is small (with estimates in the 10−16N range for an input laser beam intensity of 5×1011Wcm−2) but experimentally attainable.
A potential physical system is given by Figure 4.
Several other groups have tackled alternative methods of harnessing chiral forces to achieve enantiomer separation.
For example, at the micron scale, an optofluidic experiment to separate chiral materials has been reported by Tkachenko and Brasselet [78], although related nanoscale studies have yet to be realised.
Theory on molecular-level chiral separation is increasingly expansive, and includes notable descriptions by Jia and Wei [79], Canaguier-Durand et al.
[80,81] and Cameron et al.
[82,83].
The extent of interplay between the fields of material and optical chirality is now expanding, and at an accelerating pace, well beyond the most familiarly associated spheres such as optical rotation.
Part of the drive for many of the recent advances can be attributed to developments in metamaterials; advances in theory and in optical technology are also both contributors to the progress.
Taking an overview of high level theoretical and technical advances, it is clear that that much of the current work promises foundations for entirely new realms of application, based on the fundamental property of chirality.
We are grateful to the Leverhulme Trust (DSB, DLA), grant no.
RPG-2012-475, and Engineering and Physical Sciences Research Council (JML, DLA), grant no.
EP/K020382/1, for funding our research.
We also thank an anonymous referee for helpful insights.
Modelling of gas–solid turbulent channel flow with non-spherical particles with large Stokes numbers This paper describes a complete framework to predict the behaviour of interacting non-spherical particles with large Stokes numbers in a turbulent flow.
A summary of the rigid body dynamics of particles and particle collisions is presented in the framework of Quaternions.
A particle-rough wall interaction model to describe the collisions between non-spherical particles and a rough wall is put forward as well.
The framework is coupled with a DNS-LES approach to simulate the behaviour of horizontal turbulent channel flow with 5 differently shaped particles: a sphere, two types of ellipsoids, a disc, and a fibre.
The drag and lift forces and the torque on the particles are computed from correlations which are derived using true DNS.The simulation results show that non-spherical particles tend to locally maximise the drag force, by aligning their longest axis perpendicular to the local flow direction.
This phenomenon is further explained by performing resolved direct numerical simulations of an ellipsoid in a flow.
These simulations show that the high pressure region on the acute sides of a non-spherical particle result in a torque if an axis of the non-spherical particle is not aligned with the flow.
This torque is only zero if the axis of the particle is perpendicular to the local direction of the flow.
Moreover, the particle is most stable when the longest axis is aligned perpendicular to the flow.The alignment of the longest axis of a non-spherical particle perpendicular to the local flow leads to non-spherical particles having a larger average velocity compared to spherical particles with the same equivalent diameter.
It is also shown that disc-shaped particles flow in a more steady trajectory compared to elongated particles, such as elongated ellipsoids and fibres.
This is related to the magnitude of the pressure gradient on the acute side of the non-spherical particles.
Finally, it is shown that the effect of wall roughness affects non-spherical particles differently than spherical particles.
Particularly, a collision of a non-spherical particle with a rough wall induces a significant amount of rotational energy, whereas a corresponding collision with a spherical particle results in mostly a change in translational motion.
Knowledge of the dynamics of turbulent gas–solid flows has a great importance for the successful design and determination of optimum operating conditions of numerous industrial applications, e.g.
pneumatic transport, cyclone separators, fluidised beds, dust collectors, and pulverised-coal combustors to name a few.
These systems exhibit complex flow dynamics and interactions between flow components.
In particular, the complexity of the interaction between particles and gas-phase turbulence (Vreman, 2007) and the effect of particle–particle and particle–wall collisions (Sommerfeld and Kussin, 2003) have stimulated research work in recent years.
Turbulent gas–solid flows have been studied experimentally (e.g.
Snyder and Lumley, 1971; Kulick et al., 1994; Kussin and Sommerfeld, 2002) and numerically.
Numerical simulations can be done in an ensemble-averaged framework, in which the particle properties are represented by their mean or a PDF (e.g.
Simonin et al., 1993; Minier and Peirano, 2001; van Wachem et al., 2001a).
Alternatively, the location and other properties of each individual particle can be tracked, the so-called Lagrangian approach (e.g.
Tsuji, 1993; Tsuji et al., 1992; van Wachem et al., 2001b; Kuang et al., 2008).
With this approach, various frameworks can be used to account for the interaction of the particle with the surrounding fluid.
Most common is the so-called “point-particle” approach, in which an empirical expression is used to estimate the interaction between the fluid and the particle, which is added as a momentum source to the fluid.
A valid empirical relation between the local fluid properties and the interaction forces for the specific particle must exist in order to use this approach.
Moreover, a point-source approach is only valid if the particle is sufficiently small with respect to the Kolmogorov scale of the fluid.
Otherwise, a more detailed coupling algorithm must be used, which takes into account the no-slip condition on the surface of each particle (e.g.
Patankar et al., 2000; Mittal and Iaccarino, 2005; Mark and van Wachem, 2008).
Although this type of coupling is more accurate, it is also computationally very expensive and currently very restrictive in the number of particles it can deal with.
The majority of studies involving gas-particle flows assume that particles are perfect spheres.
This assumption is very convenient because of several factors: perfect spheres are simple to model, their behaviour is well known, and lastly there is a large availability of models in the literature which describe the particle–fluid interactions (e.g.
Fan and Zhu, 1998).
However, assuming the particles are perfect spheres may be unrealistic, because most applications deal with non-spherical particles.
Analysis of flows with non-spherical particles is considerably more complicated than flows with spherical particles.
While a sphere is characterised by its diameter only, even a very simple non-spherical particle like a disc or a fibre needs at least two parameters to be uniquely defined.
This makes the rigid body dynamics of non-spherical particles more complex than the corresponding dynamics of spherical particles.
Moreover, additional complexities arise in describing the interaction of a non-spherical particle with a fluid.
In a uniform flow a sphere experiences only a drag force, whereas a non-spherical body is also affected by a transverse lift force, a pitching torque and a counter-rotational torque.
Moreover, all of these forces acting on a non-spherical body depend not only on the Reynolds number, but also on the angle between the axes of the particle and the direction of the incoming flow.
Additionally, the framework for describing collisions requires a different approach compared to the one used for perfect spheres; for instance, the orientation of the particle must be taken into account.
All of the factors above contribute to the complexity of the investigated problem and are addressed throughout this article.
A comprehensive overview of the available methods to describe the shape, the resulting drag force based on correlations and their associated behaviour of non-spherical particles is presented in Chhabra et al.
(1999), Mandø and Rosendahl (2010).
A common approach to describe the particle shape is by using a so-called “sphericity factor”, Φ (Wadell, 1934).
Sphericity is defined as the ratio of the surface area of a sphere over the surface area of a non-spherical particle with the equivalent volume.
By definition, the sphericity is less than or equal to one.
In most engineering handbooks (e.g.
Crowe, 2005) and papers (e.g.
Hölzer and Sommerfeld, 2008) the drag of a non-spherical particle is estimated from correlations for spherical particles which are modified to take into account the sphericity factor.
The majority of papers concerning the simulation of the behaviour of non-spherical particles use the framework of Brenner (1964) to determine the hydrodynamic drag interaction and Jeffery (1922) to describe the hydrodynamic torque acting on a particle from a flow (e.g.
Marchioli et al., 2010; Marchioli and Soldati, 2013; Njobuenwu and Fairweather, 2013; Zhao and van Wachem, 2013a).
However, both models assume creeping flow and Stokes flow conditions, and are in principle not valid to describe gas-particle flows where there is a slip between the particle and fluid velocity.
Hence, simulations carried out using these models cannot resolve gas-particle flows with non-spherical particles, where there is a slip between the particle and the fluid flow, i.e.
particles with finite Stokes numbers.
In Zastawny et al.
(2012), the development of models for the drag, lift and torques acting on non-spherical particles with a significant slip has been researched by means of true direct numerical simulation.
The term “true” emphasises that not only all the flow scales are resolved but also a no-slip boundary condition is applied at the surface of particle.
As all the existing flow scales are resolved, there are no assumptions required at this scale to capture the interaction of the particles with the fluid flow.
The true direct numerical simulations in this paper are shown to be grid independent, and a large number of simulations have been performed for each particle shape.
Although there is a good agreement from the new drag, lift and torque model with the analytical models of Brenner (1964) and Jeffery (1922), the models show that the behaviour of non-spherical particles at larger slip velocities is quite different from the models put forward by Brenner (1964) and Jeffery (1922).
The most notable difference of the forces on a non-spherical particle in a flow with a significant slip velocity, is the detachment of the flow at the acute edges on the particles.
This is illustrated by a result of the resolved direct numerical simulation shown in Fig.
1.
This figure shows an ellipsoid in a flow with a slip velocity between the particle and the fluid, the Reynolds number based on the slip velocity is 200.
It can be clearly seen that the acute edges of the particle cause the flow to separate.
This leads to high pressure regions near these points of detachment, as is indicated by the colours of Fig.
1.
This was also confirmed in Hölzer and Sommerfeld (2008).
These high pressure regions cause a net fluid torque to act on the particle, and as a consequence the particle will rotate until the pressure gradients are of equal magnitude on both sides of the particle.
Thus, the configuration as shown in Fig.
1 is unstable, and the resulting net torque on the particle originates from the difference in pressure gradients on either side of the particle.
This will result in a rotation of the particle in the flow, until the pressure gradients are maximum and of equal magnitude on both sides of the particle.
Hence, a non-spherical particle will tend to maximise its drag once there is a slip velocity between the particle and the fluid.
This is also commonly observed in nature, as described for instance in Hoerner (1965): leaves that fall from a tree do not fall as fast as possible, but maximise their drag and their falling time.
There are numerous other examples of this in nature.
In studying the orientation of non-spherical particles with a slip velocity between the particle and the fluid, the application of the drag and torque model as put forward by Brenner (1964) and Jeffery (1922) is not applicable.
The application of these models will not result in the particles to have a preferred orientation with respect to the mean flow, as has been shown in numerous papers (Marchioli et al., 2010; Zhao and van Wachem, 2013a).
However, as discussed above, for the gas-particle case with a slip velocity between the fluid and the particle, a preferred orientation is to be expected.
The aim of this paper is to investigate the influence of the particle shape on interacting particles flowing in a horizontal turbulent channel flow, for particles with a significant Stokes number.
To achieve this, large eddy simulations (LES) of a horizontal turbulent channel flow laden with five different particle shapes, incorporating the drag, lift and toque model derived in Zastawny et al.
(2012), are performed.
The well-documented horizontal channel flow case described in Kussin and Sommerfeld (2002), who study spherical particles, is used as a reference case.
The measurements in their work was done with phase Doppler anemometry (PDA), to measure the fluid and particle velocity simultaneously.
The numerical framework applied in this paper has been previously validated for spherical particles in Mallouppas and van Wachem (2013).
In that paper, it is shown that the comprehensive discrete element model (DEM) is more accurate in determining the behaviour of the particles in this horizontal gas–solid channel flow that the hard-sphere model.
Moreover, this paper showed that the fluid mechanics are accurately modelled using the LES framework.
In the current paper, this framework is extended to account for non-spherical particles.
The details of the five particles researched in this article are shown in Table 1.
The results of the simulations are compared to experimental and numerical data for spheres and the effect of non-sphericity will be discussed.
Moreover, the effect of wall roughness of the channel walls on the behaviour of the five different particle shapes is researched and is shown to be very important.
In the large-scale simulation of non-spherical particles the true DNS framework, where the flow around each particle is calculated, is not yet feasible because of the large number of particles and the relatively high Re number.
Therefore, a point-source approximation of particles in the flow combined with LES is pursued (e.g.
Portela and Oliemans, 2003).
This approach relaxes the necessity to resolve the flow around each individual particle.
The individual particles are not “seen” by the fluid, but their presence is approximated through momentum source terms and, if applicable, a local volume fraction.
The momentum source terms arise from approximations of the drag and lift forces.
The simulations are four-way coupled, that is the effect of the fluid on the particles, the effect of the particles on the fluid, and the effect of particle–particle and particle–wall collisions are all taken into account.
The individual components of the models are described in the following sections.
The Reynolds number of the flow experimentally examined by Kussin and Sommerfeld (2002) is far too high to make DNS a feasible option.
In Sommerfeld (2003) an empirical velocity profile and velocity fluctuations are assumed.
It is also possible to use the Reynolds averaged Navier–Stokes (RANS) approach to determine the flow, although this requires a significant amount of empirical parameters and a non-trivial treatment of the boundary layer near the wall.
In this research paper, large eddy simulation (LES) is pursued.
In this approach, there are no fitting parameters (Sagaut, 2005) and the resulting filtered momentum equations are(1)∂(ρfv∼jf)∂t+∂(ρfv∼jfv∼if)∂xi=-∂p∼∂xj+∂(τ∼ij)∂xi-∂(τija)∂xi+Sjf+β(f,p)v∼^jf-v∼jpwhere v∼f represents the filtered fluid velocity and v∼p the particle velocity in the Eulerian framework.
The last two terms on the right hand side of Eq.
(1) represent the general source terms, Sjf and the momentum exchange between the fluid phase and the particle phase.
The fluid velocity in the inter-phase momentum exchange term is the undisturbed fluid velocity, therefore it is denoted by v∼^f.
The equations arising from filtering are very similar to the Navier–Stokes equations, except for the addition of one term, describing the behaviour of the sub-grid scale (SGS) stresses, namely τija.
To close the subgrid-scale stresses, two different approaches have been applied: the Smagorisnky model with van-Driest damping near the wall and the dynamic model proposed by Germano et al.
(1991) and Lilly (1992).
Both models are outlined further and the results are compared to each other for horizontal turbulent channel flow in Mallouppas and van Wachem (2013).
In Mallouppas and van Wachem (2013) it is shown that the framework gives an accurate prediction of the flow dynamics; various LES models are compared with each other and with the experimental data.
Although the motion or dynamics of a spherical particle is relatively straightforward (e.g.
van Wachem et al., 2001b), the dynamics of a non-spherical particle are more complicated.
The rigid body dynamics of a non-spherical particle concern its motion and behaviour during one or more collisions.
The ordinary differential equation describing the translational position and velocity are the same as for a spherical particle (Newton’s second law),(2)DvpDt=ap(3)ρpVpap=FD︸drag+FL︸lift+Vpρpg︸gravity+Vp∇P︸Archimedes+ρpVpac︸collisionswhere Vp is the volume of the particle, ρp the density, vp the velocity of the particle in the Lagrangian framework, and ac represents the forces due to collisions, which will be discussed later.
The added mass and history forces are neglected in the equation, as they are not significant in the case studied in this paper.
All the forces are combined in the acceleration term ap.
The position and velocity are solved using the Verlet scheme (Allen and Tildesley, 1989),(4)xp(t+Δt)=2xp(t)-xp(t-Δt)+ap(t)Δt2+O(Δt4)(5)vp(t+Δt)=xp(t+Δt)-xp(t-Δt)2Δt+O(Δt2) The velocity is determined with less accuracy than the position, but this is not essential in this scheme, as the velocity itself is not directly involved in updating the position of the particle; i.e.
Eq.
(4) does not directly depend on the velocity.
The rotational motion of a non-spherical particle is very different compared to that of a spherical particle.
For a non-spherical particle the orientation is important, unlike for a spherical particle.
To derive the rotational equations of motion for a non-spherical particle, it is convenient to introduce two types of Cartesian spaces: body space and world space, see Fig.
2.
For all variables in body space, the superscript b is employed.
All variables without this superscript represent the variables in world space.
Due to the absence of singularity and Gimbal lock problems (e.g.
Evans and Murad, 1977), unit Quaternions are increasingly popular to represent rotation of a non-spherical particle.
General Quaternions do not only change the orientation of a vector, but also scale the length of a vector.
Therefore, the equation for representing rotation cannot be a simple Quaternion multiplication, as the length of the vector could change.
To represent rotation by Quaternions, the length of the Quaternions must be exactly unity.
Rotation without scaling is performed by unit Quaternions, see Eberly (2002), Hoffmann (1978).
Quaternions were first introduced by Sir Hamilton (Hamilton, 1844; Gsponer and Hurni, 1993) in the nineteenth century and are widely used to represent rotation for modelling dynamic systems in the past decades.
They are expressed in a complex number system, consisting of a scalar part and a vector part.
Hence, there are a total of 4 unknowns.
In dynamics, the physical meaning of a Quaternion is to scale the length and change the orientation of a vector (Ibanez, 2001).
A Quaternion is defined by:(6)q=q0,qwhere q0 is the scalar part, and q is the vector part.
A vector s rotated by a pair of unit Quaternions is defined by(7)s′=qsq-1where q is a unit Quaternion, q-1 represents the conjugation of q,(8)q-1=q0,-qand the vector s is interpreted as a Quaternion as s=[0,s], thus with the scalar part of the Quaternion equal to zero.
The multiplication of two Quaternions is defined by the Grassman product,(9)pq=[p0q0-pq,p0q+q0p+p×q] The unit Quaternion q can be directly expressed in a form containing the vector around which the rotation takes place and the angle of the rotation (Betsch and Siebert, 2009; Karney, 2007)(10)q=cosα2,sinα2qˆwhere qˆ is the normalised vector around which the rotation takes place and the angle α indicates the rotational angle.
In the unit Quaternion q, the coefficients q0,q1,q2 and q3 are sometimes referred to as Euler parameters (e.g.
Betsch and Siebert, 2009), which are not independent of each other, as they must always satisfy(11)‖q‖=q02+q12+q22+q32=1 Many integration algorithms do not inherently respect this constraint and explicitly re-normalise the Quaternion after the algorithm is applied, by defining the corrected Quaternion as(12)qˆ=qq02+q12+q22+q32 This is, however, not the same as inherently embedding the unit length of the Quaternion, as expressed by Eq.
(11), into the algorithm itself.
Applying Eq.
(12) affects the relation between the four parameters of the Quaternion, therefore modifying the rotation it represents.
Most research papers applying Quaternions to represent the orientation of non-spherical particles still determine the corresponding rotation matrix explicitly to perform the rotation of vectors and tensors.
Obtaining the rotation matrix requires an inverse relationship between rotation matrices and unit Quaternions and may introduce additional inaccuracies.
Therefore, the current article uses Quaternions only, without the necessity of computing the rotation matrix.
The rotation of a vector by a Quaternion is given by Eq.
(7).
The transformation of second order tensors by unit Quaternions can be expressed as(13)I‾‾′=(q(qI‾‾q-1)Tq-1)TFollowing the above analysis, unit Quaternions can be used to transform vector properties during rotation, but also to transform tensors properties directly.
Accordingly, rotation matrices can be completely replaced by corresponding unit Quaternions only, and the rotation matrix is no longer required.
This will save a significant amount of computer memory (4 instead of 9 floating point numbers per particle), and increase the accuracy introduced by round-off errors, as fewer operations are required.
The equations of motion describing non-spherical rigid particles consist of translational and rotational components.
The position of a particle can be described equally simple in world space and in body space, but for the orientation of a particle the equations are significantly more complex in world space than body space.
To describe the rotation of non-spherical particles, the most common and convenient way is to compute rotational properties of particles in body space and, if required, transform them into world space.
The governing dynamic equations are determined by the angular momentum equations in body space, and the differential equation of the angular momentum is given by(14)Lḃ+ωb×Lb=τbwhere the Lb represents angular momentum, the ωb is angular velocity and τb is torque acting on the body.
The superscript b means the variables are evaluated in the body space framework.
The angular momentum is related to the angular velocity by(15)Lb=I‾‾bωbwhere the second order tensor I‾‾b is the moment of inertia in body space, which is constant for a rigid body.
The time derivative of angular momentum in Eq.
(14) is determined by(16)Lḃ=I‾‾ḃωb+I‾‾bω̇bin which the first item on the right hand side of the above equation is equal to zero, because the tensor I‾‾b is constant.
Therefore, the angular acceleration is given as(17)ωḃ=Ib‾‾-1(τb-ωb×Ib‾‾ωb) The method to numerically integrate the unit Quaternion put forward in this paper approximates the angular velocity with a basic Lie–Euler method.
In this paper, we propose the application of the predictor–corrector direct multiplication (PCDM) method (Zhao and van Wachem, 2013b), which is not based on Taylor series, but applies the predictor–corrector and direct multiplication algorithms.
Firstly, the variables which describe the rotational motion of a particle are transformed into body space from world space at current time level n(18)ωnb=qn-1ωnqn(19)τnb=qn-1τnqn The angular velocity expressed in body space at the mid-point of the next time level, ωn+12b and at a quarter of next time level, ωn+14b, are determined by(20)ωn+14b=ωnb+14ω̇nbδtωn+12b=ωnb+12ω̇nbδtwhere the angular acceleration in body space, ω̇b, is given by Eq.
(17).
The predicted angular velocity at a quarter at next time level in world space, ωn+14, can be directly based on the unit Quaternion qn(21)ωn+14=qnωn+14bqn-1 Then, a prediction of the unit Quaternion at the half time interval, qn+12′, is determined by the velocity ωn+14.
The prime on the variable emphasises that it concerns a prediction of the variable, not its final value.
(22)qn+12′=cos‖ωn+14‖δt4,sin‖ωn+14‖δt4ωn+14‖ωn+14‖qn Using this predicted unit Quaternion qn+12′, the angular velocity ωn+12 at mid-point of next time level in world space is determined by(23)ωn+12=qn+12′ωn+12bqn+12′-1 Then, the corrected unit Quaternion qn+1 at the new time level can be determined as(24)qn+1=cos‖ωn+12‖δt2,sin‖ωn+12‖δt2ωn+12‖ωn+12‖qn Finally, the angular velocity in body space at the new time level can be determined and transformed to the angular velocity in world space,(25)ωn+1b=ωnb+ωḃn+12δt(26)ωn+1=qn+1ωn+1bqn+1-1 The method as outlined above presents a consistent and accurate predictor–corrector direct multiplication (PCDM) method to determine the unit Quaternion representing the orientation of a non-spherical particle and its angular velocity.
Moreover, this method does not use a rotation matrix and does not mix time-levels inconsistently in its final correction (Zhao and van Wachem, 2013b).
In Zhao and van Wachem (2013b) the PCDM method is compared to other numerical integration schemes for non-spherical particles, and is validated using 3 test cases.
This paper showed that the PCDM method has an increased order of accuracy compared to other methods, and conserves both momentum and energy.
Moreover, the method is validated with a number of analytical solutions.
The non-spherical particles are assumed rigid and homogeneous, which implies that the density, ρp, throughout the particles is constant.
The mass of a particle is then given by(27)mp=ρpVp There is no simple equation for calculating the mass and mass centre of a non-spherical particle directly.
However, it is straightforward to do this by first determining the volume of the particle computationally.
The volume is determined by generating points in an imaginary box enclosing the particle.
The ratio of points which fall inside the particle, Np, over the total number of points, N, gives the ratio of the volume of the particle over the volume of the imaginary box, as the total number of tried points becomes very large,(28)VbodyVbox=limN→∞NpN The centre of mass of the non-spherical particle can be found in a similar way, by summing over the positions of the imaginary points which fall inside the particle, rp, (29)xp(t)=limNP→∞1NP∑n=1Nprp,n(t) In order to determine the moment of inertia, a similar method as outlined above is employed, where a different expression is used for the diagonal terms as for the non-diagonal terms,(30)Ip,iib=mpNPlimNp→∞∑n=1Np(rp,n,jb-xp,jb)2+rp,n,kb-xp,kb2(31)(summationoverjandk)Ip,ijb=Ip,jib=mpNP∑n=1Np(rp,n,ib-xp,ib)·(rp,n,jb-xp,jb)∀i≠j The moment of inertia in body space is constant, and is expressed as I‾‾b, and the relation between the moment of inertia in body space and world space is determined by application of Eq.
(13).
The moment of inertia in world space thus depends on the orientation of the non-spherical particle and varies with time.
It needs to be recomputed at every time step by application of Eq.
(13).
At sufficient high particle loadings both particle–particle and particle–wall collisions are important for predicting the behaviour of the flow.
Therefore, all potential collisions must be correctly detected in order to determine their contribution.
Moreover, the particle–wall collisions are required to keep the particles in the domain.
There are various frameworks to describe particle collisions.
In the hard-sphere, or event driven, framework the collisions are dealt with using global conservation of momentum and energy.
In the soft-sphere framework, the dynamics of the actual collision are resolved, using approximations from elasticity theory.
In this paper we consider the soft-sphere approach, thus contact forces and torques are determined for particles which are actually slightly overlapping.
This overlap is a representation for the local deformation, or displacement, and a Hertzian force model can be used to predict the resulting repellent force.
Therefore, each pair of near-neighbour particles is checked for overlap.
This is possible through describing the particle surfaces with a mathematical function (Delaney and Cleary, 2010) or by building the body from spheres (Langston et al., 2004).
In this work, we have pursued the latter approach.
Each body is then filled with a number of overlapping fictitious spheres, typically with varying radii, where the number of fictitious spheres determines the accuracy of the surface representation of the body.
An example is shown in Fig.
3.
This framework allows for a similar contact detection approach as for spherical particles, as described, for instance, in Allen and Tildesley (1989).
The effect of rough walls has shown to be important in a number of gas-particle flows because the particles that collide with a rough wall have a tendency to be re-suspended into the flow more often (Sommerfeld and Kussin, 2004).
In particle-laden horizontal channel flow simulations, neglecting the effect of wall roughness predicts a large number of particles “grazing” the bottom wall.
It is shown experimentally by Kussin and Sommerfeld (2002) that the wall roughness strongly enhances the transverse dispersion of the particles and their fluctuating velocities throughout the channel.
The measurements have also revealed that the wall roughness causes a significant reduction of the mean horizontal velocity of the particles.
Numerical simulations of this flow have also been able to show these effects (Mallouppas and van Wachem, 2013; Konan et al., 2011; Lain and Sommerfeld, 2007).
The most obvious approach to model a rough wall is a deterministic approach, where the wall roughness is resolved.
However, because of the rapidly changing normal of the wall, a fully deterministic approach is quite costly.
Therefore, a stochastic approach to model wall roughness is adopted.
There are a number of stochastic approaches described in the literature (e.g.
Tsuji et al., 1987; Fukagata et al., 2001), the most frequently applied model is of Sommerfeld (1992), later corrected for the so-called shadow effect, in Sommerfeld and Huber (1999).
A stochastic model usually works with a virtual wall concept, which changes the orientation of the wall with angle γ, which is sampled from an experimentally determined distribution of wall roughness.
Using the soft-sphere model, as done in this work, the collision between a particle and the wall is fully resolved.
To account for this deterministic nature of the collision, a novel wall roughness model was derived and validated in Mallouppas and van Wachem (2013).
The results of this novel rough wall model for soft-sphere collisions provides very good results, the same as the rough wall model for hard-sphere collisions as put forward in Sommerfeld and Huber (1999) and the further improvements to account for secondary collision effects by Konan et al.
(2009).
These secondary collision effects are inherently captured by the wall roughness model for soft-sphere collisions.
The algorithm for the rough wall can be summarised for collision with a non-spherical particle as follows:1.When the shortest particle–wall distance is the wall roughness amplitude (taken to be 10% of the particle diameter) one virtual wall is generated at the point of the particle which is closest to the wall.
The virtual wall is generated with the original algorithm Konan et al.
(2009), Sommerfeld and Huber (1999).
This virtual wall is locally treated as deterministic, it remains at the location until all integration steps associated with the particle–wall collision are finished.
If the shortest particle–wall distance becomes half of the distance at which the virtual wall was inserted, i.e.
the particle has moved closer to the wall, a second virtual wall is introduced, with a newly randomly sampled angle.
This is shown in Fig.
4.
The addition of new virtual walls is repeated until the particle is moving away from the wall.
The required standard deviation for the normal distribution is taken from the experimental data provided by Kussin and Sommerfeld (2002).
In the analysed flow, up to three virtual walls are required to deal with the rough wall collision, although almost all collisions are dealt with by application of a single rough wall.
The soft-sphere collision model is applied to resolve particle–particle and particle–wall collisions.
The comparison between the hard-sphere and the soft-sphere model for spherical particles has been presented in Mallouppas and van Wachem (2013), showing that both are in good agreement for the channel flow conditions as studied in the current work.
However, the hard-sphere model is not suitable for non-spherical particles and therefore a soft-sphere model has to be adopted.
The soft-sphere model essentially determines the slight overlap, or the displacement, of two particles or a particle and a wall.
This overlap is used as a measure to estimate the local deformation of the particle at the point of collision, by assuming the contact point is locally axi-symmetric with a constant local radius, and leads to normal and tangential forces based upon Mindlin and Deresiewicz (1953),Fn(t)=Kn(t)δn32(t)n(t)Ft(t)=minμFn(t),Kt(t)δt(t)where μ is the coefficient of friction, δn(t) is the scalar representing the normal displacement, δt(t) is the vector representing the total tangential displacement mapped onto the current reference frame.
The tangential displacement vector is determined by integrating the successive tangential displacements and mapping this into the current frame of reference of the collision.
Kn and Kt are the spring constants for the normal and tangential forces respectively, as predicted Hertzian contact theory (Mindlin and Deresiewicz, 1953)Kn,l(t)=43E∗r(t)Kt(t)=8G∗r(t)δt(t)where E∗ represents the Young’s modulus of the pair of colliding particles, G∗ is the ratio of the Young’s modulus and Poisson’s ratio plus one for the pair of colliding particles, r(t) represents the local radius of the particle (the distance from the centre of the particle to the contact point) and the subscript l represents loading, i.e.
the particles moving towards each other.
When the particles move away from each other, the subscript u, representing unloading will be used.
To account for the dissipative nature of the collision, a coefficient of restitution is introduced to determine the spring constant value for unloading, represented by the subscript u, following Walton (1993)(32)e=Kn,uKn,l The total force on the body is determined by adding the gravity force, the fluid force, and summing the force contributions of all collisions of each particle(33)a(t)=g+Ff(t)mp+∑c=contactsFn,c(t)+Ft,c(t)mpwhere mp indicates the mass of the particle, g represents the gravitational acceleration, Ff represents the total interaction force with the fluid, and Fn,c and Ft,c represent the normal and tangential forces from the collision of the particle.
The torque on the body is determined by adding the torque arising from the fluid and the contributions of all collisions of each particle(34)τ(t)=τf(t)+∑c=contactspc-xp(t)×Fn,c(t)+Ft,c(t)where pc is the point of contact of the particle with another particle, and xp is the centre of mass of the particle.
The fluid exerts two types of forces on the particle: drag force in the direction of the flow velocity and a transverse lift force.
Additionally a pitching and counter-rotational torques are present.
These interactions are given by the following equations (Zastawny et al., 2012):(35)FD=CD12ρṽ2π4dp2(36)FL=CL12ρṽ2π4dp2(37)τP=CT12ρṽ2π8dp3(38)τR=CR12ρdp25|Ω|Ωwhere FD are the drag force, FL is the lift force, τP is the pitching torque, τR is the rotational torque, CD,CL,CT and CR are the shape specific force and torque coefficients, ṽ=v^f-vp is the velocity of the particle relative to the local undisturbed fluid velocity, ρ is the fluid density, and dp the equivalent particle diameter, i.e.
the diameter of a sphere with the same volume as the considered particle.
The relative rotation of the particle with respect to the fluid is given by(39)Ω=12∇×ṽ-ωpwith ωp representing the angular velocity of the particle.
The total fluid induced force is determined by adding the drag and lift forces and the total fluid induced torque is determined by adding the two torques.
As all of the considered particles in this paper are axi-symmetric, the force vector therefore consists of two principal components, the drag force acting in the direction of the flow, and the lift force acting in the perpendicular direction of the flow.
Also because of the axi-symmetry of the particle, the effective angle between the flow and the longest axis through the body can be described by a single angle of incidence.
The definition of this angle is shown in Table 1 for each of the particles.
The angle of incidence is determined in body space, by transforming the local fluid velocity from world space to body space.
The fluid velocity as seen by the particle in body space is computed as(40)vb(t)=q(t)v(t)q-1(t) The angle of incidence is determined between the fluid velocity in body space and the xb axis of the particle, corresponding to the unique principle axis of the particle, i.e.
the length of the ellipsoid and the fibre and the thickness of the disc.
Hence, the angle is determined as(41)φ=arctanvshbvlnb2where vshb is the velocity projection on the axis containing the smallest dimension of the body and vlnb is the projection of the velocity on the axis containing the longest dimension.
The consequent aerodynamic forces acting on the body in body space are then defined as(42)FDb=12ρ14πdeq2CD(φ,Re)vbvb(43)FLb=12ρ14πdeq2CL(φ,Re)vb2where the force coefficients depend on the angle of incidence and the local particle Reynolds number.
Note that the lift force above is given as a scalar and is applied to the perpendicular direction of the fluid velocity.
Therefore, the force components in the body space are the sum of drag and lift contributions and for the prolate ellipsoids have the following form,(44)Ff,xb=12·ρ·14πdeq2·CD(φ,Re)·|vb|vxb+FL·sinφ·sign(-vxb)(45)Ff,yb=12·ρ·14πdeq2·CD(φ,Re)·|vb|vyb+FL·cosφ·vybvyb2+vzb2(46)Ff,zb=12·ρ·14πdeq2·CD(φ,Re)·|vb|vzb+FL·cosφ·vzbvyb2+vzb2 The specific expressions for the components are slightly different for the case of the disc, but follow the same idea.
Once the forces are determined in the frame of body space, they are converted to world space and applied to the particle equation of motion.
When determining the torque on the particle, two mechanisms have to be considered.
The first mechanism occurs if there is an oblique angle between the fluid velocity vector in body space and any of the principle axis of the body.
When the position of the centre of pressure on the particle does not coincide with the centre of mass of the particle, a pitching torque will act in the axis perpendicular to the force plane.
As the bodies considered in this paper are all axi-symmetric around the principal xb axis, the contribution of this mechanism to the torque along the direction of the xb axis in body space is always zero.
The other two components for prolate ellipsoids are given by(47)τyb=14ρ14πdeq2dpCT(φ,Re)vbvzbvyb2+vzb2·sign(vxbvzb)(48)τzb=14ρ14πdeq2dpCT(φ,Re)vbvybvyb2+vzb2·sign(vxbvyb) Slightly modified expressions are applied in the case of particles with a disc-like shape, similarly as the correction to the lift force for this particle.
The second mechanism occurs if the body rotates with respect to the fluid framework of motion.
In this case, the torque acts on the particle to counteract the rotation and is proportional to the angular velocity of the particle, as given by Eq.
(38).
In the case of axi-symmetric non-spherical particles it is reasonable to divide the rotation into two components, namely one along the primary axis of symmetry, i.e.
along the axis xb, and one axis perpendicular to this.
Hence, the torque components in body space have the following form:(49)Txb=-CR(φ,Re)ρ2dp25Ωxb2(50)Tyb=-CR(φ,Re)ρ2dp25|Ωyzb|Ωyb(51)Tzb=-CR(φ,Re)ρ2dp25|Ωyzb|Ωzbwhere the angular velocity of the particle relative to the fluid in body space is denoted by Ω, and is determined by transforming Eq.
(39) in body space.
It should be noted that the Saffman nor Magnus lift forces are not considered in the above analysis.
This is the lift force caused by a local fluid velocity gradient over the particle.
As the particles studied in this work have a high Stokes number, this contribution is assumed to be negligible.
The correlations for the drag, lift, pitching torque and rotational torque to predict the forces on the individual particles are taken from Zastawny et al.
(2012).
The equations have been validated for the same particle shapes as used in this research paper and have been determined as a function of angle of incidence (φ), the Reynolds number, and the rotational Reynolds number.
The Reynolds number is defined with the equivalent particle diameter, dp, as(52)Re=ρfudpμfand the rotational Reynolds number is defined using the magnitude of the angular velocity(53)ReR=ρfdp2ωμas the evaluation of the forces occurs in body space, the angular velocity of the particle in body space should be applied.
For the drag coefficient the expression which fits all the particle shapes best is given as (Zastawny et al., 2012)(54)CD(φ)=CD,φ=0o+(CD,φ=90o-CD,φ=0o)sina0φwhereCD,φ=0o=a1Rea2+a3Rea4CD,φ=90o=a5Rea6+a7Rea8where the ai represent empirical parameters given in Table 2 for the various particle shapes.
The lift force considered is the force occurring if the flow is not aligned with one of the axes of symmetry of the particle.
This lift is zero at 0 and 90 degrees.
The expression for the lift force coefficient that describes this phenomenon is given as (Zastawny et al., 2012)(55)CL=b1Reb2+b3Reb4sin(φ)b5+b6Reb7cos(φ)b8+b9Reb10where bi represent empirical parameters given in Table 2 for the various particle shapes.
The pitching torque coefficient, CT, is zero at the angles of incidence of 0 and 90 degrees.
For the pitching torque coefficient, the expression is (Zastawny et al., 2012)(56)CT=c1Rec2+c3Rec4sin(φ)c5+c6Rec7cos(φ)c8+c9Rec10where ci represent empirical parameters given in Table 2 for the various particle shapes.
The rotational torque is given by the expression (Zastawny et al., 2012):(57)CR=r1(ReR)r2+r3(ReR)r4where ri represent empirical parameters given in Table 2 for the various particle shapes.
The form of these correlations is the same for all particle shapes, but the parameters in the correlations depend on the particle shape.
These parameters are all displayed in Table 2.
The drag, lift and torque models as outlined above are the result of many true direct numerical simulations, which were presented in Zastawny et al.
(2012).
In these true direct numerical simulations all the flow structures are resolved, and the interaction of the particles with the fluid is determined without any assumption.
The models have been validated by grid refinement, and match the analytical expression for the drag, lift and torque proposed by Brenner (1964) and Jeffery (1922) for cases in which the Reynolds number approaches zero.
Moreover, the flow patterns and findings of these models are in agreement with the experimental findings of Hoerner (1965) and the computational work of Hölzer and Sommerfeld (2008).
The large-scale simulations are performed in the Eulerian–Lagrangian framework and the predictions are compared to the experimental work of Kussin and Sommerfeld (2002).
In their work, a horizontal channel with a height of 35mm, a width of 175mm and a length of 6m, corresponding to approximately 170 channel heights.
A flow of an air-particle mixture with various particle sizes and mass loadings is introduced in the horizontal direction.
The mass loading is defined as the ratio of the mass particles introduced in the domain with the mass of the fluid (i.e.
air) in the same domain.
This paper focuses on the experimental results obtained by Kussin and Sommerfeld (2002), for the two-phase flow with mass loading, ϕ=1.0, with the particles of 195μm.
At this mass loading both fluid-particle as well as particle–particle interactions are expected to be important.
The experimental Reynolds number considered based on the channel height is 42,585, arising from the average air velocity of Uav=19.7m/s, air density of ρf=1.15kg/m3 and a viscosity of μf=18.62Pas.
The friction Reynolds number based on the half channel height is Reτ=600.
The particles considered are glass beads, ρp=2500kg/m3.
In the simulations, particles are tracked for 47 TL, where TL is the Lagrangian integral time scale of turbulence at the centre of the channel.
The Stokes number of the particles depends on the changing fluid timescale.
The Stokes number is much larger than one, irrespective of location or precise definition.
The simulations are carried out in a three-dimensional domain of 0.175m×0.035m×0.035m, where the X direction corresponds to the direction of the flow and the Y direction is the direction of gravity.
The X and Z directions are taken to be periodic and the lengths of the domain in these directions has been verified to not affect the results.
The domain used for the simulations is sketched in Fig.
5.
The simulations are carried out with the in-house code Multiflow (van Wachem et al., 2012; Mallouppas and van Wachem, 2013; Denner and van Wachem, 2014), which is a fully coupled parallel computational fluid dynamics code based on finite volume discretisation and various types of particle and fluid models.
The the flow is initialised by setting a mean velocity of 19.7m/s based on the Reynolds number.
On top of the mean, synthesised turbulence is added as randomly sampled from a von Karman spectrum, using the Fourier modes of the fully developed turbulent spectrum.
The initial condition does not impose a flow profile; the flow profile is formed as a result of solving the Navier–Stokes equations and enforcing the no-slip condition for velocity at the wall.
The boundary conditions at the walls are set as no-slip conditions.
A constant forcing term is introduced everywhere in the flow domain to keep a constant mass flow rate, ṁ=0.027044kg/s, matching the pressure drop required to overcome the wall shear stress in equilibrium.
The average resulting pressure drop equals the total wall shear stress in the channel.
The pressure is fixed to a reference value on one arbitrary cell face inside the domain.
The pressure on the wall faces is determined by extrapolation from the flow domain.
The particles are introduced uniformly in the domain with a small random slip velocity compared to the local fluid velocity.
The number of particles in the domain is determined by the mass loading and is 24,000 corresponding to a mass loading of ϕ=1.
The computational mesh contains a total of 870,000 computational cells and the wall boundary layer is resolved by 5 mesh poins within the y+=10 layer.
Near the wall a DNS resolution is obtained by using the y-coordinate for the nth gridpoint:(58)yn=ymax121+tanhRnΔyymax-12tanh(12R)where R is a constant set to 7.0 and defines the amount of refinement near the wall, Δy is the average mesh spacing, and ymax=35.0mm, is the channel height.
In addition, in every x+=50 and z+=30,1 mesh point is uniformly added.
Mesh refinement results have been presented in Mallouppas and van Wachem (2013), showing that the applied mesh is sufficiently fine to capture the flow details.
Moreover, spectra of the single-phase flow computations are presented in Mallouppas and van Wachem (2013), showing that all the energetic eddies are captured satisfactorily by the mesh and the assumptions for LES are met.
The results of these simulation are in very good agreement with the experimental results of Kussin and Sommerfeld (2002).
The discretisation of the Navier–Stokes equations is done using a finite volume approach, combined with a second order accurate three point backward Euler time discretisation for the temporal terms and a second order accurate central differencing scheme for the advection term.
The pressure velocity coupling is done in a fully coupled framework, using one outer iteration per time-step.
This outer iteration is solved with typically 4 iterations of the stabilised bi-conjugate gradient stabilised method using incomplete LU decomposition to precondition the linearised matrix.
As the particles move in a Lagrangian framework and the fluid is solved in a fixed Eulerian framework, the coupling between these frameworks requires special attention.
The fluid velocity as determined on the Eulerian mesh must be accurately interpolated to each of the Lagrangian particles.
Some properties of interpolation schemes between the Eulerian and the Lagrangian frameworks are discussed in Franklin and Lee (2010).
A frequently used interpolation scheme is the tri-linear interpolation, which has a number of favourable properties, such as continuity and ease of implementation, but suffers from a strong filtering of higher frequency velocity fluctuations and is probably not suitable for LES.
Therefore, we have used a polynomial spline interpolation, where a property of the fluid at the particle is approximated by(59)ϕf@p=∑n=1N∑i,j,kI,J,Kan,ijkΔxiΔyjΔzkϕf,nwhere the summation over n is over the independent points and the summation over (i,j,k) is over the polynomial integer values, and an,ijk is the constant coefficient corresponding to independent point n and the polynomial powers of (i,j,k) for the three directions.
The number of independent fluid velocity points, N, used to evaluate the spline is 27, and the order of the polynomial used is, therefore (I,J,K)=(3,3,3).
The simulations were performed on the HPC facility of Imperial College London, using 16 cores per simulation.
Simulations of the single-phase flow and simulations of spherical particles have been presented in Mallouppas and van Wachem (2013).
This article will focus on the flow with non-spherical particles.
The simulations with the non-spherical particles take approximately 40h of computational time on 16 cores to achieve steady-state statistics for the particle phase.
It is observed that it takes much longer time for the statistics of the particle phase to become steady than for the fluid phase.
Simulations with non-spherical particles are approximately 30% more computationally expensive than the simulations of spheres.
This is due to the fact that the orientation of the particle has to be resolved, requiring a significantly smaller time-step compared to the spheres.
are steady.
From the initial conditions, simulations have run for 35 TL before starting to sample data, and the sampling is done for a time duration of 7 TL.
TL represents the Lagrangian integral time scale of turbulence at the centre of the channel.
It has been verified that the obtained statistics are steady.
The plane averaged relative concentration for all shapes of particles considered in this study as a function of height in the channel is shown in Fig.
6 for the experiments (spheres) and all the simulations, for both the cases considering the effect of wall roughness and neglecting wall roughness, indicated as smooth walls in the figure.
The relative concentration is calculated by averaging the number of particles in each horizontal plane, and normalising this average throughout the channel.
Simulations without considering the effect of wall roughness show that the particles tend to graze near the bottom of the channel, as the relative concentration is much higher near the bottom than near the top.
The reason for this is that next to turbulent dispersion and particle–particle collisions, which are both very weak phenomena in this case, there is no mechanism to re-suspend the particles back into the bulk of the flow.
This effect is even more pronounced with non-spherical particles as their orientation changes significantly, compared to in the bulk of the flow, as they graze near the bottom of the channel.
Because non-spherical particles in the near wall region align along the wall, the concentration becomes even higher compared to spherical particles.
This effect is strongest for fibres and discs, as can be expected from these strongly elongated shapes.
Including the effect of rough walls in the simulation changes the concentration profiles enormously.
Fig.
7 shows an instantaneous distribution of the location of the fibres for the simulation considering the effect of wall roughness, (a), and the simulation neglecting this effect, (b).
It can be clearly seen that in case (a) the fibres are fairly homogeneously distributed in the channel, whereas for case (b) the fibres tend to flow in the bottom part of the channel.
The wall roughness provides an additional mechanism for transferring momentum from the horizontal direction into the vertical direction.
Even though this effect is not very strong, it is sufficient to enable the spherical particles to move through the channel so that there is almost no concentration gradient in the vertical direction, as observed by the experiment Kussin and Sommerfeld (2002) as well as earlier simulations of spheres, reported in Mallouppas and van Wachem (2013).
There are significant changes when considering the behaviour of non-spherical particles compared to spherical ones.
Fig.
8 shows the relative concentration profiles for all the shapes of particles and the experimental data from Kussin and Sommerfeld (2002) for rough walls only.
For spherical particles, the effect of the wall roughness is sufficient to re-suspend the spheres into the flow.
The collisions with the rough wall provide an additional, stronger, mechanism for converting momentum from the horizontal direction into the vertical direction.
However, for non-spherical particles a significant part of the horizontal momentum is not converted to vertical translational momentum by a collision with a rough wall, but to rotational momentum.
The non-spherical particles tend to rotate more strongly in the near-wall region due to the rough walls, and this rotational momentum does not contribute to re-suspending particles back into the flow.
Therefore, the effect of rough walls is not as pronounced for non-spherical particles as it is for the spheres, as can be clearly seen from Fig.
8.
The particle and corresponding fluid velocities for all shapes of particles including and excluding the effect of wall roughness are shown in Fig.
9.
The velocity of the spheres and the experimental measurements are in very good agreement with each other.
Also, the velocity of the fluid is in good agreement with the experimental measurements.
It is observed that all the non-spherical particles flow faster through the channel than the spherical particles.
This is because the non-spherical particles tend to locally maximise the drag force, by aligning their longest axis perpendicular to the local flow direction.
The fluid profiles are slightly influenced by the particles, as can be seen from the variations in fluid velocity shown in Fig.
9.
However, there is very little effect of the particles on the fluid Reynolds stresses if the flow, as can be seen from Fig.
10.
The plane averaged particle velocities predicted by the simulations including wall roughness are compared to the particle velocities resulting from simulations excluding wall roughness in Fig.
11.
For the simulations including the effect of wall roughness, the particle velocity profiles are more or less symmetrical, but the simulations without wall roughness show a strong asymmetric profile, due to the very strong concentration gradient as earlier shown in Fig.
6, especially for the non-spherical particles.
It can be concluded from Figs.
6, 8, 9, and 11 that ellipsoids 2 almost behave as the spherical particles, which again match very well with the experimental measurements.
The particle concentration as a function of channel height, as well as the particle velocity as a function of channel height are very close to those of the spheres.
This can be seen as a validation of the computational framework, as the sphericity of ellipsoids 2 is near unity, 0.99.
The average angle of attack of the non-spherical particles is shown in Fig.
12.
As confirmed by the average velocities of the particles, shown in Fig.
9, the particles tend to align their longest axis perpendicular to the flow, hereby maximising their local drag.
This results in a high average angle of attack, a high particle flow velocity, and a higher pressure drop, compared to the spheres.
Near the walls, the effect of the collisions can be clearly seen.
A measure for the oscillation or rotation of the particles is the root mean square of the fluctuations of the angle of attack, shown in Fig.
13 as a function of channel height.
The disc-shaped particle has the highest average angle of attack, over 70°, and the lowest root mean square fluctuating angle of attack, so this shape oscillates the least, closely followed by ellipse number 2.
Of all the shapes considered in this paper, the disc-shaped particle shows the most stable flow.
The fibre shaped particle and ellipse number 1 show the lowest average angle of attack, although it is still well above 55°.
However, the root mean square of the fluctuating angles is quite large for both the fibre as well as ellipse number 1.
This is because the fibre shape and the elongated ellipse shape exhibit the least stable flow behaviour compared to the other non-spherical shapes, making these shapes relatively sensitive to fluid velocity fluctuations.
The wall roughness seems to have a small effect on the average angle of the particles, as it de-stabilises the position of the particle a little.
When a non-spherical particle collides with a wall, rotation of the particle is induced.
A particle collision with a rough wall induces slightly more rotation, leading to a smaller average angle of attack.
This can be observed from the difference in average angle of attack between simulations incorporating rough walls compared to where this effect is neglected in Fig.
12.
This is also consistent with the observation that the root mean square of the fluctuating angle of attack is higher in cases incorporating wall roughness as shown in Fig.
13.
This paper describes a complete framework to predict the behaviour of interacting non-spherical particles with large Stokes numbers in a turbulent flow.
A summary of the rigid body dynamics of particles is presented in the framework of Quaternions, showing a novel algorithm to convert tensors from body space to world space as well as a novel algorithm to integrate unit Quaternions efficiently.
This new approach does not rely on the renormalisation of the Quaternion, but uses a method which inherently conserves the unity of the Quaternion in time.
The integration framework for Quaternions has been scrutinised and validated by Zhao and van Wachem (2013b).
To describe the interaction of the non-spherical particles with the fluid, the drag and lift forces and the torque are determined through the closures determined from DNS as described and validated in Zastawny et al.
(2012).
The collisions between the particles themselves and the particles and the walls are also taken into account.
This is achieved by identifying all the contact points of the particles, and determining a collision force assuming a visco-elastic deformation in the contact point.
A subsequent repulsive force is determined acting at the point of contact, leading to a force and a torque on the particle.
Finally, a model to deal with the interaction of rough walls and particles using a visco-elastic approach is applied to non-spherical particles.
The framework is applied to turbulent channel flow, of which the experiments for spherical particles are presented in Kussin and Sommerfeld (2002).
Five differently shaped particles are considered in this flow, each with an equivalent diameter of around dp=195μm and a mass loading of ϕ=1, matching the experimental parameters of Kussin and Sommerfeld (2002).
The channel flow is resolved with a hybrid DNS-LES approach.
The results for single-phase flow and the flow laden with spherical particles is in very good agreement, and has been reported in Mallouppas and van Wachem (2013).
Moreover, the results of nearly spherical ellipsoids, with a sphericity factor of 0.99, are very close to those of the spheres, which serves as an additional validation of the complete framework.
From the results of the flow of non-spherical particles, it is generally observed that non-spherical particles try to locally maximise their drag, although the inertia of the particles and the local fluid velocity fluctuations and gradients prevent this from occurring instantaneously.
This phenomena is further investigated in this paper by performing resolved true direct numerical simulations of a non-spherical particle in a flow.
These results show that in case of slip between the particle and the fluid, two pressure gradients on the acute sides of the particle exist, and cause a net torque if the axis of the particle is not exactly perpendicular to the flow direction.
The only stable orientation of the particle is if the longest axis of the particle is aligned perpendicular to the flow direction.
This phenomenon is not captured by using the torque model of Jeffery (1922), which is used by other research papers, because it is derived for flow situations where there is no slip velocity between the particles and the fluid.
The average angle between the longest axis of the non-spherical particles and the direction of the average flow channel is as high as 70°.
Because of this effect, the non-spherical particles move considerably faster through the channel than the spherical particles.
This also results in a larger pressure drop.
The disc-shaped particle exhibits the most stable flow behaviour, with the highest average angle and the lowest root mean square of fluctuation of angle.
This can be explained by the fact that the acute angle of the disc is the sharpest compared to the other particle shapes.
On the other hand, the elongated fibre shows the least stable behaviour; it shows the most oscillating motion around its axes as it moves through the channel.
The wall roughness has a very important effect on the flow of non-spherical particles, even more so than for spherical particles.
Non-spherical particles that are grazing near the bottom of the channel are returned into the flow by the roughness, but the roughness also induces additional rotation of the non-spherical particles.
This leads to a lower mean angle between the shortest axis of the particle and the average channel flow direction and a higher fluctuating angle.
The authors are grateful to the Engineering and Physical Sciences Research Council (EPSRC) for their financial support (Grant No.
EP/G049262/1).
The authors would like to thank Prof. M. Sommerfeld for providing the experimental data.
The authors are also grateful to the European Cooperation in Science and Technology (COST) Action FP 1005 on “fiber suspension flow modelling” for the fruitful meetings and discussions.
Massively parallel kinetic Monte Carlo simulations of charge carrier transport in organic semiconductors A parallel, lattice based Kinetic Monte Carlo simulation is developed that runs on a GPGPU board and includes Coulomb like particle–particle interactions.
The performance of this computationally expensive problem is improved by modifying the interaction potential due to nearby particle moves, instead of fully recalculating it.
This modification is achieved by adding dipole correction terms that represent the particle move.
Exact evaluation of these terms is guaranteed by representing all interactions as 32-bit floating numbers, where only the integers between −222 and 222 are used.
We validate our method by modelling the charge transport in disordered organic semiconductors, including Coulomb interactions between charges.
Performance is mainly governed by the particle density in the simulation volume, and improves for increasing densities.
Our method allows calculations on large volumes including particle–particle interactions, which is important in the field of organic semiconductors.
Kinetic Monte Carlo (KMC) methods are widely used for simulating the time evolution and equilibrium behaviour of systems of particles.
These methods have proven to be valuable techniques for describing chemical reactions [1], physical systems [2,3], layer growth [4–6] surface reactions [7–9], defect mobility [10,11] and biological networks [12,13].
KMC models systems by calculating all possible transition rates that transform a system from one state into another, followed by randomly picking one of these transitions for execution [14].
Depending on the system under consideration, it may take billions of transitions before steady state is reached.
This computational burden is further exacerbated by the large simulation volumes that are required to reduce measurement errors or by inclusion of particle–particle (p–p) interactions.
These interactions may for instance originate from Coulomb attraction between charges [15] or elastic forces between other particles [16].
Various methods exist for reducing the extra workload due to these interactions.
One method is by applying the fast multi-pole method (FMM) [17], where distant charges are clustered to decrease the number of interactions that need to be calculated [18,19].
A great benefit of this method is the existence of very precise error bounds that allow for a custom choice between precision and speed.
In the work of van der Holst et al.
long range Coulomb interactions are calculated by solving the discrete 1D Poisson equation, which is much faster than calculating direct interactions [20].
Because the short range interactions are still calculated directly, they introduce a method to prevent double counting of charges in the Poisson equation.
For some types of interaction, accurate estimations of the overall interaction potential can be made.
For simulations of hetero epitaxial growth, Schulze et al.
determined an approximate, easy to calculate, upper bound for the elastic energy of a certain configuration [21,16].
This provides an upper limit for the transition rate to the corresponding state.
Once this transition is selected for execution, the real interaction is calculated, and a rejection algorithm determines whether the transition is performed.
Instead of directly reducing the computational effort in calculating the interaction potential, the simulation itself can also be adapted.
In the next reaction method (NRM) [22], transition rates are only updated when the corresponding particle moved.
This reduces the amount of interaction calculations dramatically, but the transition rates become insensible for nearby particle movement.
Another approach for increasing performance is numerically solving an approximation of a master equation [23,24].
Because this type of simulation operates on particles densities rather than on explicit particles, the interaction potential can be solved using a finite differences approach.
However, this type of simulation is not able to capture all physical detailed that can be acquired using a KMC simulation.
An approach to accelerate KMC simulations of large systems is to perform the calculations on parallel computers: the simulation volume is divided into multiple sections that are each treated as a single simulation [25–28].
It is important that the simulation time of all sections remains synchronized.
This can either be achieved by independently running all sections for a fixed period of time [26] or by equalizing all transition rates by introducing null-transitions [27].
Particles that leave their section are simulated in a ghost region, until a periodic synchronization event communicates all particles to their corresponding segments [25].
Rigorous methods use a roll-back procedure to restart the simulation at the time that the boundary transition was received [29,28].
Computationally less expensive non-rigorous algorithms deal with these transitions periodically, introducing an error with respect to the serial algorithm [26,30].
One special type of parallel computer is the General Purpose GPU (GPGPU), an interface that supports thousands of simultaneous threads.
Previous work on lattice based KMC algorithms that were implemented on a GPGPU used a similar approach as the CPU version, but did not consider p–p interactions [31,30].
The work of Plimpton et al.
was one of the first implementations of the parallel KMC code on a GPGPU for simulating thin film growth, and used a similar method as the parallel KMC algorithms discussed above [31].
Another example of GPGPU on lattice based KMC is the simulation framework that was designed by Arampatzis et al.
[30].
In their general approach, they introduce an architecture that supports simultaneous independent execution of segments for a short time span on different processors.
They also provide a thorough error analysis of the non-rigorous parallel KMC algorithm.
An example of a rigorous off-lattice KMC simulation is the work by Andersson et al.
[32].
They described a 2D system of hard disks that can move in free space.
During each iteration, all disks make a trial move.
If the move is accepted by the metropolis acceptance criterion, it is executed.
Boundary conflicts are avoided by rejecting trial moves that leave a cell.
The boundaries of different regions are changed periodically, to allow particle movement throughout the entire volume.
In this work, we study a method for implementing a parallel lattice KMC simulation including long range p–p interactions that runs on a GPGPU.
We use a synchronous method that is similar to the work by Martínez et al.
[33].
Boundary conflicts are solved non-rigorously by using a sublattice approach [34].
We use this method because the inclusion of p–p interactions is computationally expensive, and roll-back mechanisms decrease performance further.
Typically, the bottleneck in GPGPU is caused by copying CPU data back and forth to the GPU interface.
We prevent this by performing the entire procedure on a GPGPU interface.
As discussed above, proper inclusion of these interactions requires segment synchronization after every iteration.
In order to minimize simulation errors due to incorrect values of the interactions potential, segments are synchronized after every iteration.
Although relatively long communication times between remote processors may hinder this process in typical parallel computers, this is not the case for GPGPU architectures.
Still, full recalculation of the interaction potential after each iteration is time consuming.
Instead, the algorithm corrects the current potential by adding dipole contributions for every nearby charge that hopped during the previous iteration.
Full updates of the interaction potential are only required for the grid points that are related to charges that hopped during the last iteration.
Accumulative rounding errors that arise due to repetitive addition and subtraction are solve this by rounding all interaction potentials to a uniformly spaced range of floating point numbers.
Finally, we validate the simulation by modelling charge transport in organic semiconductors [35].
Charge transport through these materials is characterized by an effective mobility μ, that depends on the degree of disorder σ, temperature T, the electric field F and the particle density n. We compared the results with and without Coulomb interactions to numerically solving the corresponding master equation [23], and investigated the performance of the algorithm.
The simulation is implemented on a cubic grid of sites, that contains N particles.
Each particle has transition rates to its six nearest neighbours.
Other transitions are ignored, because the rate expressions in section 3 contain a term that decreases exponentially with distance.
To prevent multiple particles from occupying the same lattice site, rates between occupied sites are set to zero.
The transition rates depend on the energy difference Δϵij=ϵj−ϵi between the initial state i and final state j.
Negative values of Δϵij indicate a decrease in the interaction potential energy of the particle and therefore result in higher transitions rates than positive values of Δϵij.
Effects that contribute to the interaction potential are differences in static site energies, external fields and p–p interactions.
The simulation volume is divided into S spatial segments, where segment si contains Ni particles {pij|j=0...Ni−1} with transition rates {wijk|k=0...5}.
During one simulation step, each segment randomly selects a transition wi with probability wijk/Wi, where Wi=∑j,kwijk.
The dimensions of each segment are arbitrary, but for simplicity we assume them to be cubic and of equal size.
The reason for this is that we expect a homogeneous particle concentration throughout the simulation volume, which results in a constant number of particles in each segment.
Moreover, by setting the segment size to the maximum interaction distance, interacting particles are always in the same segment or in an adjacent segment.
For cases with inhomogeneous concentrations, however, a custom distribution of variously sized segments is also possible.
Boundary conflicts are prevented by using a checker board approach with 8 colours {ci|i=0...7} [32,34].
At the beginning of each iteration, a random sequence of these colours is drawn.
If transitions from segments with different ci result in a conflict, the transition from the segment with lowest position of ci is rejected.
Particles that leave their segment are transferred to a new segment at the end of the KMC cycle.
After the simulation volume has been initialized by randomly placing particles throughout the lattice, the KMC code is executed.
The entire algorithm is executed on the GPU, because copying large volumes of information back and forth between CPU and GPU memory is computationally expensive.
The flowchart of the simulation is shown in Fig.
1.
A CPU function repeatedly starts the main GPU function until a preset amount of simulation time has passed.
This GPU function performs a number of KMC cycles, and has a separate thread i for each segment si.
The sequence of operations is as follows:1.Calculate all wi and Wi.
This is done by starting another GPU function that is explained below.
Determine Wmax=max⁡(Wi) for all segments.
This is done in the main GPU function by a parallel reduction procedure of the S threads.
Randomly select transition wi with probability WiWmax or the null-transition with probability Wmax−WiWmax.
This is again done in the main function, where every thread draws a random number rnd and rejects ri if rnd>WiWmax.
If the null-transition was not selected, mark the locations corresponding to the transition.
This procedure is performed in the main GPU function.
If a location has already been marked by another thread, a conflict occurred.
In case of a conflict, retrieve the position l in the colour permutation of the colour that corresponds to the segment.
The segment with the highest value of l gets to execute its selected transitions.
The other transitions are rejected.
This action is again performed in the main GPU function.
When the null-transition was not selected, perform the transition.
This procedure is performed by a separate GPU function that (i) updates the variables containing the locations of the particle and its possible destinations, (ii) updates the transition rates of the particle, (iii) resets the transition rate of particles that are adjacent to the old particle position, and (iv) resets the transition rates of adjacent particles to the new location to zero.
Increase simulation time with −ln⁡(rnd)Wmax and calculate μ, where rnd∈(0.0,1.0].
This step is performed by only one thread, because the simulation time in all segments is synchronized.
Calculating Wi and selecting wi is the most time consuming part of the simulation (when p–p interactions are not taken into account).
Optimizing this function is important to improve the overall performance.
A single GPU function for calculating wi and Wi would require thousands of inter-communicating threads.
Because our hardware limits this number to 1024, the calculations are split in two levels, as shown in Fig.
2.
The first level operates on sets {wijk|j=32l...32l+31,k=0...5} of 32 transitions, where i refers to the segment and the maximum value of l=Ni−132+1.
For each of these sets, the GPU kernel reduces the transitions to an intermediate sum of transition rates Wilk, and randomly selects a transition wilk′.
The kernel is run in blocks of 256 threads that can simultaneously operate on 8 sets.
A large number of these blocks is required to performs all calculations in the simulation volume.
Before this procedure is started, an array is build that assigns all sets to a block.
A single set of transition rates is processed by 32 parallel threads m∈{0...31} that perform the following tasks:1.All threads load the interaction potentials and site energies from memory.
Thread m calculates transition rate ωm=(wilk)m. Each thread m determines the partial sum σm=∑i=0mωm by using a parallel scan algorithm.
Set the sum of all rates Wilk′=σ31.
Given a random number ci,0∈[0.0,1.0), find m for which σm−1≤σ31⋅ci,0<σm.
This selects the intermediate transition wilk′=wilm.
Only one unique random number is required for the first level calculations of the sets corresponding to the same segment, because in the end only one transition will be selected.
The second level reduces the intermediate transitions and rates into a Wi and wi.
The total number of interactions per block is now reduced by a factor 32.
When segment of 16×16×16 nodes are used, there are 768 intermediate transitions left for the entire simulation volume.
For organic semiconductors, it is safe to assume that the maximum occupancy of each segment is one in eight sides, reducing the number of interactions to 96.
The GPU kernel consists of S blocks of each 96 parallel threads (m,k)∈{(0...15,0...5)} that perform the following tasks:1.Each thread (m,k) loads wimk′ and determines the partial sum σimk′=∑i=0mwimk′ by using an efficient parallel scan algorithm.
Set the sum of all rates Wi=σ31′.
Given a random number ci,1∈[0.0,1.0), determine (m,k) for which σi(m−1)k′≤σ31⋅ci,1<σimk′.
This selects the transition wi=wimk′.
A sufficient condition for the convergence of a KMC simulation is detailed balance:(1)piRij=pjRji Here, pi is the probability that the system is in state i, and Rij is the rate at which the system transform to state j.
Detailed balance is satisfied when the distribution reaches its thermodynamic equilibrium.
In our case, this should only occur when no electric field is applied, as particles do not reach their equilibrium position otherwise.
For the Miller–Abrahams hopping rates that are used in section 3, the hop-rates depend exponentially on the energy difference between the initial and final state.
No matter what sequence of states is traversed, a cyclic travel through a random number of states will always result in a net energy difference of zero.
The product of the hop-rates that correspond to any cyclic interval will therefore always be equal to 1.
This means that Kolmogorov's criterion is satisfied, and that the simulation satisfies detailed balance.
The inclusion of p–p interactions does not change this principle, because the overall energy level of a state is determined by the sum of the static energy level and the interaction energies with surrounding particles.
The energy level is characteristic for the state, and a cyclic traversal through random states will always result in zero energy difference.
Under the condition that boundary conflicts are handled rigorously, the work of Martínez et al.
[33] shows that the same master equation is solved exactly when using a parallel simulation with null-transitions.
Similar work by Martínez et al.
[34] introduces a non-rigorous method to prevent boundary conflicts, and gives a quantification of the error that is introduced by this method compared to a simulation were conflicts are handled rigorously.
The analysis shows that the deviation from a serial simulation decreased with increasing the segment size.
Our approach deals with conflicts in a similar way, and will therefore result in a similar error.
The work in section 3 is based on segments of 4096 sites.
Although the number of particles is lower, the probability that a selected particle is located next to the interface is identical.
Therefore, the error in our simulation is comparable to the case with 4096 particles per processor.
This results in an error that deviates at most 0.5% from the serial simulation (see figure 3 and 4 of [34]).
This is well within the error margins that are important for our applications.
The interaction potential at location x→ due to p–p interactions for a particle at xj→, in a system of N particles located at xi→ is given by(2)Φj(x→)=∑i≠jcj⋅ci|xi→−x→|, where ci resembles the sign of the particle.
Calculating these interactions is a challenging task: the scaling with N2 causes direct summation of the terms to be computationally expensive, even for a relatively low number of particles.
A cutoff radius r that ignores particles for which |xi→−x→|>r reduces the required amount of work, but too small values of r affect the simulation outcome [36].
Our approach also employs a cutoff radius, but further reduces computational work by updating Φj(x→) after a nearby particle moved, instead of by fully recalculating it.
Fig.
3 shows the principle of the approach.
The change in interaction potential of location X with positive cj, is given by a dipole that has a negative pole at the old location and a positive pole at the new location.
Although the locations with a grey point still require a full recalculation using equation (2), the total number of calculations is dramatically reduced.
For each particle, the interaction potential must be calculated at 7 different positions: the current location of the particle as well as its possible destinations.
When considering a single particle transition in a system of N interacting particles, a full update will thus require 7N2 interaction calculations, whilst the dipole update method requires only 9N calculations.
Although the dipole correction method is not new, it has not been applied yet in parallel KMC simulations to the knowledge of the authors.
The spatial ordering of particles and the synchronized nature of our method are beneficial when implementing a parallel version of this method.
The segmentation in cubes with dimensions of the cutoff radius allows fast communication of dipole terms to it's interaction particles.
And the synchronize nature of the simulation causes multiple dipole terms to be processed simultaneously, which is computationally favourable.
The interactions are included into the simulation as follows.
The p–p interactions are calculated after all particle movements have been executed.
For segment i, the following operations are performed (Fig.
4):1.Gather all movements from the surrounding segments.
These are all the contributions that are within the cut-off radius of the charges inside segment i.
Perform the dipole update: iterate through all particles of segment i, and add a dipole contribution for each of the gathered moves if it is within the cut-off radius.
Perform the full update: iterate through the destination locations of all gathered moves, and calculate the contribution of all particles in segment i to the interaction potential moved charges.
Implementing this approach results in problems due to accumulative rounding errors due to repetitive summation and addition of interaction potentials.
These rounding errors originate from the non-uniform interval between consecutive floating point numbers.
We resolve this issue by transforming all interaction terms to a subset of the floating point numbers that does have uniform spacing.
This results in exact calculations, whilst maintaining fast floating point operations.
The subset of choice is the integer range of the floating point numbers: [−222,222].
Single precision floating point number cannot fully represent integers outside this range, and rounding to the set can be done using intrinsic operations.
In order to obtain adequate precision, the maximum possible interaction potential of a particle must map to 222.
This maximum interaction depends on the interaction distance, and the maximum expected particle density.
For the calculations done in the next section on segments of 16×16×16, a cutoff radius of 16 lattice spacings is used.
Assuming a maximum filling of 12.125% and r=16a, the maximum expected interaction is 66.75, which results in a Unit in Last Place of 7.96×10−6.
Even though the calculation of p–p interactions is accelerated by using our method, further increasing the cutoff radius is computationally expensive.
This issue can be resolved by replacing the direct calculations of the most distant interactions by modern approximative schemes like the FMM.
A good candidate is the massively parallel implementation by Lashuk et al.
[19].
Because the impact of distant particle movement on the interaction potential is minimal, the long range approximations need to be updated occasionally.
Although use of the FMM is a good replacement for the direct calculation of very long interactions, nearby particles still require direct evaluation.
We validate our method by modelling the transport of positively charged particles (holes) in disordered organic semiconductors [37].
Charge transport is characterized by phonon assisted hopping of charges between localized sites (Fig.
5) [35].
Each site i has a static, gaussianly distributed, on-site energy ϵ0,i with standard deviation σ.
We assume Miller–Abrahams hopping, because its results are well known from the literature [38–40].
The hopping rate νij for a hole from site i to site j is given by(3)νij=ν0exp⁡(−2αijaij)⋅exp⁡(|Δϵij|+Δϵij2kBT), where ν0 is a constant rate, αij is the inverse localization length, aij is the inter-site distance, kB is the Boltzmann constant and T is the temperature of the system.
For site i, ϵi is given by(4)ϵi=ϵ0,i+eVz,i+e24πϵ0ϵrΦi, where ϵ0 is the vacuum permittivity, ϵr is the relative dielectric constant, e is the fundamental charge, Vz,i is the local electric potential due to an externally applied electric field Fz in the z direction, and Φi is the local p–p interaction term.
The sequence of hopping events in the lattice can be translated into a hole mobility μp, according to(5)μp=ν0σn+−n−Fza⋅N⋅δt.
Here, n+ and n− are the number of charge hops in respectively the positive and negative z direction, during the simulation time interval δt.
The behaviour of μp can be described in terms of the dimensionless degree of disorder σˆ=σkBT, and the dimensionless electric field Fˆ=eFzaσ.
Increasing disorder results in larger energy differences between sites, reducing μp.
Large electric fields decrease the energy difference between adjacent sites, thus increasing μp.
The simulations are performed on a cubic lattice with dimensions of 512a×512a×128a and full periodic boundary condition.
The segment size is set to 16×16×16 lattice sites.
We use a maximum particle density of one particle per ten lattice sites, this is much larger than the typical densities that are found in space charge limited diodes (between one in 100 and 1,000,000).
We are mainly interested in the behaviour when including pp-interaction at large particle concentrations, because these are hard to calculate in a serial version of the algorithm.
As we will discuss later on, the current configuration is not optimized for calculating situation with low particles density.
The hopdistance aij is fixed to a and αij is set to 10a [40].
Energy levels are randomly assigned to all sites, and holes are placed on the sites with the lowest energy levels.
For the Coulomb interactions, we employ a cutoff radius of 16a, as this is adequate for particle densities down to 0.01% [41].
Finally, a relative dielectric constant of 4.0 is used, which is a common value for organic semiconductors.
All calculations were performed on a NVidia k20c board.
First, the results of the simulation without p–p interactions are compared to numerically solving the corresponding master equation [23].
This numerical solution requires the same parameters, but uses a simulation volume of only 128a×128a×128a.
A charge density of 10−3 is used to prevent artifacts due to site correlations [42].
Fig.
6 shows μp versus Fz for different values of σˆ.
The agreement between both methods validates our approach.
Moreover, the difference in simulation volume indicates that finite size effects are not important for the selected density [43].
Next, we consider the effect of p–p interactions on the field dependence of μp.
The range of parameters comprises of typical values that are hard to address using serial KMC: low Fz, high σˆ and high densities.
Fig.
7 contains the field dependence of the mobility for different values of σˆ, with (blue lines) and without (red lines) taking into account Coulomb interactions.
The results for μp including Coulomb interactions deviate slightly from the values without Coulomb interactions: μp reduces for low Fz and increases for high Fz.
This matches the calculations of van der Holst [41] and is contributed to the formation of an unidirectional potential barrier around each charge.
At low fields, this barrier reduces the hopping rates to all adjacent sites, which leads to a slight decrease in mobility.
At high fields, more and more charge hops are down in energy, causing the mobility to approach the values without Coulomb interactions.
Although the effects of coulombic interactions on the mobility are small, including the interactions is crucial for a good understanding of organic light emitting diodes and solar cells.
In general, the materials in these devices possess a low relative dielectric constant (between 3.0 and 4.0).
Therefore, electrons and holes have a large attractive force on each other, and a proper description of generation and recombination processes requires the inclusion of these interactions [44].
We benchmark the simulation by determining the number of KMC iterations per second as a function of the average charge concentration in the device.
The same simulation configuration with segments of 16×16×16 nodes is used as in the validation part.
Fig.
8 shows the number of iterations per second for different particle densities, both including and excluding p–p interactions.
At low densities, the number of iterations per second is limited by the overhead that occurs because the algorithm is run on a GPU.
This effect is even worse for the case including p–p interactions, because additional GPU functions have to be executed.
Insufficient calculations can be done in parallel to compensate for the relatively low serial speed of the GPU.
Under these conditions, a CPU will outperform our algorithm.
This changes as the charge concentration is increased.
The number of iterations per seconds drops, indicating that more time is spent on performing actual calculations.
As more calculations are performed in parallel, the GPU is used for efficiently.
For the case without p–p interactions, the turning point occurs around a concentration of 10−3.
At this point, calculating transition rates and selecting transitions becomes the bottleneck of the simulation.
The efficiency at which these calculations are now executed is higher than when they are performed serially.
For increasing concentrations, this effect becomes stronger, and a GPU is preferred over a CPU.
For the case including p–p interactions, the turning point will occur much earlier, at a concentration of 10−5.
The bottleneck is caused by the calculation of p–p interactions.
Given a fast parallel method for determining the interaction, this means that performing KMC simulations including long range interactions in parallel is more beneficial compared to KMC simulations without these interactions.
Moreover, a fast implementation of the p–p interactions is crucial, and the implementation of very fast routines for the transition rates becomes less important.
We also looked at the parallel efficiency that is obtained for different charge concentrations.
Fig.
9 contains the parallel efficiency versus N for σˆ=0.01 and Fz=0.1.
This figure shows the parallel efficiency for the case without p–p interactions.
The impact is p–p interactions should be limited, because it will only modify the interaction potentials slightly.
Therefore, we only show the parallel efficiency for the case without p–p interactions.
This efficiency is defined as the average fraction of segments where transitions are carried out during each KMC iteration.
Because this number is related to the probability that a null-transition is executed, it will depend on the deviation of the overall transition rates of all segments.
Since the transition rates of charges should be independent of the segment, the deviation in overall rate is expected to decrease for increasing concentrations.
This explains why the parallel efficiency increases with particle density.
Initially, the parallel efficiency is found to increase linearly, this occurs because not all segments contain charge yet.
At concentrations above 2⋅10−4, all segments contain at least one charge on average, and the scaling becomes sub-linear.
The maximum efficiency that we reach occurs at a concentration of 10−1, where 7 out of 10 segment perform transitions simultaneous.
Parallel efficiency is also influenced by σ and Fz.
Increasing σ will cause more disorder in the overall transition rates of segments, which results in relatively larger maximum.
This increases the chance of selecting null-transitions in other segments.
For large Fz, the transition rates in the field direction and against the field direction will become more similar.
This will cause smaller deviations in the overall transition rates of the segments, resulting in higher parallel efficiencies.
Because the parallel efficiency increases with charge density, and the serial efficiency decreases, an optimum exists where the simulation performs best.
The overall efficiency is defined as the product between the parallel efficiency and the number of sequences, and represents the total number of charge moves per second.
Fig.
10 contains the normalized overall efficiency of the simulation, both for the cases including and excluding p–p interactions.
The case including p–p interactions uses the same parallel efficiencies as the case without p–p interactions.
For the current configuration with segments of 16×16×16 nodes, the efficiency shows a maximum at carrier densities of 10−2, regardless of the presence of Coulomb interactions.
This indicates that the current configuration is optimized for relatively high particle concentrations.
For applications involving lower particle concentrations, the simulation can be improved by increasing the segment size.
The procedures for calculating transition rates and p–p interactions operate on groups of 32 particles simultaneously.
When the occupation level of segments becomes too low, these calculations are not performed in parallel anymore.
Given that the cutoff radius remains equal, larger segments prevent this issue, because the number of particles per segment increases.
Also, larger segments reduce the number of segments that is required to span the simulation volume: this lowers execution times.
Finally, using more particles per segment will also increase the parallel efficiency, because the overall transition rates of the segments will increase.
Our method can be further improved by dynamically segmenting the simulation volume, such that an optimal occupancy of all segments is ensured.
In an optimal segmentation, the overall transition rates of all segments are equal, leading to a parallel efficiencies close to 100%.
Alternatively, segments can be setup to contain equal amounts of particles.
This can also be beneficial for implementing approximative schemes like FMM that calculate the interaction potentials.
These improvements also allow efficient simulation of volumes with non-homogeneous particle distributions, for instance in organic solar cells or light emitting diodes.
We have developed a parallel, time synchronous, lattice based Kinetic Monte Carlo simulation that includes particle–particle interactions and runs on a GPGPU board.
Determining transition rates and selecting transitions for execution was done using fast GPU methods, where an eight colour checker board prevents boundary conflicts.
Boundary transitions were propagated after each iteration, providing a good description of particle–particle interactions.
The modifications of the interaction potential due to nearby particle moves were taken into account by adding dipole correction terms, thereby reducing the required number of operations.
Exact evaluation of the interactions obtained by representing all interaction terms as 32-bit floating numbers, where only the integer range between −222 and 222 was used.
We have validated our method by modelling the charge transport in disordered organic semiconductors including Coulomb interactions between charges.
The results were in good agreement with values obtained from numerically solving the approximation of the corresponding master equation.
Performance is mainly governed by the density of particles in the simulation volume.
For low densities, the limited amount of parallel work was unable to gain from the massively parallel architecture.
Performance improved for large densities, both due to better use of the architecture and to increased parallel efficiency.
The method allows calculations on large volumes, which is crucial in the field of organic photovoltaics.
Moreover, the fast evaluation of particle–particle interactions allows simulations with high particle concentration.
Further improvement may be obtained by using a dynamic segmentation algorithm that optimized the parallel efficiency, and accelerates the calculation of transition rates and p–p interactions.
But we leave this addition for future work.
This work is part of the research program of the Foundation for Fundamental Research on Matter (FOM), which is part of the Netherlands Organization for Scientific Research (NWO).
This is a publication by the FOM Focus Group ‘Next Generation Organic Photovoltaics’, participating in the Dutch Institute for Fundamental Energy Research (DIFFER).
Mesh adaptation on the sphere using optimal transport and the numerical solution of a Monge–Ampère type equation An equation of Monge–Ampère type has, for the first time, been solved numerically on the surface of the sphere in order to generate optimally transported (OT) meshes, equidistributed with respect to a monitor function.
Optimal transport generates meshes that keep the same connectivity as the original mesh, making them suitable for r-adaptive simulations, in which the equations of motion can be solved in a moving frame of reference in order to avoid mapping the solution between old and new meshes and to avoid load balancing problems on parallel computers.The semi-implicit solution of the Monge–Ampère type equation involves a new linearisation of the Hessian term, and exponential maps are used to map from old to new meshes on the sphere.
The determinant of the Hessian is evaluated as the change in volume between old and new mesh cells, rather than using numerical approximations to the gradients.OT meshes are generated to compare with centroidal Voronoi tessellations on the sphere and are found to have advantages and disadvantages; OT equidistribution is more accurate, the number of iterations to convergence is independent of the mesh size, face skewness is reduced and the connectivity does not change.
However anisotropy is higher and the OT meshes are non-orthogonal.It is shown that optimal transport on the sphere leads to meshes that do not tangle.
However, tangling can be introduced by numerical errors in calculating the gradient of the mesh potential.
Methods for alleviating this problem are explored.Finally, OT meshes are generated using observed precipitation as a monitor function, in order to demonstrate the potential power of the technique.
The need to represent scale interactions in weather and climate prediction models has, for many decades, motivated research into the use of adaptive meshes [3,34,38].
R-adaptivity – mesh redistribution – involves deforming a mesh in order to vary local resolution and was first considered for atmospheric modelling more than twenty years ago by Dietachmayer and Droegemeier [14].
It is an attractive form of adaptivity since it does not involve altering the mesh connectivity, does not create load balancing problems because points are never created or destroyed, does not require mapping of solutions between meshes [26], does not lead to sudden changes in resolution and can be retro-fitted into existing models.
Variational methods exist which attempt to control resolution in different directions for r-adaptive meshes (e.g.
[23,25]).
Alternatively, the solution of the Monge–Ampère equation to generate an optimally transported (OT) mesh based on a scalar valued monitor function is a useful form of r-adaptive mesh generation because it generates a mesh equidistributed with respect to a monitor function and does not lead to mesh tangling [7].
We will see that the optimal transport problem on the sphere leads to a slightly different equation of Monge–Ampère type, which has not before been solved numerically on the surface of a sphere, which would be necessary for weather and climate prediction using r-adaptivity.
At first glance, r-adaptivity does not look ideal for adaptive meshing of the global atmosphere; Dietachmayer and Droegemeier [14] pointed out that the resulting meshes can be quite distorted which leads to truncation errors and it is not always possible to control the resolution in individual directions, just the total cell size (area or volume); with r-adaptivity, it is not possible, for example, to increase the total number of points around the equator, just re-distribute them [17].
However, if the mesh redistribution starts from a mesh with enough points around the equator, then these points can be redistributed according to transient features of the flow.
Models of the global atmosphere are being developed with accurate treatment of non-orthogonality and which allow arbitrary grid structures [21,29,11,28,39].
The time may therefore be right to reconsider r-adaptive modelling of the global atmosphere.
A powerful form of adaptivity that, like r-adaptivity, retains the same total number of points, is centroidal Voronoi tessellation using a non-uniform density (or monitor) function to control the mesh spacing, using Lloyd's algorithm [32].
Lloyd's algorithm generates smoothly varying, orthogonal, near centroidal isotropic meshes suitable for finite-volume models and is being used by the Model for Prediction Across Scales (MPAS, [35]).
Lloyd's algorithm alters the mesh connectivity meaning that, if it is used in conjunction with dynamic mesh adaptivity, mapping between old and new solutions is needed and there is an additional layer of complexity involved with changing the data structures and moving information between parallel processors.
Also, Lloyd's algorithm is extremely expensive, using an explicit solution to find an equidistributed mesh – an elliptic problem.
The cost per iteration is proportional to the number of points, N, [24] and, in one dimension, the number of iterations is proportional to N [15].
Therefore, overall, the cost is proportional to N2.
Conversely, generating optimally transported meshes using a semi-implicit technique, has convergence independent of the mesh size and the overall cost is proportional to Nlog⁡N [5].
We therefore propose r-adaptivity which uses cheaper mesh generation and fixed data structures associated with the mesh.
In section 2 we describe mesh generation by optimal transport in Euclidean space leading to a Monge–Ampère equation.
We then show how these concepts can extend to mesh generation on the sphere, leading to an equation of Monge–Ampère type.
Existing numerical solution techniques in Euclidean geometry are reviewed in section 3.
In section 4 we describe the new numerical methods for solving the Monge–Ampère type equations, both on a Euclidean plane and on the sphere.
In order to address issues of mesh distortion, a range of diagnostics of mesh quality are presented.
These diagnostics, along with the diagnostics of solution convergence, are described in section 5 and the diagnostics of the meshes generated are presented in section 6.
The meshes generated, both on the plane and on the sphere, are shown and described in section 6 and the meshes on the sphere are compared with centroidal Voronoi meshes generated using Lloyd's algorithm [32] with the same monitor function.
In order to demonstrate the performance of the mesh generation using real data as a monitor function, meshes are generated using a monitor function derived from reanalysis precipitation in section 6.
Final conclusions and recommendations for future work are drawn in section 7.
A mesh is equidistributed with respect to a monitor function when the product of the cell volumes and the monitor function in the cell is constant across all mesh cells.
The equidistribution principle alone does not lead to a well-defined problem for mesh generation.
Indeed this problem is ill-posed in more than 1 dimension and so requires the imposition of an extra constraint.
Budd and Williams [6] introduced optimal transport for mesh generation to find a map from the original mesh (or computational space, Ωc) to the new mesh (or physical space, Ωp).
This technique was further developed by Budd et al.
[7] and extended to 3 spatial dimensions by Browne et al.
[5].
The optimal transport constraint says that the new mesh should be as close as possible to the original mesh – we seek to minimise the distance between the two meshes in a certain measure which we shall discuss.
We write this minimization problem:(1)minx∈Ωp⁡d(ξ,x)2 where d is the distance metric between the two meshes and ξ∈Ωc maps to x∈Ωp.
In Cartesian space [0,1]n this metric can simply be the sum of the Euclidean distance between all of the corresponding points defining the meshes.
Brenier's theorem [4] then tells us that the unique, optimal transport map from x to ξ is the gradient of a convex scalar potential, ϕ, so that the new mesh locations are given by:(2)x=ξ+∇ϕ.
The change in cell volume under the coordinate transform is given by the determinant of the Jacobian of the map, |J(ξ)|=|∇x(ξ)|, the gradient of x with respect to ξ.
Therefore, for equidistribution with respect to a monitor function, m, the new mesh locations should satisfy(3)|∇x|m(x)=c where c is a constant, uniform over space, which will be determined once the numerical method is defined.
Taking the determinant of the gradient of eqn.
(2), we can see that |∇x|=|I+∇∇ϕ|=|I+H(ϕ)| where I is the identity tensor and H is the Hessian.
Consequently, for the mesh to be optimally transported and equidistributed, the mesh potential, ϕ, must satisfy a Monge–Ampère equation:(4)|I+H(ϕ)|m(x)=c.
The presence of the identity tensor in this Monge–Ampère equation will be exploited in the linearisation to create a novel numerical algorithm.
Mesh tangling is caused by a local loss of invertibility of the Jacobian of the map from the original to the transported mesh.
Given that the solution of the Monge–Ampère equation, ϕ, is convex, the determinant of the Hessian of ϕ is positive and hence the Jacobian determinant of the map is positive and thus is invertible and the mesh will not tangle [7].
A naive approach to r-adaptivity on the sphere, S2, would be to map the surface onto the plane, use an established method to solve a mesh redistribution problem on the plane, then map back to the sphere.
As shown in Fig.
1, the desired map T could be written as a composition of mappings as T=g−1∘t∘g.
A map g:S2→R2 must be chosen and an optimal transport map t found.
The boundary conditions for the problem of finding t must be specified, and those boundary conditions would necessarily depend on g. For example in the case where the mapping g is simply the lat-lon decomposition of S2, the boundary conditions for the mesh redistribution problem on the plane will then be periodic in the zonal direction.
In the meridional direction, Neumann boundary conditions would not be appropriate as the poles will not be free to move and they will be mapped back to their original location under g−1.
The Hairy Ball Theorem tells us that there must be at least one fixed point of the map T:S2→S2.
The decomposition T=g−1∘t∘g would then be possible if g maps the fixed points of T to a Neumann boundary of R2.
However the location of the fixed points of T are not known a priori, and hence choosing g appropriately would form a significant problem by itself.
Hence we will seek a direct optimal transport map, T:S2→S2 which will be described in this section.
On the surface of the sphere, we would still like to define an optimally transported mesh satisfying equidistribution:(5)r(ϕ)m(x)=c where r=Vξ/Vx is the ratio of the volumes of the original mesh cells, Vξ, with vertices at positions ξ, and the volumes of the new mesh cells, Vx, with vertices at positions x.
We need to ascertain if unique solutions of (5) exist which minimise the distance between the original and resulting meshes.
On the sphere S2, the appropriate distance metric is the Riemannian distance on the surface of the sphere between all of the corresponding points defining the meshes.
We cannot use Brenier's theorem on the sphere.
Instead, we appeal to the generalised version of Brenier's theorem given by McCann [27], a detailed discussion of which is given in Villani [37].
Definition 1c-convex functionThe c-transform ϕc of a function ϕ:S2→R is defined as(6)ϕc(y)=supξ∈S2⁡{−c(ξ,x)−ϕ(ξ)}.
The function ϕ is said to be c-convex, or cost-convex, if (ϕc)c=ϕ.
Theorem 1(A combination of Theorems 8 and 9 of [27].)
Let M be a connected, complete smooth Riemannian manifold, equipped with its standard volume measure dx.
Let μ,ν be two probability measures on M with compact support, and let the objective function c(ξ,x) be equal to d(ξ,x)2, where d is the geodesic distance on M. Further, assume that μ is absolutely continuous with respect to the volume measure on M. Then, the Monge–Kantorovich mass transportation problem between μ and ν admits a unique optimal transported map T where T pushes forward the measure μ onto ν.
Then, (using classical optimal transport notation):(7)T#μ=ν such that(8)x=T(ξ)=expξ⁡[∇ϕ(ξ)] for some d2/2-convex potential ϕ. Corollary 1There exists a unique, optimally transported mesh on the sphere that satisfies the equidistribution principle.
Moreover, that mesh is defined by a c-convex scalar potential function that satisfies the Monge–Ampère type equation(9)m(expξ⁡[∇ϕ(ξ)])|J(ξ)|=c.
ProofClearly M=S2 satisfies the conditions on M in Theorem 1.
The first probability measure of interest, μ, we define to be the scaled Lebesgue measure such that:(10)dμ=dx∫S2dx.
The target probability measure, μ, is the Lebesgue measure appropriately scaled by the monitor function to be equidistributed, such that:(11)dν=m(x)dx∫S2m(x)dx.
As M=S2 these are trivially compactly supported.
μ is absolutely continuous.
Hence by Theorem 1 we have that there exists a unique solution, T, to the mass transportation problem between μ and ν.
From (8) we can see that any point in the new mesh, x, is defined by the action of the exponential map on the scalar potential, ϕ.
To see that this map, T, will give a mesh that satisfies the equidistribution principle, consider a cell Aξ in the original computational mesh, ΩC with volume Vξ.
The mapping of the cell under T gives the new cell, Ax in the physical mesh Ωp.
As T is a (optimal) transport map, then the integral over a set with respect to the measure μ equals the integral over the image of that set with respect to ν.
Hence:(12)∫Aξdμ=∫Axdν⇒Vξ∫S2dx=∫Axm(x)dx∫S2m(x)dx.
The ratio of the integral of the monitor function over the new cell with the total integral of the monitor function is equal to the proportion of the volume that the original cell occupied in the original mesh.
This is precisely what it means for the monitor function to be equidistributed on a discretised mesh.
Using a change of variables, we have:(13)Vξ∫S2dx=∫Axm(x)dx∫S2m(x)dx=∫Aξm(expξ⁡[∇ϕ(ξ)])|J(ξ)|dξ∫S2m(x)dx where |J(ξ)| is the determinant of the Jacobian of the map T(ξ)=expξ⁡[∇ϕ(ξ)].
As (13) must hold for arbitrary sets Aξ∈ΩC, the following equation of Monge–Ampère type on the sphere results:(14)m(expξ⁡[∇ϕ(ξ)])|J(ξ)|=∫S2m(x)dx∫S2dx=c.
□ Corollary 2The optimally transported mesh on the sphere satisfying the equidistribution principle does not exhibit tangling.
The choice of cost function c to be the squared geodesic distance is crucial to the proof of uniqueness in Theorem 1.
Indeed simply taking c to be the square of the Euclidean distance is not sufficient [1].
The squared geodesic distance is necessary to ensure that the classical twist condition holds, i.e.
T given in (7) is injective and hence is a map.
The injectivity of this map ensures that (8) is locally invertible, i.e.
for each point in the new mesh, x, there is a unique point in the original mesh, ξ, which maps to it – i.e.
mesh tangling is not present.
□ The fully non-linear, second-order, elliptic Monge–Ampère equation is:(15)|H(ϕ(ξ))|=f(ξ,ϕ) for independent variable ξ∈Ω and Ω⊂Rd where ϕ is the (scalar) dependent variable, f is a known scalar function of ξ and ϕ, H=∇∇ is the Hessian (the tensorial gradient of the gradient) and |H| is the determinant of the Hessian.
Froese and Oberman [19] give an excellent review of some numerical methods for solving this equation and this review draws from and adds to their review.
There are two challenging parts to solving the Monge–Ampère equation.
Firstly we need spatial discretisation methods both for the Hessian, H, and for the source term, f (although f is a known function, it can be a function of ϕ or of ∇ϕ, so numerical approximations are necessary).
The spatial discretisation leads to a set of non-linear algebraic equations.
Secondly, the algebraic equations require a numerical algorithm to find solutions.
The discretisation should ensure that the solutions is convex based on a discrete definition of convexity.
We will start by considering the spatial discretisation of the Hessian, H. Budd et al.
[7] used finite differences on a structured, Cartesian grid to discretise the Hessian, a technique that was extended to three dimensions by Browne et al.
[5].
Convexity was ensured by filtering the monitor function and smoothing the non-converged solution.
Oberman [30] describe a finite difference method that uses a wide stencil to calculate the Hessian on a structured Cartesian grid.
This was extended to three dimensions by Froese and Oberman [19].
The wide stencil was needed to ensure monotonicity of the iterative solution to the convex, numerical solution.
Froese et al.
[20] study the slightly different, 2-Hessian equation and describe how they rotate the coordinate system so that the Hessian is diagonal and hence the solution is convex and the discretisation is monotone.
Feng and Neilan [16] approximate the Monge–Ampère equation by a fourth-order quasi-linear equation in order to use mixed finite elements for the spatial discretisation.
Dean and Glowinski [12,13] also use mixed finite-elements on triangulations of the unit plane.
All of these techniques have been used on either 2D or 3D Euclidean geometry.
When solving the Monge–Ampère equation for mesh adaptation, the RHS of eqn (15) depends on ∇ϕ.
Froese [18] pointed out that standard centred differences are not monotone for discretising this term and so used wide stencil finite differences.
Saumier et al.
[33] experimented with second and fourth order centred finite differences and a spectral method for discretising ∇ϕ.
Once the Monge–Ampère equation is discretised in space, it is necessary to solve the resulting non-linear algebraic equations, the part of the method that we describe as the “algorithm”.
Budd and Williams [6], Budd et al.
[7] introduced a parabolic version of the Monge–Ampère equation which is solved by time-stepping, including an implicit relaxation term to smooth the transient solution and to speed up convergence:(16)(I−γ∇2)∂ϕ∂t=(m(∇ϕ)|I+H(ϕ)|)1d where γ is a scalar parameter defining the amount of smoothing applied, ∇2 is the Laplacian operator and d is the number of spatial dimensions.
The time-stepping effectively creates fixed-point iterations but it may be possible to create more convergent iterations, without smoothing towards a uniform mesh.
Benamou et al.
[2] also used fixed-point iterations by linearising the two-dimensional Hessian term with a Laplacian:(17)|H(ϕ)|=12(∇2ϕ)2−ϕxx2+ϕyy22−ϕxy2.
After some manipulation, this results in a Poisson equation which can be solved implicitly with the non-linear terms on the right hand side.
Froese and Oberman [19] describe this as a semi-implicit method and use it to find the starting point for a Newton method.
A Newton method is a common algorithm for solving the algebraic equations [12,19,10].
However the cost and complexity of the Newton method may not be necessary for mesh generation.
In this paper we focus on fixed-point iterations, although the new linearisation proposed may also be beneficial for calculating the first guess for a Newton method.
Equations of Monge–Ampère type have not before been solved numerically on the sphere.
The description of the optimally transported mesh problem using the Monge–Ampère equation relies on properties of Euclidean geometry [7].
The numerical solution technique for the optimal transport problem on the surface of a sphere will be described in section 4.
There are two aspects to solving equations of Monge–Ampère type in order to calculate optimally transported (OT) meshes.
The spatial discretisation describes how to calculate the gradient and the Hessian of the mesh potential, ϕ, from discrete values (in this instance in finite volume cells).
This will convert the PDE into a set of non-linear algebraic equations.
The algorithm describes how to linearise and solve the large set of non-linear algebraic equations.
A fixed-point iteration sequence to solve eqn.
(4) can be found by observing that the linear terms of |I+H(ϕ)| are in fact 1+∇2ϕ where ∇2 is the Laplacian operator (linearising about ϕ=0).
Eqn.
(4) can then be written as fixed-point iterations:(18)1+∇2ϕn+1=1+∇2ϕn−|I+H(ϕn)|+cnm(xn) where n is the iteration number and where:(19)xn=ξ+∇ϕn.
This is simpler than the fixed-point iterations used by Feng and Neilan [16], Benamou et al.
[2] because of the presence of the identity tensor in our Monge–Ampère equation which simplifies the linearisation.
These fixed-point iterations are similar to the solution of the parabolic Monge–Ampère equation by Browne et al.
[5] but could have advantages because the Laplacian term should initially accelerate convergence whereas the Laplacian smoothing used by Browne et al.
[5] was only used to smooth intermediate iterations.
Given suitable spatial discretisations, eqn.
(18) can be solved for ϕn+1 given known values ϕn.
Assuming periodic boundary conditions, for the Poisson equation (18) to have a solution, cn must take the value(20)cn=∑|I+H(ϕn)|Vξ∑Vξm(xn) where Vξ are the volumes of the original, computational mesh cells and the summations are over all cells of the computational mesh.
Without a monotone spatial discretisation, numerical solutions of the Monge–Ampère equation can become non-convex leading to artificial oscillations in the numerical solution and non-convergence [20].
The spatial discretisation described here is not monotone and the numerical solution can become non-convex.
Therefore, in order to improve stability of the fixed-point iteration sequence, the Laplacian terms of eqn.
(18) can be multiplied by a factor, 1+α, where α≥0:(21)(1+α)∇2ϕn+1=(1+α)∇2ϕn−|I+H(ϕn)|+cnm(xn) which clearly has no affect on a converged solution but will alter the convergence of the fixed-point iterations used to find ϕ and can help to keep the numerical solution smooth.
This is a form of under-relaxation and the value of α will be defined in section 4.3.1.
A solution of the Monge–Ampère equation only controls |I+H(ϕ)|, not the individual eigenvalues of I+H(ϕ).
If one of the eigenvalues gets large and the other small, the Laplacian preconditioning will not lead to convergent iterations without the under-relaxation.
In order to solve the optimal transport problem on the sphere, we solve eqn (5) directly rather than eqn (4).
However, in order to define fixed-point iterations, we need to find a linearisation of eqn.
(5).
For small maps, we assume that maps lie on a tangent to the sphere and so eqn.
(5) can be approximated by eqn.
(4).
We then use the same linearisation as in section 4.1.1 and the same fixed-point iteration sequence:(22)(1+α)∇ϕn+1=(1+α)∇2ϕn−r(ϕn)+cnm(xn) where(23)xn=expξ⁡[∇ϕn(ξ)].
No further approximation is needed for r=Vξn/Vxn since the old and new cell volumes can be computed explicitly at every iteration.
The linearisation will now be less accurate than in the Euclidean case due to the curvature of the sphere, so it may be necessary to increase α further to avoid divergence.
We have found by experience, trial and error and by analogy with numerical solution techniques for the rotating shallow-water equations (e.g.
[36]), some desirable properties of the spatial discretisation in order to achieve convergence.
Further work to improve the spatial discretisation and prove convergence is needed.1.The discretisation of |I+H(ϕn)| should be consistent with the discretisation of 1+∇2ϕ otherwise the linearisation will not be close and the iterative solution will not converge quickly.
In this context, consistent means that the trace of the discretised H(ϕ) must be equal to ∇2ϕ, as occurs analytically.
This is only possible when solving eqn (21), not eqn.
(22) since the relationship between r and 1+∇2ϕ is not known numerically.
The spatial discretisation should be at least second-order accurate and the errors should be smooth.
If we have rough truncation errors or first-order accurate truncation errors then truncation errors could lead to mesh tangling.
To avoid grid-scale oscillations in the solution of ϕ, the spatial discretisation should be as compact as possible so that grid-scale oscillations of ϕ are not hidden in the discretisations of |I+H(ϕn)| and m(x).
If the solution, ϕ, is convex or locally convex, then convex cells in the initial mesh should remain convex in the mapped mesh.
This implies that ∇ϕ should have bounded variation.11We postulate that, following the TRiSK discretisation on polygons [36], the divergence of the mesh map on the initial (primal) mesh should be a convex combination of the divergence of the mesh map if it were calculated on a dual mesh (e.g.
a triangulation).
For cell i with faces f∈i, the simplest, most compact discretisation of the Laplacian, suitable for an orthogonal grid, using Gauss's divergence theorem, is:(24)∇i2ϕ≈1Vi∑f∈i∇nfϕ|Sf| where cell i has volume Vi, Sf is the outward pointing normal vector to cell i at face f with area equal to the face area so that |Sf| is the face area and gradient normal to each face is:(25)∇nfϕ=ϕif−ϕi|df| where cell if is the cell on the other side of face f from cell i and |df| is the (geodesic) distance between cell centre i and if in the computational domain.
This simple form ensures curl free pressure gradients (assuming that the curl is calculated using Stokes circulation theorem around every edge of the 3D mesh).
If cell i has centre ξi then df=ξi−ξif in Euclidean geometry.
On the surface of the sphere, |df| is the great circle distance between ξi and ξif.
Locations ξi and ξif, vector Sf and df for cells i and if are shown in Fig.
2(a).
Two approaches are taken to calculate the Hessian.
The first we define a finite-difference approach (which uses both finite volume and finite difference approximations).
The second uses the fact that, in solving the Monge–Ampère equation for mesh generation, we are approximating the change in cell volume by the determinant of the Hessian.
Therefore, rather than calculating a discretised Hessian, we can simply use the change in cell volume, r. This is the geometric approach.
The geometric approach is always used on the surface of the sphere.
For a discretisation of the Hessian consistent with the discretisation of the Laplacian, we use Gauss's theorem:(26)H(ϕ)i=∇∇iϕ=1Vi∑f∈i∇fϕSf where ∇fϕ is the vector gradient of ϕ located at face f of cell i.
The vector gradient, ∇fϕ, is reconstructed from normal components, ∇nfϕ using a least-squares fit which is derived by assuming that ∇ϕ is uniform so that it is first-order accurate on non-uniform meshes.
This approach starts by reconstructing a cell-centred gradient from surrounding normal gradients using a least squares fit:(27)∇iϕ=(∑f∈iSˆfSfT)−1∑f∈i∇nfϕSf.
where Sˆf=Sf/|Sf|.
Next, a temporary value of the vector valued gradient at each face is calculated:(28)∇f′ϕ=λf∇iϕ+(1−λf)∇ifϕ where λf is the coefficient for linear interpolation.
For consistency with the Laplacian, we must have ∇fϕ⋅Sf=∇nfϕ|Sf| which can be enforced with an explicit correction:(29)∇fϕ=∇f′ϕ+(∇nfϕ−∇f′ϕ⋅Sˆf)Sˆf.
The Hessian calculated using eqn.
(26) is not symmetric, as the analytic version would be.
A numerical approximation for calculating H will introduce truncation errors so instead we can simply use the change in cell volume:(30)ri=|I+Hi(ϕ)|=Vi(x)Vi(ξ) where Vi(x) is the volume of the transported mesh cell i and Vi(ξ) is the volume of the original cell.
Volumes are calculated on the surface of the sphere by decomposing every polyhedron (on the original and new meshes) into tetrahedra with curved surfaces which are flat in spherical geometry.
The volumes of these tetrahedra are found using the formula for the area of a spherical triangle.
In order to calculate the mesh map and consequently to calculate m, we must calculate ∇ϕ at the mesh vertices, ∇vϕ.
(This is in contrast to [18,33] who discretise the gradient of ϕ at the same locations where ϕ is stored.)
Ideally, the calculation of ∇vϕ should not produce any non-convex cells and it turns out to be particularly sensitive to the numerical approximation and its stencil.
In section 4.2.3.1 we will describe a large stencil gradient, for which grid-scale oscillations in ϕ can grow which are not seen in ∇vϕ and convergence is slow.
In section 4.2.3.2 we will describe a small stencil gradient which can lead to grid-scale oscillations of ∇vϕ on a hexagonal mesh since the calculation of the gradient does not lead to a gradient with bounded variation.
This leads to locally distorted meshes.
Section 4.2.3.3 describes a compromise; a new Goldilocks stencil that combines the advantages of both large and small.
In order to calculate ∇vϕ at the vertices, ∇fϕ at the faces is calculated using eqn.
(29).
These values are then interpolated onto the vertices using linear interpolation.
On a mesh of squares, four values of ∇fϕ are averaged to calculate each ∇vϕ at a vertex and on a mesh of hexagons, three values of ∇fϕ are averaged to calculate each ∇vϕ.
Including the calculation of ∇fϕ, the reconstruction of ∇vϕ from ϕ uses a stencil of 10 hexagons on a hexagonal mesh and 12 squares on a mesh of squares.
Due to these large stencils, the gradients calculated are smooth even if the ϕ field is not smooth.
Due to the averaging (interpolation) of the gradient from the cell centres to the vertices there will be some consistency between gradients at different vertices and so cells may remain convex.
The vector gradient at each vertex, ∇vϕ, can be reconstructed directly from the normal component of the gradient at the surrounding faces using a least squares fit(31)∇vϕ=(∑f∈vdfdfT)−1∑f∈v(df∇nfϕ) where f∈v is the set of faces which share vertex v. This approximation is exact for a uniform vector field, ∇ϕ, and is consequently first order accurate on an arbitrary mesh.
However on a hexagonal mesh, eqn.
(31) only uses information from three surrounding faces and three surrounding hexagons and the resulting gradients are prone to grid-scale oscillations which can lead to the creation of non-convex cells.
The small amount of information used at every vertex means that neighbouring vertices can have very different gradients.
We therefore need a larger stencil, but not as large as the stencil used in section 4.2.3.1.
On a mesh of squares, ϕ at four squares is sufficient to reconstruct a smooth ∇vϕ to second order.
The Goldilocks stencil should be large enough to calculate a smooth gradient (with bounded variation) but without including averaging which can hide grid-scale oscillations in ϕ.
The stencil used includes the faces which share vertex v and the face neighbours attached by a vertex to those faces (Fig.
3).
The vertex gradient is then reconstructed using a least squares fit:(32)∇vϕ=(∑f∈v′∈f′∈vdfdfT)−1∑f∈v′∈f′∈v(df∇nfϕ) where f∈v′∈f′∈v is the set of faces shown in Fig.
3.
In the least squares fit in eqn.
(32), the central faces are counted three times (making the fit more accurate near the centre, following Weller et al.
[40]).
Exponential maps are used to move vertices on the surface of the sphere.
The direction of the map is given by the direction of ∇vϕ at vertex v (i.e.
the direction is along the great circle in the plane of ∇vϕ).
The distance moved is the geodesic distance |∇vϕ| so that the vertex is rotated around the sphere by an angle |∇vϕ|/a where a is the radius of the sphere.
Spatial discretisation of eqns (21), (22) leads to a set of linear algebraic equations, which can be written as a matrix equation, Aϕ(n+1)=b(n), where ϕ(n+1) is the vector of all of the values of the unknown, ϕ(n+1).
This matrix equation is solved using the OpenFOAM GAMG solver (geometric algebraic multi-grid, [31]) using diagonal incomplete Cholesky smoothing with 50 cells in the coarsest level.
The residual for the solver tolerance is defined as:(33)∑|b−Aϕ|∑(|b|+|Aϕ|) where the sum is over all cells of the mesh (i.e.
over all elements of the vectors b and Aϕ).
For each fixed-point iteration, the values of ϕ from the previous iteration are used as an initial guess for the solution of the matrix equation, so the initial residual should converge to the final residual as the fixed-point iterations converge.
At each fixed point iteration (i.e.
each value of n in eqn.
(21)) the matrix equation is solved with a tolerance equal to the maximum of 0.001 times the initial residual and 10−8.
The matrix equation is not solved all the way to 10−8 at every fixed-point iteration to save computational cost but, when the fixed-point iterations have converged, the initial residual will be less than 10−8.
A weaker tolerance is probably acceptable for mesh generation but we are using a tight tolerance to have more confidence that the numerical method is convergent.
If the initial mesh is Voronoi and it is required that the transported mesh is also Voronoi, then the Voronoi generating points can be moved using eqn.
(2) using the cell centre gradient, reconstructed from the face gradient using volume weighting.
Then the moved generating points can be re-tessellated to create a new Voronoi tessellation.
However, the re-tessellation may not have exactly the same connectivity due to edge swapping in the Delaunay algorithm.
This technique therefore may not be so suitable for r-adaptivity.
When using r-adaptivity, the mesh monitor function (that controls the mesh density) will need to be mapped from the previous mesh onto the new transported mesh so that it can be evaluated when solving eqns.
(21) or (22).
In section 6, we first present results using an analytic monitor function, which is evaluated at the transported mesh cell centres.
We then use observed meteorological data to calculate a monitor function by mapping the data to the computational grid and then apply Laplacian smoothing, as described in section 4.3.2.
Here we describe how α is calculated.
We start by defining the source terms of the eqns (21), (22) to be sn=|I+H(ϕn)|−c/m(xn) and sn=r(ϕn)−c/m(xn) respectively.
For convergence to occur, we would like the source term to decrease relative to the Laplacian term, ∇2ϕ.
Initially, the source term has order 1.
In the tests undertaken, both on the plane and on a sphere, it has been sufficient to keep the ratio of the Laplacian to the source term greater than four and to always ensure that α increases with iteration number, n. So α is set to be:(34)1+αn+1=max⁡(1+αn,4max⁡(1/4,max⁡(|sn|))) Following Browne et al.
[5], we experimented with smoothing the monitor function and this smoothing certainly improved convergence and generated meshes with smoother grading and hence lower anisotropy and skewness and better orthogonality (see section 5).
However the purpose of this work is to describe a robust solution of the Monge–Ampère equation on the sphere for any monitor function.
So smoothing of the monitor function will not be considered for the analytically defined monitor functions.
However, when using meteorological data to define a monitor function, the monitor function is smoothed on the computational grid during each iteration using Laplacian smoothing:m=m′(expξ⁡∇ϕ)+14∇⋅(|df|2∇m′(expξ⁡∇ϕ)) where m′ is the monitor function mapped from the meteorological data onto the physical grid at position expξ⁡∇ϕ and m is the monitor function used in the source terms of eqns.
(21), (22).
The diffusion coefficient used is the square of the mesh spacing, |df| on the computational grid.
Convergence is measured in two ways.
Firstly, convergence is measured by plotting the initial residual (eqn 33) of the matrix equation at every fixed point iteration as a function of iteration number.
This gives an indication of how much the solution is changing for each fixed-point iteration.
Secondly, the convergence of the final solution is assessed by plotting the change in cell area between the initial mesh and the final iteration for every cell in comparison to c/m.
At convergence, these should be equal.
The test cases considered use an axi-symmetric monitor function, m, so this measure is plotted as a scatter plot against distance to the axis of symmetry.
This tells us where the solution is not converging to the required monitor function and also, for solutions using |I+H(ϕn)| instead of r(ϕn) (i.e.
using the finite difference Hessian rather than the geometric Hessian) in the Monge–Ampère equation, it tells us how well |I+H(ϕn)| approximates r(ϕn).
The diagnostics of mesh quality consider cell centres, defined as cell centroids or centres of mass of the moved cells, and face centres, defined in the same way.
These will also use the face area vector, Sx, the normal vector to each face with magnitude equal to the face area and dx, the vector between cell centres either side of a face of the deformed mesh (see Fig.
2(b)).
In order to measure mesh quality, firstly we will consider mesh spacing, |dx|, between adjacent cell centres for each cell face as a scatter diagram as a function of distance to the axis of symmetry (for the axi-symmetric cases).
This informs us about the aspect ratios of the cells since cells with high aspect ratio will give a large scatter of values of |dx| for a given distance to the axis of symmetry.
If the mesh is perfectly equidistributed then cell areas should be given by c/m.
Therefore, for the meshes of quadrilaterals, |dx| will be compared with c/m and for the meshes of mostly hexagons, |dx| will be compared with 2ctan⁡(π/3)/3m.
The second mesh quality diagnostic is non-orthogonality for each cell face which is measured as(35)non-orthogonality=cos−1⁡Sx⋅dx|Sx||dx|.
The third mesh quality diagnostic is the face skewness, measured as the distance, ds, between the face centre and the crossing point between the vector dx with the face, normalised by |dx|:(36)skewness=ds|dx|.
The skewness distance, ds, is shown as a short grey line in Fig.
2(b).
This definition of skewness is a feature of the non-linearities of the map generating the mesh and is different quantitatively and qualitatively from that of Budd et al.
[8] which can be calculated directly the Jacobian of the map.
The skewness metric, Q, from Budd et al.
[8] gives information about isotropy and orthogonality, not face skewness.
Optimally transported meshes are generated in two-dimensional planar geometry to compare with those generated by numerical solution of the parabolic Monge–Ampère equation by Budd et al.
[8].
Next, OT meshes are generated on the surface of the sphere in order to compare with the centroidal Voronoi meshes generated by Ringler et al.
[32] using Lloyd's algorithm.
Finally, OT meshes are generated on the sphere using observed precipitation to define a monitor function.
Meshes are generated on a finite plane, [−1,1]2, using the radially symmetric monitor function used by Budd et al.
[8] defined for each location xi:(37)m(xi)=1+α1sech2(α2(R2−a2)) where xc is the centre of the refined region (the origin for these results), R is the distance of xi to xc and α1, α2 and a control the variations of the density function.
Following Budd et al.
[8] we generate two types of mesh with this monitor function, the first we call the ring mesh using a=0.25, α1=10 and α2=200 and the second the bell mesh using a=0, α1=50 and α2=100, both using periodic boundary conditions for ϕ.
The computational meshes on which the optimal transport problems are solved are uniform grids of 60×60 squares.
The ring and bell meshes generated using both the finite difference and the geometric Hessian on the plane are shown in Fig.
4.
The convergence diagnostics will be presented in section 6.2.
Mesh quality for these meshes was analysed by Budd et al.
[8] and this is not repeated here.
The meshes in Fig.
4 calculated using both Hessian techniques are similar to each other and they are also similar to the meshes generated by Budd et al.
[8].
Fig.
5 on the left shows the initial residual of the matrix solution as a function of iteration number for the calculation of all of the meshes on the plane.
Using the finite difference Hessian, the solution converges rapidly but convergence stalls when using the geometric Hessian.
There are two possible reasons for the stalling.
Firstly, the Laplacian is no longer a good linearisation of the geometric Hessian and secondly, a solution at this resolution may not exist.
Smoothing the monitor function removes the stall in convergence and speeds convergence of all solutions (not shown) since smoothing removes the very abrupt changes in the monitor function.
However this is not the topic of this paper.
The underelaxation factor, 1+α, is shown in the right of Fig.
5.
It never rises above the initial value because the source term never increases above its initial value.
The initial value of 1+α is simply 4max⁡|1−c/m| and so 1+α is independent of the Hessian calculation method.
In order to diagnose how closely the final mesh equidistributes the monitor function, we plot the cell area as a scatter plot for every cell in the mesh as a function of the distance from the axis of symmetry in Fig.
6 in comparison with c/m.
Using the finite difference Hessian, there are discrepancies between c/m and the cell area where the second derivative of c/m is high.
This is because the discrete calculation of the Hessian is not a good approximation of the cell area in these regions, where the derivatives of ϕ are varying rapidly.
However, for the purpose of mesh generation, these discrepancies do not look problematic.
If the OT mesh is smoother than that specified by the monitor function then it could be beneficial.
The meshes generated using the geometric Hessian are more accurately equidistributed with respect to the monitor function, despite the lack of convergence of the initial residual.
Meshes are generated using the geometric Hessian in order to compare with the locally refined centroidal Voronoi meshes generated using Lloyd's algorithm by Ringler et al.
[32].
We use a monitor function given by the square root of the density function of the corrected eqn.
(4) of Ringler et al.
[32]:(38)m(xi)=12(1+γ)(tanh⁡β−‖xc−xi‖α+1)+γ where xc is the centre of the refined region which has a latitude of 30° and a longitude of 90°.
‖xc−xi‖ is the geodesic distance between the points and is computed as cos−1⁡(xc⋅xi).
α and β are in radians and they control the size of the refined region and the distance over which the mesh changes from fine to coarse resolution.
We follow Ringler et al.
[32] and use α=π/20 and β=π/6.
γ controls the ratio between the finest and coarsest resolution and we use γ=(1/2)4, γ=(1/4)4, γ=(1/8)4 and γ=(1/16)4 for meshes with finest mesh spacing factors of 2, 4, 8 and 16 times smaller than that of the coarsest.
Following Ringler et al.
[32], these meshes are referred to as X2, X4, X8 and X16.
The computational meshes are hexagonal icosahedra which consist of 12 pentagons and 10(22n−1) hexagons for n=3,4,5,6.
These quasi-uniform meshes can be referred to as the X1 meshes.
The X1 meshes are not shown but the X1 centroidal Voronoi meshes and the OT meshes are slightly different.
The X1 centroidal Voronoi meshes are generated using Lloyd's algorithm which guarantees that the X1 meshes are nearly centroidal (the Voronoi generating point is co-located with the cell centre) whereas the X1 OT meshes are the Heikes and Randall [22] version of the hexagonal icosahedron, optimised to reduce face skewness.
The X2, X4, X8 and X16 meshes of 2562 cells are shown in Fig.
7 with the ratio between the cell area and the average cell area coloured.
The centroidal Voronoi meshes in Fig.
7 are orthogonal, close to centroidal and the mesh topology (connectivity) is different for all the refinement levels.
(Lloyd's algorithm generates meshes that are centroidal relative to a density function which means that they are not centroidal when the centroid is simply the centre of mass.)
The OT meshes all have the same connectivity and they are centroidal but not orthogonal.
(Orthogonality could be achieved by Voronoi tessellating the meshes, at the expense of centroidality, see section 6.6.)
All of the OT meshes in Fig.
7 have regions of anisotropy in between the fine and coarse regions whereas the centroidal Voronoi meshes remain isotropic and the mesh topology changes between resolutions.
The anisotropy will be investigated further in section 6.5.
Before looking in more detail at the mesh quality in section 6.5, we will examine diagnostics of convergence in section 6.4.
Convergence of the initial residual is shown in Fig.
8 for the X2–X16 meshes of various resolutions.
Convergence is rapid for the X2 and X4 meshes but slows after around 100 iterations, once the non-linearities have grown and the Laplacian is no longer a good approximation for the Hessian and once the exact solutions becomes difficult to achieve at finite resolution.
It appears from Fig.
8 that the number of iterations reduces as mesh size increases.
The underelaxation factor, 1+α, is shown in Fig.
9.
Unlike in the Euclidean case, 1+α does rise after initialisation.
This implies that the maximum of the source term increases before it decreases.
However the initial residual is monotonically decreasing during these early iterations.
This is because the initial residual is a mean over the whole domain whereas 1+α is set from the maximum of the source term.
The convergence of the cell area with the monitor function is shown in Fig.
10 as scatter plots of cell area change as a function of distance to the centre of the refined region.
As occurred in Euclidean geometry, the geometric Hessian gives accurate equidistribution.
Scatter plots of the cell-centre to cell-centre distance, |dx|, as a function of distance to the centre of the refined region are shown in Fig.
11 for the X2–X16 meshes of 2562 cells on the sphere.
This shows that the centroidal Voronoi meshes are close to isotropic whereas the OT meshes have high anisotropy where the second derivative of the monitor function is high.
In particular, a region of anisotropy is indicated by a blue ring for the X4 OT mesh in Fig.
11: there is a wide range of |dx| at the same distance to the centre of the refined region, indicating anisotropy.
This anisotropy could be reduced by smoothing the monitor function.
Unlike the meshes on the plane, the meshes on the sphere are isotropic in the uniformly coarse region, due to the isotropy of the domain relative to the centre of refinement.
This could be an advantage of using r-adaptivity on the sphere over its use in Euclidean geometry with corners.
However the meshes on the sphere still have a bulge in |df| on the edge of the coarse region.
This is not ideal for atmospheric simulations since global errors are often proportional to the largest |dx| [32].
The orthogonality and skewness of the faces of the OT X2–X16 meshes on the sphere are shown in Figs.
12 and 13 in comparison to the centroidal Voronoi meshes.
Lloyd's algorithm with a non-uniform monitor function generates exactly orthogonal, non-centroidal meshes and so for comparison with the OT meshes, the Voronoi meshes are made exactly centroidal at the expense of orthogonality by using the cell centroid as the cell centre rather than using the Voronoi generating point.
Even so, they remain very close to orthogonal in comparison to the OT meshes which have high non-orthogonality where the second derivative of the monitor function is high (for this test case).
In fact the non-orthogonality reaches over 70 degrees for some cells in the X16 mesh.
This is unlikely to be a good mesh for simulation.
This problem will be investigated further in section 6.6.
The OT meshes have less face skewness, ds/|dx|, than the centroidal Voronoi meshes (Fig.
13) which could be advantageous for numerical methods whose errors depend on skewness.
For example, Heikes and Randall [22] described how to optimise orthogonal meshes to reduce skewness for low-order finite-volume discretisations.
The OT X16 meshes presented in sections 6.3–6.5 have some large non-orthogonality at regions where the resolution is changing rapidly (Fig.
12).
The reason for this can be seen more clearly in a zoomed regions of the meshes in the second row of Fig.
14.
The double zoomed plot shows that some of the cells are not convex.
This implies that the calculation of ∇vϕ has in fact not yielded a smooth vector field, despite the development of the Goldilocks stencil with the aim of achieving a smooth ∇vϕ on the smallest possible stencil.
The Goldilocks stencil does give a much smoother ∇vϕ than the small stencil (first row of Fig.
14).
If instead we interpolate ∇ϕ from faces onto vertices which entails the use of the larger stencil (secn 4.2.3.1), the non-convex cells are not generated (third row of Fig.
14).
Alternatively, a Voronoi tessellation can be created using the cell centres of the Goldilocks stencil mesh as generating points (bottom row of Fig.
14).
This also eliminates non-convex cells.
The problem with the large stencil calculation of ∇vϕ is that convergence is slowed and orthogonality is only reduced a little (Fig.
15).
Therefore it is necessary to consider the Voronoi tessellation of the cell centres (final row of Fig.
14).
This modification does not affect the convergence since the Voronoi tessellation is calculated after convergence of the Monge–Ampère solution.
This mesh is insensitive to the calculation of ∇vϕ but is no longer exactly equidistributed because the cell areas change a little (locally) when the Voronoi tessellation is calculated (Fig.
16).
However these area changes are very small and simply smooth out the curve where the monitor function flattens out into the coarse region.
Fig.
16 also shows that the Voronoi version is more orthogonal than the large stencil version, the anisotropy is similar and the skewness is increased.
However, the connectivity may be changed slightly.
In order to demonstrate the numerical solution of the Monge–Ampère type equation using realistic data as a monitor function, meshes are generated based on the daily average precipitation rate from the NOAA-CIRES 20th Century Reanalysis version 2 ([9], http://www.esrl.noaa.gov/psd/data/gridded/data.20thC_ReanV2.html) on 9 Oct 2012.
The numerical solution of the Monge–Ampère equation uses two near uniform hexagonal–icosahedral meshes of 2562 and 10,242 cells.
The re-analysis precipitation ranges from zero to pmax=8.73×10−4 kgm−2s−1.
A strictly positive, non-dimensional monitor function, m, is defined from the precipitation rate, p using:(39)m=p+pminpmax+pmin where pmin=10−5 kgm−2s−1.
The resulting meshes are shown in Fig.
17 (and are highly sensitive to the value of pmin used).
Precipitation clearly could not be used as a monitor function for a dynamically adapting simulation of the global atmosphere since it is strongly resolution dependent.
Instead, monitor functions with less resolution dependency should be developed.
Reanalysis precipitation is used here just as a demonstration of the solution when using realistic meteorological data.
The meshes resolved based on precipitation show excellent refinement along fronts, particularly looking at South America.
The Inter-tropical convergence zone is also refined in the latitudinal direction.
However, based on the limitations of r-adaptivity, the Inter-tropical convergence zone cannot be refined everywhere around the equator in the longitudinal direction.
If this were a requirement, a mesh starting with more points around the equator should be used.
This is the subject of future research.
A technique for generating optimally transported (OT) meshes, solving a Monge–Ampère type equation on the surface of the sphere, has been developed in order to generate meshes which are equidistributed with respect to a monitor function.
Equations of Monge–Ampère type have not before been solved numerically on the surface of a sphere.
We show that a unique solution to the optimal mesh transport problem on the sphere exists and exponential maps are used to create the map from the old to the new mesh.
We introduce a geometric interpretation of the Hessian rather than a numerical approximation which is accurate on the surface of the sphere.
In order to create a semi-implicit algorithm, a new linearisation of the Monge–Ampère equation is proposed which includes a Laplacian term and the resulting Poisson equation is solved at each fixed-point iteration.
To validate the novel aspects of the numerical method, we first reproduce some known solutions of the Monge–Ampère equation on a two dimensional plane and find that the geometric interpretation of the Hessian leads to more accurate equidistribution than a finite difference discretisation.
We also generate OT meshes of polygons on the sphere to compare with the centroidal Voronoi meshes generated by Ringler et al.
[32].
The geometric Hessian accurately equidistributed meshes on the surface of the sphere.
The algorithm is found to be sensitive to the numerical method used to calculate the gradient of the mesh potential (the map to the new mesh) with a compact stencil leading to non-convexity and a large stencil leading to very slow convergence.
The mesh tangling can be eliminated by creating a Voronoi tessellation of the cell centres of the final mesh.
The exact solution of the OT problem on the sphere is c-convex which means that the mesh should not tangle.
A numerical method which reproduces this property will be the subject of future work.
The meshes generated have advantages and disadvantages relative to centroidal Voronoi meshes generated using Lloyd's algorithm.
In principle, OT meshes should be much faster to generate, although we do not yet have timing comparisons.
OT meshes do not change their connectivity with respect to the base, uniform mesh, so these meshes can be used in r-adaptive simulations.
In comparison to centroidal Voronoi meshes, the OT meshes are non-orthogonal and less isotropic but have less face skewness.
In order to overcome the non-orthogonality of OT meshes, the OT technique can be used to generate Voronoi meshes.
Finally, we generate a mesh using a monitor function based on reanalysis precipitation.
This mesh refines smoothly along atmospheric fronts and convergence zones and provides inspiration for using r-adaptivity for global atmospheric modelling.
Suitable monitor functions for r-adaptive simulations is also the subject of future work.
Weller acknowledges support from NERC grant NE/H015698/1 and Browne from NERC grants NE/J005878/1 and NE/M013693/1.
Budd acknowledges support the Pacific Institute for the Mathematical Sciences (PIMS) who have funded his sabbaticals via the PIMS Distinguished Visitor Grant.
Analysis of events related to cracks and leaks in the reactor coolant pressure boundary The presence of cracks and leaks in the reactor coolant pressure boundary may jeopardise the safe operation of nuclear power plants.
Analysis of cracks and leaks related events is an important task for the prevention of their recurrence, which should be performed in the context of activities on Operating Experience Feedback.
In response to this concern, the EU Clearinghouse operated by the JRC-IET supports and develops technical and scientific work to disseminate the lessons learned from past operating experience.
In particular, concerning cracks and leaks, the studies carried out in collaboration with IRSN and GRS have allowed to identify the most sensitive areas to degradation in the plant primary system and to elaborate recommendations for upgrading the maintenance, ageing management and inspection programmes.
An overview of the methodology used in the analysis of cracks and leaks related events is presented in this paper, together with the relevant results obtained in the study.
The integrity of the reactor coolant pressure boundary (RCPB) is important to safety because it forms one of the three defence-in-depth barriers.
For that reason the RCPB is designed and manufactured so as to have an extremely low probability of abnormal leakage, which can be caused by different degradation mechanisms as shown in Fig.
1.
Components of the RCPB are designed to permit periodic inspection and testing of important areas and features to assess their structural integrity.
Leak detection contributes to the prevention of reactor coolant system (RCS) loop breaks by detecting any through-wall cracks that may appear in service before they reach a critical size.
Evaluation of operating experience is a powerful tool for the safety assessment of nuclear power plants (NPP) (Schulz, 1991; Weil and Apostolakis, 2001; Michel, 2012).
When applied to cracks and leaks related events, the analysis aims to find response to critical questions, such as:•How relevant are the events, treated together, to their categorization (design, plant status, component, event cause, etc.)?
What conclusions can be drawn on the safety impact and the corrective measures taken?
What are the lessons learned for each category of event?
What are the recommendations to prevent the repetition of such events?
The EU Clearinghouse on NPP Operational Experience Feedback (OEF) (Noël, 2010; Mühleisen, 2011), https://clearinghouse-oef.jrc.ec.europa.eu/, carries out on a regular basis technical work to disseminate the lessons learned from past operating experience as well as background scientific research in OEF.
Additionally, the EU Clearinghouse is conducting work on exchange of OEF, as well as collaborating with international organisations.
The EU Clearinghouse is managed by the JRC of the European Commission and fosters the collection of operating experience from European nuclear regulators and/or operators, assessing the potential value of lessons learned, and providing support for events relevant for the global OEF to be reported systematically and in consistent manner to the IRS system (IAEA, 2010) operated by NEA/IAEA.11NEA: Nuclear Energy agency of the Organisation for Economic Co-operation and Development (OECD); IAEA: International Atomic Energy Agency.
One of the EU Clearinghouse tasks is to provide topical reports of events with similar features or causes, conducting precursor studies of events at selected European NPPs facilitating the trend analyses and enabling better understanding of the main patterns in operational experience events.
Fig.
2 shows the organisations involved in the EU Clearinghouse and their main deliverables.
This publication is based on the results of the topical study on cracks and leaks related events performed by the JRC in collaboration with IRSN and GRS for the EU Clearinghouse.
Other two independent analyses conducted recently are on events involving emergency diesel generators, see (Kancev and Duchac, 2013) and NPP modifications, (Zerger, 2011; Zerger et al., 2013).
Four different databases were used in this study.
Namely, the IAEA International Reporting System database IRS, the US Licensee Events Reports database LER, the French (IRSN) database SAPIDE and German (GRS) database KomPass.
The screening period runs for 20 years, from 1991 to 2011 for IRS and LER, and from 1990 to 2010 for French and German databases.
After screening, 145 IRS reports and 75 LERs were found to be applicable, to which 129 French event reports and 61 German event reports were added.
The total number of events considered is 409.
To help identify generic recommendations, the events were classified into families and sub-families according to their safety significance.
To evaluate the lessons learned the following approach was applied:(1)Data preparation: extraction of the relevant information from each database.
Categorization of the events according to previously agreed families, such as design, plant status, component, sub-component, event cause and type of detection.
Additional information on safety impact and corrective actions were derived for the analysis.
Screening of the most relevant cracks and leaks events, classified according to families.
Deriving lessons learned for each category of events.
Elaboration of recommendations to prevent the repetition of such events.
The overall process is depicted in Fig.
3.
The event investigation was performed from different angles (by components, sub-components, degradation mechanisms, plant conditions, etc.
), and the study was very exhaustive.
To limit the length of this publication below are presented only some illustrative results classified by component type (pipes, vessel, pressuriser, valves and flanges).
The pipes under pressure in the RCS or connected to RCS are usually made of austenitic or austenitic & ferritic stainless steel.
Most connections are welded.
The pipes may be exposed to various degradation phenomena (diverse hazards, mechanical fatigue, thermal fatigue, stress corrosion, etc.).
Event screening in the databases showed a total of 116 events (33 related to cracks and 83 to leaks).
Three main causes for failure were identified, namely, fatigue, corrosion and the presence of manufacturing defects.
Human factor induced defects proved to have little impact – less than 10% of the cases could be attributed to operation errors.
Fatigue was found being induced by several factors: excessive vibration, pressure shocks and the thermal regime of operating the pipe, as well as by combinations of these factors.
Corrosion was induced, in most of the cases, by a non-appropriate choice of alloys while not taking into account the chemical parameters of the fluid inside pipes.
Manufacturing defects mostly dealt with welding related problems and deviation from the design documentation during post-weld heat treatment.
Screening the databases revealed a total of 66 events (7 related to cracks and 59 to leaks) involving the reactor pressure vessel (RPV) and the pressuriser.
The main cause of failure identified was corrosion.
The events dealing with corrosion pointed to various causes, like steam impingement, high oxygen concentration, or use of materials containing water-soluble chlorides (asbestos) as an insulation material.
9 of all the 66 events selected were found dealing with inadequate material selection (mainly alloy 600).
For example, all of the six events involving RPV head penetrations recorded into the SAPIDE database were related to the vulnerability of Inconel 600 to stress corrosion cracking.
Other events were caused by the following deficiencies: deviation from modern design recommendations, inadequacy of the detailed written procedures, improper process of manual electric arc welding, debris on the vessel flange and post weld heat treatment at the manufacturer's works.
Lack of feedback from operating experience was also detected in some events.
Screening the databases resulted in 88 events (14 related to cracks and 74 to leaks) involving SGs.
The main cause of the SG tube defects was corrosion.
It was found that corrosion was induced mainly by improper material selection (mainly alloy 600), and just in few cases by design deficiencies.
A second important contributor represents the manufacturing defects, mainly resulted as failures of the Quality Assurance (QA) programme implemented during the fabrication process.
Some events showed that a circumferential through-wall cracking can occur with a rapid degradation kinetic.
In French PWRs, the tubes material varies depending on the year of construction, so the following tubes are found, respectively: “Inconel 600 MA” (Mill Annealed) tubes, “Inconel 600 HT” (Heat-Treated) tubes, and “Inconel 690 HT” (Heat-Treated) tubes.
“Inconel 600” and, in particular, “Inconel 600 MA,” are susceptible to stress corrosion, which leads to cracking.
For the period 1990–2010, the French operators reported 31 events involving a primary-to-secondary leak rate that was excessive with reference to the thresholds and limits defined in the technical operating specifications.
Screening the databases revealed 29 events (5 related to cracks and 24 to leaks) dealing with reactor coolant pumps.
The events recorded into the IRS database were related to inadequate seal design (upper rotating seal ring degradation/failure was the failure mode), to inadequate inspection method, to foreign substance intrusion into reactor coolant during operation and/or maintenance, and to the fact that damages on the seal water injection line filters could lead to clogging downstream of the seal housing.
19 events were recorded into the French national database alone.
By their classification, there were:•Events involving damage to dynamic seals and leakage into the nuclear island vent and drain system (or into the reactor containment), Events involving external leakage from a static seal, Events involving lack or loss of screws or bolts tension and generalised corrosion in screws and bolts contributing to maintaining the integrity of the second containment barrier (with said lack or loss of tension causing borated water leaks which in turn result in generalised corrosion), Events involving inadvertent interruption of coolant flow in the thermal barrier, followed by subsequent damage, events involving damage to the backseat seal, followed by leakage into the nuclear island vent and drain system (or into the reactor containment), One event involving external leakage from a temperature sensor and One event involving incorrect installation of seal no.
1.
Screening the databases revealed 18 events (2 related to cracks and 16 to leaks) involving safety relief valves.
The events in the IRS database were related to the safety relief valves installed on the pressuriser (6 events).
The analysis of French national database was limited to the safety relief valves (protection and isolation) installed on the pressuriser of the French PWRs.
The analysis only retained events characterised by a failure or leak affecting the relief valve tandems of the pressuriser (valve, control cabinet and connections).
The result of screening the French national database shows 8 events involving safety relief valves, of which 4 events involved “Banjo” fitting and/or “Jet” seal failures.
Screening the German national database identified three events involving safety relief valves, all of them occurred at BWRs.
The first event was caused by a malfunction of testing equipment whereas the second one was caused due to degradation of a control relay – both of them were leaks.
The third safety relief valve event had as cause a broken spring, which was detected during revision, but did not lead to a leakage.
The main causes for the German events were, apart for human errors, periodic inspection programme deficiencies and spurious actuation, in combination with minor design deficiencies.
All these events underline the importance of instrumentation in and surrounding the relief valves and control cabinets, and their associated measurements and alarms.
There were found a total of 65 events (7 related to cracks and 58 to leaks) in the databases involving these types of valves.
Screening IRS database revealed that 5 of the 12 events dealt with maintenance induced corrosion, while corrosion alone caused just 2 events.
Regarding the US NRC database, it was found that 4 out of the 6 recorded events dealt with manufacturing defects, caused in half of the cases by design deficiencies.
By far, the highest number of events is recorded in the SAPIDE database.
The events observed concern internal (upstream-downstream) and external (packing) leaks that were collected into tank or systems.
Of 14 events involving packing gland failure, six involved pressuriser spray valves.
Screening the KomPass database revealed a total of 7 events involving valves.
Out of them, 6 events occurred at BWRs, and were caused by corrosion.
Although usually there are several causal factors that cause the events, for the purpose of this study it was investigated the proportion of events caused by failure of manual operated valves in events.
This examination revealed 23 events, recorded in all 4 databases, involving manual operated valves.
So, manual operated valves inflict a large share of leaks.
The flange joints and mechanical connections are mainly found in small-diameter pipes (instrumentation, venting and drain) and in the auxiliary pipes connected to the RCS.
The leaks in the flange joints often are characterised by a slow flow rate and by damage localised in the seal.
The observations of leaks (boron deposits), are often followed by the replacement of the seal.
Screening the databases revealed 24 events dealing with flanges (1 related to cracks and 23 to leaks).
Diverse causes were identified.
It was found that the “O” rings used by the manufacturers were slightly oversized, so could not be contained inside the groove and was swelling outward under pressure.
Other causes for leak were the lack of QA during maintenance and staff training, the utilisation of materials sensitive to intergranular and transgranular stress corrosion cracking on the flange contact surfaces and deficiencies in the installation procedures.
All the 409 events analysed involving cracking or leakage of reactor coolant have different root causes or sources.
In more cases, the events did not have just one cause, but a combination of them.
The causes of the analysed events are the following:•Corrosion of different types for 35% of the cases.
Manufacturing defect (e.g., ferritic pollution of stainless steel) for 15% of the cases.
Maintenance anomaly (e.g., improperly placed seal, insufficient torque, etc.)
for 11% of the cases.
Control or operating error (e.g., manual valve in inappropriate open position or in incompletely closed position, etc.)
for 9% of the cases.
Fatigue for 11% of the cases Unknown or other causes in 19% of the cases.
Fig.
4 lists the degradation mechanisms that may be present in the different components of the RCPB, see (IAEA, 2005; GALL, 2010; IGALL, 2014).
Not all these mechanisms were found in the analysis of events due to the limited number of events in the databases.
Corrosion is the main root cause of the events analysed.
In France, many corrosion induced events are due to Inconel 600 (SG tubes and vessel head penetrations).
If we do not take into account these events, which are numerous, corrosion is not the main degradation mechanism.
In France, corrosion is often a consequence of an external leak (boric acid).
Manufacturing defects are the second largest root cause.
These events are nearly all related to welding faults, but also it could be founded cases in which the manufacturing defect was just the “apparent cause”, while it's precursors were dealing with inappropriate QA measures at the manufacturer.
Fatigue is also an important degradation mechanism for the analysed events.
Both mechanical fatigue and thermal fatigue were identified during the analysis.
Mechanical fatigue appeared in certain cases in combination with another cause, like manufacturing or maintenance defects, especially on welds.
Thermal fatigue usually appears mainly in conjunction with “Farley–Tihange” phenomenon, but also could be triggered by high temperature differences from one side to the other of a tube, for example.
The event investigation showed that the crack and leak related events occurred in specific equipment, components or sensitive areas.
SG tubes; SG nozzle dams and associated drain plugs required and operated during outages; reactor vessel head penetration areas; small lines for bypass, instrumentation, venting and drainage and corresponding isolation valves; RCS pumps; RCS relief valves and associated control cabinets; valve housings; all the lines, components and accessories located in such areas where they are exposed to various hazards and quality failures during outages when manual operations are required; flange joints and their seals, as well the mechanical couplings or fittings, in particular those subject to frequent operating or maintenance interventions.
The assessment performed shows that cracks have a low safety impact – all the leaks were discovered with the plant in cold shutdown, by performing inspection, and their effect on operation was almost negligible.
Besides, it was observed that when a crack is discovered, the operators take the needed precautionary measures to limit or to control the propagation of the crack – usually the cracks discovered during power operation are repaired during the next outage.
The mechanism of cracks development varies from case to case, based on the type of the plant and on the type of operation and/or maintenance of the equipment.
Regarding the cracks only, it was found that fatigue (both mechanical and thermal) was the main contributor to the appearance of cracks – for the other cases ageing, corrosion or combinations of these three contributors were the initiating stressors of the systems, structures and components affected.
Contrary to cracks, once detected the leaks usually request for an immediate action.
The OEF analysis indicates that leaks often have a significant safety impact, mainly on staff radiation protection, as usually decontamination and cleaning operation require numerous human resources to enter controlled areas.
Also, leaks can induce corrosion of base materials and of external surfaces of the pressure boundary, i.e., vessel head had to be cleaned several times because of boron deposits.
Leaks also induced production of unaccounted wastes that must be treated according to the waste management plans.
Some leaks were difficult to detect, to locate and to stop.
Event reporting has become an increasingly important aspect of the operation and regulation of all public health and safety related industries.
Diverse industries such as aeronautics, chemicals, pharmaceuticals and nuclear all depend on operating experience feedback to provide lessons learned about safety.
For events involving failures in operating devices or in human and organisational performance, it is important to analyse the event, to identify its causes and draw lessons, in order to avoid the recurrence of similar events or to ensure with additional defences that their consequences remain small.
Numerous recommendations have been elaborated in the analysis of the cracks and leaks related events and are extensively documented in (Renev et al., 2014).
The analysis was performed from different perspectives.
Only a summary of the methodology used and some relevant results are presented in this paper.
Plant operating experience has shown that significant increases in the leakage rates below the limits established in the plant technical specifications, but above the baseline values, may indicate a potentially adverse condition.
Plants should periodically analyse the trend in the unidentified and identified leakage rates.
Evaluating the increase in the leakage rates is important to verifying that the plant will continue to operate within acceptable limits.
Prompt corrective action requires continuous online monitoring for leakage, which is important to ensuring the safe operation of a facility because it provides an indicator during reactor operation that a potentially adverse condition may exist.
Photocatalytic water splitting with acridine dyes: Guidelines from computational chemistry The photocatalytic splitting of water into H and OH radicals in hydrogen-bonded chromophore-water complexes has been explored with computational methods for the chromophores acridine orange (AO) and benzacridine (BA).
These dyes are strong absorbers within the range of the solar spectrum.
It is shown that low-lying charge-transfer excited states exist in the hydrogen-bonded AOH2O and BAH2O complexes which drive the transfer of a proton from water to the chromophore, which results in AOHOH or BAHOH biradicals.
The AOH and BAH radicals possess bright ππ∗ excited states with vertical excitation energies near 3.0eV which are predissociated by a low-lying repulsive πσ∗ state.
The conical intersections of the πσ∗ state with the ππ∗ excited states and the ground state provide a mechanism for the photodetachment of the H-atom by a second photon.
Our results indicate that AO and BA are promising chromophores for water splitting with visible light.
Splitting of water into H and OH radicals via photosensitization with a redox-active chromophore which absorbs within the range of the solar spectrum could be a particularly straightforward and cost-efficient way of generating clean and sustainable energy directly from sunlight.
The ideal catalyzer for this reaction should absorb strongly in the visible and be able to abstract a hydrogen atom from a water molecule in its excited state.
Denoting the photocatalyzer by X, the first step of the water-splitting reaction is:(1)X-H2O+hv→X∗-H2O(2)X∗-H2O→XH+OH,where XH2O denotes a hydrogen-bonded complex of the chromophore X with a water molecule.
By photodetachment of the surplus H-atom from the XH radical with a second photon(3)XH+hv→X+H,the photocatalyzer X is recovered and a water molecule has been split into H and OH radicals.
Exothermic radical–radical recombination reactions may yield H2 and H2O2 as closed-shell products.
H2O2 can finally be catalytically decomposed into H2O and O2.
In recent work, we explored the mechanisms of the photoreactivity of pyridine (Py) and acridine (Ac) in hydrogen-bonded complexes of Py and Ac with a single water molecule using state-of-the-art computational methods [1–3].
We constructed minimum-energy reaction paths, their energy profiles and two-dimensional relaxed potential-energy (PE) surfaces for H-atom transfer from water to Py and Ac, respectively, in low-lying ππ∗ and nπ∗ excited states in the singlet and triplet manifolds.
The results reveal the mechanisms by which photoexcited Py or Ac can abstract an H-atom from water via an electron-driven proton-transfer process.
A key finding were hitherto unknown excited states of charge-transfer character (involving electron promotion from the p-orbital of water to the π∗ orbital of the chromophore) which are separated by low energy barriers from the spectroscopic states of the chromophore-water complexes.
The charge-separated electronic states are strongly lowered in energy by the transfer of a proton from water to the chromophore, which results in XHOH (X=Py, Ac) biradicals.
The excess energy of the H-atom transfer reaction is sufficient to dissociate the biradicals into X and OH free radicals.
Due to the existence of a low-lying repulsive 2πσ∗ state in the XH radicals, the radicals can be photodissociated with a second photon.
This regenerates the Py or Ac chromophores, which thus become photocatalyzers [1–3].
While Py absorbs in the far UV, Ac absorbs at 450nm in aqueous solution [4] and thus at the upper edge of the solar spectrum at the surface of earth.
Therefore, both chromophores are not suitable catalyzers for efficient water splitting with sunlight.
Herein, we explore the photochemistry of two chromophores derived from acridine which absorb well within the solar spectrum, acridine orange (tetramethylacridine-3,6-diamine) (AO) and benzacridine (BA).
The aim of this work is to provide evidence that these easily accessible chromophores are potential photocatalyzers for the splitting of water with sunlight.
Ab initio multi-configuration multi-reference perturbation methods as well as propagator methods were employed for the calculation of electronic excitation energies, excited-state reaction paths and conical intersections among the relevant electronic states of the AOH2O and BAH2O complexes as well as of the AOH and BAH radicals.
In earlier investigations of the PyH2O and AcH2O complexes, we explored the reaction mechanisms in both singlet and triplet manifolds [1–3].
While the bright 1ππ∗ electronic states are populated by the absorption of light, the triplet states may come into play by intersystem crossing (ISC), which may compete with direct photoreactions in the singlet manifold.
It was found that the topographies of the PE surfaces of the triplet states are quite similar to those of the singlet states in PyH2O and AcH2O (see Refs.
[1–3] for details).
Since the excited-state electronic-structure calculations are considerably more tedious and expensive for the larger chromophores considered herein, we have confined the calculations to the singlet manifold.
The second-order Møller–Plesset (MP2) method was employed for the determination of the ground-state equilibrium geometries of the AOH2O and BAH2O complexes.
Vertical excitation energies were calculated with the complete-active-space self-consistent-field (CASSCF) method, the CASPT2 method (second-order perturbation theory with respect to the CASSCF reference) as well as with the ADC(2) method.
ADC(2) is a single-reference many-body Green’s function method [5,6].
Vertical excitation energies, geometries of minimum-energy reaction paths and saddle points on the excited-state PE surfaces were determined with the ADC(2) method.
Although ADC(2) is a single-reference method, it has been found to be reliable for the prediction of vertical excitation energies of singly-excited states of closed-shell systems [7].
In particular, ADC(2) PE surfaces are well behaved near conical intersections of exited electronic states, although the method may possibly fail near conical intersections of excited states with the electronic ground state.
In recent investigations of the pyridine-water complex, we evaluated the accuracy of the ADC(2) method in comparison with CASSCF/CASPT2 results [1,2].
It was shown that the ADC(2) method is a qualitatively reliable and computationally efficient alternative to multi-reference perturbation methods and coupled-cluster-type methods for the calculation of excitation energies and the characterization of local regions of excited-state PE surfaces for such systems.
Therefore, most of the calculations in the present work, in particular all excited-state geometry optimizations, were performed with the ADC(2) method.
Being derived by diagrammatic perturbation theory for systems with a closed-shell ground state, ADC(2) is not readily applicable to the open-shell AOH and BAH radicals.
The calculations for the AOH and BAH radicals were therefore performed with the CASSCF and CASPT2 methods.
The reaction paths were constructed as so-called relaxed scans.
For the calculation of the reaction path for the H-atom-transfer process from water to the N-atom of the chromophore, the bond length of the OH bond of the water molecule involved in hydrogen bonding was chosen as the driving coordinate, while all other internal nuclear coordinates of the complex were relaxed in the electronic state under consideration.
The distance between the O-atom of H2O and the N-atom of the chromophore was taken as the second driving coordinate in the calculation of two-dimensional PE surfaces in the vicinity of the saddle point of the H-transfer reaction.
The latter calculations were performed as rigid scans on a two-dimensional grid in the ROH and RON coordinates, since two-dimensional relaxed scans turned out to be prohibitively time-consuming.
The reaction path for the photodetachment of the hydrogen atom from the AOH and BAH radicals was constructed as a rigid scan of the NH bond length, because the relaxation of the internal coordinates of the system is of little importance in this case [8].
The saddle points for the H-atom transfer reactions were estimated from the two-dimensional relaxed PE surfaces.
The minimum-energy geometries of conical intersections were optimized using the CIOpt program of Martinez and coworkers [9].
In the CASSCF calculations for the chromophore-H2O complex, 14 electrons were distributed in 13 orbitals, including the five highest π and five lowest π∗ orbitals the chromophore, the n orbital of the N-atom, one p orbital of the O-atom and the lowest σ∗ orbital of water.
The energies of the S0 state and the lowest 1ππ∗ and 1nπ∗ states were averaged with equal weights in the calculations of the singlet states.
The active space of the CASSCF calculations for the XH radicals (X=AO, BA) consisted of 13 electrons in 12 orbitals: the five highest π orbitals and five lowest π∗ orbitals of the chromophore as well as one σ orbital and one σ∗ orbital of the NH bond.
The CASPT2 calculations for the vertical excitation energies were carried out as single-state, single-reference calculations and a level shift of 0.3au was employed.
For the calculation of the PE functions for hydrogen detachment, a smaller active space had to be employed due to the computational cost of these calculations and due to convergence problems with the larger active space.
This smaller active space consisted of 10 electrons in 9 orbitals: the four highest π orbitals and three lowest π∗ orbitals of the chromophore as well as one σ orbital and one σ∗ orbital of the NH bond.
The state-averaging included the three lowest A′ and the three lowest A″ states.
The MP2 and ADC(2) calculations were carried out with the TURBOMOLE program package [10], making use of the resolution-of-the-identity (RI) approximation [11].
The CASSCF and CASPT2 calculations were performed with the MOLPRO program package [12].
Cs symmetry (that is, co-planarity of the water molecule with the chromophore) was enforced throughout the calculations.
The correlation-consistent split-valence double-ζ basis set with polarization functions on all atoms (cc-pVDZ) [13] was employed in the calculations for the AOH2O and BAH2O complexes.
The Rydberg character of the 2πσ∗ state of the AOH and BAH radicals requires the augmentation of the cc-pVDZ basis with diffuse basis functions.
The aug-cc-pVDZ basis was therefore employed for the calculation of the PE functions of the D0, 2ππ∗ and 2πσ∗ states of the radicals.
The equilibrium structures of the AOH2O and BAH2O complexes, optimized at the MP2 level with Cs symmetry constraint, are shown in Fig.
1.
The H2O molecule acts as a hydrogen bond donor to the N-atom acceptor of the chromophore.
The vertical excitation energies of the three lowest singlet states (two 1ππ∗ states and one 1nπ∗ state) calculated with the CASSCF, CASPT2 and ADC(2) methods are given in Table 1.
The vertical excitation energies of the AcH2O complex are shown for comparison (slight deviations of the CASPT2 excitation energies of AcH2O from those reported in Ref.
[3] are due to differences in the state averaging of the CASSCF reference wave function).
For all three complexes, the two 1ππ∗ states are the lowest excited singlet states.
For AOH2O and BAH2O, the energy of the lowest singlet excited state is lower than the lowest singlet state of AcH2O by about 0.8eV at the ADC(2) level.
At the CASPT2 level, the lowering of the energies of the 1ππ∗ states of AOH2O and BAH2O relative to AcH2O is less pronounced (about 0.5eV), which may be a consequence of the limitation of the active space to 13 orbitals, which is more restrictive for the larger π-systems.
The 1nπ∗ state is substantially higher in energy than the two lowest 1ππ∗ states (it is blue-shifted relative to the isolated chromophore).
Since it does not carry oscillator strength, it will not be considered in what follows.
The oscillator strengths of the 1ππ∗ states are about the same for the three chromophores (at the ADC(2) level, which is considered to be more reliable for response properties).
Fig.
2 shows energy profiles along minimum-energy reaction paths for H-atom transfer from the hydrogen-bonded water molecule to the chromophore.
The energy profiles were constructed as relaxed scans at the ADC(2) level.
The reaction coordinate ROH is the bond length of the OH group involved in the hydrogen-bonding between the H2O molecule and the N-atom of the chromophore.
Small values of ROH (≈1.0Å) correspond to the equilibrium geometry of the chromophore-H2O complex.
Large values of ROH (≈2.5Å) correspond to a biradical structure, consisting of the hydrogenated chromophore and the OH· radical.
The vertical dashed line in Fig.
2a–c separates the reaction path optimized in the electronic ground state (black dots for ROH<1.2Å) from the reaction path optimized in the lowest charge-transfer (CT) state (red squares for ROH>1.2Å).
The energies of the remaining states were computed along these optimized reaction paths.
The PE profiles to the left of the vertical dashed line show the ordering of the excited states in the Franck–Condon (FC) region.
The PE functions of these locally excited states are seen to be parallel to the ground-state PE function.
These states are thus non-reactive with respect to the transfer of a proton or H-atom from the solvent molecule to the chromophore.
The pronounced lowering of the energy of the lowest 1ππ∗ state in AOH2O and BAH2O relative to the lowest 1ππ∗ state of AcH2O is clearly visible.
The PE functions to the right of the dashed vertical line, designated as 1ππ∗(CT) and 1nπ∗(CT), on the other hand, correspond to CT states in which an electron has been transferred from a p-orbital of H2O to the lowest π∗ orbital (LUMO) of the chromophore.
It is seen that the energies of the CT states are strongly stabilized by the transfer of a proton from H2O to the chromophore.
This electron-driven proton-transfer (EDPT) process [8] results in electronic states of biradical character.
The energy of the closed-shell electronic ground state, on the other hand, moves up in energy by several electron volts in all three systems upon the transfer of a proton.
The energy of the ground state crosses the energy of the optimized 1ππ∗(CT) state at ROH=1.50Å, 1.65Å and 1.43Å, respectively, in AcH2O, AOH2O and BAH2O.
The crossing of the S0 energy with the 1nπ∗ energy is allowed by symmetry, while the crossing of the S0 energy with the 1ππ∗ energy is a weakly avoided crossing.
These energy crossings become conical intersections when out-of-plane vibrational modes are taken into account.
The 1ππ∗/S0 and 1nπ∗/S0 conical intersections coalesce, in fact, into a 1ππ∗/1nπ∗/S0 three-state conical intersection.
We located the minimum energy of this three-state intersection at the CASSCF level without symmetry constraints.
The location of the optimized three-state conical intersection with respect to energy and ROH is marked by the green star in Fig.
2a–c.
At the 1ππ∗/S0 and 1nπ∗/S0 conical intersections, nonadiabatic transitions from the CT excited states to the S0 state can take place.
If these transitions happen, the H-bonded complex is restored in its electronic and vibrational ground state after vibrational energy relaxation on the S0 surface.
Otherwise, 1ππ∗ or 1nπ∗ biradicals are formed.
The excess energy available from the proton transfer reaction is by far sufficient to break the weak hydrogen bond between the radicals, resulting in free XH and OH radicals (X=Ac, AO, BA).
Inspection of Fig.
2 reveals that the locally excited (spectroscopic) 1ππ∗ states as well as the CT 1ππ∗ states are lowered in energy by the extension of the π-conjugated system.
This bathochromic shift in the electronic absorption is attributable to a larger density of π and π∗ orbitals upon increasing conjugation.
As a result, the barrier (relative to the minimum of the locally excited 1ππ∗ state) which connects the locally excited (LE) and the charge-separated singlet states near ROH=1.2Å, is approximately the same for the three chromophores.
To visualize these barriers explicitly, rigid scans of the PE surfaces as functions of the ROH and RON (the distance between the oxygen atom of H2O and the N-atom of the chromophore) were computed with the ADC(2) method and are displayed in Fig.
3.
These two-dimensional energy surfaces display the adiabatic connection of the ππ∗ PE curve (red) on the left hand side of Fig.
2 to the ππ∗ PE curve (red) on the right hand side of Fig.
2.
For AcH2O, the rigid scan shown in Fig.
3a can be compared with the corresponding relaxed scan shown in Fig.
5b of Ref.
[3].
Although the energy of the peak of the barrier with respect to the S0 minimum is about 0.2eV higher in the rigid scan than in the relaxed scan, the barrier height with respect to the 1ππ∗(LE) minimum is the same (≈0.4eV).
This result indicates that the barriers relevant for H-atom transfer in chromophore-water complexes can be reliably estimated from rigid scans.
Fig.
3b and c reveal that the barrier height for H-atom tunneling relative to the 1ππ∗(LE) minimum is about 0.5eV in both AOH2O and BAH2O and thus marginally higher than in AcH2O.
The somewhat larger barrier height suggests that the rate of H-atom tunneling from the water molecule to the chromophore may be lower in the AOH2O and BAH2O complexes than in the AcH2O complex.
The lowest vertical excitation energies of the AcH, AOH and BAH radicals, calculated with the CASPT2 method, are given in Table 2.
The lowest excited state is of 2ππ∗ character and corresponds to the excitation from the highest doubly occupied π orbital (HOMO) to the singly occupied π orbital (SOMO).
The next two 2ππ∗ states correspond to excitation from HOMO and HOMO-1 to the lowest unoccupied π orbital (LUMO).
The 2πσ∗ state corresponds to the excitation from the HOMO to the σ∗ orbital, which is antibonding with respect to the NH bond.
At the ground-state equilibrium geometry, the σ∗ orbital is of 3s Rydberg character and localized outside the π-conjugated system, which renders the 2πσ∗ state highly polar.
Upon extension of the NH bond, the σ∗ orbital collapses to the 1s orbital of the H-atom, as is well known for pyrrole, indole and related systems [14].
As shown by Table 2, the vertical excitation energy of the 2πσ∗ state is lowest in AOH and highest in BAH.
In AcH and BAH, the vertical 2πσ∗ excitation energy is above all three 2ππ∗ excited states.
In AOH, on the other hand, the 2πσ∗ state is quasi-degenerate with the second 2ππ∗ state.
The 2πσ∗ state is dark, while the three 2ππ∗ states carry comparable oscillator strengths.
Although the 2πσ∗ state cannot be excited directly by light, it can be populated by vibronic coupling with quasi-degenerate 2ππ∗ states or via a radiationless transition from the higher-lying 2ππ∗ states.
The mechanisms of the H-atom photodetachment reaction from the AcH, AOH and BAH radicals are illustrated by the NH-stretching PE profiles displayed in Fig.
4.
While the PE functions of the three 2ππ∗ excited states are bound with respect to the NH-stretching coordinate, the PE function of the 2πσ∗ state is, apart from a low barrier, strongly repulsive.
The 2πσ∗ PE function crosses the PE functions of the 2ππ∗ states as well as that of the electronic ground state (D0).
These crossings are symmetry-allowed crossings, since the ground state and the 2ππ∗ states transform as A″ in Cs symmetry, while the 2πσ∗ state transforms as A′.
The energy crossings become conical intersections when out-of-plane vibrational coordinates are taken into account.
The repulsive 2πσ∗ state opens a channel for efficient H-atom photodetachment from the radicals.
It should be noted that the dissociation threshold of the 2πσ∗ state of AOH, at 2.5eV, is less than half of the dissociation threshold of the ground state.
In BAH, the dissociation threshold of the 2πσ∗ state is about 0.5eV higher.
When the 2ππ∗ states are excited below the minimum of the crossing seam with the 2πσ∗ state, H-atom photodetachment can occur by tunneling, as is well established for phenol, for example [15–17].
When the 2ππ∗ states are excited above the minimum of the 2ππ∗/2πσ∗ crossing seam, a rapid nonadiabatic transition to the 2πσ∗ state is expected to take place, followed by ultrafast dissociation through the 2πσ∗/D0 conical intersection or, alternatively, internal conversion to the D0 state of the radical.
For closed-shell systems like pyrrole and phenol, the efficient predissociation of the bound 1ππ∗ states by repulsive 1πσ∗ states has been experimentally confirmed [18–20].
In the AcH, AOH and BAH radicals, one or two additional bound 2ππ∗ PE surfaces have to be crossed by the dissociating wave packet on the 2πσ∗ PE surface.
The effect of these additional surface crossings on the quantum yield of H-atom detachment should be explored by future quantum dynamics calculations.
Ab initio electronic-structure calculations were employed in this work to explore the photochemistry of the AOH2O and BAH2O hydrogen-bonded complexes.
We have characterized the minimum-energy reaction paths connecting the locally excited singlet states of the chromophores to hitherto unknown charge-separated excited states.
These CT states are difficult to find in the FC region of the complexes because they are high in energy, are strongly mixed with the spectroscopic 1ππ∗ and 1nπ∗ states and their energies are very sensitive to the geometry of the complex.
However, the CT states become the lowest electronic states of the AOH2O or BAH2O complexes when a proton moves from the H2O molecule to the chromophore, as shown in Fig.
2.
When the proton is fully transferred, the electronic charge separation is neutralized.
The reaction products are XHOH (X=AO, BA) neutral biradicals.
The excess energy available after the EDPT process is sufficient to dissociate the biradicals, which yields XH and OH free radicals.
A substantial part of the energy of the photon is stored as chemical energy in these radicals.
The topographies of the 1ππ∗ and 1nπ∗ PE surfaces in the vicinity of the barrier for proton transfer from H2O to the chromophore were characterized by two-dimensional scans as functions of the ROH and RON coordinates at the ADC(2) level.
Barriers of the order of 0.5eV relative to the minimum of the lowest locally excited 1ππ∗ state were determined, which indicates hydrogen atom transfer from water to the chromophore may occur by tunneling on time scales of hundreds of picoseconds to nanoseconds.
The results obtained for the AOH2O and BAH2O complexes were compared with earlier results obtained for the AcH2O complex with the same computational methods [3].
As expected, the first two spectroscopic 1ππ∗ states of AO and BA are significantly lower in energy than in Ac due to the extension of the π-conjugation.
Importantly, it has been found in the present work that the energies of the CT states are lowered by roughly the same amount as the energies of the spectroscopic states.
Consequently, the barrier separating the locally-excited 1ππ∗ states and the charge-separated 1ππ∗ states is approximately the same in all three systems.
ISC may compete with the singlet-state driven photochemistry of organic chromophores.
For PyH2O and AcH2O, the topographies of the 3ππ∗ and 3nπ∗ states were explored in earlier work [1–3].
The topographies of the PE surfaces of the triplet states were found to be largely similar to those of the singlet states.
The lowest locally excited 3ππ∗ state is special in so far as it is much lower in energy than the lowest 1ππ∗ state in the FC region due to the large exchange matrix element in extended organic systems.
As a result, the barrier connecting the lowest spectroscopic 3ππ∗ state with the lowest 3ππ∗(CT) state is substantial (≈0.8eV in AcH2O [3] and most likely even higher in AOH2O and BAH2O).
The existence of a long-lived 3ππ∗ state in these complexes indicates the possibility of a two-photon excitation process, wherein the first photon excites one of the low-lying 1ππ∗ states.
After ISC and internal conversion within the triplet manifold to the long-lived lowest 3ππ∗ state, a triplet–triplet absorption may take the system to a higher triplet state [21,22], from which a barrierless H-atom-transfer reaction can readily take place.
While more input of excitation energy is required, it is likely that the photoreactions are barrierless and that the quantum yields of the relevant processes are high via this mechanism.
The second step of the water-splitting reaction is the photodetachment of the surplus H-atom from the AOH or BAH radicals.
Our calculations reveal that these hypervalent aromatic radicals exhibit low-lying (≈3.0eV) bound 2ππ∗ PE functions which are intersected by dissociative 2πσ∗ PE functions.
The repulsive 2πσ∗ state intersecting the two light-absorbing 2ππ∗ states and the ground state provides a channel for H-atom photodetachment.
While the 1πσ∗ driven photodetachment of closed-shell aromatic chromophores with acidic groups is nowadays experimentally well established and theoretically well understood [15–20], such photoreactions have hitherto received much less experimental and theoretical attention for radicals.
The conical intersections of the lowest 2πσ∗ state with several bound 2ππ∗ states in the radicals may cause a trapping of the population of the 2πσ∗ state en route to dissociation, which may reduce the overall yield of H photoproducts, depending on the strength of the nonadiabatic couplings at the 2πσ∗/2ππ∗ conical intersections.
This issue deserves further investigation by quantum dynamics calculations based on accurate ab initio PE surfaces.
Fig.
5 gives an overview of the theoretically estimated absorption spectra of AcH2O, AOH2O and BAH2O and the corresponding AcH, AOH and BAH radicals in comparison with the solar spectrum at the surface of earth.
An empirical bandwidth of 0.3eV FWHM was chosen for the simulated spectra.
While Ac is seen to absorb at the upper edge of the solar spectrum, the absorption profiles of AO and BA fully overlap with the high-energy part of the solar spectrum, see Fig.
5a.
Further extension of the π-conjugated system, e.g.
in dibenzacridine, will lead to a further bathochromic shift of the absorption profile.
Fig.
5b shows that the absorption profiles of the AcH, AOH and BAH radicals exhibit good overlap with the high-energy part of the solar spectrum.
It is suggested that AO and BA are promising lead structures for the development of simple and efficient organic water-oxidation photocatalysts.
The available experimental data indicate that Ac and AO can photo-abstract hydrogen atoms from hydrocarbons and alcohols, although not from water [23–25].
However, the photo-oxidation potential of the chromophores can be manipulated by suitable substitution.
Electron-withdrawing substituents, such as cyano groups, may lower the energy of the CT states in the XH2O complexes and may consequently lower the barriers for H-atom transfer from the solvent to the chromophore.
We postulate on the basis of the present computational results that suitably substituted AO or BA chromophores should be able to photo-oxidize neat water with measurable yields.
The hydroxyl radicals can be scavenged and detected by their reaction with benzoic acid, salicylic acid or terephtalic acid.
The hydroxylated forms of these non-fluorescent acids are strongly fluorescent and the build-up of the characteristic fluorescence is proportional to the formation rate of the OH• radicals [26].
Alternatively to H-atom photodetachment from the intermediate radicals, the latter may serve as reducing agents.
Evidence has been reported in recent years that the pyridinyl radical (PyH) is an exceptionally strong reducing agent which can even reduce CO2 to formaldehyde, formic acid or methanol with suitable catalyzers [27–29], albeit the mechanisms of these reactions are currently poorly understood [30–32].
The theoretically predicted dissociation thresholds of the AcH, AOH and BAH radicals are about 2.7eV, 2.5eV and 3.0eV, respectively (see Fig.
4), while the predicted dissociation threshold of the pyridinyl radical is much lower, about 1.7eV [1].
Pyridinyl is thus a significantly stronger reductant than acridinyl and related radicals.
It is therefore not expected that the latter will be able to reduce carbon dioxide in dark reactions.
Another issue of considerably practical interest in photoinduced water splitting is chemical damage of the photocatalyzer by the highly reactive OH radicals generated in the water-splitting reaction.
Since OH radicals primarily attack aromatic systems by abstracting hydrogen atoms from CH groups, perhalogenation may considerably enhance the robustness of the aromatic chromophores.
In addition, OH radical scavengers such as benzoic acid or manganese oxide may be employed to suppress undesired hydroxylation reactions in future exploratory water-splitting experiments.
Very recently, Eisenhart and Dempsey demonstrated the formation of AOH radicals by excitation of a solution of 40μM AO and 1mM tri-tert-butylphenol in acetonitrile at 425nm [33].
The authors tracked intermediates with nanosecond time-resolved spectroscopy and demonstrated the efficient formation of AOH radicals by photoinduced coupled electron/proton transfer from phenol to AO.
Phenol is somewhat easier to oxidize than water and thus can serve as a sacrificial agent for photoinduced water splitting.
A 100μs lifetime was established for the AOH radial under these conditions [33].
This lifetime should be sufficient to demonstrate the photodetachment of the H-atom from the AOH radical with photons of about 500nm, as proposed in the present work.
In aqueous or alcoholic solutions, the photoejected H-atoms form solvated electrons with appreciable quantum yield [34–36].
The latter can conveniently be detected by their strong and characteristic absorption near 750nm [37].
The authors declare that there is no conflict of interest.
This work was supported by the National Natural Science Foundation of China (X.L., Grant No.
61177017), by the National Science Centre of Poland (A.L.S., Grant No.
2012/04/A/ST2/00100) and by a grant of the Deutsche Forschungsgemeinschaft (W.D.).
T.N.V.K.
thanks the University Foundation of the Technical University of Munich for a postdoctoral research fellowship.
A.L.S.
acknowledges support by the DFG cluster of excellence “Munich Centre for Advanced Photonics”.
Stochastic generation of synthetic minutely irradiance time series derived from mean hourly weather observation data Synthetic minutely irradiance time series are utilised in non-spatial solar energy system research simulations.
It is necessary that they accurately capture irradiance fluctuations and variability inherent in the solar resource.
This article describes a methodology to generate a synthetic minutely irradiance time series from widely available hourly weather observation data.
The weather observation data are used to produce a set of Markov chains taking into account seasonal, diurnal, and pressure influences on transition probabilities of cloud cover.
Cloud dynamics are based on a power-law probability distribution, from which cloud length and duration are derived.
Atmospheric transmission losses are simulated with minutely variability, using atmospheric profiles from meteorological reanalysis data and cloud attenuation derived real-world observations.
Both direct and diffuse irradiance are calculated, from which total irradiance is determined on an arbitrary plane.
The method is applied to the city of Leeds, UK, and validated using independent hourly radiation measurements from the same site.
Variability and ramp rate are validated using 1-min resolution irradiance data from the town of Cambourne, Cornwall, UK.
The hourly irradiance frequency distribution correlates with R2=0.996 whilst the mean hourly irradiance correlates with R2=0.971, the daily variability indices cumulative probability distribution function (CDF), 1-min irradiance ramp rate CDF and 1-min irradiance frequency CDF are also shown to correlate with R2=0.9903,1.000, and 0.9994 respectively.
Kolmogorov–Smirnov tests on 1-min data for each day show that the ramp rate frequency of occurrence is captured with a high significance level of 99.99%, whilst the irradiance frequency distribution and minutely variability indices are captured at significances of 99% and 97.5% respectively.
The use of multiple Markov chains and detailed consideration of the atmospheric losses are shown to be essential elements for the generation of realistic minutely irradiance time series over a typical meteorological year.
A freely downloadable example of the model is made available and may be configured to the particular requirements of users or incorporated into other models.
cloud coverage fraction (C/10) cloud coverage in okta (0–8) elements per minute in matrix white-noise multiplier for kc variations irradiance, specified by subscript (Wm−2) random start point within row vector clear-sky index (G/Gcs) number of elements within a re-sampled cloud length row vector transition probability matrix probability of x to occur random variable between 0 and 1 resolution of primary x (100m/el) number of states in Markov process time-step in Markov process wind speed (ms−1) u measured 10m above ground (ms−1) horizontal cloud length (m) state at point t in Markov process cloud cover row vector of 1’s and 0’s cloud cover row vector adjusted by ψ cloud height (m) roughness length (m) used to represent clear sky minute used to represent a clouded minute used as a conversion for secs to min coefficient defined by xmax single power law exponent tilt angle from horizontal of inclined plane coefficient defined by xmin minutely fluctuation from kc solar incidence angle normal to angled plane solar zenith angle std.
dev.
around hourly mean of kc sampling rate beam (direct) denoting a Boolean matrix clear minute cloudy minute clear sky denoting cumulative probability in matrix diffuse the ith state at time t-1 the jth state at time t minutely maximum value minimum value the nth order of a Markov process panel (arbitrary alignment) value at a reference measurement number of states in Markov process Solar irradiance varies on a minutely time scale (Sayeef et al., 2012).
The fluctuations are driven by cloud dynamics, atmospheric losses (Calinoiu et al., 2014), and the transport of airborne pollutants (Vindel and Polo, 2014a).
Changes in irradiance that occur on the same time scale as changes in electricity demand will impact the benefits of storage and self-consumption in a domestic or community PV system (Marcos et al., 2014).
Integrated electricity demand, PV supply, and storage simulations must operate on a minutely time scale to capture these effects, and therefore require minutely irradiance time series as an input (Widen et al., 2015; Sayeef et al., 2012; Hummon et al., 2012; Cao and Sirén, 2014).
Calibrated minutely irradiance datasets are generally the output of isolated research projects and are often limited in duration, measurement consistency, and location.
Hourly weather data, however, is widely collected and made available through national meteorological offices.
This hourly data fails to capture the intermittent nature of solar irradiance (Sayeef et al., 2012), therefore some solar irradiance models use hourly weather datasets to artificially generate minutely irradiance time series.
The focus of solar irradiance models can vary from predicting the future irradiance, to providing a general expected irradiance at any location globally.
Many examples of these models have been reviewed, analysed and validated in literature (Gueymard, 2012).
The methodology of interest is a sun obscured type approach.
This is where the cloud cover is predicted or determined, thereby implying when the solar beam irradiance will be obstructed.
A more complex methodology is outlined and developed by Morf (1998, 2011, 2013) where cloud cover is two-dimensionally modelled to replicate sky with certain clouded conditions, whilst a random number generator driven model separates irradiance into its subcomponents of diffuse and beam.
Atmospheric transmission losses from extraterrestrial irradiance to global horizontal irradiance is extensively detailed in literature, however its inclusion on a time series irradiance generation model is less so.
Simplistically, clouded periods can be subjected to a random variate to represent these losses (Ehnberg and Bollen, 2005), however there is scope for a more sophisticated approach.
Geographically dependent monthly clearness index distributions can be used to deterministically select the transmission losses during clouded periods (Morf, 2013).
Probabilistic methods are commonly seen to generate irradiance data by stochastically selecting the clearness index based on real observations (Ngoko et al., 2014), or by determining monthly probability distributions of clearness index (Graham et al., 1988), or stochastically producing cloud cover and applying beam irradiance transmission losses (Aguiar et al., 1988).
There are numerous methodologies that can be utilised and developed upon.
The methodology presented within this paper is a temporal only sun obscured type model with two distinct sections, the first generates 1-min resolution cloud cover, and the second calculates the irradiance based on whether the ensuing minute is clear or cloudy.
A distinct aspect of this methodology is the use of a multitude of Markov chains to stochastically determine future weather condition states of pressure, wind speed, cloud height and okta,2An okta is the representation of cloud amount reported in eighths, where zero okta represents the complete absence of cloud, through to 8 okta representing full cloud cover; an additional value of 9 okta represents full coverage due to fog or other meteorological phenomena (UKMO, 2010).2 incorporating the weather variation influence of season, the diurnal dependency and variations caused by pressure.
Furthermore, the pattern of cloud cover is generated with greater complexity than in previous literature by using a one-dimensional method that considers cloud height, the speed of the cloud, and the statistical distribution of the horizontal cloud length.
The irradiance is calculated in three stages, firstly the atmospheric transmission for each minute is determined, secondly the theoretical clear-sky irradiance is calculated based on earth–sun geometry, and thirdly the irradiance is broken down into its different components in order to obtain irradiance on an arbitrary plane.
The most distinct element of the irradiance calculations is the production of clear-sky index probability distributions based on the okta number, and the deterministic approach to applying atmospheric losses minute-by-minute with the inclusion of an intra-hour dependency.
The methodology is geographically flexible, so long as the input data exist for the location desired.
However, individual simulations at nearby locations would not correlate due to the non-spatial nature of the methodology.
The model output is a synthetic global solar irradiance time series upon an arbitrary plane at a 1-min resolution.
It is a temporal data series only, and does not include a spatial dimension.
Its applications are therefore limited to cases where the spatial element is not integral, such as small scale studies where a single high-resolution irradiance data series input is ideal.
The methodology has suggested application in the improved modelling of small-scale PV supply, demand, and storage systems, through the calculation of electricity supply on a time scale that matches the demand flows.
Regular demand flows operate with high power across a small duration, meaning that mean hourly averages fail to capture electricity peaks in the supply and demand.
To appropriately capture electricity flows within the residence that can accurately calculate efficiencies, self-consumption losses, battery charge and discharge states for example, an appropriately high-resolution time scale is required (Darcovich et al., 2015; Torriti, 2014).
This methodology offers an improved input estimation of the solar irradiance to these types of studies.
A validation of the methodology is carried out using hourly input observation data from Leeds, UK, and with 1-min input observational data from Cambourne, Cornwall, UK.
Comparisons with the irradiance output have been made against independent radiation observation data for the same location, and the key features of this methodology are discussed.
An example of the output can be seen in Fig.
1.
Fig.
1a is the mean okta number for the hour, Fig.
1b is the one-dimensional cloud cover where black indicates the presence of a cloud, and Fig.
1c is the corresponding horizontal global irradiance.
The simulated day selected was typical of June for Leeds, UK.
The inputs required for this methodology are the hourly mean weather observation measurements of sea level pressure (hPa), 10m wind speed (knots), cloud base height (decameters), and okta number (okta).
In order to capture accurately the probability statistics of a typical meteorological year, a minimum of 10years of observational data are recommended.
A two-dimensional approach to producing cloud cover could be used to replicate the measurement type of an okta number (Morf, 2011).
The methodology presented, however, requires only a single passage of beam irradiance from sun to the solar panel; a representation of the whole sky is not necessarily required.
The typical method for measuring cloud cover is to use a cloud base recorder which uses a vertical laser pulse to track cloud presence (UKMO, 2010).
This measurement device takes readings in a single direction, the okta number derived from this type of measurement is considered an acceptable representation of obscured beam irradiance once it is adapted to a synthetic temporal representation.
Unlike previous methodologies, the cloud cover will be generated to follow a size distribution for a single linear dimension.
Horizontal cloud size distributions are shown to be well-represented using a single power-law relationship using an exponent of β<2 (Wood and Field, 2011; Stull, 1988; Leahy et al., 2012; Pressel and Collins, 2012; Ray and Engelhardt, 1992)(1)P(x)=αx-βwhere x is the horizontal cloud length, β is the exponent taken at 1.66 between 0.1 and 1000km (Wood and Field, 2011), α is a constant and P(x) is the probability that x will occur.
Eq.
(1) represents the probability of x without horizontal cloud length limits.
Eq.
(2) introduces these limits and to allow for pseudo-random number extraction(2)x=(α+δr)11-βwhere r is a random variable that is uniformly distributed between 0 and 1, and α and δ are coefficients defined by the upper and lower limits of x as(3)α=xmax1-βand(4)δ=xmin1-β-αwhere xmin is the minimum cloud length and xmax is the maximum cloud length.
These equations allow a 1 metre resolution row vector of cloud cover, x, to be generated with 1s and 0s representing single metres of cloudy and clear intervals respectively.
A signal poly-phase filtering technique (implemented using the built in Matlab ’resample’ function (Oppenheim et al., 1999; Matlab, 2012a)) is applied that decimates vector, x, by a sampling rate, ψ.
This converts the cloud length row vector (in m) into a smaller vector that is representative of increasing cloud speeds, and so the resultant vector, xψ, is a time series.
The clouds are assumed to be travelling at the concurrent wind speed and cloud height for each hour of simulation.
The sampling rate used in the signal poly-phase filtering technique is(5)ψ=Ru×60where u is the wind speed (ms−1), 60 is a conversion from seconds to minutes, and R is the resolution of x (m/element).
xψ is the minutely time series cloud cover row vector as a function of the wind speed.
Hourly periods are selected at random from xψ and the hourly mean coverage value, C, is determined through Eq.
(6).
C is a fractional representation of the duration of obscured sun, where 010 is clear sky and 1010 is fully obscured;(6)C=∑i=n×ri+59xψ60‾where the overline represents the mean, n is the total number of elements in xψ, and i is a random start point along the length of xψ.
Hours are selected from xψ until there is a cloud cover database consisting of 1000 examples of an hour of cloud cover for each possible u at each C. During each hour of simulation, an hour of cloud cover is selected from the database using the stochastically determined values of u and C as a reference.
To implement the stochastic element to the modelling, a Markov process is used.
Markov models are a very popular method of stochastic data generation and have been used in many applications, from wind estimates (Masseran, 2015), solar energy estimations (Bhardwaj et al., 2013; Vindel and Polo, 2014b; Hocaoglu, 2011), and in weather generation (Yang et al., 2011).
A Markovian process is a probabilistic mathematical method whereby transitions from one state to the next are directed by discreet probabilities taken from the statistics of real-world processes.
In the case of this methodology, statistics are developed from real observational transitions of mean hourly okta, wind speed and cloud height.
From these statistics, transition matrices can be constructed.
This methodology uses a single order Markov model whereby only one previous time-step, t-1, influences the transition process from t-1 to t. Higher order transitions exist, from first order t-1, through to the nth order t-n. Ngoko et al.
(2014) describes how a Markov process (Xt,t=0,1,2,…) that has s allowable states (1,2,…,s), is in state j at time t if Xt=j.
In this first order Markov process, given that the process is in state i at time t-1, the probability that it will transition to state j at time t is given by discreet probability Pij, this is calculated as(7)Pij=P(Xt=j|Xt-1=i) These probabilities are stored in a transition matrix P1 and can be represented asP1=P11P12…P1sP21P22…P2s⋮⋮⋱⋮Ps1Ps2…Pss Constructing a transition matrix requires the conversion of a variable’s range of magnitudes into discreet states.
Fortunately for this study all the variables are extracted in appropriate states, for example each okta number (0,1,…,9) is its own state.
The linear time series of observed states must be converted into a statistical representation of transitions.
Firstly, the frequency of transitions fst-1,st must be found.
This is done by tallying every transition of states found in the observational data in the appropriate fst-1,st location.
Secondly, the frequencies are converted into probabilities through dividing fst-1,st by the total number of occurrences that st-1 transitioned, fst-1∗.
In the case where the observed frequency of state i transitioning to state j is fij, and where fi∗ is the total number times state i has transitioned.
The probability of this transition can be expressed as(8)Pij=fij/fi∗ A pseudo-random number generator is used to determine st from st-1 within the simulation.
To do this, the cumulative sum of the probabilities PC1 is found asPC1=P11P11+P12…∑n=1sP1nP21P21+P22…∑n=1sP2n⋮⋮⋱⋮Ps1Ps1+Ps2…∑n=1sPsn To implement the Markov process in Matlab r2012a, a random variate r, evenly distributed between 0 and 1, is introduced at each transition.
PC1 is queried against r in a logical statement, resulting in a Boolean matrix(9)PC1<r→PB1=1where → denotes a logical if statement, and PB1 is the resultant Boolean matrix.
For example, if each of the cumulative transition probabilities are less than r, a value of 1 is assigned, else a value of 0 is set.
The future state st is then given by(10)st=1+∑(PB,st-11)where the subscript st-1 indicates the matrix row to sum using the previously determined state.
In the subsequent iteration st becomes st-1 and the process is repeated.
Twenty different Markov chains are utilised to stochastically select weather variables for the subsequent hour, based on the conditions of the current hour.
The variables selected through the Markov process are the okta number for each season during both above and below average pressures (8 matrices), wind speed for each season (4 matrices), cloud height for each season (4 matrices), and the diurnal changes for each season (4 matrices).
The transition probabilities are calculated through statistical analysis of the transitions between hours within the observation data.
The wind speed and cloud height Markov chains are produced accounting for seasonal variations.
A Markov chain is used for each variable representing each of the four seasons, capturing the variability at different times of the year, totalling four chains each.
The okta number Markov chains also consider the effect of season, with the inclusion of impacts from pressure and diurnal variation.
Eight okta Markov chains are produced that are split by above and below average pressure for each season, and four additional morning okta Markov chains are produced to capture the diurnal variation for okta transitions between 00:00 and 05:00am for each season.
The intent is to capture the variation in transition probability that occurs as a result of weather changes due to the presence of solar energy.
5am is considered the cut-off because it is a typical sunrise in the summer for the applied study locations.
5h represents 5 okta transitions and is considered an appropriate duration for the slight propensity to shift towards an increased okta to be represented, Fig.
8 demonstrates the diurnal transition differences.
Fig.
2 visually demonstrates the mean okta Markov chain for the entire year, whilst the effect of season can be seen in Fig.
11.
The hourly mean sea level pressure is considered to be either above or below average at any hour, this is to account for weather condition variation between high and low pressure systems.
The mean pressure is calculated from the observation data and the frequency distribution of below and above average pressure durations are produced.
The duration of the pressure system is then selected using a pseudo-random number generator that follows the duration probability distribution.
The pressure systems always alternate between periods when they are above and below average.
Wind speed measured by weather stations is typically recorded at 10m above ground level.
For clouds in the atmospheric boundary layer (below 1km) the wind speed can be extrapolated using the stochastically selected cloud base height as the target reference height, zref.
The extrapolated wind speed is assumed to be the speed at which the cloud travels.
To extrapolate the wind speed from the 10m measurement to a reference cloud height, the following logarithmic profile extrapolation (Best et al., 2008) is used(11)uref=u10ln(zref/z0ref)ln(10/z0ref)where uref is the wind speed at the reference height (ms−1), u10 is the wind speed at 10m above ground level (ms−1), zref is the vertical reference height, and z0ref is the roughness length of the location (set to 0.14 for rural locations (Best et al., 2008)).
Above 1km, the geostrophic wind speeds in the free atmosphere are influenced by pressure and thermal gradients (UKMO, 1997).
Estimating geostrophic wind speeds is difficult using mean hourly surface observational data; presenting one of the limitations of the current state of the methodology.
Without knowledge of pressure and thermal gradients, alternatives must be used.
Excellent methods exist such as the wavelet variability model (Lave and Kleissl, 2013), however the data input required is an irradiance profile, an input avoided in the rationale of this methodology once clear sky statistical analysis has been performed.
Future development is required to truly address the cloud speed, the validation results however were found to be relatively insensitive to this aspect, and so at this time a guided estimation is utilised.
Using the same range as used in the wavelet variability model (0-25ms) and typical geostrophic wind speeds of 12.5kmh−1 (Mathiesen et al., 2013), the estimated wind speed at cloud heights above 1km is determined allowing variation between the suggested range with the mean found at the typical free atmosphere wind speed as u>1km=Gamma(2.69,2.14).
For each hour of simulation, the wind speed, cloud height and okta are stochastically selected from the appropriate Markov chain.
From here, an hour of cloud cover at a minutely resolution can be selected from the cloud cover database, resulting in a temporal Boolean vector representing 1min cloud cover.
Once the cloud cover is generated for the entire time series desired, the irradiance is calculated.
The irradiance model is in three parts.
Firstly, atmospheric transmission for each cloudy and cloud-free minute is determined.
Theoretical clear-sky irradiance based on earth–sun geometry is then calculated and the atmospheric transmission factor applied to obtain simulated global irradiance.
Lastly, simulated all-sky direct and diffuse irradiance is calculated and then applied to obtain irradiance on an arbitrary plane.
The atmospheric transmission is parameterised in terms of the clear-sky index, kc=G/Gcs, where Gcs is a theoretical cloud-free irradiance value.
To account for the varying optical thicknesses of clouds, it is appropriate to choose kc from a distribution rather than to assign a fixed value.
To this end, observations of G from the 91 MIDAS3MIDAS: the UK Met Office Integrated Data Archive System who have maintained and updated meteorological observations from land and marine surface stations since 1853 (UKMO, 2014).3 stations that provided observations of both cloud okta and hourly global irradiance in the year of 2012 were used.
Hours in which the mean solar zenith angle was greater than 80° were rejected to minimise the impact of horizon effects and exclude hours during which the sun rose or set.
Values of Gcs were calculated by the DISORT radiative transfer solver (Stamnes et al., 2000) in the libRadtran package (Mayer and Kylling, 2005).
In the radiative transfer calculation, atmospheric profiles of temperature, O3 concentration, precipitable water vapour and surface albedo were obtained for each month of 2012 from the European Centre for Medium-Range Weather Forecasts (ECMWF) ERA-interim reanalysis data (ECMWF, 2014) at a resolution of 1.5°×1.5°.
Monthly aerosol data was provided by the GLOMAP model at a resolution of 2.8°×2.8°, specifying scattering and absorption coefficients and asymmetry parameters for 6 bands in the shortwave spectrum for sulphate, sea-salt, black carbon and particulate organic matter aerosols in four size modes (Scott et al., 2014).
As each simulated hour is to be aligned with actual data to obtain kc, it is critical to obtain the solar geometry to high accuracy.
The latitude and longitude of each MIDAS station was therefore provided to four decimal places, from which the effective cosine-weighted solar zenith angle for each hour was calculated using the Blanco-Muriel et al.
(2001) algorithm (accurate to within 0.01°) included in libRadtran.
Clear-sky index observations were grouped by corresponding cloud okta for the hour and histograms of the observations created (Fig.
3).
There were 111090 observations in total.
From this, the distributions from okta 0, 6, 7 and 8 were kept.
Periods of okta 9, which represents sun obscured due to fog or other meteorological phenomena, were treated as overcast hours.
For hours where okta is in the 1–6 range, the distribution for okta 6 is used, as the aim is to obtain a distribution for cloud optical thickness for partially cloudy hours which differs from cloud optical thickness in overcast or nearly overcast hours, but is relatively unaffected by periods of clear sky.
As okta 1–5h include a significant amount of clear sky, it is difficult to estimate a sun-obscured kc distribution for these hours.
Furthermore, observations of okta 1–5 are comparatively rare.
The distributions were fitted using maximum likelihood estimation (Table 1), except in the case of okta 0 where a normal distribution with mean of 0.99 and standard deviation of 0.08 was selected to provide a good visible fit to the peak of the histogram.
In the okta 0 histogram, there are several kc values which are much less than 1, indicating that even when no cloud is registered, the irradiance reported can be much lower than expected.
This can be explained by the fact that the cloud sensor is pointing directly upwards whilst the sun is in a different portion of the sky and may be obscured by cloud.
This is a particular issue in the UK where the sun is always at least 27° from zenith.
To ensure realistic clear-sky values where the sun is shining, these low values have been rejected and a normal distribution fitted visually to the spike of the histogram, which represents genuinely clear hours.
For each day, a baseline cloudless clear-sky index, kc,clear, is selected from the okta 0 distribution, and applied to all minutes where the sun is not obscured.
For cloudy minutes, the baseline cloud-obscured clear-sky index, kc,cloud, is selected from the appropriate distribution.
If the sky is overcast for an extended period (hourly okta 8 for >6 hours), the model sets the baseline value of kc,cloud for each interval within the overcast period from the okta 8 distribution.
This is justified because the clear-sky index of overcast skies does not vary extensively (Skartveit and Olseth, 1992).
For partially cloudy hours (okta 1–7), the hourly kc,cloud value is allowed to change each hour and is drawn from the okta 6 distribution if the hourly okta C8 is 6 or less, and the okta 7 distribution if C8=7.
If the value of kc,cloud selected from the distribution exceeds 1, a new value from the distribution is drawn.
To move from hourly averages of kc,cloud to minutely instantaneous values, a linear interpolation between two values of kc,cloud was performed to obtain baseline minutely values kc,cloud,m.
To produce more realistic irradiance profiles, the kc,cloud,m were allowed to fluctuate between intervals of 6min.
The model allows fluctuations at intervals that are a factor of 60.
Allowing fluctuations >10min intervals did not offer similarities in terms of ramp rate occurrences or the variability indices, whilst fluctuations at <5min intervals saw too much variability.
The intention is to capture the gentle rolling of the kc,cloud,m observed in real profiles.
Following this, minutely clear-sky index variations were introduced for both cloudy and clear minutes by using a Gaussian white noise multiplier based on the hourly cloud okta:(12)kc,m=fkc,m,f∼N(1,σ)where(13)σ=0.01+0.003C8for cloudy minutes, and(14)σ=0.001+0.0015C8for clear minutes.
Eq.
(13) gives the greatest variation in cloudy minutes for higher okta; Eq.
(14) gives the greatest variation in clear minutes with increasing cloud cover.
Clear-sky irradiance values, and their direct and diffuse components, were obtained from the HELIOSAT method (Hammer et al., 2003).
HELIOSAT is fully described by Hammer et al.
(2003) and so is not repeated here.
Whilst clear-sky irradiance from the radiative transfer code can be used, the HELIOSAT method is flexible enough to calculate minutely clear-sky direct and diffuse irradiances GB,cs and GD,cs for any location of choice in very little computational time.
The global clear sky horizontal irradiance is the sum of the components(15)Gcs=GB,cs+GD,csand the global all-sky irradiance is given by(16)G=kc,mGcs.
A further two adjustments are made to ensure that unrealistic values of G are not seen.
If the value of kc,m is less than 0.01, it is set equal to 0.01.
At the other end of the scale, there are many situations where kc,m exceeds 1 in the distribution tails.
This is most likely to happen at low solar elevations where horizon and ground effects are more pronounced, and the division of a small G by a very small Gcs leads to high values of kc.
The largest values of kc from the observed irradiances were found to obey the relationship given by (R2=0.9931)(17)kc,max(θz)=27.21exp(-114cosθz)…+1.665exp(-4.494cosθz)+1.08so that kc,m is set equal to kc,max if the value drawn from the distribution exceeds this upper threshold.
The decomposition of global irradiance into direct and diffuse components is necessary in order to calculate the irradiance on a plane of arbitrary alignment.
Direct irradiance under all sky conditions is related to clear-sky index and clear-sky direct irradiance by an adjustment (Müller and Trentmann, 2010) such that(18)GB=0kc<1969;GB,cs(kc-0.38(1-kc))2.51969⩽kc⩽1;GB,cskckc>1.Diffuse all-sky irradiance is the difference between the global and direct horizontal values, GD=G-GB.
The direct and diffuse horizontal irradiances are translated to the irradiance incident on a panel of arbitrary inclination and aspect GP using the Klucher model (Klucher, 1979) which is found to perform well even when compared to more recent models (Gueymard, 2009):(19)GP=GD1+cosβ21+Fsin3β2(1…+Fcos2θisin3θz)+GBcosθicosθz,where θi is solar incidence angle normal to the angled plane, and F=1-(GD/(GB+GD))2.
All data processing was performed using the commercial software package Matlab r2012a (Matlab, 2012b).
Hourly weather observational data are taken from the British Atmospheric Data Centre (BADC) (BADC, 2013) which stores data from MIDAS’ land surface monitoring stations, which are well geographically dispersed across the UK.
As monitoring stations are occasionally taken off-line for repairs or upgrades for months at a time, 12years of data sets are used to allow at least 10years data for each variable.
For the application of the model, 12years of hourly weather observations for the monitoring site at Leeds Church Fenton (Source ID: 533) were taken from the BADC MIDAS data sets to produce the Markov chains.
In order to validate the model, 12years of radiation observation data were taken for the same monitoring station for comparison.
Missing data points are assumed to insignificantly impact the validation.
In cases where duplicate hourly measurements exist, the hour that has undergone quality control by MIDAS is selected.
Figs.
4 and 5 illustrate direct comparisons between modelled and observational mean hourly irradiance, averaged across 10years.
The 10year mean annual irradiance is also indicated by the + scatter point at 112.5Wm−2 observational and 113.5Wm−2 modelled, giving approximately a 0.9% yearly irradiance overestimate for the location of Leeds.
Whilst the intent of the model is not to produce hourly mean data in this format, averaging the minutely irradiance generated by the model over hourly timesteps shows a strong correlation when compared with the observational data (R2=0.9715).
Fig.
6 displays the mean 10-year frequency distribution of the mean hourly irradiance, normalised to a single year.
The correlation between the modelled and observational outputs is R2=0.9963.
Validation against a minutely irradiance dataset is necessary to confirm that the model can achieve statistically representative minutely resolution.
To demonstrate this, 7years of minutely radiation data was taken from the World Radiation Monitoring Centre – Baseline Surface Radiation Network (WRMC-BSRN) from BSRN station number 50, located in Cambourne, Cornwall, UK (WRMC-BSRN, 2014).
Missing data points were ignored and deemed to not significantly impact the distributions for comparison.
The model inputs were adjusted for the location of the Cambourne weather station using a latitude of 50.2178, longitude of -5.32656, and height above sea level of 87m.
12years of hourly weather observations for the monitoring site at Cambourne (Source ID: 1395) were taken from the BADC MIDAS data sets to produce the appropriate Markov chains.
Three metrics are used to validate the intermittent nature of the model output, the variability index CDF, the irradiance CDF and the ramp rate CDF.
The variability index is the ratio between the rate of change of the measured irradiance and the rate of change of the reference clear sky irradiance (Stein et al., 2012).
The more intermittent the day’s irradiance, the higher the variability index.
Furthermore, the 2-sample Kolmogorov–Smirnov (K–S) test was carried out for each metric, for each day of the year.
The subset of each K–S test consisted of 7 of the same day’s minutely data.
For example, 7 modelled samples of the 1st-January represents one subset and is compared against 7 samples of the same day from the observational data.
The comparison of the CDFs of the mean daily variability indices for the model output and the measured validation data is shown in Fig.
7a.
The two curves correlate well at R2=0.9903.
The model generated variability indices have a slightly increased frequency of mildly variable days between variability indices 10–25 as is indicated by the steeper slope, whilst having a slightly reduced frequency of extremely variable days.
Table 2 indicates that 94.38% of days analysed with the 2 sample K–S test reject the null hypothesis that the minutely variability indices of each day of the year observed and modelled cdfs are not from the same sample, when using a significance level of 99%.
No annual temporal bias was observed from the days that accept the null hypothesis.
The comparison of the observational ramp rate occurrence CDF and that of the model have an excellent correlation, as shown in Fig.
7b with R2=1.
Both the 1-min interval of ramp-up and ramp-down events are captured excellently using this methodology.
This is furthermore demonstrated with the results from the day-by-day K–S tests shown in Table 2.
All days tested reject the null hypothesis across all tested significance levels with mean asymptotic p-value of 6.7365e-29.
Fig.
7c shows the comparison of the CDFs of the irradiance occurrence frequency between the 1-min generated data and the 1-min observational data across 7years.
The probability distribution functions correlate well at R2=0.994 with a very slight underestimation of the mid-range irradiance occurrences.
A possible cause for this is combining the clear sky indices of okta values 1–6 with the same distribution.
The day-by-day irradiance frequency distribution compare well with observational data as seen in Table 2, which shows that over 95% of days tested reject the null hypothesis at a significance level of 99%.
Irradiance is generated over a 10year simulation using Leeds, UK, input observational data to complete the methodology, the same simulation was used in the validation.
A distinctive element of this methodology is the use of 20 different Markov chains in order to capture seasonal, diurnal and pressure based variations.
Combined, the Markov chains successfully replicated the okta frequency distribution of the 12years of observation data.
The mean percentage error between the modelled and observed okta transitions is -0.03%, which demonstrates that the overall statistics for Leeds are retained even with 20 separate Markov chains in use, whilst capturing more detailed transition characteristics at certain times of a year.
The annual correlation of modelled and observational okta frequency distribution is R2=0.9956, showing a highly accurate reproduction of the location’s okta statistics.
Each type of okta Markov chain (diurnal, pressure, and season) is analysed through examining the variation away from the annual mean okta Markov chain (Fig.
2) to assess the impact each of the different types have on the transition probability.
The mean okta Markov chain is produced with every transition in the observational data, and not separated for season, pressure and season.
The most significant deviation of transition probabilities away from the mean is seen with the morning diurnal dependency type Markov chain.
A comparison of the mean of the diurnal Markov chains (so that variations caused by pressure and season are excluded) minus the mean okta Markov chain is shown in Fig.
8.
It illustrates the most significant deviations from the mean, as the differences in probability below ±0.01 are removed from the plot.
There is a very distinct pattern for the probability of an okta number to remain the same from one state to the next, with a decrease of between 0.01 and 0.25 for okta 0–6.
Okta 7, 8 and 9 however have an increased probability of remaining the same meaning that cloudier weather states are longer and more prevalent.
Significant probability increases for an okta state transitioning to a higher okta number are seen across all okta states.
There is also a slight increase for okta 3, 4 and 8 to transition to okta 0.
The physical implication of this is that during the morning period (00:00–05:00am), the current okta value has an increased tendency to transition towards an okta value of 6–8 or 0, and so cloud state is most likely to develop into either fully obscured or complete clear sky.
This increased tendency towards both okta extremes during the morning period is not captured when using a single Markov chain for all times of the year.
The reason for the inclusion of transitions based on pressure is to attempt to capture variations in weather that are caused by high and low pressure systems.
The approach is simplified to be either above or below the average pressure.
Fig.
9 shows a slightly different analysis to the diurnal comparison for simplicity.
The mean below average (BA) transition probabilities are subtracted from the mean above average probabilities (AA).
The darker shade (blue) represents occurrences where the BA transition probability is greater than that of AA, for that particular transition only.
The lighter shade (green) represents where the AA transition probability is greater than that of BA.
The most significant key point to note is that the probabilities for okta 0–3 to transition to a cloudier state are consistently and significantly greater during periods of BA pressure.
The transitions are more stable during AA pressure, with an increased tendency to remain at the same okta state.
For okta 1–2 during AA pressure, there is a slight increase in probability to become less cloudy.
Okta 7–9 during BA pressure has a higher tendency, +0.1, to remain cloudy.
The variation due to the different seasons is detailed in Fig.
11, where the deviation from the mean okta Markov is shown for each of the four seasons.
Again, in order to observe the most significant of differences, deviations of ±0.015 are removed.
Autumn variations are the closest to the mean.
The frequency of occurrence of okta 2–6 are the lowest and so differences in this region offer the least impact on the overall model.
The tendency for okta 1 to remain the same is significantly larger for autumn.
Spring transitions are similar to those of autumn, in that the most significant deviations occur during the least sensitive okta range.
There is however a distinct shift to favour lower okta values.
Summer and winter contain the most significant variations from the mean.
This is most important in summer as this is the period receiving the majority of the year’s irradiance.
Summer sees an increase in the probabilities along the diagonal (x=y), hinting towards more stable transitions, directly contrasting the pattern observed in winter.
Interestingly, winter has an increased tendency to move towards the two extremes of clear sky and fully obscured.
It is worth noting that the differences reported here are specific to the location of Leeds, whilst the differences may be similar in pattern, the seasonal, diurnal and pressure differences are specific to the input observational data.
Hourly averages of clear sky indices derived from the model were compared with kc values from the MIDAS data (Fig.
10).
It can be seen that the model recreates bi-modal structure of the real-world distribution of clear-sky indices.
The peaks from the simulated data are of similar height to the MIDAS data, although the intermodal spread is lower and in particular there are fewer extreme high or low values in the simulated data.
Fig.
12 displays the diurnal root mean square error (RMSe) performance of the simulated data for each season and annually, averaged across 10years.
The RMSe is calculated as a direct comparison of 10years of modelled mean hourly irradiance data aligned with 10years of mean hourly observational data.
The time of day of most importance is around midday during the summer season during maximum clear-sky irradiance as this is when the potential for the largest ramp-rates and peak outputs are found.
Typical un-obscured irradiance outputs during these times are around 900Wm−2 at Cambourne.
The RMSe in Fig.
12 at midday in summer is at 14Wm2, offering potential mean hourly irradiance error of ±1.5%.
The vast majority of hourly RMSe values fall below 15Wm2.
The correlation of 10years observational and generated mean hourly irradiance profiles is R2=0.9715.
This paper presented a stochastic sun obscured type synthetic irradiance generation methodology with the intention of taking readily available hourly resolution input data and generating statistically accurate 1-min resolution irradiance profiles.
One-dimensional cloud cover has been generated so that the horizontal cloud length follows a power law relationship.
Twenty different Markov chains are used in setting the hourly conditions that determine the cloud cover.
These Markov chains allow the stochastic selection of wind speed and cloud height whilst accounting for seasonal variability, and the selection of cloud cover accounting for seasonal, diurnal and pressure variability.
The use of diurnal Markov chains is shown to have the most significant influence on the output.
The variability of okta transition due to pressure system and season also allows for greater accuracy in capturing the weather conditions of a geographical location.
The distribution of hourly clear-sky indices are recreated whilst allowing for minutely clear sky index fluctuations.
This methodology is validated on an hourly basis using radiation observations from Leeds Church Fenton, UK; and on a 1-min basis using radiation observations from Cambourne, Cornwall, UK.
The model was shown to capture the irradiance frequency distribution with R2=0.9963 and the 10-year mean hourly irradiance with R2=0.9715.
The 10-year mean diurnal comparison was shown to perform well for each season, the maximum RMSe during summer midday is at 14Wm-2 for a single hour.
The largest RMSe equates to an error during summer height outputs of ±1.5%.
The use of clear sky index frequency distributions as a function of the okta number allows the typical cloud characteristics inherent during okta to be produced.
The 1-min validation in Cambourne showed the same performance on an hourly resolution as shown for Leeds, however it has been demonstrated that the 1-min irradiance ramp-rates, irradiance frequencies and the variability indices distributions correlated very well at R2 values of 1.000,0.9994, and 0.9903 respectively.
Day by day 2-sample K–S tests were carried out on the modelled and obscured CDFs of the variability index, ramp rate occurrences, and the irradiance frequency; the percentage of days that reject the null hypothesis at significance levels of 99% are 94.38%, 100%, and 95.89% respectively.
Whilst not a predictive model, visual inspection of both the 1-min resolution data displayed in Fig.
13 shows how the observational and modelled irradiance profiles have very similar characteristics during overcast and intermittent moments.
This model can be used to generate statistically accurate 1-min time series irradiance data from readily available 1-h meteorological input data.
The generated irradiance can be utilised as an input into other applications such as temporal intermittency studies, small scale grid impacts modelling associated with residential PV, and residential PV integrated battery storage modelling.
Furthermore, this methodology is not limited to a minutely resolution and can theoretically be applied to produce secondly time series, however, in order to have confidence in the accuracy of the output, higher quality resolution observation data is required.
This methodology could be extended by the inclusion of a spatial dimension using geographic smoothing techniques such as those demonstrated in Lave et al.
(2012), as well as exploring potential improved accuracy through the use of higher order Markov models.
The solar resource model has other possible applications and a freely downloadable example of the model will be provided in the hope that researchers will adopt and adapt it for their own purposes (Bright et al., 2015).
Thanks are given to Dave Allen for his support and advice, and to the editor and anonymous reviewers for their thorough and helpful critiques.
This work was financially supported by the Engineering and Physical Sciences Research Council through the University of Leeds Centre for Doctoral Training in Low Carbon Technologies (Grant No.
: EP/G036608/1).
Inelastic neutron scattering study of binding of para-hydrogen in an ultra-microporous metal–organic framework Metal–organic framework (MOF) materials show promise for H2 storage and it is widely predicted by computational modelling that MOFs incorporating ultra-micropores are optimal for H2 binding due to enhanced overlapping potentials.
We report the investigation using inelastic neutron scattering of the interaction of H2 in an ultra-microporous MOF material showing low H2 uptake capacity.
The study has revealed that adsorbed H2 at 5K has a liquid recoil motion along the channel with very little interaction with the MOF host, consistent with the observed low uptake.
The low H2 uptake is not due to incomplete activation or decomposition as the desolvated MOF shows CO2 uptake with a measured pore volume close to that of the single crystal pore volume.
This study represents a unique example of surprisingly low H2 uptake within a MOF material, and complements the wide range of studies on systems showing higher uptake capacities and binding interactions.
Hydrogen (H2) is a promising alternative energy carrier not only because it can potentially achieve zero-carbon emission at the point of use, but also because H2 has a high energy density (33.3kWh/kg) compared to hydrocarbons (12.4–13.9kWh/kg) [1].
The major scientific challenge for on-board H2 applications is that of inventing effective and efficient H2 storage materials, and there is an ever-increasing worldwide interest in meeting the United States Department of Energy’s (DoE) H2 storage targets of 5.5wt% gravimetric and 40gL−1 volumetric by 2017.
It is important to note that the DoE targets refer to storage within the whole system rather than within the storage medium alone, with a target operating temperature of −40 to 60°C and an operating pressure below 100atm.
Although solid-state H2 storage based on chemisorption and physisorption has been extensively studied over recent years, so far no material is able to meet this DoE target thus presenting a major impediment for the realisation of the “Hydrogen Economy”.
Nevertheless, physisorption of molecular H2 based upon the non-dissociative interaction in porous solids is an especially attractive option since it shows fast kinetics and favourable thermodynamics over multiple adsorption and release cycles [2].
Thus, enormous efforts have been focused on developing new porous solid materials for high capacity H2 storage.
Metal–organic framework (MOF) complexes are a sub-class of porous solids which show great promise for gas storage and separation due to their high surface area, low framework density, and tuneable functional pore environment [3].
MOF materials are usually built up from metal ions or clusters bridged by organic linkers to afford 3D extended frameworks with the formation of cavities ranging from microporous to mesoporous region.
Several members within this MOF family have achieved impressively high H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77K) [4] with a record of ∼16wt% total uptake capacity observed in NU-100 [5] and MOF-200 [6].
However, these high uptake capacities drop dramatically with increasing temperature, and thus none is a practical material.
There is thus particular emphasis on optimising the interactions between MOF hosts and adsorbed H2 molecules, and the identification of specific binding interactions and properties of gases within confined space represents an important methodology for the development of better materials that may lead us to systems of practical use.
In situ neutron powder diffraction (NPD) at below 10K has been used previously to determine the locations of D2 within a few best-behaving MOF materials incorporating exposed metal sites [7–12].
It has been found that D2 can bind directly to vacant sites on metal centres, and that the adsorbed D2 molecules have molecular separations comparable to that to D2 in the solid state.
These studies have provided invaluable structural rationale for their observed high gas adsorption capacities.
Research has thus focused understandably on MOFs with high H2 uptake capacities, while materials showing very low H2 uptake and/or incorporate fully coordinated metal centres are often ignored for this study.
Therefore, information on binding interactions within those low-uptake MOF systems is entirely lacking, but can still give important complementary data and potential understanding for the subsequent design and optimisation of hydrogen storage materials.
It is critical to the success of the NPD technique that the MOF complex adsorbs a significant amount of D2 to boost the observed signal.
This technique therefore has disadvantages when studying the binding interaction within MOFs with low uptakes.
Furthermore, static crystallographic studies cannot provide insights into the dynamics of the adsorbed gas molecules.
Thus, it is very challenging to probe experimentally the H2 binding interactions within a porous host system which has very low gas uptake due to the lack of suitable characterisation techniques.
We report herein the application of the in situ inelastic neutron scattering (INS) technique to permit direct observation of the dynamics of the binding interactions between adsorbed H2 molecules and an aluminium-based porous MOF, NOTT-300, exhibiting moderate porosity, narrow pore window and very low uptake of H2.
This neutron spectroscopy study reveals that adsorbed H2 molecules do not interact with the organic ligand within the pore channels, and form very weak interactions with [Al(OH)2O4] moieties via a type of through-spacing interaction (Al-O⋯H2).
Interestingly, the very low H2 adsorption has been successfully characterised as weak binding interactions and, for the first time, we have found that the adsorbed H2 in the pore channel has a liquid type recoil motion at 5K (below its melting point) as a direct result of this weak interaction to the MOF host.
Synthesis of [Al2(OH)2(C16O8H6)](H2O)6 (NOTT-300-solvate) and of the desolvated material NOTT-300 was carried out using previously reported methods [13].
H2 sorption isotherm was recorded at 77K (liquid nitrogen) on an IGA-003 system at the University of Nottingham under ultra-high vacuum from a diaphragm and turbo pumping system.
H2 gas used was ultra-pure research grade (99.999%) purchased from BOC.
In a typical gas adsorption experiment, ∼100mg of NOTT-300 was loaded into the IGA, and degassed at 120°C and high vacuum (10−10bar) for 1day to give fully desolvated NOTT-300.
INS spectra were recorded on the TOSCA spectrometer at the ISIS Neutron Facility at the Rutherford Appleton Laboratory (UK) for energy transfers between ∼−2 and 500meV.
In this region TOSCA has a resolution of ∼1% ΔE/E.
The sample of desolvated NOTT-300 (∼2.5g) was loaded into a cylindrical vanadium sample container and connected to a gas handling system.
The sample was degassed at 10−7mbar and 120°C for 1day to remove any remaining trace guest solvents.
The temperature during data collection was controlled using the instrument built-in cryostat and electric heaters (5±0.2K).
The loading of para-H2 (99.5%) was performed volumetrically at 40–50K in order to ensure that H2 was adsorbed into NOTT-300.
Subsequently, the temperature was reduced to 5K in order to perform the scattering measurements with the minimum achievable thermal motion for H2 molecules.
NOTT-300 crystallises in a chiral space group I4122 and has an open structure comprising infinite chains of [AlO4(OH)2] moieties bridged by biphenyl-3,3’,5,5’-tetracarboxylate ligands L4− (Fig.
1(a)).
The Al(III) ion in NOTT-300 has an octahedral coordination environment with six oxygen atoms, four of which are from carboxylate groups and two of which are hydroxyl groups, giving an [AlO4(OH)2] moiety.
These aluminium oxide moieties are further linked to each other via the corner-sharing hydroxyl groups μ2-OH.
Al(III)-carboxylate MOFs are usually constructed from the 1D aluminium oxide chains linked by the carboxylate ligands (Fig.
1(c)) [14–19].
Two distinct types of aluminium oxide chains have been reported previously.
The aluminium chain in MIL-120 is composed of [AlO2(OH)4] octahedra linking to each other via a common edge defined by two μ2–(OH) groups [14].
The different positions of the common edge in the two crystallographically distinct Al sites induce a cis–trans connection mode of the octahedral units, and thus zigzag chains are generated.
The aluminium oxide chains in MIL-53 and MIL-118 are composed of [AlO4(OH)2] octahedra linked to each other via vertex-sharing μ2–(OH) groups [16–18].
In both case, the connections of the [AlO4(OH)2] octahedra adopt trans configurations, generating straight, rod-like aluminium building blocks.
Depending on the coordination mode of the carboxylate groups, the aluminium chains in MIL-53 and MIL-118 show small differences in linkage of the octahedral nodes.
However, in NOTT-300 the corner-sharing [AlO4(OH)2] octahedra in the aluminium oxide chains display a cis configuration, and in order to accommodate the hydroxide groups, adjacent [AlO4(OH)2] octahedra are rotated by 90o with respect to each other, thereby generating 41 screw axes.
This type of connection is distinct from the other two examples, and, therefore, represents a new type of aluminium oxide building block (Fig.
1(c)).
The chirality of the NOTT-300 framework therefore arises from the formation of helical chains of [AlO4(OH)2] octahedra induced by the cis-configuration of μ2-OH groups.
This overall connectivity affords a porous extended framework structure with square-shaped 1D channels with hydroxyl groups protruding into them, endowing the pore environment with free hydroxyl groups over four different directions (Fig.
1(b)).
The diameter of the channel window, taking into account the van der Waals radii of the surface atoms, is approximately 6–7Å.
Desolvated NOTT-300 has a pore volume of 0.38ccg−1 and a BET surface area of 1370m2g−1 and so the general porosity of NOTT-300 is moderate within the family of MOF complexes.
The H2 isotherm (Fig.
2) at 77K for NOTT-300 shows exceptionally low adsorption uptakes (26ccg−1 or 0.22wt%), albeit NOTT-300 shows very high uptakes of CO2 (3.30Å) and SO2(4.11Å), both of which have a larger kinetic diameter than that of H2 (2.89Å).
The uptake of H2 increases sharply in the low pressure region and reaches saturation at ∼1bar.
By using the pore volume of NOTT-300 and the liquid density of H2 at its boiling point (20.3K), it is estimated that NOTT-300 can hold a maximum of 2.7wt% H2 (302ccg−1) at saturation.
Surprisingly, the experimental uptake is 10 times lower than this estimation, suggesting that NOTT-300 has unusually weak binding interaction to H2 molecules, even though the pore size of NOTT-300 (6–7Å) is believed to be optimal to afford strong overlapping potential to H2 molecules and thus boost the adsorption uptakes.
This anomalous H2 adsorption behaviour motivated us to further investigate the interactions between adsorbed H2 molecules and NOTT-300 host, and thus to understand its very low uptake.
Direct visualisation of the interaction between adsorbed H2 molecules and the NOTT-300 host is crucial to understanding the detailed mechanism of interaction and hence rationalising the unusually low observed uptake capacity.
INS is a powerful neutron spectroscopy technique which has unique advantages in probing H2 binding interactions by exploiting the high neutron scattering cross-section of hydrogen (82.02barns) [20].
As a result, the INS spectrum is ultra-sensitive to the vibrations of hydrogen atoms and the rotations of the hydrogen molecule, with hydrogen being ten times more visible than other elements.
In this study, we successfully used the INS technique to investigate the binding interaction for the NOTT-300/H2 system albeit it has such a low H2 uptake capacity.
The INS spectrum for the bare NOTT-300, collected at ∼5K to minimise the thermal motion of the adsorbed H2 and the framework host, was found to be similar to those for MOFs containing polyphenyl rings [18,21–23], and this experimental spectrum is in good agreement with the INS spectrum obtained from DFT calculation (Fig.
3(a)) [24].
Upon loading with H2 (0.25 H2/Al and 0.50 H2/Al) at 40–50K, the background of the INS spectra increases due to the recoil of the H2 molecules, and a broad hump is observed at low energy transfers (<30meV) confirming uptake of H2 by NOTT-300 (Fig.
3(b)).
The difference plots, calculated by subtraction of the background spectrum (bare MOF material and sample container) from the data collected for each H2 loading, display a broad hump centred at ∼20meV with only one small energy transfer peak at 8.8meV (Figs.
3(c) and (d)).
The rotational transitions of molecular H2 give a molecular proof of the local environment that the H2 molecules experience when adsorbed on a solid surface or strongly hindered site.
The rotational energy levels for a diatomic molecule are given by (1):(1)EJM=J(J+1)Brotwith J and M the angular momentum number and Brot is the rotational constant, that in the case of H2 Brot=7.35meV .
There are two nuclear spin isomers of molecular hydrogen, para-hydrogen (p-H2) with spins paired or antiparallel (↑↓) and ortho-hydrogen (o-H2) with the spins unpaired or parallel (↑↑).
Because quantum mechanical restrictions of the symmetry of the wave-function are responsible for the existence of both species, transitions between them are forbidden in optical spectroscopy, but in the case of INS the transitions between p-H2 and o-H2 are allowed because the neutron can exchange spin states with the molecule.
For p-H2 in the solid state, the environment is isotropic and the main rotational transition is J(1←0) that manifests itself as a very sharp peak at 14.7meV (Fig.
3(e)).
Such a peak has also been observed on high loadings of H2 on MgO thin films, indicating that H2 molecules are not interacting with the material surface [25].
In addition, a strong and broad shoulder with some weak overlying features appears at higher energies.
This shoulder peak, centered at ca.
37meV, is smooth except for a sharp curtailment at energies below the rotational transition, the intensity in this shoulder coming from rotational transitions displaced by the translational recoil of the H2 molecule.
While the rotational line at 14.7meV disappears completely and the onset of recoil occurs below the rotational transition for H2 in the liquid state, only the recoil features are apparent for H2 within NOTT-300 (Fig.
3(f)).
Fig.
3(c) clearly shows the spectra of adsorbed H2 to be in a liquid-like state within the pore channel of NOTT-300 and not in the form of a solid on the solid surface.
Even at higher loading where H2 adsorption in NOTT-300 reaches saturation, a very weak and broad peak at 14.7meV observed (Fig.
3(d)).
This peak indicates the presence of a very small amount of bulk H2 populated on the surface of NOTT-300, but the predominant feature is the recoil signal for H2 in the liquid state.
This observation is distinct from previous studies on adsorbed H2 which show binding to open metal sites which induce strong host–guest interactions to H2 molecules [12,26].
Thus, comparison of the INS spectra suggests that adsorbed H2 molecules have very weak interactions to the NOTT-300 host and, therefore, can rotate freely in the channel to give recoil rotational motion.
For H2 adsorption on the surface of a solid material or strongly hindered active site, the degeneracy of a transition at 14.7meV can be lifted as the freedom for the H2 molecule to rotate in all directions is restricted, thus resulting in splitting of the peak.
Depending on the energy of the interaction, the peak can split differently, and the shift and splitting of the peaks can therefore provide important information regarding the different adsorption sites and their geometry [27].
Further information can also be gleaned from the change in the peaks on loading with H2, demonstrating site saturation and/or site interference.
In this study, in addition to the broad hump at ∼20meV, a small peak appeared at low energy transfer, and the centre of mass of the rotational line of H2 is significantly shifted to 8.8meV, indicating the presence of a type of specific NOTT-300-H2 interaction.
To probe the site of the H2-framework interactions, it is important to evaluate the accessible voids of the MOF host.
The Al(III) centre in NOTT-300 is coordinated via six oxygen atoms to form a full octahedral coordination sphere, and can therefore be considered to be unavailable for direct interaction with adsorbed H2 molecules.
The small increase in the intensity of this peak in the difference spectra upon increasing the H2 loading from 0.25 to 0.5 H2/Al suggests that the interacting site reaches saturation quickly, presumably owing to space constraints.
Analysis of the crystal structure of NOTT-300 offers two possible void sites that could interact with adsorbed H2 molecules: the organic benzene rings and the inorganic [Al(OH)2O4] moieties.
The surface area around the benzene rings is sufficiently large to hold one H2/Al.
However, INS studies of carbon materials show that the phenyl ring only forms weak interactions with adsorbed H2 molecules, resulting in a small splitting or shift in the 14.7meV rotational line.
For example, in the INS spectra for H2-loaded activated carbon materials, the splitting of the free rotor is very small, with peaks observed at 12.5 and 15meV [28].
An even smaller splitting was observed in H2-loaded carbon nanotubes, with peaks at 13.5 and 14.5meV [29].
Such a small energy shift implies that the adsorbed H2 molecules are encountering relatively little hindrance for rotation, probably because of the weak van der Waals interactions between hydrogen and carbon.
In addition, no change to the molecular motion of the aromatic hydrogen atom on the phenyl rings of NOTT-300 (at ∼125meV suggested by the DFT calculation) was observed upon H2 loading.
Therefore, the population of the INS peak in this study cannot be attributed to H2 interaction with the benzene rings due to the observation of a significant shift in the rotational line to 8.8meV.
Therefore, this leaves the [Al(OH)2O4] moiety as the likely sites within the channel to interact with the H2 molecules.
It has been found that the interaction between H2 and oxygen atoms can cause a significant shift in the rotational line of H2.
For example, the INS spectra for H2 adsorbed on MgO surface show that the rotational line is shifted to 11meV [25].
Furthermore, INS studies on MOF-5, in which the Zn(II) centres are also fully coordinated by oxygen donors to form a [Zn4O(O2CR)6] building block, show two distinct peaks at 10meV and 12meV, which are attributed to H2 interactions with oxygen atoms from the [Zn4O(O2CR)6] building block and with the benzene ring, respectively [21,30].
Consistent results were also obtained from DFT calculations of H2-loaded MOF-5, where the adsorbed H2 molecules are found to interact most strongly with the [Zn4O(O2CR)6] clusters and least strongly with the benzene rings [31].
The peak at 8.8meV in the spectra for NOTT-300·nH2 (n=0.5,1.0) in this study is entirely consistent with the INS peak (10meV) observed for MOF-5, confirming that the adsorbed H2 molecules in NOTT-300 are interacting with [Al(OH)2O4] moieties (Fig.
4).
Due to the space constraints and the possible repulsive interaction with the active hydroxyl groups μ2—OH around [Al(OH)2O4] moieties, this site is saturated very quickly upon H2 loading consistent with the observed low uptake capacity.
INS studies on the H2-loaded material NOTT-300 have revealed the detailed binding interaction within this system.
The adsorbed H2 molecules in NOTT-300 are found to have recoil motion along the pore channel with freedom to rotate in all directions, reminiscent of the behaviour of liquid H2.
[Al(OH)2O4] moieties within the channel can only provide binding interactions to few H2 molecules and reach saturation quickly consistent with the experimentally observed low H2 uptake for this material.
The unusually low uptake of H2 in this study is related to the quantum effect of H2 which has a very low molecular mass [32] and the very weak interaction between the H2 and the NOTT-300 host is thus not sufficient to overcome this quantum effect, and therefore very little H2 is taken up by this porous material.
We have also confirmed that the ultra-low H2 uptake is not due to incomplete activation or MOF decomposition as the desolvated MOF shows very high CO2 uptake with a measured pore volume close to that of the single crystal pore volume.
SY gratefully acknowledges receipt of a Leverhulme Trust Early Career Research Fellowship and a Nottingham Research Fellowship, and MS receipt of an ERC Advanced Grant and EPSRC Programme Grant.
We are especially grateful to the STFC ISIS Neutron Facility for access to the TOSCA Beamline.
We thank the user support group at ISIS (Chris Goodway and Mark Kibble) for the technical help at TOSCA beamline.
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.chemphys.2013.11.004.
Supplementary data 1This doc contains supplementary information.
Perspectives on oblique angle deposition of thin films: From fundamentals to devices The oblique angle configuration has emerged as an invaluable tool for the deposition of nanostructured thin films.
This review develops an up to date description of its principles, including the atomistic mechanisms governing film growth and nanostructuration possibilities, as well as a comprehensive description of the applications benefiting from its incorporation in actual devices.
In contrast with other reviews on the subject, the electron beam assisted evaporation technique is analyzed along with other methods operating at oblique angles, including, among others, magnetron sputtering and pulsed laser or ion beam-assisted deposition techniques.
To account for the existing differences between deposition in vacuum or in the presence of a plasma, mechanistic simulations are critically revised, discussing well-established paradigms such as the tangent or cosine rules, and proposing new models that explain the growth of tilted porous nanostructures.
In the second part, we present an extensive description of applications wherein oblique-angle-deposited thin films are of relevance.
From there, we proceed by considering the requirements of a large number of functional devices in which these films are currently being utilized (e.g., solar cells, Li batteries, electrochromic glasses, biomaterials, sensors, etc.
), and subsequently describe how and why these nanostructured materials meet with these needs.
Surface engineering is a technological area of high scientific and industrial interest thanks to the wide set of applications benefitting from its contributions.
In addition to classical areas such as optics, tribology and corrosion/wear protection, where high compactness and a low porosity are important microstructural requirements, the advent of emerging technologies requiring porous or highly structured layers has fostered the development of thin film deposition procedures aimed at enhancing these morphological characteristics.
A typical experimental approach in this regard is the use of an oblique angle geometrical configuration during deposition.
Evidence of highly porous, optically anisotropic thin films grown using this technique were first reported more than a hundred years ago [1,2], but it was not until the 1950s–1970s that focus was given to the tilted columnar microstructure of these films and the factors controlling its development [3–9].
Following these initial steps, the last 20–25years have witnessed the systematic application of oblique angle deposition (OAD) procedures for the development of a large variety of devices in fields such as sensor technology, photovoltaic cells, magnetism, optical devices, electrochemistry and catalysis; all of which require strict control over porosity, anisotropy and/or crystallographic texture of the film.
A substantial number of papers, excellent reviews and books [10–20] have been published during this period, providing a well-documented background into both, the principles and the scientific/technological impacts of this synthesis method as well as the films it can produce.
The recent publication of a book on this exciting topic [21] clearly demonstrates the ample, ongoing interest that the scientific community has in this subject, and readers are addressed to these aforementioned reviews and books to acquire an initial insight into the main features and possibilities of the OAD of thin films.
In general, the term OAD or other widely used alternatives such as “glancing angle deposition” (GLAD), and “ballistic deposition”, are all associated in the literature with the physical vapor deposition (PVD) of thin films prepared by evaporation, which usually entails electron beam (e-beam) bombardment.
Since the OAD concept can be more broadly applied whenever the source of deposition particles and the substrate surface are obliquely aligned, it is used in this review to describe a particular geometry in the deposition reactor rather than a particular technique.
The intent here is to critically discuss the OAD of thin films from a perspective that is not restricted to evaporation, but also considers plasma- and laser-assisted deposition methods such as the magnetron sputtering (MS) technique, in which the presence of gas molecules may induce the scattering of particles and alter their otherwise rectilinear trajectory.
Ion beam-assisted deposition procedures, in which a significant amount of energy is released onto the film surface by obliquely impinging particles, are also briefly discussed.
A second scope in this review is to provide an atomistic insight into the mechanisms controlling the morphological development of OAD thin films, particularly its porosity, the tilt orientation of the nanostructures and any preferential texture.
Although different geometrical models and empirical rules have been proposed in the last few decades to roughly account for these features in films prepared by e-beam evaporation, and to a lesser extent by MS, we believe that the time is ripe to discuss with better accuracy the atomic mechanistic effects controlling the development of morphology and crystallography of OAD thin films.
Overall, the increasing interest shown by the scientific community in these films has been a direct consequence of their unique morphology, which has fostered the development of new applications and devices with specific functionalities.
As such, the third and final scope of this review is the description of some of the more outstanding applications and new devices that incorporate OAD thin films.
This review is organized into five sections in addition to this introduction.
Targeting readers with no previous knowledge on this subject, Section 2 presents a phenomenological description of OAD thin films and their unique morphology, which is formed from tilted nanocolumns promoted from the so-called “surface shadowing” effects.
The outline of this section is similar to that in other published reviews and books [10–21], thus readers may complete their understanding of the basic principles by consulting the relevant literature on the subject.
Generally speaking, the OAD term refers to a configuration in which the material flux arrives at the surface of a substrate at an oblique angle.
The most widely used approach to achieve this is the evaporation of material from a crucible through the impingement of electrons, although OAD thin films prepared by resistive evaporation under vacuum have also been reported [22–26].
Starting with this concept, Section 3 describes the different thin film processing techniques in which restrictions imposed by the deposition method or the presence of a gas in the chamber may alter the directionality of the particles being deposited.
Here, MS [27], plasma enhanced chemical vapor deposition (PECVD) [28], pulsed laser deposition (PLD) [29] and vacuum liquid solid deposition (VLS) [30] are all considered and subjected to specific analysis.
We also briefly discuss in Section 3 the effect of interaction momentum and energy exchange between the substrate and obliquely impinging energetic particles.
High-power impulse magnetron sputtering (HIPIMS) [31] and ion-beam-assisted deposition (IBAD) [32] are two typical examples of these latter procedures.
For the presentation of the majority of the results, we have assumed that readers are already familiar with common material characterization techniques such as electron microscopy and X-ray diffraction.
However, some mention has been made on less conventional methods such as grazing incidence small-angle X-ray scattering (GISAXS) [33] and reflection high-energy electron diffraction (RHEED) [34] that have been used recently in the study of OAD thin films, and which have contributed to deepening the understanding of their properties.
Contrary to the organization typically adopted by other reviews into OAD thin films, where basic properties such as their porosity, nanocolumnar shape or bundling association are correlated to their tilted nanocolumnar microstructure, we have opted to include a discussion on these features in Section 3.
The main reason for this is that mechanistic factors other than the “surface shadowing effect” are strongly affected, not only by the preferential oblique directionality of vapor atoms, but also by additional specific energetic interactions that are discussed for the first time in this review.
The discussion on methods other than evaporation, along with the analysis of bundling effects, adsorption properties and texturing effects in particular are all novel aspects that have not been systematically addressed in previous reviews on this subject.
Section 4 accounts for the main OAD features described in Sections 2 and 3 from a mechanistic perspective, aiming to explain the atomic-scale evolution of a thin film’s microstructure during growth by e-beam evaporation and MS at oblique angles.
Here, new fundamental ideas developed in the last years by our group are introduced, with these concepts allowing our understanding of the OAD phenomena to move beyond classical paradigms such as the tangent rule [9], as well as consider the film growth by methods other than evaporation.
Particularly relevant in this regard is a systematic assessment of the mechanistic aspects involved in MS depositions that are not present in conventional e-beam evaporation.
Section 5 describes a wide set of applications in which OAD thin films are incorporated into devices.
Unlike previous reviews and books [10–21], where a systematic and thorough description of thin film properties constitutes the backbone of the analysis, we have elected in this review to go directly to the end use so as to critically assess the many successful cases of OAD films being employed.
This exercise has resulted in an astonishing and overwhelming number of papers (approximately 300) being identified from the last seven-to-eight years in which a new applications or devices where OAD thin films have been introduced.
To cope with such a volume of results and to get a comprehensive description, we have relied on just a brief analysis of the expected performance of such films, providing a summary of the contributions made by different authors active in the field.
Finally, Section 6 presents a short critical evaluation of the potentiality of up-scaling the OAD technology for industrial production.
It will become apparent throughout this review that a critical shortcoming of OAD methods is their limited capability to be homogeneously applied over large surface areas, or to coat a large number of thin-film samples in a reproducible and efficient manner.
This final section therefore discusses some of the more recent proposals and considerations to overcome these limitations and make the OAD methodology commercially competitive.
Although the literature selected for this review encompasses the previously mentioned papers extending from the beginning of the twentieth century to the present, we have focussed on only those published in the last 10–15years; or in the case of the applications and devices discussed in Section 5, on the literature that followed the comprehensive review of Hawkeye and Brett in 2007 [16].
We expect that this systematization of the most recent literature on the subject, its critical analysis, and the proposal of new paradigms in thin film growth and description of emerging applications will be of particular interest to researchers already active in the field and, most importantly, attract new scientists from other areas that may be able to contribute to the development of this exciting topic.
In this section, we address the basic concepts involved in the deposition and processing of thin films in an oblique angle configuration by developing a comprehensive survey of the already well-established knowledge on the topic.
Though we describe the fundamentals of OAD by referring to the available literature, this knowledge can be complemented by reading some of the cited reviews on evaporated OADs [10–21].
This description is framed within the context of classical concepts in thin-film growth such as the Thorthon’s structure zone model (SZM) and other related approaches [35–37].
The core concept of this section is the so-called “shadowing mechanism”, which is the main governing process responsible for the appearance of singular nanostructures within the OAD of thin films.
The deposition of thin films at low temperatures (i.e., when thermally activated mobility processes are rather limited), while keeping the substrate in a normal geometry with respect to the evaporated flux of material, gives rise to compact thin films.
Within the classical SZM [35,36], the resulting microstructure is recognized as being Type I, which is characterized by very thin, vertically aligned structures and a high compactness.
When the deposition flux arrives at an oblique angle at the substrate surface, an additional variable is introduced into the growth process that has a significant influence on the development of the film’s microstructure and compactness.
It is generally accepted that the mechanistic factor controlling the nanostructural evolution of the films is a “shadowing effect”, which prevents the deposition of particles in regions situated behind initially formed nuclei (i.e., shadowed regions) [9,11].
Under these conditions, the absence of any diffusion of ad-particles means that vapor particles are incorporated at their point of impact, a process that is usually recognized as ballistic deposition giving rise to tilted columnar and highly porous microstructures.
Different descriptions and geometrical models accounting for these shadowing effects can be found in previous publications on the subject [38–41].
Although most recent mechanistic models for the formation of nanocolumns will be described in detail in Section 4, we will recall here some of the basic concepts that intuitively account for the importance of shadowing in controlling the microstructure of an OAD thin film.
Fig.
2.1(a) and (b) presents two ideal OAD geometrical schemes for the deposition of atoms from a small source or from a large area, respectively.
The first scenario is typical of e-beam assisted evaporation, which is the most common experimental arrangement for the OAD of thin films.
With the exception of a few thermal evaporation experiments [22–26], the second situation applies exclusively to MS and other deposition techniques that are performed in the presence of a certain gas pressure, a situation that is of relevance to the techniques and results reviewed in Section 3.
These schemes highlight the geometrical parameters relevant to the OAD of thin films, namely the zenithal angle of alignment between the source and the film (α), the azimuthal angle (ϕ) and the polar angles (δ) and (θ).
The zenithal and azimuthal angles are well defined in e-beam assisted evaporation experiments, as the target source can be considered located at a fixed point.
In contrast, the direction from which deposition particles can arrive from a large source can vary significantly, and so, for the sake of convenience, the value of α is in this case defined as the angle between the direction normal to the substrate and the center of the target.
According to Fig.
2.1(c), which illustrates the most basic notions of OAD of thin films [9–12], the first nuclei formed during the earliest stages of deposition project a shadow behind them that prevents the deposition of any further evaporated material within these ”shadowed” regions.
As deposition progresses, these nuclei induce the formation of tilted and separated nanocolumns which, as discussed in detail later in this section and in Section 3, is a feature that creates useable properties such as porosity, birefringence and magnetic anisotropy.
This relation has been recognized in early works dealing with OAD thin films, and since then, several attempts have been made to correlate the tilt angle of the nanocolumns (β) with the zenithal evaporation angle, α (see Fig.
2.1(b)).
The so-called tangent rule (Eq.
(2.1)) [6,9] and cosine rule (Eq.
(2.2)) [38] have to date been the two most popular heuristic expressions correlating the oblique angle of evaporation and the tilt angle of the nanocolumns:(2.1)tanα=2tanβ,(2.2)β=α-arcsin(1-cosα)/2.
Although other, more complicated, empirical descriptions have also been proposed [42–44], understanding the fundamental factors controlling the tilt angle of the nanocolumns is still an open question that requires additional considerations, as explained in Section 4.
Indeed, none of the proposed semi-empirical expressions have completely succeeded in accounting for the tilt angle of nanocolumns, particularly in the case of depositions with high zenithal angles (i.e., when α>60°).
Systematic studies of a large variety of metals recently carried out by Zhao et al.
[40,45] have clearly demonstrated that the tilt angle of the columns is a material-dependent property, and therefore cannot be solely attributed to simple geometrical relationships.
Based on atomistic considerations, we propose in Section 4.2 a model to explain why the tilt angle of the nanocolumns depends on the chemical nature of the deposited material, as well as the characteristics of the deposition reactor itself.
From a geometrical point of view, it is obvious that the shadowing effects associated with the directionality of the incoming particles will be relaxed when they are produced from a large area source (Fig.
2.1(b)) and/or when their directionality is randomized through scattering processes by gas molecules [46,47], as it is typically the case in MS, PECVD and PLD techniques.
Yet, even under these conditions, a preferential direction of arrival can be preserved to some extent, thereby inducing the growth of OAD thin films with tilted nanocolumnar structures.
The simple geometrical considerations regarding shadowing that are shown schematically in Fig.
2.1(b) have direct experimental implications with regard to the microstructural development in the e-beam assisted OAD of thin films (i.e., when evaporation occurs from a well-defined punctual source).
The first issue is the critical sample size at which deposition is homogeneous.
For instance, in an experimental configuration such as that depicted in Fig.
2.1(a), the zenithal deposition angle is at a maximum on the uppermost side of the substrate, and at a minimum on its lower side.
Consequently, the sample size needs to be constrained to within few square centimeters to maintain a well-defined geometrical configuration (i.e., a well-defined angle of incidence, α).
The up-scaling of this OAD methodology has therefore been a significant impediment to its industrial implementation, and recent proposals in this regard will be discussed in Section 6.
Vapor collimation is another important experimental factor, which as discussed in Section 4, may significantly affect the microstructure of the thin film.
Therefore, adjusting the target to substrate distance, or using slits and other alternatives, favours the effective control over the nanostructuration mechanisms of OAD thin films.
A basic assumption made in the previous considerations on the evolution of OAD thin film morphologies is that growth proceeds at low temperatures, and thus thermally activated diffusive processes on the film surface make little contribution to the nanostructuration of the film.
Within the framework of SZM [35–37], this implies a transition from Zone I to Zone T; the latter occurring when Ts/Tf>0.3 (where Ts is the temperature of the substrate and Tf the melting temperature of the deposited material).
In Zone T, thin films deposited with a normal configuration possess a microstructure formed by perpendicular and compacted columnar grains that are thicker than those produced in Zone I.
Similarly, heating the substrate during OAD produces a change in microstructure due to an increase in the mobility of ad-particles [49].
One way of rationalizing these nanostructural changes is to assume that dominance of shadowing in controlling the film growth is partially reduced by the diffusion of ad-particles from their point of impact to otherwise inaccessible “shadowed” regions.
Deposition rate also plays an important role in controlling surface diffusion and atom relocation, as the rapid arrival of new particles could bury and/or interfere with particles diffussing onto the substrate or thin film surface.
It stands to reason that high temperatures and low deposition rates would allow ad-particles to diffuse over greater lengths, with the opposite tendency being expected at low temperatures and high deposition rates.
This should be accompanied respectively by either a decrease or increase in the nanocolumn tilt angle (β), which was confirmed by Nakhodkin and Shaldervan [50] for a large variety of materials.
Nevertheless, in a systematic investigation of the OAD of magnetic metals and alloys (Co, Ni, Fe), Hara et al.
[51–56] showed that although the aforementioned relation between temperature and deposition rate was generally maintained, other effects besides diffusion must be involved to explain unexpected deviations in the growth tendencies and the appearance of more complex morphological patterns.
For a large variety of metals and oxides deposited by DC magnetron sputtering, Deniz et al.
[49] determined the nanostructuration temperature threshold θT=Ts/Tf, above which ad-atom surface diffusion becomes dominant over surface shadowing, thereby preventing the formation of a typical OAD nanocolumnar morphology.
For metals, this temperature threshold has been tentatively established at around 0.33Tf, while for oxides the value is approximately 0.5Tf.
Variation in the morphology of OAD thin films with deposition conditions was thoroughly explored by Abelmann and Lodder [11] by considering that changes in temperature, deposition rate and the extent of surface contamination by adsorbed impurities may have a significant influence on the tilt angle of nanocolumns, as well as on other morphological characteristics such as their bundling association.
According to these authors, the diffusion of ad-particles in either a random manner, or with specific directionality resulting from momentum exchange between the deposition particles and the thin film surface, can be affected by all these factors.
In Section 4, we will come back to the importance of momentum exchange and other mechanisms in determining the morphology of OAD thin films.
Similarly, the bundling association of nanocolumns, a morphological feature generally overlooked in the most recent literature on the subject, will be treated specifically in Sections 3.6.2 and 4.2.2.
In addition to the tilt angle, the shape and size of the OAD thin film nanocolumns can be altered by changing the temperature and deposition rate.
Although a quantitative evaluation of these changes is not yet available, various works have addressed this problem and studied the morphological changes and other nanostructures [57–59] induced as a function of temperature.
For example, using a simple nucleation model, Zhou et al [41] proposed that the evolution of the nanocolumn growth depends on the diameter of the nuclei formed during the initial stages of deposition.
That is, if larger nuclei are formed at high temperatures and/or they possess a low melting point, then thicker nanocolumns will develop during OAD.
A thickening of nanocolumns and a decrease in their tilt angle has been clearly observed in different materials by Vick et al.
[60] and Tait et al.
[38], who attributed these effects to an enhanced diffusion of ad-particles during growth.
However, in light of other results [11,53], it is evident that the general use of these diffusion arguments to predict OAD thin film morphologies needs to be critically discussed.
A quick overview of the literature on OAD thin films reveals that the concept sculptured films is used for both, the simple, tilted OAD morphologies hitherto discussed, as well as other more complex thin film microstructures in which the orientation and/or width of the nanocolumns adopt a singular form (e.g., oblique matchsticks, chevrons, multiple zigzags, S’s, C’s, helices and even superhelices) in response to moving the substrate during deposition [61].
Interest in these sculptured thin films is driven not only by scientific curiosity, but also because of the many potential applications that such nanostructures might have [16–18].
Different sculptured thin films can be obtained by changing either alternatively or simultaneously the zenithal (α), azimuthal (ϕ) and/or polar (θ) angle (c.f., Fig.
2.1(a) and (b)) to alter the incoming direction of the deposition flux during growth.
As demonstrated by Monte Carlo simulation analyses [62], shadowing effects can be regarded as the main factor in determining the large variety of nanocolumnar morphologies that can be achieved by moving the substrate around its different axes along a predefined pattern.
For illustrative purposes, Fig.
2.2 provides some selected SEM micrographs of TiO2 thin films that depict some of the different morphologies that can be obtained by tilting and/or rotating the substrate during the deposition.
Much work in this area has been carried out by Brett et al.
[16,17], who along with other authors [63–92], have systematically employed different substrate movements to tailor the morphology of sculptured thin films.
To illustrate the possibilities that can be offered by such films, Table 2.1 gathers together a selection of sculptured thin films and schematically describes their morphology, the substrate motion responsible for their formation, the type of material, and any related properties and/or target applications.
Note that rotating the substrate around the azimuthal axis (ϕ) in Fig.
2.1(a)) while maintaining a constant zenithal angle (α) provides a single degree of freedom to control the film nanostructure in what is sometimes referred to as “dynamic” OAD.
In this way, singular shapes such as zig-zags, helixes or spirals, or vertical nanocolumns can be obtained by controlling the rate, direction, etc.
of the movement.
For example, a zig-zag microstructure is obtained by tilting the substrate back and forth by 180° over set periods of time, while helical or spiral structures are obtained by slowly and continuously rotating the substrate around the azimuthal angle.
If the speed of rotation around ϕ is sufficiently high, then a vertical nanocolumn is obtained due to an averaging of the incoming flux angle during growth; the areal density and width of the nanocolumn being determined in this case by the rotation rate.
The control of this morphological quantity, therefore, represents a very simple method of tuning the density and other macroscopic properties of thin films.
In addition, S-type microstructures can be obtained by combining a tilt movement with a rapid azimuthal rotation during deposition.
Similarly, tilting around the polar angle θ (Fig.
2.1(a)) for a given period of time, while simultaneously rotating the substrate around the azimuthal angle ϕ, has been used to fine tune the density and refraction index of thin films.
Besides azimuthal rotation at a constant zenithal (α) angle commonly used to average the growth process and obtain perpendicular nanocolumns, tangential or flipping rotation of the sample substrate (see the last rows in Table 2.1) has been proposed by some authors as a way to fabricate low density films [89–92].
Even if such movements do not produce “sculptured microstructures” in the strict sense of a well-defined and shaped nanostructure, we consider them here because they may represent an interesting approach to up-scaling the OAD process (see Section 6).
Other sample rotations and/or movements include the so-called PhiSweep, wherein the substrate is rotated back and forth around ϕ, and left for controlled periods of time at each of these angular positions [93].
Meanwhile, with swing tilting rotation, the substrate is smoothly rotated back-and-forth along its azimuth within an angular range of Δϕ, which is called the swing angle.
These methods have also been used to obtain 3D nanostructures on substrates with previously deposited seed patterns [94,95] (see Section 2.5).
Another way to control the surface shadowing effect so as to achieve specific morphologies and layer properties is to carry out deposition using substrates with a well-controlled surface topography.
The idea of modifying the nanostructure of a film using a pre-deposited template is quite simple: it exploits the shadow effect by utilizing features already present on the surface.
This artificially generated shadowing mechanism is illustrated schematically in Fig.
2.3(a), where it is considered that no deposition may take place behind an obstacle.
The key point, therefore, is to control the shadow cast by a series of patterned hillocks, protrusions or other well-defined features on the surface.
Some model calculations have been proposed to determine the dimensions of these features in ordered arrays, their relative arrangement in the plane and the distances between them so as to effectively control the shadowing effect and ultimately obtain new artificially designed nanostructures [96].
The simplest version of OAD on a template uses a rough substrate surface; the elongated mounds or facets promoting the accumulation of deposited material onto the most prominent features, while at the same time preventing the arrival of vapor flux at the shadowed regions [97].
Expanding on this simple approach, the use of substrates with a well-defined lithographic pattern opens up a range of new and unexpected possibilities for the nano-structuring of OAD thin films.
To this end, patterned substrates consisting of well-ordered nano-structured array patterns or seed layers have been prepared through a variety of lithographic methods, laser writing, embossing or other top-down fabrication methods [24,87,98–109].
Among these, the use of colloidal lithography (i.e., OAD on a substrate pre-coated with packed nanospheres of different materials) has gained much popularity over the last few years thanks largely to its simplicity [110–113].
With the aid of selected examples, Fig.
2.3 illustrates the possibilities that these approaches provide for the tailored growth of all-new supported nanostructures.
Other options for controlling the thin film morphology arise when the deposition onto a template pattern is combined with the controlled movement of the substrate.
For example, S-shaped 3D photonic structures used as polarizers (c.f.
Table 2.1) have been fabricated by Phisweep or swing rotation of substrates with a pre-deposited template structure [93,94].
At present, this use of templates in the OAD of thin films has transcended pure scientific interest, and is now developed for advanced technological applications.
For example, mass-produced metal wire grid polarizers are being fabricated via the OAD deposition of antireflection FeSi and SiO2 layers on previously deposited aluminum columnar arrays [87,114,115].
Other emerging applications in biomedical areas [107,116,110,117], or for controlling the wetting properties of substrates [102,118], also rely on the use of patterned surfaces rather than the direct OAD of thin films (see Section 5.5).
In most of the deposition techniques discussed thus far the evaporated material originates from a single target source, and thus the chemical composition of the film produced is homogeneous.
An alternative methodology for the OAD of thin films relies on the use of two sources positioned at oblique polar (δ) angles to the substrate at an equal or different zenithal (α) angle (Fig.
2.1(a)) in what has been named glancing angle co-deposition (GLADCO) by some authors [119].
Typically, such a configuration of evaporation sources produces nanocolumns with an inhomogeneous composition when operated at constant evaporation rates.
This inhomogeneous growth was critically evaluated by Van Kranenburg et al.
[10], who also studied the development of bundling associations between nanocolumns in a direction perpendicular to the flux.
This type of inhomogeneous growth process was also studied theoretically by means of MC models [120], and more recently by statistical models [45].
In the search for new functionalities and properties, different authors have used these two aforementioned approaches to tailor the composition and distribution of components in a growing nanostructure.
Simultaneous or alternant evaporations have been used along with control over the relative evaporation rate of each source to tailor the morphology and composition of OAD thin films.
A survey of the different possibilities offered by this technique was recently undertaken by He et al.
[19], who described new options such as controlling the nanocolumn composition along its length or diameter, or incorporating nanoparticles into either single or more complex nanostructures by simply moving the substrate.
These principles have been applied by various authors to prepare laterally inhomogeneous nanocolumns [111,113,121,122], or to fabricate alloy or multicomponent nanocolumnar films [123–129].
The combination of the template approach discussed in the previous section and evaporation from two sources opens the possibility of simultaneously controlling both, the microstructure (i.e.
thickness, orientation, etc.)
and the lateral composition of the nanocolumns.
Fig.
2.4 gives an example where OAD has been carried out using two sources on a substrate with packed nanospheres to produce thick nanocolumns with a laterally inhomogeneous chemical composition.
Yet despite these advances, the possibilities offered by this method to further control the nanocolumnar morphology and other properties of the films remain largely unexplored.
This was made clear in a recent work by He et al.
[125], which demonstrated the possibility of enhancing the porosity of Si–Cu and SiO2–Cu nanocolumnar arrays through the selective removal of copper with a scavenger chemical solution.
It is likely considered that e-beam, and to a lesser extent thermal evaporation, have been the most widely utilized methods for the OAD of thin films.
This has certainly been true regarding the effective control of the geometry of the nanofeatures through shadowing effects (e.g., the growth of sculptured thin films); however, there are other alternatives that make use of the OAD geometrical configuration to develop new microstructures, textures and general properties in films.
In these other methods, several physical mechanisms, including the thermal diffusion of ad-particles discussed in the previous section, can affect the deposition process and the efficiency of shadowing mechanisms.
An interesting effect is the relocation of deposited particles through liquid–solid preferential agglomeration at sufficiently high temperatures, which constitutes the basis of the so-called vacuum liquid solid (VLS) method [30].
Another possibility is to alter the direction of the deposition particles by introducing a gas to scatter them during their flight from the source to the substrate, thereby inducing a certain randomization in their trajectories [46,130].
Such a situation is encountered when deposition is carried out by MS, PLD and PECVD, among others.
Total or partial randomization of the particle trajectory leads to a softening of the geometrical constraints imposed by a pure “line of sight” configuration, and in doing so causes the microstructure of the films to deviate from the hitherto discussed typical morphological patterns.
In the presence of a gas, not only is the geometry of the deposition process critical to controlling the film microstructure, but also the mean free path of the deposition particles.
Consequently, in order to retain at least some of the features typical of OAD growth, particularly the tilt orientation of the nanocolumns and a high porosity, it is essential that the deposition particles arrive at the film surface along a preferential direction, thereby allowing shadowing mechanisms to take over the nanostructural development of the films.
The vapor–liquid–solid (VLS) technique was first proposed by Wagner and Ellis [131] as a suitable method for the fabrication of supported nanowires.
It is a high temperature procedure, in which liquid droplets of a deposited material act as a catalyst to dissolve material from the vapor phase, which then diffuses up to the liquid solid interface and precipitates as a nanowire or nanowhisker.
Typical VLS growth therefore proceeds through two transition steps, vapor-to-liquid and liquid-to-solid, making it vital to maintain the substrate temperature at a critical value capable of enabling these diffusion processes.
The combination of VLS with OAD has proven very useful in tailoring nanostructures with very different shapes (nanowires, nanotrees, other branched structures, etc.).
For example, by keeping the substrate at typical VLS growth temperatures, crystalline Ge nanowhiskers with a nanostructure determined by the angle of material flux have been obtained [132].
Fancy ITO (indium tin oxide) 2D-branched nanostructures have also been fabricated by combining VLS and OAD while azimuthally rotating the substrate [133–137].
In this case, liquid indium–tin alloy droplets act as both the catalyst and the initial nuclei for branch formation.
This means that the final branched structure can be controlled by rotating the substrate or varying the zenithal angle according to a predefined pattern to adjust the shadowing effect created during the arrival of vapor flux (see Section 5.1) [134,135].
In the MS technique, a flux of sputtered atoms or molecules is produced by the bombardment of plasma ions onto an electrically biased target.
The sputtered atoms are then deposited onto a substrate, which is usually placed in a normal configuration with respect to the flux direction and therefore parallel to the target [27].
Owing to its high efficiency, reliability and scalability, this technique is widely used industrially [138].
The MS technique has been also used in OAD configurations by placing the substrate at an oblique angle with respect to the direction normal to the target (see Fig.
2.1(b)).
Despite the complexity of the mechanisms involved (see Section 4), no systematic correlations have yet been established between the deposition parameters (e.g., gas pressure, electromagnetic power of the magnetron, pulse regime, target–substrate distance, deposition angle, etc.)
and the resulting thin film microstructure.
Besides the existence of particle scattering events, another significant difference with respect to the e-beam evaporation is the large size of the material source (see Fig.
2.1(b)).
In Section 4, we will discuss some of the mechanistic effects that may induce changes in the microstructure of MS-OAD films.
While e-beam evaporation has been used for the deposition of a large variety of materials, including amorphous and crystalline semiconductors [80,86,106,139,140], metals [29,141–146], oxides [48,79,89,147–149] and even molecular materials [71,103,150], MS has been the most widely used method for the deposition of metals and oxides when control over the crystalline structure and/or surface roughness is an issue.
Within the scheme of SZMs [35–37], MS deposition results in films that grow in Zone T over an ample range of pressures and temperatures.
Moreover, new physical effects that can affect the microstructure and crystallographic structure of the films can occur under OAD conditions.
Nevertheless, the growth of separate nanocolumnar structures tilted usually in the direction defined by “shadowing” mechanisms can be considered a characteristic feature of MS-OAD films.
Fig.
3.1 shows a series of nanostructured Mo thin films deposited by MS at oblique angles, wherein the tilt angle of the nanocolumns and their separation clearly increases with the zenithal angle of deposition, α [151].
There are numerous examples of nanostructured thin films (mainly oxides, nitrides and metals) in the literature on MS-OAD [68,69,151–172]; but in general, the quantification of the nanocolumn tilt angle and film porosity is not as straightforward with MS as it is with e-beam deposition.
To approach conditions existing during vacuum evaporation and promote tilted nanocolumnar growth, MS deposition at low pressures and/or at short target–substrate distances [60] has been proposed as a way of reducing gas-particle scattering effects.
In a recent work on gold deposition by MS, we studied the influence of gas pressure and substrate inclination on the nanocolumnar development and porosity of films [172].
Through this, we identified a continuous microstructural variation with pressure and tilt angle of the substrate from compact to porous films, the latter being formed by tilted nanocolumns or continuous layers with elongated pores extending along the deposition direction.
The mechanistic factors controlling this evolution have been rationalized within an extended structure zone model scheme that will be discussed in detail in Section 4 (see Section 4.3.4).
Additional movement of the substrate, typically in the form of azimuthal rotation (i.e., dynamic OAD), has also been used during MS-OAD to create modified thin-film microstructures.
In general, the effects obtained by incorporating this additional degree of freedom are similar to those reported for the evaporation technique, except the microstructure may now progressively evolve from separated nanocolumnar nanostructures to homogeneous porous layers by increasing the gas pressure or target–substrate distance [68,151,170–172].
MS in OAD geometries has emerged as a powerful tool to tailor the crystallographic texture of films along both out- and in-plane directions; i.e., it makes it possible to obtain layers in which individual crystallites are aligned along preferential crystallographic directions perpendicular to the substrate and/or parallel to the substrate plane.
Crystallization is very common in metals, even when deposition takes place at room temperature, and can occur in oxides if the substrate temperature is high enough.
Thus, crystallization can be promoted by increasing the energy and momentum given to the surface via ion bombardment [11,157].
A more detailed description of the texturization effects in OAD thin films will be presented in Section 3.7.
In the PLD technique, a laser beam impinging onto a target ablates its uppermost layers that form a plasma plume with numerous energetic particles.
This plume is directed toward a substrate, where the ablated material is deposited in the form of a thin film [29].
Remarkably, a key feature in this technique is the conservation of the target stoichiometry when employing multicomponent targets.
Moreover, in general conditions, the highly energetic character of the species in the plume promotes that thin films usually grow compact, although nanocolumnar and porous thin films can be grown by intentionally introducing typical OAD geometries [173–178].
Much like the OAD of thin films prepared by evaporation or MS, the main purpose in most cases is the effective control of texture alignment so as to obtain a columnar microstructure and/or porous layers.
For example, in cobalt ferrite oxide thin films, preferential orientation along the [111] crystal direction and compressive strain tunable as a function of film thickness can be obtained by PL-OAD, whereas almost strain-free and random polycrystalline layers are produced when using normal-incidence PLD under the same conditions [177].
Nanocolumnar mixed oxide thin films, including YBa2Cu3Ox [179] and La0.7Sr0.3MnO3 [180], have been prepared by PL-OAD to take advantage of the preservation of the target stoichiometry.
Thin films with simple composition, enhanced porosity and controlled microstructure have also been deposited by PL-OAD, for instance to grow porous nanocolumnar ZnO thin films [175,176] by controlling both, the zenithal and azimuthal angles of deposition.
Similarly, porous carbon thin films consisting of perpendicular nanocolumns [173] (see Fig.
3.2(a)) or more complex zig-zag morphologies [174] were obtained through azimuthal rotation of the substrate to an oblique angle configuration.
In all these cases, even though the plasma plume was full of highly energetic neutral and charged species, shadowing effects are still of relevance during deposition, promoting the formation of an open film microstructure.
This means that parameters such as the temperature of the plume species (related to the power and other characteristics of the laser pulses), the distance between the target and substrate, the pressure in the deposition chamber and the effective width/length of the plume need to be carefully controlled to effectively tailor the final morphology of a film.
Plasma-assisted deposition, usually called plasma enhanced chemical vapor deposition (PECVD), is a typical thin film fabrication technique employed to homogeneously coat large-area substrates that is only rarely used for the preparation of layers with singular nanostructures [27,181].
In this process, a volatile-metal precursor is dosed in a reactive plasma chamber, where it decomposes to deposit a thin film, releasing volatile compounds (e.g., CO2, H2O, etc.)
that are evacuated together with the gaseous reaction medium.
Depending on the plasma characteristics, thin films of metal, oxide, nitride, oxinitride or other compounds can be obtained.
By definition, this procedure requires a certain plasma gas pressure to operate, a feature that seems to preclude the intervention of shadowing effects associated with the preferential arrival of deposition particles travelling in a particular direction.
It should be noted, however, that this does not exclude nanostructuration due to shadowing effects of randomly directed species [46].
However, mechanisms inducing certain preferential directionality of deposition species have been incorporated in plasma-assisted deposition by making use of particular effects.
The best known example of this is the preparation at elevated temperatures of supported carbon nanotubes (CNTs) and other related nanostructures (e.g., graphene) [182–185].
The growth of CNTs is catalytically driven by the effect of metal nanoparticles that are initially deposited on the substrate, and which remain on the CNT tips during growth.
During this process, it is assumed that the electric field lines within the plasma sheath are perpendicular to the surface (a common feature of plasma-conductive wall interactions), which is a decisive factor in the vertical alignment of the CNTs [182,183].
As previously reported [185], tilted CNT orientations can be achieved by applying an intense external electric field at an oblique angle to the substrate and introducing a precursor gas parallel to it, which is believed to induce the preferential arrival of charged species along the externally imposed electric field.
In the so-called remote PECVD processes, where the plasma source is outside the deposition zone, the preferential direction of arrival of precursor species moving from the dispenser toward the vacuum outlet has been used to fabricate metal-oxide composite layers consisting of well-separated vertical and tilted nanocolumns, as well as other types of branched nanostructures [186–188].
An example of such a nanostructure is depicted in Fig.
3.2(b), which shows a series of zig-zag Ag@ZnO nanorods created by the plasma deposition of ZnO on substrates with deposited silver nanoparticles.
Note that in this experiment, the substrates were tilted to impose a preferential direction of arrival to the precursor molecules.
In addition to this geometrical OAD arrangement, plasma sheath effects and the high mobility and reactivity of silver under the action of an oxygen plasma seem critical factors in forming a nanocolumnar microstructure while inhibiting the growth of a compact film that would be expected with conventional PECVD [189].
The possibilities offered by the combination of different plasma types and OAD to tailor the microstructure of thin films and supported nanostructures are practically unexplored, and therefore very much open to new methodological possibilities.
A good example of work in this direction is that of Choukourov et al.
[190], in which tilted nanocolumnar structures of Ti–C nanocomposites were fabricated by adding hexane to the plasma during the MS-OAD deposition of Ti, altering the composition of the film and its microstructure.
A significant number of other interesting examples of the newly coined term “plasma nanotechnology” can be also be found in recent literature on the subject [191].
In the previous section, it was indicated that in addition to deposition species, other neutral or ionized particles generated in the plasma may interact with the film during its growth, particularly in the case of PECVD, MS and PLD.
That is, both the deposition particles and plasma species tend to exchange their energy and momentum with the substrate surface.
In early models dealing with the analysis of OAD thin films prepared by evaporation, the influence of particle momentum on the evolution of the film’s microstructure was explicitly considered [11], but its effectiveness in controlling the microstructure was not always clear due to the small amount of momentum exchanged in most OAD processes.
For instance, in thermal or electron beam evaporation processes, the energy of the deposition particles is only in the order of 0.2–0.3eV, and so its influence on the development of a specific thin-film microstructure and/or crystalline structure can usually be neglected.
Yet, in conventional PECVD or MS, the kinetic energy of some species can reach much higher values in the order of a few tens of eV due to ions impinging onto the growing surface, although the overall amount of energy exchanged per deposited species usually stays low enough to not induce any appreciable change in the film nanostructure.
This situation changes, however, when the energetic interactions are increased beyond a certain level [36].
In this section, we will address situations in which the arrival of deposition material at the substrate is accompanied by a massive transfer of energy and momentum carried by either the deposition particles themselves, or by rare gas ions and other molecules present in the system.
The pronounced transfer of energy and momentum to a surface along an oblique orientation creates significant changes in the microstructure and other characteristics of thin films.
Thus, even if the supplied energy contributes to a higher compactness by increasing the mobility of ad-particles [31,184], the film nanostructure may still keep some of the typical characteristics of an OAD film such as morphological asymmetry (i.e., the appearance of a tilted pattern) or a preferentially oriented crystallographic texture.
Although a detailed description of the mechanisms involved is far beyond the scope of the present review (readers are instead referred to more specific books or reviews on this subject [32,192,193]), we will provide here some clues for the OAD of thin films prepared by high-power impulse magnetron sputtering (HIPIMS) [31] and ion-assisted deposition (IAD) [32].
This deposition method operates under extremely high power densities in the order of some kWcm−2, with short pulses (impulses) of tens of microseconds and a low duty cycle (on/off time ratio <10%).
Under these conditions, there is a high degree of ionization of sputtered material and a high rate of molecular gas dissociation.
The impingement of these charged species and their incorporation onto the growing film also likely contribute to its densification [31], which is the most valuable feature of HIPIMS from an application point of view.
Another advantage of this technique is the fact that it offers less line-of-sight restrictions than conventional MS, thus ensuring that small surface features, holes and inhomogeneities are well coated.
The combination of high densification, homogenization and conformity are linked to the high energy and momentum of obliquely directed neutral particles and the perpendicular incidence of plasma ions produced in the HIPIMS discharge.1Plasma ions generally follow the lines of electric field in a plasma.1 This peculiar transport behavior results in a variation in the flux, energy and composition of the species impinging on the substrate as the deposition angle is changed.
Specifically, the ion to neutral ratio is generally higher and the average ion energy lower when substrates are placed perpendicular (i.e., in a OAD configuration) to the target than instead of classic parallel configuration [194].
The composition of the deposited films is also modified at oblique angles due to the angular distribution of the neutral species, the ejection of which tends to be favored along the target normal direction.
This combination of effects has been utilized in controlling the composition and microstructure of ternary Ti–Si–C films [195], an example of the so-called MAX phases [196] that are characterized by their attractive combination of metallic and ceramic properties.
Although the influence of the deposition angle in combination with other deposition parameters on the microstructure and texture of HIPIMS-deposited thin films has not been systematically investigated, differences between conventional MS and HIPIMS have been observed in the deposition of Cu and Cr films [197,198].
That is, with these two metals, it has been found that the tilt angle of the nanocolumns is lower in HIPIMS-OAD than in conventional MS-OAD.
For copper films grown at room temperature, the tilt angle is independent of the vapor flux’s degree of ionization, whereas with Cr at elevated temperatures it is affected by ionization.
These differences, as well as the changes induced in the crystallographic texture of the films, have been accounted for by a phenomenological model that incorporates atomic shadowing effects during the early nucleation steps [197,198].
Ion-beam-assisted deposition is a classic method for producing highly compact thin films through the simultaneous impingement of deposition particles and energetic ions [32].
This term encompasses a large variety of procedures, which vary depending on how the deposition species and ions are produced or the ion energy range.
Herein, we will comment only on the effect of low energy ions; i.e., with typical energies in the order of some hundreds of eV or lower.
In the first possible scenario under these conditions, thin film growth initially occurs through the aggregation of neutral species, being assisted by ions stemming from an independent source.
For the sake of convenience, we shall refer to this as ion-assisted OAD (IA-OAD) [199–204].
In the second case, a thin film is formed by the impingement of a highly or completely ionized flux of deposition species arriving at the substrate along an oblique direction.
We will refer to this experimental arrangement ion-beam OAD (IB-OAD) [205,206].
A straightforward application of the IA-OAD method concerns the control of the crystalline texture of the deposited films [199,204]; i.e., the variation of the crystallographic planes of the crystallites with respect to the substrate by changing the impingement angle of the ions.
Moreover, as neutral species are produced in IA-OAD by e-beam evaporation, this has been proposed as a method capable of also controlling the film morphology (i.e., the tilt angle of the nanocolumns, areal density, etc.
), and possibly also the birefringence of the films [201,202,207–209].
Indeed, it has been demonstrated that the tilt angle of the nanocolumnar structures can be increased in a large set of oxides and fluoride materials [201], and their areal density generally decreased, when the growing films are bombarded with neutralized ions with a kinetic energy of 390eV.
An enhanced surface diffusion of ad-particles due to heating and/or momentum transfer from the accelerated species and/or sputtered particles have been suggested as possible reasons for these morphological variations.
However, the large number of parameters involved in these experiments and the factors affecting the physical processes involved (e.g., the effects of sputtering efficiency [203] and relocation phenomena on the angle of incidence and energy of the impinging ions [210]) have so far prevented any predictive description of tendencies.
The IB-OAD processes, wherein the majority of deposition species are ionized and impinge along a well-defined off-normal angle with respect to the substrate, has not yet been extensively studied.
A nice example of the nanostructuration that can be achieved in the vacuum arc deposition of Ni and C, given in Fig.
3.3(a) [206], shows a selection of cross-sectional electron micrographs and GISAXS (glancing incidence small angle X-ray scattering) patterns of this type of thin films.
The red arrow indicates the impingement direction of the ionized atoms which, in this experiment, had energies ranging between 20 and 40eV.
These micrographs show that the films are compact and formed from tilted layers or lamellas containing C and Ni.
They also show that a decrease in the tilt angle of the layers and a widening of their periodic structure occur when the Ni content and ion energy are increased.
These results suggest that, on average, the ion-induced atomic mobilization in the growing film is not random, but instead proceeds preferentially along the direction of momentum of the impinging ions.
The importance of momentum and energy transfer is further highlighted by the opposite tilting orientation of the Ni and C lamellas with respect to that of the nanocolumns whenever they grow as a result of surface shadowing mechanisms; e.g., by e-beam or MS OAD.
This figure also shows other GISAXS characteristics [211] for tilted nanocolumnar films prepared by e-beam OAD.
Although this characterization technique has not yet been extensively used to study OAD thin films, it has been used recently to obtain information regarding the tilting orientation of the nanostructures and the correlation distances from the asymmetry and position of maxima in the GISAXS patterns [33,171,206].
In the previous section, we implicitly assumed that the development of tilted and/or sculptured nanostructures is the most typical morphological characteristic of OAD thin films.
In this section though, we develop a more detailed discussion of other relevant microstructural features such as roughness, porosity or preferential association of the nanocolumns (a phenomenon usually referred to as bundling); features that grant OAD films some of the unique properties required for a large variety of applications.
A discussion on the development of specific crystallographic textures will also be presented later in this section.
Although most of these characteristics stem from shadowing mechanisms, the contribution of energetic interactions or other random deposition processes outlined previously will also be taken into account as part of this analysis.
The termination of the nanocolumnar features at the surface of a film generates a peculiar topography that is characterized by a high surface roughness.
Surface analysis of OAD evaporated thin films by atomic force microscopy (AFM) reveals that the RMS surface roughness increases with the deposition angle (α) and thickness of the films [70,212,213].
This tendency stems from the general finding that the diameter of the nanocolumns increases with their length in a competitive process, wherein the growth of some nanocolumns prevails against that of others; a situation that is shown schematically in Fig.
2.1(c).
Depending on film thickness and deposition angle, typical RMS roughness values ranging from 2–3 to 15nm have been reported for OAD thin films prepared by e-beam evaporation [70,212,213].
The progressive widening of individual nanocolumns with film thickness has been systematically analyzed within the context of the dynamic scaling theory of surface growth [214,215].
This conceptual framework analyzes the evolution of surface patterns and roughness with deposition time (i.e., with thin-film thickness) to give a scaling law of: ws∼tp, where ws is the RMS roughness and p the so-called growth exponent, a parameter whose value depends on the dominant mechanisms governing the film growth.
This association between surface roughness and the width of column terminations has prompted many authors to check for the existence of a similar power law correlating nanocolumn width and height; i.e., w∼dp′, where w is the width of the nanocolumn at a certain length, d, with respect to the substrate, and p′ is the growth/scaling exponent [22,23,109,216–221].
The validity of this relation has been proven for many metals and oxides, which paved the way for different mechanistic studies of OAD thin film growth.
Interesting morphological features, such as a correlation in lengths between nanocolumns or an increase in nanocolumnar diameter, have been related to the scaling parameter [219].
Furthermore, the influence of temperature on the power law has also been recently examined along with the relations between the exponent values and the growth process [220].
A simple power law description of nanocolumn width evolution with film thickness cannot hold true when shadowing is not the prevalent mechanistic factor of growth, as is the case in MS-OAD and other deposition processes discussed in Section 3.5.
Thus, crystallization and growth in Zones II or T of SZM clearly preclude any description of the growth mechanism according to simple rational schemes based on dynamic scaling concepts [161].
For example, thin film crystallinity may significantly alter the dependence of roughness on thickness when facetted growth of nanocrystals suddenly occurs.
An example of such a sudden modification in surface roughness in relation to crystallinity was reported by Deniz et al.
[171] in AlN MS-OAD thin films, where a sharp increase in RMS roughness was found when a given nitrogen concentration was added in the plasma gas.
An initial assessment of the nanocolumnar arrangement in OAD thin films might suggest the absence of any clear organization and a random distribution of nanocolumns within the film.
However, some experimental evidence by AFM and SEM analysis of the surface and morphology of thin films directly contradict this simple view [48,219–222].
For example, in the case of evaporated OAD thin films, it has been found that well-defined correlation lengths and intercolumnar distances emerge depending on the deposition conditions.
What is more important, these can be formally described with a power law scaling approach [219].
Similarly, through SEM analysis of the surface of a series of OAD thin films, Krause et al.
[222] identified repetitive correlation distances between surface voids separating the nanocolumns that were dependent on the deposition parameters.
Using AFM, Mukherjee et al.
[220] were also able to determine the period of surface roughness features in different OAD thin film materials, and correlated the values obtained with specific growth exponents.
To confirm the existence of correlations between nanocolumns in the interior of films, not just at their surface, we have previously used a bulk technique such as GISAXS [33].
With this method, it is possible to determine both the tilting orientation of the nanocolumns and the correlation distances among them.
Fig.
3.3 shows a series of GISAXS patterns corresponding to TiO2 thin films that were prepared by e-beam OAD at different zenithal angles.
We see from this that while the asymmetric shape of the patterns clearly sustains a tilted orientation of the nanocolumns, the position of the maxima in each pattern provides a rather accurate indication of the correlation distances existing in the system [33,48].
It is also interesting that these patterns depict two well-defined maxima, suggesting the existence of a common correlation distance of about 14nm in all the films, as well as a second much longer correlation distance parameter that progressively increases from 30 to 200nm with increasing film thickness and deposition angle.
We tentatively attributed the smaller of these correlation distances to the repetitive arrangement of small nanocolumns/nuclei in the initial buffer layer formed during the first stages of deposition, with the larger of the two being attributed to the large nanocolumns that develop during film growth.
Owing to the bulk penetration of X-rays the values obtained were averaged along the whole thickness of the film, the progressive increase observed with thicker films confirming that the nanocolumn width extends from the interface to the film surface.
In early studies of evaporated OAD thin-film materials [10,11,54,56], it was soon recognized that nanocolumns can associate in the form of bundles; i.e., laterally connected groupings of nanocolumns arranged in a direction perpendicular to the vapor flux.
This bundling association has since been reported in a large variety of OAD thin films prepared by either evaporation or MS [143–145,223–226], and though development of this microstructural arrangement has been mainly reported for metal thin films [10,11,54,56,144,145,223,224], it has also been utilized as a template structure with oxides to develop new composite thin films (see for example [225] and [226]).
Surprisingly, aside from some detailed discussions in early reviews [10], this bundling phenomenon has not attracted much attention in recent investigations on OAD thin films.
In the present review, we therefore highlight its importance and refer the reader to Section 4, where we present a critical discussion of the growth mechanisms contributing to the development of this microstructural arrangement.
For many applications such as sensors, electrochemistry, catalysis, electrochromism, and antireflective layers, the porous character of an OAD thin film is a key feature in device fabrication (see Section 5).
Here, two related concepts need to be taken into account: porosity and effective surface area.
The former refers to the actual empty volume that exists within the films, whereas the latter represents the effective area created by the internal surface of these pores that makes possible the interaction with a medium through adsorption.
Note that some pores might be occluded, in which case their internal area would not affect the adsorption properties of the films.
Porosity can be assessed in an indirect way by determining the optical constants of thin films, and then deriving the fraction of empty space by means of the effective medium theory [227].
However, as this approach requires relatively complex optical models based on “a-priori” assumptions regarding layer/pore structure and connectivity, its results can only be regarded as approximate at best.
A close relation seems to exist between porosity and the growth mechanisms and conditions.
For example, the porosity of a Cu OAD thin film is qualitatively related to its deposition rate and the wetting ability of copper on a silicon substrate, in such a way that the initially formed nuclei control the posterior evolution of the thin film’s microstructure and therefore also the final porosity of the film [143].
Modeling the OAD thin film microstructure and growth by Monte Carlo (MC) simulation and other numerical methods has been carried out to better understand the evolution of porosity as a function of the deposition angle [93,228], with Suzuki et al.
[62] deducing that evaporated OAD thin films should present a maximum surface area at an evaporation angle of 70°.
Interestingly, experiments exploring different adsorptions from the liquid phase have confirmed a maximum surface adsorption capacity at this “magic deposition angle” [63,229–231].
As these past approaches have relied on indirect measurements, calculations or qualitative assessments of porosity or surface area, they have obvious limitations concerning the determination of the true value of pore volume, internal surface area or pore size distribution.
The traditional method of analyzing porosity in a powder involves determining the specific surface area through the so-called BET (Brunauer, Emmett and Teller) [232] method, in which the amount of N2 or other gas/vapor that is adsorbed at its condensation temperature onto the internal surface of open pores is quantified [233].
By assuming a certain effective area for each molecule in the first adsorbed monolayer, it is possible to determine from the amount of adsorbed gas both, the porosity and effective surface area.
The extrapolation of this procedure, however, is not straightforward in the case of OAD thin films because of the relatively small amount of material available in the micron-thick or less layers, which hampers the application of conventional BET instruments.
To circumvent this limitation, modified adsorption techniques based on the classical BET method have been developed.
One of these is a modified volumetric method based on Kr adsorption and a special set up, which was used by Krause et al.
[222] to determine the pore volume and pore size distribution of TiO2 and SiO2 thin films.
In this particular case, the pore volume ratios were found to vary from 20% to 50% depending on the type of material and the deposition geometry.
This same method was also used to characterize the porosity of IA-OAD thin films [202], showing that porosity and surface area decrease due to the ion-induced mobilization of ad-particles.
We have also developed a volumetric method using a quartz crystal monitor to measure the adsorption of water by the pores in a film [234,235], with which we assessed the porosity of TiO2 and SiO2 OAD thin films [48,63].
Analysis of the measured water adsorption isotherms revealed that the overall OAD film porosity is made up of both meso- and micro-pores (according to IUPAC, these terms are applied to pores with throats bigger and smaller than 2nm, respectively [233]), giving a total porosity of 50–60% relative to the total film volume depending on the thickness and evaporation angle.
The partition between micro and mesopores also changed with film thickness and evaporation angle, with approximate volume percentages of 30–50% meso pores to 15–20% micro pores.
Thus, as an initial approximation, the meso-pore volume can be related to the intercolumnar space.
This is clearly evident in SEM micrographs of OAD thin films, such as those given Figs.
2.1 and 2.2.
Assessment of micro-pores is less straightforward, as these must be distributed within the interior of the nanocolumns.
Nevertheless, the pore size distribution and surface area of different oxides has been determined by Flaherty et al.
[231] using a similar adsorption system consisting of a quartz crystal monitor and different vapors condensed at room temperature.
This confirmed that the maximum internal surface area per unit volume occurs when α=75°.
An important consequence of the high porosity of OAD thin films of an oxide or other transparent dielectric material is that depending on the environment [227], their optical constants vary in response to the condensation of water in their pores.
Thus, unless they are encapsulated, this effect precludes their straightforward incorporation into real-world optical devices [48].
Surprisingly, this effect has been generally overlooked in many works regarding the use of OAD thin films as an antireflective coating or multilayer in solar cells or other related environmental applications [236–238].
On the other hand, the development of humidity sensors takes advantage of this very change in the optical constants of OAD films and multilayers due to the adsorption of water vapor from the atmosphere [78].
Porosity, surface area and chemical adsorption capacity are not homologous concepts.
This idea, which is inherent to the field of adsorbents and catalysts, is not common when dealing with OAD thin film devices as only very few works have specifically addressed this issue.
Some time ago, Dohnálek et al.
[239] made use of a temperature programmed desorption (TPD) technique, very well known in catalysis [240] to study adsorption processes on flat surfaces [241], to examine the adsorption capacity of OAD MgO thin films.
They experimentally determined that the fraction of active adsorption sites in these thin films was higher than that of films prepared at normal geometry, and that the distribution of sites with different adsorption binding energies changed with deposition temperature.
Unfortunately, no systematic studies involving adsorption processes from the gas phase, together with the subsequent desorption mechanism, have been pursued thereafter.
In the liquid phase of a photonic sensor incorporating dye molecules in transparent OAD thin films (see Section 5.4), we have observed that the pH of the medium greatly modifies the adsorption capacity of cationic or anionic organic molecules from the rhodamine and porphirine families [229,230,242–248].
To account for this pH dependent behavior, we have used the classical zero point of charge concept (zpc), which is widely used in colloidal chemistry [249].
According to this, the surface of colloidal oxides becomes either positively or negatively charged depending on whether the surrounding liquid has a pH that is lower or higher than the zpc of the investigated material.
Thus, by simply adjusting the pH of the solution and the concentration of dissolved molecules, it is possible to control the amount of molecules adsorbed in the OAD thin film or selectively favor the adsorption of one type of molecule over another.
Pre-irradiation of the films with UV light to modify their surface properties has been also utilized to control the type and amount of molecules adsorbed from a liquid medium [242].
In the course of these studies, it was also demonstrated that the adsorption equilibrium of tetravalent porphyrin cations in a OAD TiO2 thin film follows a Langmuir-type isotherm, while the adsorption kinetics adjust to an Elovich model [244].
Undoubtedly, these preliminary studies are insufficient to reach a sound general conclusion, but a tight collaboration between thin film material scientists and colloidal chemists should help to deepen our understanding of the adsorption properties of OAD thin films.
Crystalline and porous OAD thin films prepared by evaporation are needed for many different applications (e.g., catalysis, photo-catalysis, sensors, etc.)
in which a combination of porosity and a well-controlled crystalline structure is essential.
Most oxides deposited by e-beam evaporation at room temperature are usually amorphous, but both metals and ceramics become crystalline when their deposition is carried out at sufficiently high temperatures.
Furthermore, the crystallization of metals, oxides and other dielectrics deposited in OAD geometries is promoted when using MS, PLD or other techniques that involve the exchange of energy and momentum with the growing film.
In most cases, in addition to being crystalline, these thin films present a well-defined texture; i.e., a preferential orientation of the crystallographic planes of their individual crystallites.
Both out- and in-plane (i.e.
biaxial) preferential orientation may occur in these thin films depending on the deposition conditions.
In the case of the former, the crystallites exhibit a preferential orientation with a given crystallographic axis perpendicular to the surface, whereas the other unit cell axes are randomly oriented.
With the latter, however, individual crystals possess a similar orientation along the direction perpendicular to the plane and parallel to the surface plane.
This second situation is similar to that of a single crystal, with the difference being that OAD polycrystalline thin films instead consist of small crystallites with similar orientation.
The development of preferential orientations during the OAD of thin films has been the subject of an excellent discussion by Mahieu et al.
[160,161], who studied the out-of-plane texturing mechanisms of MS deposited thin films within the context of the extended SZM.
According to their description, preferential out-of-plane oriented films are obtained in Zones T and II of SZM when sufficiently high ad-particle mobility leads to a preferential faceting of crystallites along either planes with the lowest growing rate (Zone T) or highest thermodynamic stability (Zone II).
As a result, the films become textured with the fastest growth direction perpendicular to the surface.
In addition to this out-of-plane orientation mechanism, the growth of biaxial thin films by OAD is favored by the preferential biased diffusion [11] of ad-particles when they arrive at the film surface according to their direction.
Obviously, this situation can be controlled by adjusting the orientation of the substrate with respect to the target, in which case this preferential diffusion can be used to ensure grain growth in the direction of the most favorable faceted crystal habit facing the incoming flux of material.
From a mechanistic point of view, both the mobility of ad-particles during growth and the angular distribution of the incoming material flux are critical for the effective growth of biaxially aligned thin films; the particles mobility favoring biaxial alignment and their angular spread contributing to its randomization.
Consequently, parameters such as pressure, target–substrate distance, deposition angle, film thickness, bias potential of the substrate, temperature and the presence of impurities may play an important role in determining the degree of biaxial orientation.
This strong dependence that the crystalline structure of OAD thin films has on the experimental parameters means that although some trends in texture can be predicted [11,161], significant deviations associated with the use of different deposition conditions and/or techniques (e.g., conventional MS, pulsed MS, HIPIMS, temperature of substrate, bias, etc.)
should be expected.
A selection of crystalline OAD thin films is presented in Table 3.1 to highlight the specific features of their crystallographic structure.
This gathered data broadly confirms that crystallization occurs when thin films are grown within Zone T and II of the SZM; i.e., in evaporated films prepared at high temperatures, or by using MS or other techniques that involve ion bombardment during deposition (e.g., IA-OAD).
Biaxially oriented thin films are of the utmost interest for many applications, for example the synthesis of oriented high-temperature thin-film superconductors [259], magnetic-oriented systems [260], coatings for mechanical applications [261], or controlling the heat transport properties of a surface [262].
To determine the texture of such films, particularly their biaxial orientation, polar X-ray diffraction analyses and, more recently, reflection high-energy electron diffraction electron diffraction (RHEED) techniques [34,263] are typically employed.
With the first of these methods the retrieved information that stems from the bulk of the film, whereas in the second, it comes from the outermost surface layers.
As an example of the possibilities of this latter technique, Fig.
3.4 shows selected RHEED pole diagrams illustrating the evolution of biaxial texturing during the MS-OAD of Mo (the morphology of the same film is shown in Fig.
3.1) [151].
It is apparent from this figure that the pole diagrams change with deposition angle, revealing the development of a (110)[11¯0] biaxial texture when α>30° (note that Mo has a cubic structure), in which the [110] axis is oriented along the out-of-plane perpendicular direction and the [11‾0] axis is along an in-plane direction.
The sharpening of the pole patterns for α=45°, and the appearance of new poles for α>60°, sustain the development of a new texture when the deposition angle increases.
The representation on the right-hand side of this figure describes the nature of these textural changes, while additional explanations are provided in the figure caption.
Table 3.2 summarizes some select examples of biaxial thin films prepared by OAD.
Most of these were prepared by MS, though the two metals prepared by evaporation have either a low melting point (e.g., Mg) or are the product of thermally activated synthesis.
This confirms the need to identify experimental conditions that favor the controlled diffusion of ad-particles during film growth to ensure effective control over the texture of the thin film.
Another point worth noting from this table is the possibility of changing the facet termination of the individual crystallites by changing the deposition angle (e.g., Mg, Mo), and the fact that the selection of specific OAD conditions is generally critical to the development of a given biaxial orientation.
In some cases, the clear prevalence of a given orientation confirms the importance of the energetic factors related to the development of a given crystallite facet for the control of texture.
Thus, epitaxial effects (e.g., Ge) and the influence of substrate roughness or film thickness in effectively controlling the texture confirm that mechanistic conditions mediating ad-particle diffusion are quite important to controlling the biaxial orientation.
The bundling association of crystallites (e.g., MgO) is another interesting feature present in some biaxially OAD thin films.
In this section we deal with the fundamentals of OAD by discussing them from an atomistic point of view.
Rather than a critical enumeration of previously reported models, we have focused this discussion on a set of new concepts that provide an updated conceptual framework for understanding the growth process by e-beam evaporation and MS at oblique angles.
For a better assessment, these concepts have been explained using classical models which, using mainly geometrical considerations, have been previously proposed to describe the basic growth mechanisms of this type of films.
In this regard, numerous works and review papers have already dealt with both, the phenomenology and key theoretical issues involved in the OAD of thin films by different deposition techniques [10,11,14,264].
The terms ‘simulations’ and ‘experiments’, which were intentionally included in the title of this section, underline the importance of a combined approach when dealing with complex atomistic phenomena such as those involved in the OAD of thin films.
From a fundamental perspective, a key model in this section is the SZM that was already mentioned in Sections 2 and 3 [35–37,142].
Despite its phenomenological basis, it provides valuable information on the competition between surface shadowing mechanisms and thermally activated diffusion, which is useful for introducing simplified assumptions in growth models under various conditions [265].
This section is organized into two well differentiated parts.
In the first of these we address the problem of e-beam evaporation and consider the deposition of particles through a purely ballistic model; i.e., we assume that there is no significant scattering of particles in the gaseous phase during their flight from the source to substrate, and that the shadowing mechanism is the predominant nanostructuration process.
In the second we explicitly address MS deposition, in which scattering interactions in the gaseous/plasma phase are considered so as to understand how they may affect the microstructure of the film.
When dominated by surface shadowing mechanisms, the aggregation of vapor particles onto a surface is a complex, non-local phenomenon.
In the literature, there have been many attempts to analyze the growth mechanism by means of pure geometrical considerations; i.e., by assuming that vapor particles arrive at the film surface along a single angular direction [38,41].
Continuum approaches, which are based on the fact that the geometrical features of the film (i.e., the nanocolumns) are much larger than the typical size of an atom [42,266,267], have been also explored.
For instance, Poxson et al.
[228] developed an analytic model that takes into account geometrical factors as well as surface diffusion.
This model accurately predicted the porosity and deposition rate of thin films using a single input parameter related to the cross-sectional area of the nanocolumns, the volume of material and the thickness of the film.
Moreover, in Ref.
[39], an analytical semi-empirical model was presented to quantitatively describe the aggregation of columnar structures by means of a single parameter dubbed the fan angle.
This material-dependent quantity can be experimentally obtained by performing deposition at normal incidence on an imprinted groove seeded substrate, and then measuring the increase in column diameter with film thickness.
This model was tested under various conditions [40], which returned good results and an accurate prediction of the relation between the incident angle of the deposition flux and the tilt angle of the columns for several materials.
Semi-empirical or analytical approaches have provided relevant information regarding film nanostructuration mechanisms; however, molecular dynamics (MD) [14] and MC [62,120,268] methods have provided further insights into the growth dynamics from atomistic and fundamental points of view.
The MD approach considers the incorporation of single species onto a film one by one, describing in detail the trajectory of each particle by means of effective particle–surface interaction potentials.
Unfortunately, given the computational power presently available, this procedure only allows simulations over time scales in the order of microseconds, even with hyperdynamic techniques [269].
Since real experiments usually involve periods of minutes or even longer, this constraint represents a clear disadvantage when comparing simulations to experimental data.
In this way, two-dimensional MD simulations carried out [270] with the intent of investigating the role of substrate temperature, the kinetic energy of deposition particles and the angle of incidence on the film morphology predicts that increasing substrate temperature and incident kinetic energy should inhibit the formation of voids within the film and promote the formation of a smooth and dense surface.
Moreover, it was also found that increasing angles of incidence promote the appearance of tilted, aligned voids that ultimately result in the development of columnar nanostructures.
In contrast to MD techniques, MC models approach the problem from a different perspective by allowing the analysis over longer time and space scales.
In this case, MD simulations are employed to describe the efficiency of different single-atom processes using probabilities, which are then subsequently put together.
Although this strategy accelerates by some orders of magnitude the simulation time, except in the case of athermal processes [212]), ad-atom diffusion must be excluded from the calculations to obtain a realistic simulation of the growth of a thick thin film.
This means that MC simulations of the deposition process are suitable to conditions within Zone I of the SZM, which is where the preparation of the majority of evaporated OAD thin films takes place.
Nevertheless, since thermal activation may also be involved in thin film growth [11,60,271], and may have certain influence on the nanostructural evolution of the films, this activation has been explicitly considered in some MC simulations of up to a few hundred monolayers of material.
For example, a three-dimensional atomistic simulation of film deposition [272], which included the relevant thermally activated processes, was developed to explain the growth of an aluminum thin film onto trenches.
In another work, Yang et al.
[273] employed a two-step simulation wherein arriving species are first placed at the landing location point, with a kinetic MC then describing their subsequent diffusion.
Want and Clancy [274] included the dependence of atom sticking probabilities on temperature to describe the deposition process, whereas Karabacak et al.
[22] considered the ad-atom thermally activated mobility by introducing a fixed number of ad-atom diffusion jumps onto the surface following deposition.
When only surface shadowing is considered in the simulations, a classical MC model with cubic geometry proceeds in the following way: punctual deposition species are thrown onto a two-dimensional substrate that defines the x–y coordinate plane, with the z-axis being defined by the direction perpendicular to it.
The three-dimensional space is then divided into a NL×NL×NH grid, in which cells are assigned a value of 1 if they contain a deposited species, or are otherwise given a value of 0.
A cell with a typical size in the order of the distance between atoms in the material represents a species in the network.
From an initial random position above the film, each deposition particle is thrown toward the substrate following a direction defined by the spherical angles θ′ and φ′, where θ′∈[0,π/2) is the polar angle of incidence (θ′=0 is the direction normal to the substrate) and φ′∈[0,2π) is the azimuthal angle.
By assuming periodic boundary conditions for the system, particle movement proceeds along a straight line until it hits the surface at a given location, where it then sticks (ballistic approach).
For each deposition particle, the angles θ′ and φ′ are randomly calculated by defining an incident angle distribution function per unit time and unit surface, I(Ω), where dΩ=sinθ′dθ′dφ′ and represents a differential solid angle.
The procedure hitherto described represents a very simple approach to simulating thin film growth under general conditions, a quite complex process in which other mechanisms may be also present.
A brief summary of these additional issues are:•Incoming vapor species may interact with the surface and not follow a straight trajectory, thus other processes resulting from the proximity of the vapor species to the surface should be introduced.
As will be described later in this section (Section 4.2.1), this is straightforwardly connected to so-called surface trapping mechanisms.
Although most OAD films analyzed had been synthesized under conditions pertaining to Zone I of the SZM model, thermally activated processes may also influence the film nanostructure to some extent [11].
Vapor species arriving at a landing location may carry enough energy to move or induce additional displacements in the material’s network.
As we will see later, this is relevant in MS depositions where the vapor species may possess hyperthermal energies.
When the plasma interacts with the film, there are numerous energetic species that may affect the film nanostructuration; e.g., plasma ions or neutral species in excited states [275,276].
High growth temperatures may promote the crystallization of the film and the appearance of surface potentials that favor atomic displacements along preferential directions/planes of the network.
The full analysis of these processes is an active research area, and the study of their combined effect on the film nanostructuration is an open field of investigation.
In the following sections we discuss some of the previous mechanisms by considering the results of some fundamental experiments carried out under simplified conditions to highlight the influence of a particular process.
From a conceptual point of view, the sublimation of a given material in a vacuum reactor, and the subsequent condensation of gaseous species on a solid surface, is a simpler problem than those encountered in PECVD or reactive MS techniques, where a strong interaction between deposition and plasma species may occur during the travel of the former from the material source to the substrate (some specific interactions involved during the MS-OAD of thin films will be addressed in Section 4.3).
As mentioned in previous sections, a general picture of the OAD growth at low temperatures only considers a purely ballistic approach; i.e., vapor species arrive at the film surface along straight, oblique trajectories and remain at the landing location, giving rise to a tilted columnar nanostructure [277].
Fig.
4.1(a)–(c) schematically describes the first stages of growth that are believed to occur during the oblique arrival of ad-atoms.
In the first stage (Fig.
4.1(a)), individual vapor species arrive at random locations on the surface along a tilted direction, which is defined by the angle α.
In the second stage (Fig.
4.1(b)), the deposited particles accumulate within certain regions in the form of grains, which then cast “shadows” behind them, preventing other particles from depositing.
Through this shadowing effect, taller surface features are more likely to grow through the incorporation of new particles, whereas regions lower in height will scarcely develop due to the “shadow” cast by the former (see Fig.
4.1(c)).
This selective growth of taller surface features introduces a competitive process whereby the taller a feature, the larger its shadow, which ultimately results in the formation of tilted columnar structures [43,278,279].
Karabacak et al.
[217] introduced the so-called shadowing length to describe nanocolumnar growth: a parameter that plays a similar role as the diffusion length during the conventional development of island growth morphologies.
As we will see later in this section, the shadowing length concept has been extended to three-dimensional growth by introducing a shadowing region and, more recently, an effective shadowing region.
In general, the low kinetic energy of a vapor species that lands on the surface (the energy of evaporated atoms is around 0.2eV) ensures that no kinetic energy-induced processes are likely to take place when using the evaporation technique.
Furthermore, the competitive growth that gives rise to a tilted columnar nanostructure is directly dependent on the angle at which the vapor species arrives [228].
The connection between the value of α and the tilt angle of the columns, β, is of outmost importance to controlling the film’s properties [39,41,44], and has been roughly described by means of the heuristic tangent rule [11] (Eq.
(2.1)).
In most cases, this rule accurately describes the relation between both angles when α⩽60° [270].
More recently, Tait et al.
[38] proposed the so-called cosine rule (Eq.
(2.2)), which succeeds in describing the relation between both angles for some materials, but fails with others.
Indeed, one of the main problems posed by these two equations is the implicit assumption that the relation between both angles is purely geometric, and therefore leaves aside any material- or experimental reactor-dependent influence.
Based on recently published results [280–282], we describe in the next section how these factors influence the nanocolumnar evolution of OAD thin films prepared by evaporation One of the most common misconceptions when considering the OAD growth of evaporated thin films lies in the idea that particles arriving at the substrate along a single angular direction, α, give rise to three-dimensional columnar structures.
In Fig.
4.2(c) we see a top view of a film surface that was simulated by assuming a unique angle of incidence for the vapor species, α=80°, from which it is clear that this condition would never give rise to volumetric three-dimensional columnar structures.
This is attributed to a lack of correlation among Π planes (see Fig.
4.2(a) for its definition), meaning that each slice of material parallel to the Π plane is independent of the one that follows, which precludes the formation of columnar, cylindrical structures.
This implies that the classical description presented in Fig.
4.1(a)–(c) corresponds to a two-dimensional model, in which the tilted structures do not develop in three dimensions.
Actually, in real experiments, it is most likely that the evaporation source is non-punctual and that some unlikely collisions may take place among vapor species.
These, and other experimental effects, may slightly broaden the angular distribution of evaporated particles along the nominal direction defined by the α angle.
In Refs.
[280,282], the effect of this broadening on the microstructure of a thin film was studied by assuming that the distribution of momentum amongst species in the gaseous phase was Gaussian (see Fig.
4.2(b)), with a mean value of α and a variance of σ.
As it is seen in Fig.
2.4(d), a slight broadening of the momentum flux when σ=1° is enough to induce the formation of a tilted three-dimensional columnar volumetric structure.
The influence of σ on the film microstructure is also evident in Fig.
4.2(d)–(f), where higher values of σ can be seen to produce an increase in the diameter of individual nanocolumns.
However, this experimental parameter alone cannot explain the different tilt angle of the nanocolumnar structures found with different deposited materials.
According to Refs.
[280,282], an atomistic process called the surface trapping mechanism can account for the existence of short-range interactions between vapor species and the growing film surface, which is a classical phenomenon that has been discussed in the literature for decades [120].
In contrast with typical ballistic models of thin film growth, where evaporated species in the gaseous phase are assumed to follow straight trajectories [215], the surface trapping mechanism considers that vapor species passing within a few angstroms from the surface may deviate in their trajectory and deposit at a nearby location (see Fig.
4.3(a)).
A way of accounting for the interaction potential between vapor species and the film surface is by defining the surface trapping probability, st, according to the following rules:(i)when a vapor species impinges head-on onto the surface, its probability of sticking at that location is 1, and when a vapor species moves over the surface at a distance of less than 4–5Å, its probability of sticking at that position is st.2Note that the trapping probability, st, is introduced per next neighbor.
That is, if the vapor atoms encounter N next neighbors at a given position, the trapping probability is 1-(1-st)N.2 The trapping probability concept should not be confused with the sticking probability, which accounts for the overall probability of a particle to be deposited onto a surface regardless of its particular location.
Furthermore, most studies into particle sticking on surfaces have only considered a perpendicular incidence and therefore only accounted for head-on sticking processes [215].
From a physical point of view, the trajectories of species in the vapor phase near the surface should depend on the interaction potential between them (chemical nature, distance, etc.
), as well as their relative velocity.
Typical van der Waals and electrostatic attractive forces mostly operate within distances of a few angstroms, which is in line with the assumptions of the surface trapping mechanism.
Thus, the low kinetic energy of vapor species under typical evaporation conditions and the oblique incidence geometry should favor trapping, as under these conditions, particles would move longer distances in the vicinity of the surface before landing.
The value of st would therefore depend on the chemical nature of both, the interacting species and the surface; i.e., on the chemical nature of the sublimated material.
Simulations of the influence of σ and st on the columnar microstructure of a thin film are presented in Fig.
4.3(b), while the effect of σ and st on the tilt angle of the nanocolumns as a function of α is shown in Fig.
4.4, along with the trends derived by applying the tangent and cosine rules.
Since the simulation results for st⩾0.1 are weakly dependent on the particular value of σ, results for σ=6° are presented.
Meanwhile, for st=0, calculation results are presented for both σ=0° and σ=6°.
Overall, it is found that the higher the value of st, the less tilted the columns are.
This tendency supports the notion that surface trapping effectively modifies the geometrical shadowing mechanism, which must now be described through an effective shadowing area (see Fig.
4.3(a)).
This concept describes the actual region around the surface features of the film that are able to trap flying vapor species, and in doing so, cast a shadow over the surface.
It is worth noting in Fig.
4.3(b) that the model results seem to follow the tangent rule when α⩽60°, a coincidence which indicates that the surface trapping mechanism is not relevant at low incident angles, but rather only introduces important deviations with respect to a pure geometrical model at high incident angles.
Further confirmation of the existence of an effective shadowing area is provided by the results presented in Fig.
4.4.
This plot shows a series of experimental and simulated values of β as a function of α for TiO2, SiO2, Ta2O5, ITO and Ti (all grown using the same experimental setup at low temperature), and for Ni, Al, Si and TiO2 (also grown at low temperature, but in a different reactor) [40].
The good concordance evident between the experimental values of β for any given material and the simulations for a given value of st indicates that this parameter only depends on the chemical nature of the sublimated material and the composition of the film.
The results for the TiO2 and Ti thin films are well described by the pairs of values st=0.1 and σ=6°, and st=0 and σ=6°, respectively, with the common value of σ=6° suggesting that the angular broadening of the deposition flux only depends on the characteristics of the deposition setup (geometry of the reactor, operating pressure, size of the source, etc.).
Further discussion regarding the effect of these two parameters on the tilt angle of the nanocolumns for thin films of other materials has been published previously in Refs.
[280–282].
In Section 3, we mentioned that the actual surface area of the films, their roughness and accessible pore volume are all relevant microstructural quantities for the use of OAD films in different applications.
Substantial simulation work has therefore been devoted to predicting the most appropriate experimental conditions required to maximize the actual surface area of OAD thin films [283].
With this in mind, Suzuki et al.
[62] used a three-dimensional MC model to show that films grown at α∼70° have the maximum surface area.
This prediction agrees with the efficiency results of dye-sensitized solar cells and the performance of other devices [48,63,284,285], wherein it was found that the maximum yield of films deposited with α∼70° was justified by their maximum adsorption capability [222].
The surface roughness of OAD thin films is another relevant microstructural feature that is associated with the termination profile of nanocolumns at the film surface.
The evolution of both the nanocolumn width and surface roughness as a function of the thin film thickness has been theoretically analyzed within the premise of the Dynamic Scaling Theory (see Section 3.6.1) [215].
This framework has been widely utilized to model the growth of a large variety of thin films, and has made it possible to correlate some empirically determined critical exponents with the mechanisms involved.
For example, the width of nanocolumns, w, as a function of column length, d, has been found to follow a power law dependence w∼dp′, where p′ is the so-called growth exponent [22].
In the same paper, a MC model was used to prove that if surface shadowing dominates the growth, then the value of this exponent is p′=0.5, while it drops to a lower value when surface diffusion plays a relevant role [220].
The tight correlation between the growth exponent and the shadowing mechanism was further demonstrated by Buzea [219] for Si thin films prepared by dynamic OAD.
This author showed that the value of p′ strongly depends on the tilt angle of the vapor flux, and that it directly controls the distance between nanocolumns.
The evolution of the surface roughness, ws, of OAD thin films has also been analyzed under similar premises, and found to generally follow the power law [215]:(4.1)ws(L)∼wsat,ifL≫LcrossoverLγ,ifL≪Lcrossover,where L is the linear size of the substrate on which the film grows, Lcrossover is the length at which saturation of the roughness, wsat, occurs and γ is the roughness exponent used to describes surface roughness evolution at small scales of length.
In addition, wsat fulfills the law wsat-wsat(0)∼ακ, where wsat(0) is the surface roughness of a reference film grown with α=0° and κ is the saturated roughness exponent.
In Ref.
[286], it was found that the roughness of OAD thin films of Ti grown by evaporation can be described by the exponent κ=7.1±0.2, which agrees well with the value of κ=6.7±0.4 that was determined by three-dimensional MD calculation [216].
In Section 3.6.2, we presented the relevant effects encountered during the OAD of thin films in relation with the anisotropic coalescence of nanocolumns into bundles.
However, despite the relevance of this to numerous applications, no systematic analysis of the meso-scale development of this bundle association has yet been carried out [82,287].
Herein, we would like to address how the trapping mechanism already presented in Section 4.2.1 in connection with the tilt angle of the nanocolumns may also account for the formation of bundles.
Fig.
4.3(c) illustrates how the trapping process also occurs with vapor species passing laterally near a growing nanocolumn.
A consequence of this preferential trapping would be a faster lateral growth of nanostructures in the direction perpendicular to the vapor flux, ultimately provoking columnar coalescence along this same direction.
To prove this hypothesis, Fig.
4.3(d) shows three simulations of equally-thick thin films grown under the assumption of an angular broadening of 6° and trapping probabilities, st, of 0, 0.12 and 1.
When st=0, the columns are small in diameter and grow separately, creating a homogeneous distribution over the substrate.
With higher trapping probabilities, the column diameter increases and the nanocolumns merge in the direction perpendicular to that of the vapor flux, leaving elongated gaps among these ‘columnar fronts’.
These simulations are compared with selected scanning electron microscopy images of Ti, TiO2 and SiO2 thin films created under the conditions defined in Fig.
4.4.3Note that the values of st agree with those reproducing the tilt angle of the nanocolumns in Fig.
4.4 for Ti, TiO2 and SiO2.3 The concordance between the experimental data and simulations demonstrates that at least in the case of the three materials investigated, the trapping mechanism is a reasonable hypothesis to explain the different tendency for bundle formation exhibited by the thin films of this series.
Furthermore, even though this analysis is restricted to a limited set of materials, it is believed that this semi-quantitative justification of bundling formation provides an indication of its general character, and will be further validated by additional studies.
The challenges that clearly need to be faced here, however, are to perform first principles calculation of the trapping probability for a given evaporated material, or achieve experimental control of the angular broadening of the evaporated material.
There is also a clear need for new paradigms in relation to the quantitative analysis of other processes, such as thermally activated diffusion during growth at high temperatures [288], as well as the existence of anisotropic surface potentials associated with crystallization rearrangements.
Numerous works in the literature have dealt with the fundamentals of MS deposition at normal incidence [289–295].
From this, it is known that classical MS depositions, in which the growth surface is parallel to the target, usually yield dense and compact films, thanks mostly to the high energy of the deposition particles and the impingement of plasma ions during growth [296,297].
In contrast, when deposition is carried out in an OAD configuration, other processes may play important roles in the development of columnar and porous nanostructures [212].
The main challenges facing the MS deposition of thin films center around the control over the chemical composition of the layers, the deposition rate and the film nanostructure.
There are many excellent reviews dealing with the first two issues when working under a normal configuration [298,299], but the third has only been scarcely addressed, even under simplified conditions.
In this section, we will focus on some of the mechanistic aspects that are important to account for the MS-OAD of thin films in connection with the development of a columnar nanostructure.
For this analysis, we have avoided the use of complex models for the transport of sputtered particles inside the plasma, and have instead employed a simplified approach based on effective thermalizing collision (ETC) theory [47,130,300,301].
This has already provided numerous results and straightforwardly applicable mathematical formulae pertaining to the deposition rate and final microstructure of MS thin films.
In the following subsection, we analyze the main differences between evaporation and MS in terms of atomistic processes.
Following this, we briefly describe the mechanism of sputtering and explain the transport of sputtered particles by means of ETC theory.
This theory will then be employed to deduce a formula that describes the deposition rate at oblique angles.
Finally, the influence of deposition conditions on the films’ morphology is described.
When comparing evaporation and MS techniques, the following key differences become apparent:•Plasma-generated species in contact with the film during growth: In MS-OAD deposition, the plasma contains numerous energetic species such as positive or negative ions, or highly reactive species that may impinge on the film during growth and affect its nanostructure and chemical composition [293,302].
Size of the material source: Under typical e-beam evaporation conditions, the material is sublimated from pellets situated very far away from the film (1m or more).
In MS deposition, the size of the source (target) is within the same order of magnitude as the target/film distance.
Thus, the deposition particles stem from a wide area racetrack, meaning that the deposition angle is not fixed but rather varies over a relatively large interval (see Fig.
2.2(b)).
Kinetic energy of vapor atoms: In MS, the mean kinetic energy of sputtered particles is in the order of 5–10eV, whereas under typical evaporation conditions it is in the order of 0.2–0.3eV.
Since the typical energy threshold required to mobilize atoms deposited on the surface is ∼5eV, the impingement of vapor atoms onto the film may cause the rearrangement of already deposited species [212,303–305].
Collisional processes in the vapor phase: The working pressures in MS are much higher than in typical e-beam evaporations, where the mean free path of evaporated species is typically greater than the source/substrate distance.
In contrast, the pressure in MS can be varied within a relatively broad interval, meaning that sputtered particles can experience a large number of collisions before their deposition [162].
These collisions have a direct impact on the kinetic energy and momentum distribution (including the direction of arrival) of the sputtered particles when they eventually reach the film surface [302,306–308].
All these differences are of great relevance for the control of fundamental atomistic processes involved in the growth of a thin film, and make the previously introduced surface trapping probability and angular broadening concept insufficient to describe the nanostructural development of MS-OAD thin films.
The sputtering of atoms in MS is caused by positive ions of the plasma, which are accelerated toward the target within the plasma sheath [289].
These ion–solid interactions are complex, and as such have been the subject of study over many decades [32].
For the purposes of this review on OAD, the quantities of interest have been limited to the so-called sputtering yield and the energy distribution of sputtered particles.
The former takes into account the number of ions ejected from the target surface per ion arriving, whereas the latter function determines the energy and angular distribution of ejection.
Based on standard binary collision approaches [309], the sputtered particle distribution at the target can be calculated to be proportional to E(E+U)-3cosξ, where E is the kinetic energy, U the solid target binding energy and ξ the ejection angle.
Thus, sputtered atoms preferentially leave the target along a direction perpendicular to the surface with an average energy of around U/2; i.e., an energy much higher than the thermal energy of gaseous species inside the reactor.
Even if the sputtered atoms leave the target with energies in the order of 5–10eV and a high preferential directionality, collisional processes with (predominantly) neutral species of the plasma gas may drastically alter both the energy distribution function and the directionality of the particles when they reach the substrate.
Sputtered particles arriving at the film surface can be broadly separated into three categories depending upon their collisional transport: (i) particles that have not collided with any gas atoms and arrive at the film surface with their original kinetic energy and direction, (ii) those that have experienced a large number of collisions with background plasma/gas atoms and therefore possess low (thermal) energy and an isotropic momentum distribution, and finally, (iii) those that have undergone several collisions, but still possess significant kinetic energy and some preferential directionality.
This transport has been thoroughly studied in the literature by means of MC models of gas phase dynamics using different elastic scattering cross-sections to determine the energy and momentum transfer in each collision [162].
However, the complexity of the mechanisms involved makes it difficult to find analytical relations between quantities of interest and experimentally controllable parameters.
With the aim of simplifying the description and deducting general analytic relations, the effective thermalizing collision (ETC) approximation has been successfully applied to describe the collisional transport of sputtered particles in a plasma [47,300,301].
This theory introduces the concept of effective thermalizing collision: an effective scattering event between a sputtered atom and gaseous species that results in the former losing its original kinetic energy and initiating a random thermal motion in the gas.
As we will see next, this enables the deduction of simple analytical equations that relate the main fundamental quantities, though the connection between actual collisional quantities in the plasma/gas and this effective mechanism remains the main issue for the practical application of these ideas.
A summary of the main concepts and approximations utilized within this theory to assess the division between ballistic and thermalized species is presented next, but interested readers may get a more detailed description in Refs.
[47,300,301].
One of the most relevant quantities describing the collisional transport of particles in a gas is the mean free path, λ=1/Nσg, which represents the typical distance covered by a sputtered particle between two consecutive collisions in the plasma/gas.
Here, σg, is defined by the cross-section of an elastic scattering event, while N is the density of gas atoms.
The ETC theory also introduces what is known as the thermalization mean free path, λT, to account for the typical average distance covered by a sputtered atom in the plasma/gas before becoming thermalized.
Likewise, a so-called thermalization cross-section, σT, is introduced through the relation λT=1/NσT.
In this way, the flux of ballistic (non-thermalized) sputtered atoms at a distance of L from the target can be expressed as Φ0exp(-L/λT), where Φ0 is the flux of sputtered particles at the source (target).
The amount of thermalized sputtered atoms per unit of time in the plasma/gas is also given as: Φ0[1-exp(-L/λT)].
Using these formulae, it is possible to account for the Keller–Simmons (K–S) formula, which is a well-known empirical equation in classical MS deposition (i.e., in non-oblique configurations) with a single sputter gas [301].
This formula describes the dependence between the deposition rate and relevant experimental parameters such as the plasma gas pressure, pg, and the distance between the cathode and the film, L, as:(4.2)rKS=p0L0pgL1-exp-pgLp0L0,where rKS is the deposition rate and p0L0 is an adjustable parameter dubbed the characteristic pressure–distance product.
This empirical equation was deduced in Ref.
[301] from fundamental principles within the ETC framework, which found the relation pgL/p0L0=L/λT.
To simplify the calculations, the quantity Ξ=L/λT was defined in Ref.
[307] as the thermalization degree of the sputtered particles.
Thus, when Ξ is much less than one, the distance between the target and the film is much smaller than the typical thermalization mean free path; i.e., most deposition particles arrive with a high energy and along a preferential direction.
However, when Ξ is much greater than one, the thermalization mean free path is much smaller than the target/film distance and most species should be thermalized when they arrive at the film surface (with low energy, and following an isotropic momentum distribution function).
By means of this quantity, the K–S formula now reads:(4.3)rKS=Φ01-exp(-Ξ)Ξ.To determine σT, we show in Fig.
4.5(a) the fitting of this theory to experimental data.
The good fitting demonstrates that this parameter is connected with the geometrical cross-section of an elastic scattering event between a sputtered atom and a plasma atom through the relation: σT=σg/ν, where ν is the average number of subsequent elastic collisions required for the thermalization of the sputtered particles, as calculated by Westwood [310].
Likewise,(4.4)λT=νλ.This Eq.
(4.4) allows σT to be calculated for any given sputtered atom or sputter gas, which implies that the complex dynamics of hyperthermal atoms sputtered from the cathode within a plasma can be simplified by means of an effective thermalization cross-section that describes the progressive loss of kinetic energy and directionality (i.e., the momentum distribution) of the sputtered particles.
In regard to the OAD of thin films, the previous assessment of the partition that exists between ballistic and thermalized sputtered atoms is of the utmost importance, as the former contributes to the film’s nanostructuration through shadowing effects and other hyperthermal phenomena responsible for, among other things, the development of nanocolumns or specific textures.
Moreover, in the OAD configuration, basic deposition magnitudes such as the deposition rate or the final composition and microstructure of complex thin film materials will be drastically affected by the thermalization degree of sputtered particles.
Given its importance to any MS-OAD process, we specifically analyze the relation between the thermalization degree and deposition rate in the next section.
An example showing the importance of the thermalization degree of sputtered particles on the control of composition and microstructure in complex MS-OAD thin films can be found in the recent work by Gil-Rostra et al.
[311] on the MS-OAD of WxSiyOz electrochromic thin films, in which a net W enrichment of the deposited film with respect to the target and continuous variation in the tilt angle of the nanocolumns were attributed to more effective scattering of Si than W by Ar atoms in the plasma gas.
That is, W has a much higher atomic mass than Si, and so its energy and momentum is less affected by binary collisions with much lighter Ar atoms.
Silicon, on the other hand, has an atomic mass similar to Ar, and so is more effectively scattered to become distributed in all directions within the deposition chamber.
Consequently, the films become enriched in tungsten and exhibit a microstructure in which, for any given deposition geometry, the tilt angle of the nanocolumns increases with W content.
Despite its importance to thin film processing, evaluation of the deposition rate in MS-OAD has only been recently addressed [130].
From the ETC concept, the total deposition rate on a tilted substrate can be written as the sum of a highly directed contribution of energetic ballistic particles and a thermalized contribution.
The ballistic contribution can be estimated as a function of the average angle of arrival, α, as per:(4.5)rB=Φ0exp(-Ξ)cosα.Meanwhile, due to their non-preferential directionality, it is reasonable to assume that the deposition rate of thermalized atoms, rD, is not dependent on the particular value of α.
On the basis of this, it can be easily calculated for α=0 as: rD=rα=0-(rB)α=0=rα=0-Φ0exp(-Ξ), where rα=0 is the deposition rate at normal geometry.
Thus, by introducing the deposition rate at oblique incidence, rα=rB+rD, and the quantity Δrα=rα=0-rα, we arrive at the formula:(4.6)Δrα/Φ0=exp(-Ξ)(1-cosα).Eq.
(4.6) relates the deposition rate to relevant process parameters such as the tilt angle of the substrate and the thermalization degree.
In this way, when Ξ is significantly low, the deposition rate is found to have a strong dependence on the deposition angle, Δrα∼Φ0(1-cosα).
This dependence stems from a highly directional arrival of deposition particles that makes the deposition rate highly dependent on the relative orientation of the film with respect to the target.
When Ξ is much above 1, it is found that Δrα∼0; i.e., the deposition rate is little affected by the substrate orientation angle.
Indeed, under these conditions most deposition particles are thermalized, and so the deposition rate should be independent of α, a concept that is in good agreement with the original hypothesis of the ETC theory.
Overall, whenever the KS formula is applicable, a general formula for the growth of magnetron-sputtered thin films at oblique angles can be derived from Eqs.
(4.4) and (4.5):(4.7)rα=0-rαrα=0=Ξexp(Ξ)-1(1-cosα).This formula establishes a relation between the deposition rate at oblique angles and at normal incidence with the thermalization degree and zenithal angle of the substrate.
The predictions of this model are of general characteristics, but have been successfully applied to describing the growth of Ti thin films at different tilt angles and background pressures, as shown in Fig.
4.5(b).
At the present point in the discussion of the influence of basic atomic interactions on the characteristics of MS-OAD thin films, it is evident that experimental parameters and magnitudes such as geometry, gas pressure, temperature and energy/momentum distribution of the sputtered particles, will have a decisive influence on the thin film properties.
However, no systematic works have yet been carried out in which their combined influences have been considered.
For thin films grown using a normal configuration, the SZM model [35–37] has proven to be quite an important contribution to qualitatively describing the morphological features of thin films deposited under different temperatures and pressures.
Clearly then, a similar approach is needed for dealing with MS-OAD thin films.
An attempt along these lines can be found in a recent work by our group [172], which provided a thorough experimental and theoretical analysis of the microstructural evolution of Au thin films deposited by MS-OAD as a function of Ξ and α.
In this, we compared the different microstructures obtained by systematically varying these two parameters in MC simulations where only the surface shadowing mechanism and collisional processes in the plasma gas were considered.
The different microstructures obtained were then rationalized in the form of a phase map along the schemes of the SZM.
Fig.
4.6 illustrates the four different generic microstructures obtained, which are characterized by the following features:•α-type microstructure: the film is compact and has no well-defined geometrical patterns in the bulk.
It is formed at low thermalization degrees with small deposition angles (typically when Ξ<1.5 and α<50°).
β-type microstructure: the film possesses a highly coalescent, tilted columnar structure (i.e., columns are not isolated from each other, but are always touching), with large elongated mesopores percolating from the surface to the bottom of the film.
It is obtained at low thermalization degrees and higher deposition angles (typically when Ξ<1 and 50°<α<75°).
γ-type microstructure: the film is defined by isolated and well-defined tilted nanocolumns.
It is formed at even lower thermalization degrees and much higher deposition angles (typically when Ξ<0.5 and 75°<α<90°).
δ-type microstructure: the film is characterized by vertical, coalescent column-like structures, with a high density of micro and mesopores occluded in the material.
It is formed at high thermalization degrees and a wide range of deposition angles (typically when Ξ>2), similar to those reported in Ref.
[312] when low-energy deposition particles arrive at the film surface following an isotropic angular distribution function.
Fig.
4.6 shows the formation of these different microstructures on a phase diagram, as well as through a series of MC simulations to visualize their main columnar and pore features.
A very good concordance in shape was revealed between these simulated structures and the experimental films.
In addition, the MC simulation provided clues to understand the formation of the different thin film microstructures by assuming a different thermalization degree for the sputtered gold atoms.
This quantity identified different surface shadowing processes during the early stages of thin film formation and subsequent growth.
A detailed description of the mechanistic effects under each working condition is reported in Ref.
[172].
The aforementioned study was carried out in order to account for the growth of gold nanostructures as a function of substrate tilt angle and working pressure during MS-OAD.
The generalization of this kind of analysis to other materials and conditions is still very much an open issue and will be addressed in the next few years.
It is worth noting here that gold represents a very simple case, which can be properly simulated by taking into account very simple phenomena such as atomic scattering in the plasma phase and shadowing effects.
To fully explain the nanostructures created in the MS-OAD of other materials, additional complex phenomena associated with the high energy of sputtered particles would need to be included.
Thus, processes such as the kinetic energy-assisted mobility of surface atoms, biased diffusion, and upwards or downwards funneling mechanisms, among others, cannot be disregarded, and very likely will need to be incorporated into the analysis (see Refs.
[303–305]).
For example, Dalla Torre et al.
[212] developed a MC model that considers a set of different interactions and mechanisms, including binary collisions between incoming gaseous atoms and those deposited on the film surface, and thermally activated processes or different hyperthermal relaxation processes that are dependent on the kinetic energy and momentum transfer such as the reflection of incoming particles, biased-diffusion, kinetic energy-assisted diffusion or re-sputtering.
Overall, they found good qualitative agreement between simulations and experimental data with the growth of Ta thin films by DC sputtering at oblique angles, and demonstrated the relevance of these processes to the nanostructural development of these films.
Moreover, in Ref.
[49], a detailed experimental study of several materials was carried out to determine the temperature threshold at which thermally activated processes on the film surface compete with surface shadowing mechanisms for control over the formation of the film nanostructure.
In agreement with the SZM, they found that for elemental metal thin films with a melting point above that of Al (933K) the value of T/Tf was T/Tf>0.33, whereas for oxide thin films it was T/Tf>0.5 (see Section 3).
Thus, the influence of thermally activated processes must be considered to account in a generalized way for the evolution of different microstructures during MS-OAD [49].
In addition to the concepts already discussed in this section, a systematic analysis of how the impingement of plasma ions affects the nanostructuration of a film at oblique angles is still notably absent.
Nevertheless, plasma ions have been widely used for numerous purposes, such as the removal of defects, the densification of materials, improving mechanical properties, smoothing surface, and the crystallization of materials.
When dealing with porous materials, however, their energy must be kept low enough to influence the film nanostructuration without producing full densification.
Yet it is only recently that this effect has been analyzed by altering the ion energy using different electromagnetic waveforms so as to change the plasma potential.
In Ref.
[313], it is demonstrated that the introduction of a shallow low-energy ion impingement on the film surface during growth affects the tilt angle of the columns and the overall film porosity.
Even though this effect has only been reported for TiO2 thin films, this result suggests that the modulation of low-range ion energies is necessary to tune the morphological features of the columnar structures in plasma-assisted depositions.
The incorporation of OAD thin films in a large variety of advanced devices is a clear indication of their maturity and excellent prospects for a successful implementation in a large range of technological fields.
Although it is impossible in the space of this review to cover all of the advanced applications and devices that rely on OAD thin films, the following sections present a review on a select number of works published in the last seven-to-ten years that, in our opinion, best illustrate the possibilities of this material type and technology.
We have grouped this selection into the following broad subjects: transparent and conductive oxides (TCOs), energy harvesting, sensors and actuators, optic and photonic devices, wetting and microfluidics, and biomaterials and biosensors.
Of the thin film materials used for the fabrication of photonic or electronic devices, transparent conducting oxides (TCOs) occupy a central position, being indispensable for the fabrication of solar cells, electroluminescent and electrochromic systems or electrochemical devices [314].
Thanks to its low resistivity (∼10−4Ωcm), high transmittance in the visible range (80–90%) and low-deposition temperature, tin-doped indium oxide (ITO) is the most popular TCO used by industry [314,315].
Indeed, it is only the need to replace the expensive indium component that has prompted the search for alternative TCO formulations, which are briefly summarized at the end of this section.
The fabrication of highly porous and/or sculptured ITO thin films has been attempted by e-beam OAD from ITO pellets, both under high vacuum conditions [316–327] and in a carrier gas flux (usually nitrogen) [320,328–330].
Fig.
5.1 shows some examples of the thin films and various microstructures that can be obtained by controlling the deposition conditions.
This rich variety of potential morphologies is made possible by strict control of deposition parameters (e.g., working pressure, residual or carrier gas, substrate geometry and temperature) that are known to have a direct impact on the growth mechanism (see Sections 2 and 3).
The morphologies obtained can be divided into three groups: (i) standard tilted nanocolumnar layers fabricated by OAD at low pressure (Fig.
5.1(A)–(C)), (ii) nanostructures with a tapered nanocolumn profile over a critical length obtained by deposition with a carrier nitrogen flux (Fig.
5.1(D)–(H)), and (iii) nanowhiskers and hierarchical, multi-branched tree-shaped nanostructures obtained by sequential application of VLS-OAD [133,134,137,331] (see Section 3.1 and Fig.
5.2 for a more detailed description of the singular branched nanostructures created using this method).
Selected applications relying on these nanostructured TCO films will be discussed in the sections that follow.
Owing to their open microstructure, controlling the electrical properties of ITO-OAD films has required the development of specific methodologies and the use of relatively sophisticated techniques.
These have included axial resistivity measurements by a cross-bridge Kelvin system [321], terahertz time-domain spectroscopy (THz-TDS) [325], or combinations of experimental and theoretical studies [324].
These, and other investigations, relating the OAD microstructure of ITO nanopillars to the deposition parameters [317] (Fig.
5.1(A)–(C)) have proved that it is possible to achieve optical transmittances and electrical conductivities comparable to those of compact ITO thin films.
The good electrical performance, high porosity and nanocolumnar structure of ITO-OAD films has fostered their incorporation as active components into liquid crystal displays (LCDs) for the alignment of calamitic liquid crystals (Fig.
5.3(A) and (B)).
Typically, an LCD display integrates different layers of transparent electrodes, LC alignment layers and birefringent compensators (Fig.
5.3(C)), functions that can be provided by just one OAD-ITO nano-columnar film (Fig.
5.3(D)).
Meanwhile, coatings with a graded refractive created by combining dense (n(500nm)=2.10) and porous (n(500nm)=1.33) stacked layers of ITO through changing the zenithal angle of deposition [88] have been used for photonic and combined photonic–electronic applications.
For example, different color filters designed and fabricated using this approach have been utilized as the bottom electrode of a LCD system (Fig.
5.3(E) and (F)).
Other photonic components such as Bragg reflectors [316,332], broadband antireflective and conductive materials [331], and transparent electrodes and antireflection contacts in organic light emitting devices [330,333] have also incorporated similar photonic and conductive layers.
OAD ITO layers have been used as transparent conducting and antireflective coatings in: organic photovoltaic solar cells [319,334], Si-based solar cells [322,323,327] and GaAs photovoltaic devices [329].
The photon absorption capacity of a solar cell is one of the main factors contributing to its global efficiency.
Thus, in order to reduce the light reflection coefficient and increase the amount of transmitted light, antireflection coatings (AR) are usually incorporated on the front side of these devices.
Because of their high porosity (see Section 3.6), OAD thin films and multilayer structures have an inherently low refraction index, making them suitable for AR-layer applications [331,332,335].
An added benefit of ITO OAD films used for this purpose resides in their high electrical conductivity.
From the point of view of cell manufacturing, another advantage is the possibility of using a single material as an AR coating rather than the multilayer structure of alternating layers of different refractive index that are typically used (see Section 5.4).
Going beyond the use of conventional ITO OAD films, Yu et al.
[329] have proposed a new AR concept based on the peculiar ITO nanocolumns presented in Fig.
5.1(D)–(H).
The characteristic tapered and bent microstructure of these nanocolumns works as a collective graded-index layer offering outstanding omnidirectional and broad-band antireflection properties for both s- and p-polarizations.
The benefit to a GaAs solar device integrating one of these ITO films (Fig.
5.4(A) and (B)) over not using an AR layer is as much as 28%, with nearly 42% of the enhancement in the photocurrent generated through the transparent gap of the window layer.
Nanocolumnar ITO films have also been used as high surface area 3D nanoelectrodes in organic solar cells [319].
Fig.
5.4(C)–(F) depicts a device consisting of an OAD-ITO electrode electrochemically modified with nanofibrous PEDOT:PSS (poly(3,4-ethylenedioxythiophene):poly(p-styrenesulfonate)) to produce a cobweb-like structure (inset in Fig.
5.4(C)), which is then infiltrated by spin-coating with a P3HT:PCBM photoactive layer (Fig.
5.4(C)).
The cross-sectional view given in this figure provides evidence that the ITO layer is effectively infiltrated by the photoactive material, a requisite for good performance with this type of solar cell device.
The cell is electrically contacted with an evaporated aluminum layer on top (Fig.
5.4(D)), which integrates the insulating electrode formed by OAD-ITO nanopillars covered with OAD-SiO2 caps (Fig.
5.4(E) and (F)).
This solar cell configuration exhibits better performance than equivalent devices based on compact ITO films.
In a very recent collaborative work between the Pohang University and the Ressenlaer Polytechnick Institute [336], ITO nanohelix arrays were used as three-dimensional porous electrodes, antireflection coatings and light-scattering layers in bulk heterojunction solar cells.
These ITO arrays provided an enhancement in both the total light absorption and the charge transported from the photoactive layer to the electrode; the combination of these effects resulting in a 10% increase in short-circuit current density and a substantial increase in the power-conversion efficiency.
As a result of their high surface area and outstanding optical/electrical properties, ITO nanopillar layers have been extensively used as biosensors to detect immobilized biomolecules within their pores by means of different spectro-electrochemical methods.
One example of this is the rapid absorption of microperoxidase within the meso- and micro-pores of OAD ITO layers, which has been used for the detection of a series of adsorbed redox biomolecules that were electrochemically monitored by derivative cyclic volt-absorptiometry [320,337].
Using another sensing approach, Byu et al.
[338] demonstrated that the performance of surface plasmon resonance (SPR) gold biosensors can be enhanced by adding an OAD-ITO layer to increase the surface area available for the target molecules.
These decorated substrates present a much higher sensitivity than bare gold films in the analysis of ethanol–water mixtures.
The high surface area of ITO-OAD films has also proven decisive in the development of resistive gas sensors for NO2 [339], in that a maximum sensitivity of 50ppb and short response time have been achieved with highly porous films prepared using high zenithal angles of deposition (see Section 3.6).
Further improvements were obtained by incorporating a bottom layer of Fe2O3 or SiO2 as seeds to favor the growth of interconnected ITO nanorods, with their inner channels being highly accessible to the surrounding medium.
The high cost and increasing demand of indium over the last few years have prompted the development of TCO films with other compositions; this also being true of TCO thin films prepared under OAD conditions.
Al-doped zinc oxide (AZO) thin films prepared by either e-beam or MS OAD [323,340–344] have been successfully used in solar cells and LED devices.
The synthesis of Sb-doped tin oxide (STO) and Nb-doped titanium oxide (TNO) films with a strictly controlled composition, microstructure and porosity has also been recently reported to achieve outstanding optical transparency and electrical conductivity [323,345,346].
In most of these preparations, MS has been the technique of choice for the simple reason that it is better suited for depositions over large areas.
Some examples in the literature include the MS-OAD of Ga-doped ZnO [168,347,348] and the development of a co-sputtering method for the fabrication of high and low refractive index layers of TNO and AZO, respectively, for use in the preparation of a transparent and conductive distributed Bragg reflector [340,349].
In this structure, the eight-period stack of AZO/TNO layers produces a reflectivity of ∼90% along a spectral region centered at 550nm, and a resistivity of less than 2×10−3Ωcm.
Because of this outstanding performance, this system has been proposed as a mirror and charge injection layer to substitute the existing dielectric Bragg reflectors in vertical cavity surface emitting lasers.
The last few years have borne witness to a tremendous expansion in the development of efficient renewable energy sources and/or the implementation of new, advanced energy storage and saving methods.
Among the solutions proposed for the development of these new methods, we have seen that OAD thin films play a special role due to their unique properties.
In this section we review some of the recent advances in the development of OAD thin films for renewable energy applications, with a special emphasis on electricity generation by photovoltaic solar cells, solar-driven photocatalytic hydrogen production, fuel cells, electric energy storage with lithium ion rechargeable batteries and electrochromic coatings.
The emerging topic of piezoelectric generators based on OAD thin films will also be introduced.
OAD nanostructures represent versatile candidates to fabricate the many diverse elements of a photovoltaic solar cell device.
Following a simplified description, OAD thin films have been used as: (i) transparent conducting layers, (ii) active semiconductor materials for electron–hole production upon absorption of photons, (iii) electron or hole conducting/blocking layers, (iv) sculptured counter electrodes, and (v) antireflection coatings.
In Section 5.1 we reviewed the latest developments regarding the OAD of ITO and other TCOs used as transparent conducting materials and AR coatings in different solar cells architectures.
In this section, we summarize the advances that have been made in the fabrication, design and optimization of other OAD nanocolumnar thin films utilized in this type of device/component.
Layers of OAD nanocolumnar materials other than TCOs have been used as AR coatings to optimize the efficiency of various types of solar cell device.
Their use is motivated by expected improvements in photon harvesting, carrier separation or carrier collection efficiencies.
Photon harvesting in photoactive solar cell components substantially increases if light is scattered or confined within the system, and so the implementation of an AR coating at the front side of a solar cell is a well-known method of confinement that enhances light absorption by the active material.
Since sunlight has a broad spectrum and its angle of incidence changes throughout the day, high-performance AR coatings must be efficient over the entire solar spectrum and a wide range of incident angles [335].
A good approach to avoid Fresnel reflections (i.e.
optical reflections at the interface between two materials with different refractive indexes) is to insert an interface with a graded refractive index between the two media.
Schubert et al.
[350,351] have shown that the interference coatings formed by stacking OAD films with different refractive indexes may provide the desired broadband and omnidirectional AR characteristics needed for photovoltaic cell applications.
There have been two general approaches adopted for the fabrication of AR OAD structures for solar cell applications: the multilayer stacking of low and high refractive index materials, and the combination of layers of the same material but with different densities (i.e.
porosities, see Sections 2.4 and 3.6).
In the case of the former, Schubert et al.
[237] have reported the lowest refractive index material known to date, with a n value ∼1.05 for SiO2.
They also succeeded in fabricating a graded-index broadband and omnidirectional AR coating consisting of alternant layers of TiO2 and SiO2 with refractive indexes ranging from 2.7 to 1.3 and 1.45 to 1.05, respectively.
As reported in Fig.
5.5, such a coating effectively reduces the Fresnel reflections when deposited on an AlN substrate (n is ∼2.03).
A typical example of an AR OAD coating used to enhance photon harvesting is one formed by stacked TiO2 and SiO2 layers [228,351–353], which in some cases are prepared through successive sputtering of the two materials [354,355].
The extension of this procedure to large area substrates, including deposition on conformable polymers, has also been reported [356,357].
Other reported attempts include the fabrication of single-material AR coatings by stacking different refractive index layers of ITO (see Section 5.1), MgF2 [358], SiO2 [358–361], TiO2 [362], alumina [363], ZnS [364] or ZnO [365].
Composite and compositionally complex thin film materials have also been used to increase the scattering of light in solar cell devices.
Some examples of this approach include: (i) antireflection and self-cleaning coatings for III–V multi-junction solar cells made of OAD layers of gold grown on nanocone arrays fabricated by plasma etching borosilicate glass [366], (ii) magnetron sputtered TixSn1−xO2 films prepared on top of a seed layer of polystyrene spheres [367,368], (iii) HfO2 films [369] combined with amorphous porous silicon with closed porosity [370], and (iv) sculptured TiO2 films [371].
The implementation of OAD thin films and nanostructures as active photon absorption components in solar cells is another relevant field of interest for these materials.
One approach in this context consists of exploiting the high surface area of columnar OAD films to increase the interface area between p- and n-type semiconductors or, depending on the type of solar cell, between a semiconductor and an electrolyte.
In order to reduce the electron–hole recombination probability, OAD nanostructures are also used to diminish the transport distance of the minority carriers from the absorbing semiconductor to the charge collection electrode.
Furthermore, the unique architecture of OAD films at the nanometer scale provides different ways to increase both the interface area and carrier lifetime in solar cells.
Various cases will be presented next to illustrate the possibilities in dye and quantum dot (QD) sensitized solar cells, organic and hybrid photovoltaic solar cells and Si-based solar cells.
Titanium dioxide nanomaterials have been extensively used as a wide-band-gap semiconductor in DSSCs [372].
In typical cells of this type, anatase thin films are made from agglomerated nanoparticles, nanotubes or nanorods, while light-absorbing dye molecules are adsorbed onto the oxide surface [372–374].
Studies into one-dimensional TiO2 nanomaterials for DSSCs have been motivated by the idea that the transport of charge carriers along tubes or nanowires may be more efficient than through a nanoparticle film, where electrons must cross many boundaries before reaching the collection electrode.
According to this, one-dimensional nanostructures should have a lower diffusion resistance than nanocrystalline films, thereby facilitating the collection of photogenerated charge carriers [375].
With this in mind, several authors have used OAD TiO2 nanocolumnar films as photo-anodes in DSSCs [48,63,64,284,376,377], achieving performances similar to that of cells based on conventional TiO2 nanoparticle films.
A schematic showing the typical structure of a DSSC incorporating an OAD TiO2 photoanode is given in Fig.
5.6(A).
In most cases, a compact TiO2 layer is first deposited between the porous OAD TiO2 and TCO substrate to prevent the direct contact between the electrolyte and electrode [63,64,284].
Ruthenium 535-bisTBA, also known as N719, is the most common dye in this type of solar cell.
In the example shown in Fig.
5.6, the electrolyte chosen was Iodolite AN-50.
Other combinations of dye and electrolyte have also been used with OAD TiO2 films, including viscous and fast-recombining ionic-liquid electrodes [284].
Most research into OAD TiO2 photoanodes has correlated photo-efficiency improvement with geometry, porosity, microstructure and the dimensions of the TiO2 nanostructure.
Indeed, much work has been devoted to determine the effect of these morphological characteristics on physical phenomena such as the amount of dye incorporated, electrolyte diffusion, light absorption and scattering or charge transport efficiency.
To this end, several OAD geometries have been studied ranging from vertical [64] to tilted [63,64,284,376] nanocolumns, and from zig-zag [63] to nanohelix nanostructures [64,284,377].
Our group has recently utilized OAD TiO2 nanocolumnar films in the fabrication of transparent DSSCs (see Fig.
5.6) [63], wherein the effects of crystallinity and deposition angle on cell efficiency were evaluated for values of α of between 70° and 85° (Fig.
5.6(B)).
After annealing the samples to acquire an anatase phase [63,64,377,378], the highest cell efficiency was found with a 70° OAD TiO2 film.
This result was attributed to the relatively high dye loading capacity of these layers (see Sections 3.6 and 4.2.2).
In the quest for a highly transparent DSSC, it was also found that zig-zag geometries produce less light scattering than straight tilted nanocolumns or nanohelixes (see Refs.
[64,377]) and impart the cell with a higher mechanical stability.
The photograph in Fig.
5.6(C) of a 5μm-thick dye-infiltrated film with a zig-zag geometry demonstrates the high transparency of the cell (90% at certain wavelengths).
Overall, a conversion efficiency of ∼2.78% was reached with 3μm thick electrodes, which is comparable with a particle electrode of similar thickness.
This was achieved together with a high cell transparency, thus supporting the implementation of these devices for glass coloring and related applications in the field of fenestration.
A modified OAD methodology named GLADOX has been reported by Kondo et al.
[369].
It combines the sputtering of Ti at oblique angles with a posterior anodization to produce highly porous TiO2 hierarchical nanostructures (nanobrushes).
Following annealing to induce crystallization to anatase, these nanoporous photoanodes were incorporated in a DSSC where they yielded an overall performance (conversion efficiency of ∼1.27% for 1.5μm thickness) that is comparable with a nanoparticulate reference cell of the same thickness.
Improved charge transport and the potential to increase photoelectrode thickness without any significant detriment to the conduction capacity of photogenerated electrons are just some of the advantages cited for this type of nanobrush system.
Generally speaking, the semiconductors used in photovoltaic devices possess a small bandgap and can potentially absorb all photons with energies above this minimum threshold.
Unfortunately, owing to the small energy difference between photo-generated electrons and holes in these systems, small bandgap semiconductors produce lower potentials than wider bandgap semiconductors.
Various approaches have been tried to combine the benefits of both large and small bandgap semiconductors in order to expand the range of absorbed wavelengths and use each absorbed photon to its full potential.
One such attempt consisted of combining semiconductors and QDs of progressively decreasing bandgap energies to promote the absorption of radiation across the entire visible spectrum [379–381].
These QDs were made from materials such as CdS, CdSe and CdTe that strongly absorb in the visible region [379,380], and which can inject photoexcited electrons into wide bandgap materials such as TiO2 and ZnO.
Moreover, these QDs display a high extinction coefficient (the capability to generate multiple excitons) and the capability of tuning their energy absorption band through the control of their size.
Utilizing these ideas, Zhang et al.
[381] proposed the OAD co-evaporation (see Section 2.6) of TiO2 and CdSe at α≈86° to deposit composite nanorods on ITO.
Femtosecond transient absorption spectroscopy analysis of the charge transfer processes in this system revealed efficient electron transfer from the conduction band of CdSe QDs to the conduction band of TiO2.
This result was attributed to the high interfacial area and a strong electronic coupling between the two materials, and represents a nice example of the possibilities that OAD and co-deposition can bring to engineering compositions and nanostructures at interfaces with a strict control over chemical states.
Very recently, Schubert et al.
[382] reported QDs solar cells based on the decoration of three-dimensional TiO2 nanohelixes with CdSe QDs, as illustrated in Fig.
5.7(A).
The SEM micrograph in Fig.
5.7(B) shows a characteristic cross section of the TiO2 nanohelixes grown on FTO, clearly showing how their large and open pores can accommodate both QDs and electrolyte.
As demonstrated by HAADF-STEM (see glossary), the distribution of QDs along the TiO2 nanohelixes is quite homogeneous and densely packed (Fig.
5.7(C)).
The enhanced electron transport properties reported for this system have been related to the high crystallinity of the nanohelixes (Fig.
5.7(D)), while their three-dimensional geometry seems to enhance light scattering effects and therefore the cell absorption capability and efficiency (Fig.
5.7(E)–(H)).
The results of this work also suggest that one-dimensional nanorods and nanotubes with diameters in the order of a determined light wavelength may act as guides for this wavelength rather than as scattering centers, as it is the case with nanoparticles in conventional electrodes.
This solar cell based on TiO2 nanohelix arrays exhibits a two-fold improvement in solar energy conversion efficiency (1.34%) with respect to QD solar cells based on a nanoparticle layer electrode (0.66%).
So far, TiO2 has been the most common material used in the fabrication of hybrid solar cells consisting of an inorganic (e.g., TiO2) and an organic component.
Here, the TiO2 acts as a good electron conductor, and is combined with conjugated polymeric poly(3-hexylthiophene) (P3HT, the most widely used compound for this function) or small-molecule p-channel materials that act as hole conductors.
OAD TiO2 thin films have been extensively used for the fabrication of different types of hybrid solar cells [383–385].
The intended function of the OAD oxide is to provide a well-defined intercolumnar space in which the small size of the P3HT infiltrated domains can limit the distance travelled by photo-generated e−-h+ pairs (i.e., excitons) until the electrons are captured by the TiO2.
Relying on the same principle of confinement, various works have reported the use of other OAD thin film materials in the fabrication of hybrid polymer and small molecule solar cells.
Examples of this include hybrid solar cells incorporating the semiconductor InN and Pb-phthalocyanine [386], or Si/P3HT solar cells in which Si nanowires are fabricated at low temperatures by hot-wire chemical vapor deposition at oblique angles [387] or typical OAD procedures [106,388,389].
The principle of operation for a fully organic photovoltaic (OPV) solar cell is similar to that of a hybrid solar cell in that solar light is absorbed by a photoactive material, thereby creating electron–hole pairs (i.e., “excitons”) with a given binding energy that must be overcome for their effective separation.
However, the short exciton diffusion lengths characteristic of organic semiconductors (e.g., up to 50nm for some metal phthalocyanines and 10nm for many polymers) tends to limit the thickness of the device, and consequently, the light absorption capacity of the cell [390].
Moreover, for an efficient exciton separation and carrier collection at the semiconductor’s heterojunction, the two materials must possess the right offset between the highest occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital (LUMO).
The microstructure needed to successfully tackle these operating restrictions is one in which electron and hole conductor materials are distributed in the form of small, interpenetrating nano-domains with a size smaller than the exciton diffusion length [390–392].
The OAD method is well suited to these morphological requirements and has been widely used to grow organic nanomaterials for OPV devices.
The majority of these OAD-OPV devices incorporate nanostructures formed by the sublimation of small-molecule materials [103,393–400], although some works have also reported the use of conjugated polymers or composites formed by small molecules and a conjugated polymer [401,402].
Fig.
5.8(A) shows a simplified view of a nanocolumnar OPV device and the molecular structure of some of its components.
The most common donor materials in small molecule OAD-OPVs are metal phthalocyanines (MPc) such as CuPc [103,150,395–397,399], PbPc [398] and ClAlPc [394].
The most common acceptor in these devices is fullerene, either in its sublimable (C60) or soluble form as [6,6]-phenyl-C61 butyric acid methyl ester.
In the latter case, the solvent choice and solution concentration critically affect the OPV performance because of difficulties in controlling the domain size and fragility of the donor/acceptor interface [396].
One of the first examples of an OPV formed through the evaporation of both donor MPc and acceptor C60 is presented in Fig.
5.8(B), wherein the device structure was developed by growing a ClAlPc film at oblique angles (α≈60°) on ITO, followed by evaporation at a normal incidence of C60 [394].
The authors demonstrated that the increase in contact area at the ClAlPc/C60 heterojunction interface leads to an increase in efficiency comprised between 2.0% and 2.8% relative to planar heterojunctions.
Equivalent experiments with OAD-CuPc/C60 OPV cells also resulted in efficiency improvements ranging between 1.3% and 1.7%.
To better control the shape and size of the organic photoactive nanocolumns, Brett et al.
[103,395,403] thoroughly investigated the evolution of the morphology and porosity of a OAD-CuPc thin film using a pre-seeded ITO substrate (Fig.
5.8(C)) [103].
Similarly, Taima et al.
[397] recently investigated the use of CuI OAD seeds to pattern the growth of ZnPc nanocolumnar arrays (Fig.
5.8(D)).
Optimized ZnPc/C60 bilayer cells fabricated following this approach presented a three-fold higher efficiency (4.0%) than an equivalent planar cell.
Additional results from nanopatterning with organic seeds can be found elsewhere [150,399].
The OAD of C60 as an electron acceptor material has also been tried in C60/P3HT cells [402] and, in combination with another hole collecting material (i.e., P3CBT), for the manufacture of inverted solar cells.
Although the efficiencies achieved in this case were rather low (i.e.
less than 1%), the values obtained were still two-to-four times greater than those for an equivalent planar cell fabricated by solution processes.
The use of hydrogen as an energy vector in transportation or for the production of electricity has been proposed as a clean alternative to hydrocarbon fuels.
At present, it is extensively used in chemical synthetic applications (e.g., ammonia production, or in the petrochemical, food and metal industries); however, its wider use as an energy vector faces serious scientific, technological and economic challenges regarding its production, storage and final combustion.
In the sections that follow we summarize some of the recent advances in the use of OAD thin film materials for applications covering the whole chain of hydrogen technology.
Considering the most commonly recurring themes addressed in recent publications on the subject, this review will be divided into three main parts: the solar generation of hydrogen, fuel cells and hydrogen storage.
In the four decades since the seminal work of Fuyishima and Honda [404], in which TiO2 was used as photo-active UV semiconductor, the energetically efficient photocatalytic splitting of water has remained a dream.
Yet since then, TiO2 has been extensively used in UV-driven photo-catalytic applications, with much effort being devoted to moving its spectral response to the visible part of the solar spectrum (e.g., by doping) or to develop alternative semiconductor compounds active in the visible spectrum and stable in aqueous media.
The solar driven generation of hydrogen generally requires a semiconductor (a metal oxide or sulfide), as well as a metal acting as an electron trapper, electrode or co-catalyst.
According to a simplified description of the process, the absorption of a photon by the semiconductor creates an electron–hole pair.
The hole then migrates to the surface of the oxide, where it becomes trapped by OH− groups adsorbed at the surface and yields oxygen.
Meanwhile, the electron reaches the active metal and forms hydrogen by reducing protons.
There are two main approaches to achieving solar-light induced water splitting: (i) the use of a photo-electrochemical cell (PEC), wherein the semiconductor and metal are connected through an external circuit, and (ii) loading a powder-based semiconductor oxide with metal catalytic particles, usually Pt, decorating its surface [405].
To efficiently convert water into molecular hydrogen and oxygen, any practical system must also fulfill the following requirements: (i) the semiconductor must possess a narrow band gap (i.e.
active in the visible) to absorb a significant amount of solar light, (ii) it should promote both proton reduction and water oxidation, and (iii) it must remain stable in the electrolyte during long irradiation periods [231].
Owing to the difficulties in meeting all these requirements, recent approaches have relied on the simultaneous use of two semiconductor materials that independently promote the oxidation and reduction processes of the water splitting reaction [406].
In this way, n-type semiconductor photoanodes for oxygen evolution and p-type semiconductor photocathodes for hydrogen evolution are combined within the same system.
Aside from some intrinsic properties of semiconductors required for the intended application (i.e.
a high light-absorption coefficient, low recombination probability, etc.
), nanostructured materials with a high surface area are always desirable for water photo-catalytic splitting [405].
Since OAD semiconductor thin films and nanostructures comply with these requirements, they result ideal candidates for PEC devices and applications.
Mullins et al.
have extensively investigated the manufacturing of OAD materials for PECs and Li-ion batteries electrodes, with a concise review from 2012 provided in Ref.
[231].
An outstanding result of these works has been the preparation by OAD of photo-active alpha-Fe2O3 photoanodes capable of collecting light in both the UV and visible regions of the solar spectrum.
These authors have also reported the controlled doping of these nanostructures with Ti, Sn or Si by co-evaporation at oblique angles (see Section 2.6) [406–408] or by LPD-OAD [409].
The same co-evaporation method has also been used for the synthesis of BiVO4 and Co-doped BiVO4, two active semiconductor materials capable of water photo-splitting using visible light [126].
More recently, the same group has explored the OAD deposition of tungsten semicarbide (W2C) which, when deposited on p-Si substrates, proved to be an efficient photo-active support for Pt nanoparticles [410].
The combination of two n-type semiconductors with comparable band gaps in a single photo-anode has been proposed as a means of optimizing both the lifetime of the electron–hole pair and the charge-separation efficiency at the surface.
To this end, Ludwig et al.
[411] prepared by MS a layered system consisting of WO3/TiO2 nanostructures.
For this, they followed a combinatorial approach, whereby a WO3 layer in the form of wedges was first deposited by controlling the sputtering pressure, and then was followed by TiO2 nanocolumnar layers deposited by MS-OAD.
Similarly, Zhao et al.
[412] prepared core/shell TiO2/WO3 and WO3/TiO2 nanorods through dynamic electron beam OAD by varying the zenithal angle, α, from 86° for the core to 11° for the shell.
Their results suggest that the use of core/shell nanorods as photo-anodes preserves the optical properties and water splitting performance of the core, while the shell composition determines the flat band potential at the surface [413,414].
These authors have also developed a plasmonic-driven PEC cell consisting of heterostructured Ag/TiO2 nanoarrays with silver nanoparticles embedded in the outer surface layers of TiO2 nanocolumns [149].
The metal nanoparticles in this structure induce an enhanced photocatalytic activity under both visible- and UV–visible illumination through a plasmon-promoted electron transfer in the case of the former, and by facilitating electron–hole separation in the latter.
The same authors have also pioneered the application of OAD ZnO as an n-type semiconductor for PEC cells [415].
In the course of these investigations, they compared the performance of materials deposited by PLD at both normal (PLD ZnO) and glancing at α=86° (PLD-OAD ZnO) incidences with electron beam evaporation at the same incident angle (named GLAD ZnO in the authoŕs work).
These deposition methods yielded different ZnO microstructures: a compact thin film (PLD), and nanoplatelet (PLD-OAD) and nanoparticulated (GLAD) thin films.
From the point of view of PEC performance, GLAD ZnO generated the highest initial photocurrent, most likely thanks to a fast charge transport kinetics resulting from a small defect concentration in this material.
Among the proposed hydrogen storage methods, the trapping of hydrogen by chemical or physical bonding into suitable materials has provided some of the highest volumetric densities reported to date [405].
The conditions required for suitable hydrogen storage applications are a high storage capacity, fast kinetics and a high reversibility of both hydrogenation and dehydrogenation processes.
Additional requirements are a low-reaction temperature and operating pressure, easy activation, minimal deterioration of the material and an affordable cost [416].
Solid-state hydrogen storage involves either chemisorption or physisorption; the latter mechanism usually involving van der Waals interactions that require cryogenic temperatures.
Chemisorption involves the dissociation of hydrogen molecules and formation of metal hydrides (e.g., AlH3, MgH2, TiH2 and LiH), which usually requires high temperatures and a catalyst.
The main problem usually encountered with these hydride compounds, however, is their high sensitivity to oxygen.
In principle, OAD nanostructures are good candidates as hydrogen storage materials because their intercolumnar space allows for a reversible volume expansion during the transformation from metal to hydride and vice-versa.
Moreover, thanks to their high surface area, nanocolumns enable faster hydrogen adsorption/desorption rates than compact materials.
A pioneering work related to this possibility using OAD nanostructures of LaNi5 was published by Jain et al.
in 2001 [417].
Following this idea, Zhao et al.
investigated the synthesis and performance of Mg nanoblades both with and without vanadium decoration [418–420].
Wang et al.
also applied the OAD method to fabricate Mg nanoblades decorated with Pd [416,421].
To increase the chemical stability of the system, the Pd/Mg nanoblades were protected with a conformal layer of parylene: a polymeric material that is highly permeable to hydrogen, but prevents the passing of other gases [422].
Meanwhile, the resistance of Mg nanorods to oxidation under conditions ranging from room temperature to 350°C was also demonstrated for MS-OAD magnesium prepared under controlled conditions [423].
Fuel cells convert chemical energy into electrical current through the chemical reaction of a fuel (H2, hydrocarbons or methanol) with oxygen or other oxidizing agents.
The main components of the cell are anode, electrolyte and cathode.
The type of electrolyte, fuel, working temperature and start-up time are the criteria utilized for the classification of fuel cells [424].
In this section, we summarize some of the recent applications of OAD thin films as components in proton exchange membranes or polymer electrolyte fuel cells (PEMFCs and PEFCs) at low temperatures, solid oxide fuel cells operated at high or intermediate temperature (SOFCs and IT-SOFCs) and direct-methanol fuel cells (DMFCs).
These cells operate at low temperatures and a key issue to improve their performance is to increase the catalytic efficiency of the cathode for the oxidation of hydrogen under these mild operating conditions.
Gall et al.
[425–428] and Karabacak et al.
[127,429–431] have extensively applied OAD methodology to the fabrication of Pt-based cathodes for PEMFCs and develop Pt/C cathodes through the dynamic MS-OAD of carbon nanorods (α∼85°) onto flat, pre-patterned substrates, followed by posterior decoration of their tips with Pt deposited by MS in the classic (non-oblique) configuration [426].
The use of these pre-patterned surfaces yielded C layers that were highly porous in the vicinity of the substrate, but became rather compact and similar to thin films obtained by conventional MS at their ends.
This topology ensures that the Pt layers deposited on top of these OAD carbon films present similar microstructures as those deposited on conventional carbon substrates.
The performance of these two types of Pt/C cathode was similar at high potentials, whereas at lower potentials the performance of Pt deposited on a pre-patterned substrate was significantly higher.
This difference in behavior was attributed to a more effective mass-transport of oxygen to the Pt reaction sites through the porous structure of the pre-patterned cathodes.
To increase the available open porosity of the system, these authors applied an additional in-situ etching treatment once the Pt/C-OAD cathodes were inserted into the cell [427].
Meanwhile, to assess the importance of thin film porosity, they also compared the performance of compact Pt layers deposited using a conventional (non-oblique) configuration, and porous MS-OAD layers of Pt.
The columnar porous electrodes were found to exhibit a higher (lower) mass-specific performance than the compact electrodes with a high (low) current density, a result that was attributed to the higher porosity of the latter and lower electrochemically active surface area of the former [425].
Karabacak et al.
[429–431] focused their investigations on the MS deposition of well-crystallized Pt nanorods to improve their efficiency in oxygen reduction reactions without carbon support.
To achieve this, they studied the formation of well-isolated, single-crystalline and vertically aligned Pt nanorods formed by dynamic OAD at an oblique angle of 85°, finding that the Pt OAD films exhibit a higher specific activity, higher electron-transfer rate and comparable activation energy when compared to conventional Pt/C electrodes.
This enhanced performance was attributed to the single-crystalline character, larger crystallite size and dominance of Pt(110) facets in these OAD thin films.
Minimizing the Pt loading in the cathode is a common strategy to reduce the cost of PEMCS.
One possibility in this regard that has been widely explored in the case of OAD films is the search for chemically stable substrates other than carbon where substrate oxidation and the resulting loss of Pt nanoparticles can be avoided or minimized.
With this in mind, the use of Pt/Ti-OAD nanocolumnar systems [432], Pt/CrN cathodes [426] or Pt decorated Cr nanorods [431] have all been proposed.
In the case of the latter two examples, nanostructured CrN and Cr layers prepared by MS-OAD act as supporting layers.
Meanwhile, in Ref.
[431], a conformal Pt layer was deposited on top of the Cr nanorods using a small angle deposition (SAD) (α<45°) approach that yields a core/shell nanostructure in which self-shadowing effects are absent at normal incidence.
This Pt(SAD)/Cr(OAD) layer configuration provides a higher oxygen reduction reaction activity and superior catalyst performance than a Pt layer deposited in a normal configuration.
With the same purpose, this group recently reported the MS-OAD of Pt–Ni alloy nanorods by dynamic co-deposition at α∼90° relative to two independent Pt and Ni sputter sources [127].
In this way, they demonstrated that the alloy composition and microstructure can be fine-tuned by controlling the OAD conditions, and that upon electrochemical cycling these Pt–Ni nanorods remain more stable than Pt nanorods or conventional high-surface-area Pt/C electrodes.
The fabrication by OAD of niobium oxides (for PEMFCs) and Ni (for alkaline direct alcohol fuel cells, or ADAFCs) as supports for Pt cathodes has also been recently reported by Brett et al.
[433,434].
Though less common, MS-OAD has been applied for the fabrication of Pt-doped CeO2 anodes for PEMFCs [435].
In this work, carbon nanotubes (CNTs) were used as both support and template to obtain a specific three-dimensional anode morphology.
Oxide layers doped with Pt were then deposited onto the tips of the CNTs by co-sputtering the two components, with the resulting configuration exhibiting satisfactory catalytic activity.
One of the challenges in this type of cell, which operates at moderate temperatures, is preventing the poisoning of the Pt catalyst by carbon monoxide (CO) formed during the methanol oxidation reaction.
Alternate layers of Pt and Ru nanorods deposited by sputtering in a OAD configuration have been proposed as a catalyst for this type of DMFC cell [436].
The aim of this Pt–Ru configuration is that the CO-poisoned platinum is regenerated through its reaction with oxygen species formed on the ruthenium (i.e., CO+Osurf→CO2).
As a result, when used in an acidic medium, the Pt/Ru OAD multilayers exhibit an electrocatalytic activity with respect to methanol electro-oxidation reaction that is higher than that of equivalent monometallic Pt nanorods.
OAD has also been used for the fabrication of electrolyte layers in SOFCs working at intermediate temperatures.
One of the first published examples in this research topic dealt with the OAD of porous yttria-stabilized zirconia (YSZ) thin films, wherein CeO2 was infiltrated by spin-coating [437].
This configuration takes advantage of the excellent oxygen ion transport of doped ceria at intermediate temperatures to reduce the operational temperature of YSZ (about 1000°C) electrolyte without affecting the power density of the device.
Indeed, the resulting composite CeO2/YSZ(OAD) system displayed a high conductivity in the 300–550°C temperature range, with values higher than those attained with a compact CeO2 layer.
For high-temperature SOFCs, Lii et al.
[438] explored the OAD of YSZ and strontia-doped lanthanum manganite (LSMO) for use as an electrolyte and oxygen reducing cathode, respectively.
The comparable thermal expansion coefficient of these two ceramic materials contributes to an “in operando” stability of the system, while the triple phase boundary formed between the cathode acting as a catalyst, the reacting gas and the solid electrolyte favors exchange reactions involving oxygen gas molecules and ions in the electrolyte.
This bilayer system was fabricated by dynamic e-beam OAD at α=80° and 200°C, and was subsequently annealed at 900°C to produce a well-crystallized material with a microstructure consisting of vertically aligned YSZ nanorods and oblique LSMO nanocolumns.
At 600°C it exhibited a resistance that was ten-times lower than that of a single porous LSMO catalyst layer.
Very recently, the same authors have pushed the OAD technology a step forward with the fabrication of both the anode (Ni/YST) and cathode (Samaria-doped ceria (SDC)/La1xSrxCo1yFeyO3d (LSCF)) of a SOFC [439].
Owing to their flexible design and high energy density, Li-ion batteries have become one of the most popular storage systems used in portable electronics.
The basic components of these batteries are cathode, anode and electrolyte.
As shown schematically in Fig.
5.9(A), they operate by extracting Li ions from the cathode and inserting them into the anode during charging [440,441].
This process is reversed during discharging, when Li ions are extracted from the anode and inserted into the cathode.
Typical cathode materials are laminar oxides (LiCoO2 and TiO2), spinels (LiMn2O4) and transition metal phosphates (LiFePO4), while common anode materials include Si, graphite, carbon and Sn.
A standard electrolyte consists of solid lithium salt in an organic solvent.
The nature of the insertion and extraction mechanisms varies from the electrochemical intercalation of layered oxides and graphite, to alloy formation with Sn or Si.
The electrode’s performance is typically quantified in terms of its charge capacity per unit weight, a functional parameter that is directly linked to the electrode’s porosity [440].
The superior performance of porous materials stems from the fact that a high electrode/electrolyte interfacial area favors a rapid ion-coupled electron transfer rate and provides direct access to the bulk and surface atomic layers (Fig.
5.9(B)) [442].
Furthermore, nanosized and structurally disordered materials can better accommodate volume changes and lattice stresses caused by structural and phase transformations during lithiation/delithiation [231].
Films deposited by OAD comply with most of these requirements, and so the technique has been applied to the fabrication of both anodes [125,231,443,80,444–447] and cathodes [231,442,448–450].
Silicon is a material very well suited to Li-ion battery anodes because its nominal charge capacity can exceed ten-times that of graphite.
However, Si anodes must also face the problem of the huge volume change (about 400%) that occurs during the insertion/extraction of Li ions.
The stress associated with this can cause cracking and pulverization of the Si anodes, eventually leading to a loss of electrical contact and fading of electrical capacity [440].
Brett et al.
[443] have demonstrated the advantages of OAD Si films used as three-dimensional Li-ion battery anodes by showing that vertical Si nanorods (porosity ca.
80%) have twice the expected capacity of a compact film, and can therefore sustain more than 70 insertion/removal cycles without any significant capacity fade [80].
Koratkar et al.
extended the OAD method to the growth of Si nanorods on a Cu foil for their direct implementation in the cell [440].
Other groups had previously demonstrated that OAD Si nanosprings display a high compliance and offer enhanced stress resilience during Li ion exchange [451].
In recent publications, different authors have also proposed advanced designs for a battery anode that combines several materials (carbon, aluminum and silicon) in each nanocolumn with a so-called strain-graded nanoscoop microstructure (Fig.
5.9(C)–(F)) [445,452].
This system can undergo rapid volume changes without interfacial cracking or delamination because its stacked microstructure provides a gradual variation of strain due to the different volumetric expansion of the three materials when alloyed with lithium.
The manufactured anodes were deposited by DC-MS and consisted of an array of C nanorods with an intermediate layer of Al capped by a scoop of Si.
The carbon nanorods were prepared by dynamic OAD with α=85°, whereas the subsequent Al and Si flux incidence was normal to the carbon nanocolumns.
Fig.
5.9(C)–(E) shows a schematic representation of the nanoscoops, together with their characteristic normal and cross-sectional views and corresponding XPS depth-profile.
Fig.
5.9(F) presents the estimated variation in volume following lithiation, showing that a large volume expansion of Si is possible without provoking device failure thanks to the introduction of an intermediate layer of aluminum.
This provides a gradual transition of strain from carbon to silicon and permits stable operation of the electrode under numerous high-rate charge/discharge processes.
Another type of composition-graded Si-based nanorods with controlled amounts of copper, which was prepared by OA co-deposition, has also shown significant improvements in conductivity and an enhanced adhesion to substrates [125,444].
In another example, pure OAD Al anodes grown on titanium/glass substrates demonstrated a specific capacity close to the theoretical maximum, as well as an average columbic efficiency of ∼91% [80].
Mullins et al.
proposed an alternative procedure to increase the stability of Si anodes, which consisted of dosing small amounts of oxygen during the growth (partial bulk oxidation) of Si OAD films, followed by posterior annealing at low temperature in air (surface oxidation).
The formation of these bulk and surface oxides provides a high capacity (2200mAh/g) with virtually no capacity loss during the first 120 cycles, and only a slight capacity fade (∼0.15% per cycle) between 150 and 300 cycles.
As a result, the anodes retained up to ∼80% of their original capacity after 300 cycles [453].
These authors have also explored the use of silicon–germanium alloys [446] and pure germanium [454] as anode materials.
Germanium is expected to be a suitable material for battery anodes as its high electronic and ionic conductivity should allow for a very high charge/discharge rate.
Thus, by systematically changing the composition of Si(1−x)Gex alloy, it was found that the anode’s specific capacity decreased and its electronic conductivity and high-rate performance increased with germanium content.
Meanwhile, an outstanding result found when using pure germanium OAD thin films in sodium-ion batteries was a high rate of operation at room temperature with this anode material [454].
For an overall view of this work on ion batteries, readers are redirected to a recent review of this group [231], where in addition to summarizing their advances in the fabrication of anodes, they also comment on the use of amorphous OAD TiO2 films as cathodes (see Ref.
[126]).
Various OAD nanostructures have been used recently as lithium battery cathodes.
For example, needle-like LiFePO4 films deposited by off-axis pulsed laser deposition have been used for this purpose [448,449], as well as FeF2 cathodes prepared by dynamic OAD with tailored periodic vertical porosity [442] (Fig.
5.9(B)).
Unlike the behavior of dense thin films, the ion and electron transport properties of these nanostructured cathodes are independent of their vertical thickness.
As shown in the bottom part of Fig.
5.9(B), this is because the vertically aligned porous microstructures of these films assures a high accessibility of Li+ ions along the whole electrode, as well as a substantial increase in area of the substrate–electrolyte-film triple point interface.
Moreover, with this particular morphology, it is possible to achieve high conductivities with cathode thicknesses up to 850nm, which is about six-times the maximum thickness attainable with FeF2 dense films due to their relatively insulating nature.
During recent years, electrochromic coatings have evolved into a practical solution for indoors energetic control, displays and other esthetic applications [455].
Although different oxides and organic compounds are also utilized, the most popular system for fenestration and house-light control is based on tungsten oxide as an active electrochromic layer that is tunable from a deep blue to a transparent state, and a nickel oxide layer as a contra-electrode.
Electrochromic film devices based on tungsten oxide consist of a reducible WO3 layer, a second thin-film electrode and an electrolyte containing a M+ cation (typically H+ or Li+) that is incorporated into the film during the reduction cycle.
Optimal device performance therefore requires rapid incorporation of M+ ions into the film and their reversible release to the electrolyte during reduction and oxidation cycles, respectively (i.e., WOx+e−+M+→WOx (M) for the reduction cycle).
Optimizing the electrochromic behavior involves increasing the incorporation capacity and maximizing the diffusion rate of M+ cations within the film structure.
These two requirements are fulfilled if the thin film of the cathode has high porosity, making OAD thin films prepared by either thermal evaporation or MS ideal candidates for this purpose.
Accordingly, Granqvist et al.
[456] reported in 1995 the preparation of WO3 electrochromic thin films by MS-OAD; however, even if the open and highly porous character of these films is very promising for their implementation as fast switchable electrochromic layers, only more recent attempts have incorporated OAD oxide thin films for this purpose [311,457,458].
These recent studies report the MS-OAD of WxOzSiy [311,457] and CoxOzSiy [458] solid-solution thin films and their implementation as electrochromic cathodes.
The high porosity of such films makes them very suitable for this application, with the integrated devices presenting fast response times and a high coloration capacity.
In addition, these mixed oxide solid solutions add the possibility of controlling the optical properties (e.g., refraction index, absorption coefficient, etc.)
in the bleached state by varying the relative amount of silicon with respect to either W or Co. One of the most promising options for the development of generators directly implementable in wireless nanosystems is to apply the piezoelectric effect to convert mechanical energy (such as body movement or muscle stretching), vibrational energy (such as acoustic or ultrasonic waves) or hydraulic energy (such as body fluid flow) into electricity.
This topic has experienced ceaseless development in recent years thanks largely to the works of Wang et al.
[459,460].
The piezoelectric effect relies on the generation of electrical voltage by imposing a mechanical stress/strain on a piezoelectric material and vice-versa.
Typical examples of piezoelectric nanogenerators are based on 1D ZnO nanostructures, although other materials such as ZnS, CdS, GaN and InN are expected to show an improved performance because of their relatively higher piezoelectric coefficient.
Although still in the very early stages, promising results pertaining to the exploitation of OAD piezoelectric materials have recently appeared in the literature [460–463].
The MS-OAD of ZnO and orientational control of obliquely aligned epitaxial InN nanorod arrays by OAD using a plasma-assisted molecular beam epitaxy system have been descried in Refs.
[462,463].
In some of these works, the piezoelectric output voltage was determined by scanning the Pt(Si) tip of an atomic force microscope along four different directions with respect to the tilt angle of the ZnO NWs.
These studies revealed an anisotropic generation of electricity as a function of the characteristic geometry of the OAD films [462].
Meanwhile, other authors have reported an increase in the output power generated by growing InN nanorods tilted along the direction of the piezoelectric field, while also applying mechanical deformation with a force normal to the surface [461].
Nanosensors is another area of application that has greatly profited from the inherent high surface area and controlled nanoporosity of OAD nanostructures when utilized as transducers for the determination of different chemical analytes.
The use of OAD thin films for biosensor applications will be reviewed in Section 5.6 and the use of photonic detection methods in Section 5.5.
Most cases discussed here refer to electrical gas sensors, although a couple of examples using acoustic and optofluidic devices for liquid monitoring are also critically described.
A short subsection devoted to particular applications in the field of pressure sensors and actuators complete this analysis of sensors.
Typical gas and vapor sensor devices that use OAD thin films as transducer elements rely on changes in resistivity or electrical capacitance upon exposure to a corresponding analyte.
Capacitance humidity sensors consisting of OAD TiO2 nanostructures deposited on interdigitated electrodes were developed some time ago by Brett et al.
[202,464–466].
Although other OAD materials respond to humidity variations (e.g., SiO2 and Al2O3), sensors utilizing TiO2 exhibit the greatest change and show an exponential increase from ∼1nF to ∼1μF when the humidity changes from 2% to 92% [467].
The same group developed room temperature SiO2 OAD sensors to selectively detect methanol, ethanol, 1-propanol, 2-propanol and 1-butanol by monitoring both the frequency dependent capacitance and impedance changes in the system [468].
In this work, it was also determined that for ethanol and 1-butanol sensor aging is reduced by UV illumination, a treatment that had no effect when detecting other alcohols.
Capacitive humidity sensors based on OAD polyimide have also been reported [469].
Different conductometric oxide sensors based on the variation in resistance of a transducer material upon exposure to the analyte (mainly oxidative or reducing gases and volatile organic compounds (VOCs)) have been prepared by OAD.
For example, ZnO nanorods prepared by MS-OAD present a high reproducibility and sensitivity, fast response and short recovery times in the detection of hydrogen and methane at mild temperatures (150°C) [470].
Similarly, tungsten oxide (WO3) nanocolumns have been used as conductometric sensors of NO, NO2 and ethanol [471–473].
To illustrate the possibilities and lay-out of a typical OAD conductimetry sensor device, Fig.
5.10 summarizes some of the results obtained with one of these WO3 sensors [471] in the detection of NO.
This particular system consists of MS-OAD WO3 films grown on interdigitated Pt electrodes.
A key feature of the WO3 nanorods (Fig.
5.10(B)) is their resemblance to intestinal villi (Fig.
5.10(A)), even after crystallization by annealing at 500°C (Fig.
5.10(C)).
Nitrogen isotherms and BET analysis revealed that the surface area of the nanostructured film was about 30-times greater than that of a flat, compact reference layer (Fig.
5.10(C)).
This resistive sensor was tested with different analytes, including NO, as a function of temperature at a relative humidity of 80% (Fig.
5.10(E) and (F)).
The results obtained confirmed a NO detection limit of as low as 88ppt and an extremely high selectivity to NO under humid conditions approximating human breath even in the presence of ethanol, acetone, NH3 or CO.
These results support the possibility of fabricating high quality sensor elements for breath analyzers to diagnose asthma, or for the detection of NO in aqueous media.
The magneto-optical detection of minority components is a new and sophisticated method of detection based on the coupling between a plasmonic signal and a ferromagnetic layer under the influence of a magnetic field.
In a recent work, we proved that the sensitivity of such a device could be enhanced by depositing a thin OAD TiO2 layer onto the magneto-optical layer structure [474].
This transparent layer ensured a significant increase in the surface area available for adsorption, without affecting the optical signal coming from the device.
One way of avoiding interference problems during the detection of multicomponent analyte mixtures is the incorporation of various sensing elements in the same device, and the use of a mathematical factor or multicomponent analysis procedure.
Electronic noses (e-noses) are a typical example of this type of devices.
Recently, Kim et al.
[475] developed a multi-sensor e-nose chip incorporating six different nanostructured gas-sensing layers prepared by OAD: TiO2 thin films, TiO2 nanohelices, ITO slanted nanorods (see Ref.
[339]), SnO2 thin films, SnO2 nanohelices and WO3 zig-zag nanostructures.
These films were monolithically integrated onto a sapphire wafer through a combination of conventional micro-electronics processes and OAD.
The thin film resistivity was measured through interdigitated electrodes, while the OAD nanostructures were tested in a top–bottom electrode configuration.
The prototype e-nose showed specific sensitivity for various gases such as H2, CO and NO2.
Detection in liquid media has also benefitted from the use of OAD thin films incorporated into ultrasonic devices or complex photonic structures produced by stacking thin film layers with different refractive indices.
Sit et al.
[476,477] studied the use of nanostructured OAD SiO2 thin films to enhance the sensitivity of surface acoustic wave (SAW) sensors for liquid monitoring.
Here, SiO2 films were deposited on top of SAW devices operating at 120MHz and were then implemented in an oscillator circuit.
The evolution of the frequency signal was monitored as a function of the relative humidity, as well as for different viscous mixtures of glycerol and deionized water.
In an early work [478], a similar approach was used for the in-situ evaluation of the elastic properties of SiO2 nanocolumns deposited on a SAW circuit.
Our group has developed a very simple but effective photonic device for the optofluidic determination of the concentration of liquid solutions.
This method utilizes a Bragg microcavity consisting of periodic and alternant SiO2 and TiO2 thin films of equivalent thickness, plus a thicker SiO2 layer acting as a defect; all of which are prepared by e-beam OAD (see Section 5.4 and Fig.
5.11(a)) [285].
The resonant absorption peak characteristic of this type of photonic structure shifts when the system is infiltrated with a liquid, and the magnitude of this shift can be directly correlated with the concentration of the solution.
This system has proven to be very useful in monitoring low and high concentrations of glucose or NaCl in aqueous solutions, or the proportion of components in a glycerol/water mixture.
Study into the mechanical properties of OAD thin films and three-dimensional nanostructures such as nanohelixes and nanosprings has received continuous attention in reviews [16] and numerous works on the subject [66,479–486].
These studies have provided very useful information regarding the elastic and mechanical properties of this type of thin films and nanostructures; knowledge that has been applied to the development of different sensor devices for the monitoring of mechanical forces and pressure.
For example, Gall et al.
[487] have reported the fabrication of a nanospring pressure sensor based on zigzag and slanted Cr OAD nanostructures.
These nanocolumnar arrays exhibited a reversible change in electrical resistivity upon loading and unloading that amounted to 50% for zigzag or nanospring structures, but only to 5% for tilted nanorods.
An accompanying change in the resistivity of these sculptured films was attributed to the formation of direct pathways for electric current when the nanosprings are compressed.
Individual metal microsprings have also been used as fluidic sensors and chemical-stimulated actuators by virtue of their reliable superelasticity; their flow rate being calibrated by determining the elongation of Ti or Cr-decorated polymeric microsprings [488].
Meanwhile, a very sensitive pressure sensor based on the piezoelectric properties of embossed, hollow asymmetric ZnO hemispheres prepared by OAD has been recently reported [489].
The analysis and modeling of the optical properties of OAD thin films has been one of the most important areas of their study.
Indeed, the optical properties represent one of the most valuable tools for assessment over microstructure and composition distribution in this type of films, as they are controlled by the deposition geometry and other experimental conditions.
Ongoing advances made in this area have been extensively reviewed in excellent reports dealing with OAD thin films [16,17], while a specific up-to-date analysis of their optical properties, including aspects such as their design, fabrication, theoretical analysis and practical examples, can be found in Ref.
[21].
A comprehensive evaluation of theoretical calculations and simulations of the optical properties of OAD films, multilayers and other less complex devices can be found in the monograph of Lakhtakia and Messier [20].
Taking into account the extensive available knowledge regarding the optical properties of OAD thin films and multilayers, and considering that a clear distinction between the optical properties and potential applications is somewhat artificial, the present review has been limited to an analysis of aspects closer to specific operating devices and their final applications.
Readers more interested in the fundamentals of optical properties are referred to the previous publications already mentioned.
As a result of their tilted nanocolumnar structure, OAD thin films are optically anisotropic; as such, transparent dielectric films deposited by OAD are intrinsically birefringent, whereas metallic absorbing films are dichroic.
The anisotropic character of OAD thin films is a precise feature that was first investigated during the earliest stages of research into these materials [1,2].
Preceding sections have provided ample evidence of the fact that the OAD technique is a straightforward method for the fabrication of transparent dielectric optical films.
This procedure has been successfully utilized for the synthesis of single-layers, multilayers and graded refractive index films, all of which have been utilized as antireflective coatings, rugate filters, Bragg reflectors, waveguides, etc.
Examples of some of the dielectric materials prepared by OAD include SiO2, TiO2, Ta2O5, MgF2, ZnO, Si, Y2O3, Eu-doped Y2O3, ZrO2, Al2O3, Nb2O5 and ITO [17,20,21].
Other transparent conducting oxides (TCO) have also been prepared by OAD for integration into a large variety of optoelectronic devices (see Section 5.1).
In addition, a considerable number of organic materials have been deposited in the form of OAD optical coatings [71,103,394,400].
Such a wide range of OAD coatings has allowed them to cover the entire spectral range required for their use as interference filters.
A characteristic feature of the OAD technique is the potential to change the nanocolumnar direction by modifying the deposition geometry.
This can give rise to two- and three-dimensional structures constructed from multisections of oriented nanocolumns, zig-zags, helices or S/C-shapes (see Section 2.4).
According to Lakhtakia and Messier [20,490], there are two canonical classes of OAD structures.
The first type, sculptured nematic thin films, includes slanted columns, zig-zag, and S- and C-shaped columns.
These materials have been extensively used for the fabrication of optical components such as polarizers, retarders or filters [66,491,492].
The second class encompasses helicoidal columns and chiral-sculptured thin films, which are able to reproduce the behavior of cholesteric liquid crystals by preferentially reflecting circularly polarized light of the same handedness as the film’s microstructure (circular Bragg phenomenon).
Chiral filters based on this principle and presenting different degrees of complexity have been successfully designed and fabricated by OAD [77,79,493–495].
As explained in Section 2.2, changing the deposition geometry or using more than one deposition source (Section 2.6) permits a gradual in-depth change in composition, density and microstructure of films.
In terms of their optical properties, this means that either by alternating the type of deposited material and/or changing the densification, the refractive index profile along the film thickness can be effectively tuned.
This possibility has been used to fabricate antireflection coatings [236,496,497] and rugate filters that are characterized by a sinusoidal index profile [90,92,498,499].
One-dimensional photonic crystals (also designated as Bragg stacks) and Bragg microcavities have also been fabricated by successively stacking oxides of different refractive indices (e.g., low index SiO2 and high index TiO2) [285,500–502] or with porous-graded Si posteriorly oxidized by high temperature annealing [503].
An example of a Bragg microcavity fabricated by stacking different layers of SiO2 and TiO2 prepared by OAD is presented in Fig.
5.11(a).
Such optical structures depict a narrow resonant peak that is affected by the infiltration of liquids in their pores, a feature that has been utilized for the development of responsive systems to determine the concentration of solutions (see Section 5.3).
More complex 3D square-spiral photonic crystals with a tetragonal arrangement of elements that exhibits well-defined band gaps in the visible, NIR or IR spectrum have also been fabricated by OAD (see Fig.
5.11(b)) on lithographically pre-patterned substrates [504–506].
An emerging topic in the field of electronics and photonics is the development of flexible devices.
A flexible optical component combining the intrinsic nanometric order of OAD thin films with an additional micron-level arrangement has been recently developed by our group through the OAD of an oxide thin film on elastomeric PDMS foils [498].
Manually bending this device gives rise to a switchable grating formed by parallel crack micropatterns (Fig.
5.12(a)).
An outstanding feature of this type of foldable optical component is that the crack spacing is directly determined by the nanocolumnar structure and material composition of the OAD film (the former controlled by the zenithal angle of deposition), but it is independent on either the film thickness or foil bending curvature.
We have attributed this microstructuration effect to the bundling association of the film’s nanocolumns (see Sections 3.6.2 and 4.2.2).
These self-organized patterned foils are transparent in their planar state, yet can work as a customized reversible grating when they are bent forming the concave surface shown in Fig.
5.12(b).
The labeling possibilities of this type of optical component are illustrated in Fig.
5.12(c).
Within this category we consider OAD thin films and more complex device structures capable of actively responding to excitation from the medium.
Luminescent films, optical sensors and plasmonic effects will be commented briefly to illustrate the potential applications in this domain.
A standard approach to the manufacturing of luminescent films is the OAD of intrinsically luminescent materials.
Here, the nanocolumnar-tilted orientation of conventional OAD films ensures that the luminescent emission of the material is linearly polarized [73].
Similarly, helical structures of luminescent materials produce a circular polarized emission [71].
In all cases, the polarization of light seems related to a widely studied filtering effect produced by the particular nanostructure of each thin film or structure [73].
Another possibility in the synthesis of luminescent OAD-based materials relies on the anchoring of luminescent molecules on the internal surface of thin films; the basic principles of which were discussed in Section 3.6.2.
The incorporation into the nanocolumnar film of a luminescent or alternative guest molecule with specific functionality [246] presents some similarities with conventional synthesis routes utilized for the sol–gel method [508–510] and wet-chemistry processes [511] used in the fabrication of hybrid luminescent materials.
In these hybrid OAD films, however, functional molecules are anchored to the chemical functionalities of oxide surfaces (e.g., the –OH groups in TiO2 or SiO2 nanoporous films) either electrostatically [243,246,512] or by forming covalent bonds [244].
The luminescence of the resulting hybrid nanocomposite depends on the dye distribution (i.e., the aggregation state of dye molecules or simply the distance between them) within the porous nanocolumnar structure of the films.
Different activation processes such as thermal treatment [243] or the UV-illumination of semiconducting host films [242] have been used to enhance or modify the luminescence properties of the films.
An interesting example of the possibilities offered by this anchoring approach is the energy transfer process made possible from the visible to near-IR spectrum exhibited by rhodamine laser dye pairs adsorbed in nanocolumnar SiO2 OAD thin films [513].
Fig.
5.13 shows that when different rhodamine pairs (all of which include Rhodamine 800 as a near-IR emitting dye) are adsorbed in the host films, excitation of the visible-spectrum absorbing rhodamine produces a very intense luminescence in the infrared.
This luminescence cannot be induced by the direct excitation of Rhodamine 800, and has been attributed to an energy transfer induced by the formation of luminescent J-heteroaggregates between the two classes of dye molecules; a phenomenon observed for the first time in these hybrid OAD films [513].
Although this is still largely unexplored, the possibilities for this type of process for wavelength-selective wireless optical communications (e.g.
inside satellites) are quite promising, as most optical detectors function in the near-IR spectrum.
OAD photonic architectures are ideal for the development of photonic sensors.
In these systems, the large pores separating the nanocolumns in the films enable the rapid access and interaction of analyte molecules either directly with their internal surface or with active molecules anchored on it.
Generally, analyte adsorption is the main limiting step of the sensing response in an OAD structure, which is in stark contrast to the diffusion-limited sensing typical of bulk sensors [17].
Filing the pores of the OAD films in the final stage of massive infiltration or adsorption can be directly monitored by (for example) measuring gas adsorption isotherms (see Section 3.6.3).
The optical properties of OAD films and multilayer structures also respond to the environment, be that a gas or liquid that condense and/or fill the pores occupying the inner volume of the films.
The dependence of the film’s refractive index and the overall optical response of these systems on the conditions of the medium have been one of the main motivations for using nanocolumnar films or nanostructures as optical sensors.
Some examples of optical sensors based on changes in the optical properties of OAD structures are the use of helical nanocolumns [490] and Bragg stacks [285,502] for infiltrated liquid sensing, or the use of photonic crystals for high-speed water vapor detection [514,515] and colorimetric detection of humidity [516] (see Section 5.4 for a more complete review of humidity sensors).
One drawback of direct sensing strategies that are based on changes in refractive index is the lack of selectivity.
That is, selective detection can only be achieved when, by a specific reaction, the analyte modifies the light absorption properties (i.e.
the color) of the nanocolumnar film.
Examples of this approach are the optical detection of H2 with Pd/WO3 OAD films that change from transparent to blue when exposed to this gas [517], and the color change of cobalt oxide nanocolumnar films when exposed to CO [518].
Another possibility to improve selectivity involves modifying the surface chemistry of nanocolumns by derivatization with functional molecules.
For example, the silane derivatization of porous TiO2 OAD films makes them insensitive to changes in ambient humidity [519].
In hybrid thin films, one way of directly enhancing sensitivity is to incorporate within the OAD films a dye molecule whose light absorption and/or luminescence changes reversibly through a specific reaction with a given analyte.
Our group has widely investigated this procedure and developed different composite sensor systems based on this principle.
For instance, acid vapors can be detected from the change in optical absorption and fluorescence of OAD TiO2 films with tetracationic metal-free porphyrin incorporated in their pores [229,244].
Typically, the selectivity of these hybrid systems is mainly determined by the chemical reactivity of the anchored molecules.
Thus, by combining a free base porphyrin with ten of its metal derivatives anchored in columnar OAD films, it has proven possible to selectively detect more than ten volatile organic compounds through spectral imaging, as illustrated in Fig.
5.14 [247].
This confirms that the selectivity and performance of these hybrid systems are determined by how the sensing molecules are bonded to the porous OAD matrix [248].
Plasmonics is a rapidly evolving area that explores the interaction between an externally applied electromagnetic field and the free electrons in a metal.
These free electrons can be excited in the form of collective oscillations, named plasmons, by the electric component of light.
At certain incident conditions, namely when the incident light frequency matches the intrinsic frequency of the free electrons, a resonant absorption occurs.
This phenomenon is called surface plasmon resonance (SPR), or in the case of nanometric metallic structures, localized surface plasmon resonance (LSPR).
OAD has been utilized by many authors for the controlled fabrication of metallic structures intended for plasmonics applications; a topic that has recently been reviewed by He et al.
[520] and Abdulhalim [521].
The OAD technique presents some competitive advantages with respect to many other fabrication techniques for metallic supported nanostructures (e.g., self-assembly, electron beam lithography, nanosphere lithography, etc.
), the most significant of these being direct control over the composition and shape of the metallic nanocolumns and the scalability of the method to large areas.
Plasmonic structures made by OAD have been successfully employed for the fabrication of dichroic systems and labels, sensor and biosensor applications through the analysis of LSPR changes, molecular detection by surface enhanced Raman scattering (SERS), metal-enhanced fluorescence (MEF), surface-enhanced infrared absorption (SEIRA), and the development of metamaterials.
A brief discussion of the possibilities offered by OAD techniques in relation to selected applications is presented next.
In non-spherical silver or gold nanoparticles, plasmon absorption resonance is highly dependent on the polarization state of light.
In practice, this polarization dependence manifests itself by the appearance of two plasmon resonance absorptions peaking at different wavelengths depending on the orientation of the electric component vector of light with respect to the largest axis of the nanostructure.
Dichroic silver nanoparticles and aggregates can be formed during the earliest OAD stages on flat and transparent substrates [522], with the polarization dependence of their SPR being attributed to their in-plane anisotropy.
This effect can be greatly enhanced by nanosecond laser treatments under ambient conditions at relatively low powers, but both anisotropy and dichroic behavior are completely lost at higher laser powers.
Strongly dichroic structures have also been obtained by us through the OAD deposition of silver onto OAD nanocolumnar SiO2 films acting as a substrate [225,523].
In this case, the in-plane anisotropy characteristic of the SiO2 nanocolumns is used as a template to induce anisotropic growth of the silver film.
The treatment of these composites with an unpolarized nanosecond IR laser induces selective melting of the silver and posterior solidification in the form of long nanostripes on the surface, which promote and enhance the dichroic response of the system.
This dichroic laser-patterning process has been attributed to a successive metal percolation and dewetting mechanism along the lines defined at the surface by the SiO2 nanocolumnar bundles (see Section 4.2.2).
Depending on the laser power, zones with a localized increase in dichroism and/or totally deactivated regions can be written on the composite film surface.
The use of this writing procedure has been suggested for the optical encryption of information at a micron level [225].
Nano-columnar silver films directly prepared by OAD have been tested for SERS (see below) and for surface-enhanced Raman [524–526].
Recently, a host–guest strategy based on the formation of anisotropic Au nanodiscs inside the intercolumnar space of bundled OAD SiO2 nanocolumns has been reported by our group [226].
The method relies on that the size, shape and orientation of gold nanoparticles are defined by the tilt angle of the nanocolumns and the intercolumnar space distance, which are experimentally determined by the zenithal angle of evaporation.
These composite materials have been applied to the development of optical encryption strategies in combination with local laser patterning.
The distinct colors of the untreated zones seen in Fig.
5.15(a) are due to the different anisotropies of the gold nanoparticles depending on the characteristics of the SiO2 host layer.
The effect of this laser treatment is to locally remove the dichroic character of the composite film by inducing melting and resolidification of the anisotropic gold nanoparticles.
This treatment can be applied to selected zones with a micron-scale level of control (Fig.
5.15(b)).
In addition, this selective writing can be applied to complex dichroic thin film arrangements in which gold nanodiscs present different orientations as a result of being prepared on SiO2 film zones with nanocolumn bundles that have grown after azimuthally turning the substrate 180°.
In this system, retrieving the original information (in this example, the complete shape of the figure drawn by the laser) requires interrogation of the system using an appropriate polarization of light and a given planar orientation of the plate (Fig.
5.15(c)).
A deeper discussion regarding the possibilities of this type of system for optical encryption and the fabrication of anti-counterfeiting labels can be found in Ref.
[226].
The characteristics of the LSPR are not only dependent on the size and shape of metal nanoparticles, but also on their dielectric environment.
Typically, most OAD metal nanoparticle films are made of either Ag or Au.
Fu et al.
[527] studied the deposition of Au/TiO2 and Au/Ti structures by OAD, with the first case producing a red shift of 30nm in the plasmon resonance when the nanostructure was covered by a TiO2 layer with a nominal thickness of 5nm, a change that was attributed to a modification in the refractive index of the medium immediately surrounding the Au nanoparticles.
In the latter case, the observed red shift was a function of both the coating thickness and coverage.
These results indicate that the OAD technique is very versatile in allowing for the deposition and fine-tuning of LSPR structures, as well as the development of sensors [521,528].
Using uniform films with tuned, narrow particle size distributions, a linear relationship between the plasmon resonance wavelength and refractive index of the surrounding media was identified in OAD Ag and Au nanocolumns immersed in liquids with different refractive indexes [529].
Similarly, a number of biosensors for the highly selective detection of different small molecules have also been developed.
Selectivity in this case is mediated by anchoring different receptor molecules on the surface of Au or Ag, which is made possible thanks to the rich surface chemistry of these metals.
Examples or this approach are the detection of anti-rabies immunoglobulin G [529], neutravidin [530] and streptavidin [531].
Surface enhanced Raman spectroscopy (SERS) is very sensitive in the detection of minute amounts of molecules in solution upon their adsorption on noble metal substrates, even in the sub-micromolar range.
A variety of noble metal nanocolumnar films, most of which have been made of Ag and fabricated by OAD [532–535], have all demonstrated excellent properties as substrates for SERS chemical and biological sensing.
Indeed, the SERS enhancement factor and reproducibility of results obtained with OAD Ag nanocolumns was similar, or even superior to that reported for other Ag nanoparticle systems [536,537].
Investigations in this field have generally concluded that the efficiency of OAD structures as SERS transducers is tightly linked to their structural and microstructural characteristics, which in turn can be tailored by controlling the experimental conditions of deposition such as the angle and thickness of the nanocolumns [538,539] or the deposition temperature [540].
Another possibility offered by OAD to enhance SERS performance is the manufacturing of complex architectures that incorporate intentional “hot-spots”; i.e., areas where an enhanced electromagnetic field amplifies the Raman scattering signals.
OAD Ag films with L-shape [541], zig-zag [542] and square helical nanostructures [543] have all been reported to promote the formation of such “hot-spots” to increase the sensitivity of SERS analyses.
Metal enhanced fluorescence (MEF) refers to the enhancement in emission intensity of a fluorophore molecule in the proximity of a metal nanostructure due to a localized increase in the electromagnetic field associated with the SPR.
Dipole–dipole interactions also play a major role in this enhancement mechanism [544].
While MEF enhancement factors of up to 70 relative to dense metal films have been reported for OAD films made of Ag and Al, equivalent films made of Au and Cu have proven to be far less effective [545,546].
In these studies, the influence of nanocolumn tilt angle, film porosity, the nature of the substrate, and the distance between the fluorophores and metallic structures were all systematically investigated.
Various applications of this MEF effect using OAD metallic structures have been reported for biosensing in water and bioimaging [544], whereby a specific detection can be enhanced through the immobilization of the fluorescent receptor onto a metal nanostructure [545–547].
Mimicking nature to obtain superhydrophobic/superhydrophilic, adhesive/anti-adhesive and, more recently, omniphobic and wetting anisotropic surfaces and coatings has been a topic of ongoing interest in the field of nanotechnology over the last two decades; a period that has witnessed the convergence of efforts from academia and industry in the search for new surface functionalities [548–550].
The wetting angle of a liquid drop on a flat surface is determined by Young’s Law, and it is the result of a balance between the cohesive forces acting on the line of contact between the drop, the solid surface and the air/gas environment.
It is therefore dependent on the interfacial energies between the solid–liquid, solid–vapor and liquid–vapor phases.
Put simply, when the solid–vapor interface presents a low surface tension, the water contact angle (WCA) increases.
Surfaces with WCAs higher (smaller) than 90° are usually referred to as being hydrophobic (hydrophilic), while those with WCAs higher than 150° (smaller than 10°) are considered superhydrophobic (superhydrophilic).
The terms oleophobic and oleophilic designate surfaces with contact angles above and below 90°, respectively, with low surface tension liquids such as non-polar alkanes, oils and non-polar solvents.
One of the most long-awaited successes in this field has been the development of reliable, simple and low-cost techniques for the fabrication of superomniphobic surfaces; i.e., coatings capable of repelling both water and low surface tension liquids [551].
The main factors controlling the contact angle of liquid droplets on a solid are the chemistry and the roughness of the surface, the latter being intimately related to the microstructure of the material [552].
Two classic models named after Wenzel (applicable to rough surfaces) [553] and Cassie–Baxter [554] (applicable to heterostructured materials with porous nanostructures) relate the contact angle on actual surfaces, characterized by a specific roughness and microstructure, to the nominal angle on an ideally flat surface of the same material.
Oblique angle deposition techniques have been extensively used to control the wettability of materials, as they permit a fine control over both the surface chemistry and roughness.
The high versatility of this technique in obtaining different nanostructures (metals, organic compounds, oxides, hybrids and other complex heterostructures) is an additional feature that supports its use for wetting applications.
Important achievements have been made in the last few years through the OAD fabrication of surfaces possessing singular adhesive properties [555], hydrophobicity [556–559], superhydrophobicity [186–188,557,560–563], superhydrophilicity [366] or superolephobicity [564,565].
Initial approaches to the development of highly hydrophobic surfaces by OAD combined the surface nanostructuration capability of this technique with the chemical modification of the surface composition by different methods.
For example, the RF sputtering deposition at oblique and normal incidence of polytetrafluoroethylene (PTFE), a hydrophobic material commonly known as Teflon, has been reported to increase the water contact angle of OAD Pt [556] and W nanorods [558,559].
In this way, superhydrophobic WCAs as high as 138° and 160°, respectively, were achieved by controlling the deposition angle, substrate rotation and reactor pressure.
Veinot et al.
[560,566] were responsible for some of the earliest works showing the formation of superhydrophobic surfaces consisting of OAD SiO2 nanocolumns and 3D nanostructures modified by the chemical grafting of siloxane molecules.
More recently, a similar approach involving the molecular vapor deposition of silane onto metal OAD nanocolumns has been proposed as a way of fabricating anti-icing surfaces [558].
Moving a step forward, Choi et al.
[561,562] have produced superhydrophobic surfaces with a dual-scale roughness that mimics lotus petal effects (i.e., the ability to pin water droplets while maintaining a large contact angle) [567].
This was achieved through the fluorosilanization of Si nanowires arranged at a micron-level via OAD onto a pre-patterned substrate decorated with gold nanoparticles.
Other pre-patterned metal OAD nanorods have been combined with Teflon deposition to control the roughness, morphology and chemistry of the surface, thereby rendering it superhydrophobic [558].
Habazaki et al.
[564,565] have expanded this hierarchical roughness concept by using aluminum sputtered OAD nanorods as a starting material, followed by their anodization and surface decoration with fluorinated alkyl phosphate.
These surfaces showed an interesting omniphobic behavior characterized by a high repellency of water, oils and hexadecane.
Active surfaces, in which the contact angle can be changed by external stimuli, are an emerging topic of interest in the field of surface wetting.
To this end, our group has developed different plasma approaches for the deposition of nanofibers, nanorods and nanowires made of inorganic semiconductors (TiO2 and ZnO) [568], small-molecules [569] and hybrid and heterostructured nanostructures (metal@metal-oxide [186–188,563] and organic@inorganic core@shell nanowires [570]) that can be activated by light illumination.
Furthermore, by tailoring the density, morphology and chemical characteristics of these 1D nanostructures, we have been able to fabricate ultrahydrophobic surfaces; i.e., surfaces with an apparent WCA of 180° [242,569].
In addition, by working with Ag@metal-oxide nanorods and nanowires prepared by PECVD in an oblique configuration, we have taken advantage of the well-known photoactivity of TiO2 and ZnO to reversibly modify the WCA of their surfaces from superhydrophobic to superhydrophilic [188,242,563] by irradiating them with UV light, and in some cases, with visible light [186,187,563].
This illumination does not alter the nanostructure of the films, but rather only their surface properties, thus enabling fine control over the final WCA of the system.
To gain deeper insight into the surface state of these photoactive ZnO nanorods grown at oblique angles after water dripping (Fig.
5.16(a)), we have also carried out a thorough study of the evolution of WCA due to the formation of surface nanocarpets [563].
The nanocarpet effect refers to the association of aligned and supported nanorods or nanowires after their immersion in a liquid, and relates to their deformation by capillary forces.
This phenomenon has attracted considerable interest since the pioneering works of Nguyen et al.
[571], where they demonstrated the self-assembly transformation of supported CNTs into cellular foams upon immersion in a liquid and subsequent drying off.
Although it is still a subject of controversy, it seems that a critical factor controlling nanocarpet formation is the penetration of the liquid into the inter-rod space of the 1D nanostructured surfaces.
Although the literature on this subject has mainly focused on CNT arrays, its expected impact in biomedical research, superhydrophobicity and microfluidics has fostered the investigation of other systems such as OAD nanorods of Si [572–574], functionalized Si [575], SiO2 [576,577], carbon [485], metal [578] and ZnO [563].
Indeed, the nanocarpet effect has already served to increase the WCA on 1D surfaces [563,573] through the formation of a double or hierarchical roughness.
From a fundamental point of view, it has also been used to provide a fingerprint of liquid droplets deposited on vertically aligned, tilted, zigzag and squared spring conformations of hydrophilic or hydrophobic materials [573,579].
In Ref.
[563] we studied the evolution of the nanocarpet morphology in UV pre-illuminated OAD ZnO surfaces and their transformation from a superhydrophobic (Cassie–Baxter) state to a superhydrophilic (Wenzel) state, passing through a modified Cassie–Baxter state.
In addition, as summarized in Fig.
5.16, we have shown the possibility of controlling the nanocarpet microstructure by pre-illuminating the surface for given periods of time (Fig.
5.16(b) and (c)), or by using samples consisting of partially hydrophobic tilted nanorods, to induce asymmetric wetting after contact with water (Fig.
5.16(d) and (e)).
Anisotropic wetting and the development of droplets with asymmetric contact angles has emerged as an appealing area of research due to the industrial interest in materials capable of inducing a preferentially oriented spreading of liquid or with an asymmetrical adhesive surface [549,580–583].
Parylene (PPX) films deposited by oblique angle polymerization (OAP) are a particularly outstanding example of this singular wetting behavior [583–587].
Fig.
5.17 provides a summary of results from the fabrication of anisotropic films with unidirectional adhesive and wetting properties [580], which clearly demonstrates the possibilities of the OAP technique for the fabrication of PPX nanorods with a pin-release droplet ratchet mechanism derived from their singular microstructure.
These nanofilms exhibited a difference of 80μN in droplet retention force between the pin and release directions, the latter being characterized by a micro-scale smooth surface on which microliter droplets are preferentially transported.
These OAP nanostructures and their unique unidirectional properties have been recently used to control the adhesion and deployment of fibroblast cells [581,588].
Liquids moving not in the form of droplets, but rather as a continuous flow (microfluidics), have also benefited from the application of OAD thin films.
Although the number of papers published on this topic is quite limited, there are a few that clearly illustrate the potential of these films to improve the handling of liquids in small channels and devices.
For example, OAD has been used to fabricate nanostructures that were subsequently embedded in PDMS microchannels using a sacrificial resist process.
These microchannels were filled with different kinds of sculptured SiO2 OAD thin films, and the resulting three-dimensional structure was used as a DNA fractionator capable of a more effective micro-scale separation of these large molecules [589].
Microfluidic systems have also been provided with additional functionalities thanks to OAD films in order to develop various kinds of responsive systems.
Fu et al.
[590] recently reported the fabrication of a microfluidic-based MEF detection system in which tilted Ag nanorod films or SiO2/Ag multilayers were integrated into a capillary electrophoresis microdevice, which was then utilized for the separation and detection of amino acids.
This system demonstrated an enhanced detection by a factor of six at half times when used in a fluorescence device, thus opening the way for further improvements and functionalities.
The singular surface topography and microstructure of OAD thin films (see Section 3) provides unique possibilities for their utilization as biomaterial substrates, and for the development of biosensors.
In this case, the key investigated feature is the effect of surface topography on the proliferation of cells and/or the adsorption of proteins that mediate cell adherence and proliferation.
In a series of works on the OAD of platinum films, either directly onto flat substrates or onto polymer-packed nanospheres to provide a second nanostructuring pattern to the layers (see Section 2.5), Dolatshahi-Pirouz et al.
[110,116,117] showed that the particular surface topography of these latter layers promotes the growth and proliferation of osteoblast cells.
For different in-vivo applications it is also critical that certain cells grow preferentially over others, a possibility that has been explored in-vitro by these same authors using OAD Pt films.
In particular, they demonstrated that fibrinogen, an important blood protein, preferentially adheres to whisker-like nano-rough Pt substrates when compared to flat Pt surfaces, and that the proliferation of human fibroblasts is significantly reduced on these nanostructured surfaces [116].
Over the course of these studies, the growth of the film nanocolumns could be described by a power law relationship (see Sections 3.6 and 4.2) between the increments in their length and width [110].
The dependence found between the power law exponent and deposition angle was subsequently used to establish indicators that can be employed to predict cell growth on OAD thin films based on the characteristics of their surface topography determined by this deposition parameter.
Another critical issue affecting the practical implementation of biomaterials in different domains is their capacity to act as a biocide layer; i.e., to prevent the development of bacteria.
In this regard, α-Fe2O3 nanocolumnar OAD thin films have demonstrated to be quite effective in both limiting bacterial growth on their surface and contributing to the inactivation of Escherichia coli O157:H7 when subjected to visible light irradiation [591].
This light response has been proposed for the development of improved visible-light antimicrobial materials for food products and their processing environments.
A study into the viability of different bacteria on a nanocolumnar thin film of Ti prepared by MS-OAD has shown that unlike Staphylococcus aureus, the growth of E. coli is significantly reduced on the nanostructured film, and that this is accompanied by an irregular morphology and cell wall deformation [592].
In a very recent publication on nanostructured Ti surfaces prepared by MS-OAD, we have claimed that while the specific topography produced by the vertical nanorods of the layers are effective in stimulating the growth of osteoblasts on their surface, it simultaneously hinders the development of different types of bacteria.
This behavior makes these substrates ideal for implants and other applications in which osteointegration must be accompanied by efficient biocide activity [593].
The high surface area of OAD thin films when compared to a compact film has been a key argument for their use in biosensor applications (as well as for their use as conventional gas and liquid sensors as discussed in Section 5.3).
However, given that biosensing needs to be carried out at room temperature in a liquid media, yet still requiring a high sensitivity, alternative transduction methodologies are normally used.
An overview of instances in which these procedures have been used in conjunction with OAD thin films is the focus of this subsection.
One example is the use of electrochemical transduction with MS-OAD NiO thin films for the enzymatic detection of urea in biological fluids [594].
On the nanostructured surface of such films, the urease enzyme becomes easily grafted while the urease–NiO system promotes a high electro-catalytic activity.
This provides detection limits as low as 48.0μA/(mM) and a good linearity over a wide range of concentrations (0.83–16.65mM).
Monitoring H2O2 is of strategic importance in many applications, not just because it is a byproduct of a wide range of biological processes, but also because it is a mediator in food, pharmaceutical, clinical, industrial and environmental analysis.
Consequently, TiN OAD nanocolumns have been recently proposed as an electrochemical sensor for H2O2, which was accompanied by a thorough study about the relation between sensitivity and catalytic activity on the one hand and the deposition angle on the other [595].
A double detection method using UV–visible absorption spectroscopy coupled with cyclic voltammetry was employed by Schaming et al.
[596] with 3D nanostructured OAD ITO electrodes for the characterization of cytochrome C and neuroglobin, two proteins that act in-vivo to prevent apoptosis.
The photonic detection of analytes has also benefitted from OAD thin films, with Zhang et al.
[597] reporting that the sensitivity of a photonic crystal consisting of linearly etched strips on silicon can be enhanced by OAD of an 80nm layer of TiO2 on its surface.
This system demonstrated an up to four-fold enhancement in sensitivity toward polymer films, large proteins and different small molecules.
Another transduction concept based on the plasmon detection of analytes was developed by Zhang et al.
[598].
These authors proposed the use of gold nanoparticles with a controlled size and homogeneous distribution, which were prepared via OAD on a substrate previously covered with a close-packed layer of polystyrene spheres acting as a template (see Section 2.5).
This template layer promotes the development of well-defined gold nanoparticles with a high plasmon resonance activity, which turned out to be very sensitive in detecting biotin–streptavidin molecules.
A detection limit of 10nM was achieved by following both the position of the plasmon resonance band and its variation in intensity upon adsorption.
A quite different approach to evaluate the concentration of H2O2 has been proposed by Zhao et al.
[122,599], who developed an original protocol for the fabrication of catalytic nanomotors using dynamic OAD.
These nanomotors consist of asymmetrically Pt-decorated TiO2 nanoarms grown on silica microbeads, and have a sensing mechanism that relies on the fact that the asymmetric distribution of Pt in the nanoarms induces their rotation in the presence of H2O2 at a variable rate of 0.15Hz per each percent of this compound in the medium.
As with conventional gas sensors, multisensing is also a challenge in the field of biosensor devices.
In this context, Sutherland et al.
[600] have reported a new method that combines protein adsorption on the surface of a nanostructured quartz crystal microbalance (QCM-D) with optical (surface plasmon resonance SPR) and electrochemical detection (cyclic voltammetry CV).
The procedure allows for the quantification of both the amount and activity of bound proteins.
The previous sections have clearly shown that OAD procedures constitute a mature technology with a clear understanding of their physical basis and a wide range of potential applications in different domains.
Yet despite this, the incorporation of OAD thin films in real industrial applications is still limited, with different factors seemingly hindering the successful transfer of this technology from research laboratories to industrial production.
The following features have been identified as the major shortfalls when it comes to the successful up-scaling of OAD methodology [19]:1.Fabrication process is too complicated when used at an industrial scale.
Low productivity because of the angular dependence of the deposition rate.
Difficult to develop a general methodology usable for the large-scale production of different nanostructures and materials in a unique experimental set-up.
For the most sophisticated nanostructures (e.g., sculptured thin films or complex multilayers), there are limitations associated with the sophisticated movements needed for a large number of substrates and/or large substrate areas at an industrial level.
Increased cost of OAD thin films when compared with other methods of nanostructuration that do not require vacuum conditions.
Both electron beam evaporation and magnetron sputtering are widely used at an industrial scale for the deposition of thin films at normal geometry, with a large variety of products manufactured by these methods, either on large area surfaces or for the production of high-tech commodities.
Moving large substrates in front of large magnetron targets or using roll-to-roll methods are just some of the approaches that have made these techniques cost-effective and highly competitive at an industrial scale [601].
The limited industrial implementation of OAD procedures is therefore quite striking given that, as indicated by Suzuki [87], large dome-shaped substrate holders (approximately 2m in diameter) are currently used in batch-type coaters for e-beam deposition of optical films, and these would require only slight modification to successfully mount substrates obliquely for up-scaled production.
As pointed out by Zhao [19], a general concern when using an oblique geometric configuration is the inherent decrease in deposition rate; however, this limitation can be easily overcome by compensating with an increase in the evaporation power.
The large-scale industrial deployment of e-beam or MS methods in an OAD configuration would certainly be cost effective if innovative and reliable procedures to handle the substrates are implemented.
Some proposals in this regard were made in 1980s by Motohiro et al.
[602], who utilized the ion-beam sputtering technique with an ion gun in an OAD configuration and a roll-to-roll procedure for the preparation of CdTe thin films intended for photovoltaic applications.
Other authors have made alternative proposals to use OAD methodology for large scale production.
For example, Sugita et al.
[603] suggested the use of a continuously varying incidence (CVI) method, which consists of a rolling system in front of an electron beam evaporator with a shadow shutter to ensure that only species arriving at a given oblique angle reach the rolling surface.
This method was used for the fabrication of tape recording ribbons consisting of Co–Cr films [604].
As already reported in Section 2.4 (Table 2.1), other authors [89–91] have used a similar procedure for the fabrication of porous thin films for thermal isolation.
In this, and other similar rotating configurations, the incoming angle of species relative to the rotating surface is systematically changed so that even if the growth of the film is dominated by shadowing processes that would produce porosity, it does not maintain a fixed and well defined microstructure due to the continuously varying zenithal deposition angle.
Although the mass production based of OAD films is not extended to the large-scale production of final products, there are various niche applications that have already benefitted from this technology in recent years.
For example, metal wire grid polarizers are already mass produced by the OAD deposition of antireflection FeSi and SiO2 layers on previously deposited aluminum columnar arrays [87,114,115] (see Section 2.5).
Magnetic recording/video tapes are another example where OAD techniques have been successfully employed in large scale production [604–607].
Very recently, Krause et al.
[608] used a simplified roll-to-roll system to analyze the geometrical conditions and translate the deposition recipes of typical OAD films and architectures from flat moving substrate to a roll configuration.
The representative structures obtained prove that a successful translation from the laboratory to mass production is possible, even for thin films with a complex shape.
In summary, although the general principles for the successful implementation of OAD methodologies at an industrial scale are already available, and some successful attempts have been made to fabricate different serial products, the large scale engineering of the process and a reduction in cost is needed to make OAD thin films competitive with alternative technologies.
We believe that the time is ripe to achieve this goal and that the near future should see new developments and applications in the market that are based on the outstanding properties and performance of thin films prepared by deposition at oblique angles.
This review has been carried out as part of the research activity of the “Nanotechnology on Surfaces” laboratory at the Instituto de Ciencia de Materiales de Sevilla, which is sponsored by different projects of the Junta de Andalucía (TEP8067, TEP5283 and P10-FQM-6900) and the Ministry of Economy and Competitiveness of Spain (CONSOLIDER CSD2008-00023 MAT2013-42900-P, MAT2013-40852-R).
One of the authors (ARGE) also wishes to acknowledge the hospitality and support of Prof. Zschech and their colleagues at the “Dresden Center for Nanoanalysis” of the Technische Universität Dresden (Germany) during a sabbatical that permitted the realization of a considerable part of this work.
An alternative to the use of a zero resistance ammeter for electrochemical noise measurement: Theoretical analysis, experimental validation and evaluation of electrode asymmetry The measurement of current and potential noise during corrosion generally requires two electrodes, coupled by a zero resistance ammeter (ZRA), and a reference electrode.
Statistical methods then enable estimation of the noise resistance, if the electrodes are identical.
In this work, a resistor is used instead of a ZRA and, consequently, the electrodes potentials are partially decoupled.
This enables evaluation of electrode asymmetry and the reliability of the estimated noise resistance.
Additionally, the simplification of the current-measuring circuit ensures that no instrumental noise due to active electronic components within the ZRA is fed back to the corroding electrodes.
When a metal electrode is immersed in a corrosive electrolyte, anodic and cathodic reactions occur simultaneously on its surface.
Generally, the anodic reaction involves the oxidation of the metal to metal ions, which are released into the environment, while the cathodic reaction involves the reduction of species present in the environment, such as molecular oxygen or hydrogen ions [1].
The rate at which the anodic and cathodic reactions proceed can fluctuate with time but, overall, the rate of the anodic and cathodic reactions are balanced, at open-circuit potential, in order to preserve electroneutrality [2].
Generally, the rate of the anodic reaction increases with increasing electrode potential, while the rate of the cathodic reaction increases with decreasing electrode potential.
Consequently, the average corrosion potential represents the potential at which the average rates of anodic and cathodic reaction are balanced.
However, if for example, a rapid increase of the anodic reaction rate proceeds, some of the charge generated by metal oxidation can be transiently stored in the capacitance that is generated due to charge separation at the metal-solution interface, known as the double-layer capacitance, before being consumed by the cathodic reaction on the electrode surface [3,4].
As a result, a fluctuation in the corrosion potential is observed; initially, the potential decreases rapidly due to the charging of the double layer capacitance and, subsequently, it recovers as a result of the progressive consumption of charge by the cathodic reaction [5].
The described process is known as an anodic event, and a similar description can be given for cathodic events.
Depending on the material-environment combination, anodic or cathodic events can be relatively large and occasional, such as for a passive material suffering metastable pitting, or relatively small and frequent, such as for materials undergoing active corrosion [6,7].
Consequently, larger potential and current transients are observed in the first case compared with the second case.
Thus, the corrosion type can be determined analyzing the potential and current noise by statistical methods [8–10], spectral analysis [6,8,10–12], by wavelet [13–16] and Hilbert [17,18] transforms or others methods [19,20].
For two galvanically coupled electrodes comprising equal areas of the same material, when one event occurs on the surface of one electrode some charge associated with that event is consumed on the same electrode and some charge is consumed on the other electrode [3,21].
In this case, one-half of the charge generated on one electrode is consumed on the same electrode, and one-half of the charge is consumed on the other electrode [21].
If the galvanic coupling between the two electrodes is realized with a zero-resistance ammeter, the coupling current can be measured simultaneously with the potential of the electrodes.
Thus, the process of electrochemical noise generation intrinsically originates from electrode asymmetry; at a particular time, for example, the anodic activity on one electrode transiently exceeds the anodic activity of the other electrode, and a current flowing through the external circuit can be measured [21].
However, if the electrochemical noise signal is acquired for a sufficiently long time, on average, the two electrodes might be considered to behave as identical (or closely similar), in the sense that, on average, the duration and number of anodic and cathodic transients generated by one electrode is identical (or closely similar) to the duration and number of anodic and cathodic transients generated by the other electrode.
From the previous, it follows that two electrodes can behave as identical on a long timescale, but they can be significantly asymmetrical on a shorter timescale.
Except for some cases such as coated electrodes or stainless steel electrodes, it has been shown that, for two identical electrodes, and under the assumptions that (i) two identical electrodes have identical resistances (impedances), (ii) a noiseless reference electrode is used to measure the potential and (iii) the electrolyte is sufficiently conductive to neglect the electrolyte resistance, the noise resistance (impedance), defined as the square root of the variance (power spectral density) of the electrodes potential divided by the variance (power spectral density) of the coupling current, provides a close estimation of the polarization resistance (electrode impedance) [3,11,22–25].
Based on the previous consideration, the values of noise resistance [3,22,24,26] or noise impedance [27–30], or their time evolution [31–34], have been used extensively to evaluate quantitatively the behavior of corroding electrodes.
Under the conditions where the value of the noise resistance, or the low-frequency limit of the noise impedance spectrum, can be considered representative of the value of the polarization resistance, their values can be used to estimate the corrosion rate from the Stern–Geary equation [35], provided that Tafel coefficients have been previously determined for the specific material-environment combination.
This results from the unique advantage of electrochemical noise with respect to other electrochemical techniques, where virtually no perturbation to the corrosion process is introduced by the measurement.
Direct (DC) or alternating (AC) current electrochemical techniques rely on imposing an electrical perturbation to the corroding electrode in order to produce a response from which information on the corrosion process is obtained.
Thus, inevitably, the Faradic current associated with the applied signal affects the rate of anodic and/or cathodic reactions on the electrode surface and modifies, to a different extent depending on the technique selected, the equilibria that would be attained on a freely-corroding electrode.
On the contrary, electrochemical noise only measures the fluctuations in current and potential that are naturally generated under free-corrosion condition and it is ideal for long term and real-time corrosion monitoring.
The validity of the estimation of the noise resistance, however, relies on two a priori assumptions, namely (i) that nominally identical electrodes have identical impedances [29,36,37] and (ii) that the noise produced by the external circuit used to measure the current is significantly less than the noise generated by the corroding electrodes [38,39].
To some extent, the first assumption can be verified by considering various statistical parameters related to the potential or current noise such as average values, skewness, and correlation coefficient [40,41].
The previous parameters might provide an indication of the symmetry between the two electrodes, but their physical meaning is not straightforward and a unique criterion to evaluate the reliability of the estimation of the noise resistance by evaluating the asymmetry related parameter remains the subject of debate.
The second assumption is, in principle, easier to verify by using an appropriate dummy cell for the evaluation of the instrument noise [39].
However, the noise generated by the circuitry within the zero resistance ammeter depends also on the electrode impedance [38,39] and, therefore, different dummy cells with different impedances must be used to completely characterize the instrumental noise.
Further, due to the intrinsic asymmetry between the current-measuring circuit and the voltage-measuring circuit, particular care must be taken during instrumental design in order to minimize errors during the measurement.
This is due to the fact that, for a specimen on the centimeter scale, the relatively small current fluctuations (nanoamps to tens of microamps) occur across the zero mean value, while the relatively small potential fluctuations (tenths of millivolts or tens of millivolts) are overlapped to a significant DC bias (hundreds of millivolts).
In addition, from the point of view of the corrosion process, it should be noted that the instrumental (current) noise generated by the zero-resistance ammeter during the current measurement is fed back to the corroding electrodes and there is the possibility that this could affect the behavior of the system [42].
Finally, but not less importantly, a non-ideal or non-perfectly calibrated ZRA might also introduce a small bias between the two electrodes, thereby affecting the corrosion process significantly.
In this work, an alternative approach to the measurement of electrochemical noise is introduced.
Specifically, the two corroding electrodes are connected by a measuring resistor instead of a zero resistance ammeter.
The corrosion events on each electrode produce current fluctuations, which result in a potential drop across the measuring resistor.
Thus, the potentials of the two electrodes are not identical.
The two individual potentials can then be physically measured by using two voltmeters, individually connected to a reference electrode (assumed noiseless) and to one of the two corroding electrodes.
By using this configuration, the perturbation produced by the measurement on the corroding surface is due to the current flowing across the voltmeter from the reference electrode to the working electrode, and it is generally negligible, due to the high impedance of the voltmeter.
Further, and unlike for the case of the measurement by ZRA where the current flows across the active electronics of the instrument, if the voltmeter is not perfectly calibrated the value of the reading might not reflect exactly the electrode potential, but this does not result in a perturbation to the corroding surface.
In summary, the measurement configuration proposed here has several advantages over the usual configuration, including (i) the possibility that noise generated by active electronics is fed back to the corroding system is excluded, (ii) the current can be calculated from the difference between the measured electrode potentials (similar), avoiding asymmetry between the current-measuring circuit and the voltage measuring circuit and (iii) due to the non-identical potentials of the two electrodes, differences in individual electrode behavior can be readily evaluated.
Considering that a distinct advantage of the electrochemical noise technique is that no perturbation is introduced to the corrosion process, the physical effect of connecting two corroding electrodes by a measuring resistor must be examined initially (Fig.
1).
If a corroding electrode and a reference electrode are immersed in an electrolyte, the corrosion potential can be measured without introducing significant perturbation to the corrosion process provided that the impedance of the voltmeter is high compared with the impedance of the corroding electrode.
If this assumption is verified, the current flowing between the reference electrode and the corroding electrode is negligible, and the corrosion process is unperturbed.
Thus, all the charge associated with, for example, an anodic event on the electrode is consumed by the cathodic reaction on that electrode.
If a second identical electrode and a second reference electrode are immersed in the same electrolyte, each of the two electrodes corrodes independently; all the charge generated by one event on one electrode is consumed on the same electrode, and the corrosion potentials of the two electrodes are uncorrelated.
Thus, the introduction of a second disconnected electrodes does not introduce a perturbation to the fundamental corrosion process on the first electrode and viceversa (Fig.
1a).
Consequently, the two electrodes can be considered to be coupled by an infinite resistance (Rm=∞).
On the other hand (Fig.
1b), if the two electrodes are coupled by an ideal connector having zero resistance, they will immediately attain identical potentials (perfectly correlated) and behave as a single electrode of double the individual electrode area.
If a corrosion event occurs on one electrode, one half of the charge associated with that event will be consumed on the same electrode [21] and one-half of the charge will be consumed on the other electrode.
Thus, considering that the electrode area per se does not affect the fundamental corrosion process, the operation of galvanic coupling (Rm=0) can be considered as non-perturbing.
If a measuring resistor is used to perform the coupling between the two electrodes, (0<Rm<∞), an intermediate situation is observed (Fig.
1c).
Specifically, the potentials of the two electrodes will only be partially correlated because a potential drop across the measuring resistance, dependent on the current value, is generated.
It should be noted that the potential drop across the measuring resistor is intrinsically different from the case of a potential difference between electrodes deliberately introduced by an active external circuit and does not introduce electrode asymmetry.
At any particular time, the potential difference between two disconnected identical electrodes freely corroding in the same electrolyte is due to statistical variations in the active anodic and cathodic areas on each electrode that result in different (but similar) corrosion potentials.
If a very high resistor is used to couple the two electrodes, the potential difference between the two electrodes will be slightly reduced compared to the case of disconnected electrodes and some current will start to fluctuate between the electrodes.
Given the high value of the resistance, only little current will flow between the two electrodes and the effects of the coupling on the corrosion of each individual electrode will be minimal.
If the value of the resistance is reduced, also the potential difference between the two electrodes will be reduced and the amount of current exchanged will be increased.
As a limiting case, the two electrodes can be short-circuited by connection with a cable or a zero resistance ammeter, and will assume identical potentials and exchange maximum current.
None of the above introduces either electrode asymmetry, preferential flow of current from one electrode to the other or a new external driving force for corrosion on any of the electrodes.
Depending on the value of the resistor, the potential drop between the two electrodes can be more or less substantial and affect where the charge generated by a corrosion event on one electrode is consumed.
For high values of resistance, most of the charge generated by an event on one electrode will be consumed on that electrode whereas, for low values of resistance, the charge provided by the other electrode will approach one-half of the charge associated with the event.
As a result, the effect of coupling one electrode to another electrode of identical material and identical area through a resistor is equivalent to coupling one electrode to another electrode of reduced area through an ideal conductor.
Overall, the fundamental corrosion process is not affected by such a procedure.
The electrochemical setup and the equivalent circuit typically used for the estimation of the noise resistance using a ZRA is presented in Fig.
2a.
Here, each electrode is represented by a current noise source, i1,2, in parallel with the electrode resistance, R1,2.
The current measured by the ZRA is I and the potential with respect to a reference electrode (assumed noiseless) is V. The following analysis is developed for electrode resistances and, in the present form, it is strictly valid for systems where the noise impedance spectrum is flat and, consequently, the noise resistance approximate well the polarization resistance.
The choice of considering the values of noise resistance rather than the noise impedance spectra has the practical advantage of simplifying the method to obtain time-resolved information.
Considering a long dataset, it is possible to evaluate the time evolution of the value of noise resistance by iteratively extracting potential and current segments, computing their variances and calculating the value of the noise resistance from Eqs.
(12)–(16).
If the noise resistance is unsuitable to describe a particular system, the analysis presented here can be readily extended to obtain a time-series of impedance spectra by replacing resistances with impedances and variances with power spectral densities in Eqs.
(1)–(16) (the resulting equations are practically equivalent to those presented in Ref.
[3]).
However, given the dependency of noise impedance on frequency, in order to apply Eqs.
(17)–(22), it would be necessary to consider the value of the noise impedance at each particular frequency or, alternatively, its average over a particular frequency range of interest.
In most practical cases, the low-frequency limit of the impedance spectrum, or the average of the values of the impedance modulus over a pre-defined low-frequency range would be more appropriate.
Fig.
2b presents the electrochemical setup and the equivalent circuit for the measurement of electrochemical noise by an external measuring resistor, Rm.
Here, V1 and V2 are the corrosion potentials of electrodes 1 and 2 respectively, measured with respect to a noiseless reference electrode.
It is evident that the current can be calculated by either physically measuring the potential drop across Rm or by physically measuring the potentials V1 and V2 and taking their difference as the value of the potential drop across Rm.
The second method has the advantage that any asymmetry between the circuit that measures potential and the circuit that measures current is avoided.
For the traditional circuit, under the assumption of uncorrelated noise sources, the analysis is well known [3] and only the expression of the noise resistance is reported here for the general case of two electrodes (non-identical)(1)Rn2=σ2(i1)[R1R2]2+σ2(i2)[R1R2]2σ2(i1)R12+σ2(i2)R22If the electrodes are assumed identical (R1=R2), Eq.
(1) reduces to(2)Rn2=2[σ2(i1)+σ2(i2)][R1]42[σ2(i1)+σ2(i2)][R1]2=R12=R22 Thus, in the usual case, if the two electrodes can be assumed to have identical resistances, the noise resistance provides an estimation of the electrode resistance.
If a measuring resistor is added to the external circuit, one value of current and two electrode potentials are measured.
The circuit of Fig.
2b, can be analyzed by the superimposition theorem, as graphically illustrated in Fig.
3.
Thus, for the circuit of Fig.
3b, the current flowing across the measuring resistor is calculated by the usual equation for a current divider:(3)I=-i1R1R1+R2+Rm Additionally, the potential V1 is given by the current source, i1, acting on the parallel between the resistor R1 and R2+Rm(4)V1=i1R1R2+R1RmR1+R2+Rm Finally, the potential V2 is given by the current I flowing across R2(5)V2=i1R1R2R1+R2+Rm An identical procedure can be followed to solve the circuit of Fig.
3c.
Thus, the overall current flowing across Rm is(6)I=-i1R1+i2R2R1+R2+Rm The expressions for the potential of the electrodes are respectively:(7)V1=i1R1R2+R1RmR1+R2+Rm+i2R1R2R1+R2+Rm(8)V2=i2R1R2+R2RmR1+R2+Rm+i1R1R2R1+R2+Rm From the previous, under the assumption of uncorrelated noise sources, the expressions of the variances can be calculated as follows:(9)σ2(I)=σ2(i1)R1R1+R2+Rm2+σ2(i2)R2R1+R2+Rm2(10)σ2(V1)=σ2(i1)R1R2+R1RmR1+R2+Rm2+σ2(i2)R1R2R1+R2+Rm2(11)σ2(V2)=σ2(i2)R1R2+R2RmR1+R2+Rm2+σ2(i1)R1R2R1+R2+Rm2 Further, by dividing each potential variance by the current variance, two apparent noise resistances are obtained(12)Rn,12=σ2(i1)[R1R2+R1Rm]2+σ2(i2)[R1R2]2σ2(i1)R12+σ2(i2)R22(13)Rn,22=σ2(i2)[R1R2+R2Rm]2+σ2(i1)[R1R2]2σ2(i1)R12+σ2(i2)R22 Under the (usual) assumption that nominally identical electrodes have identical resistances (R1=R2), it is useful to calculate the expression of the arithmetic average of the square of the two apparent noise resistances:(14)Rn,Ave2=Rn,12+Rn,222=σ2(i1)[R12+R1Rm]2+σ2(i2)[R12]2+σ2(i2)[R12+R1Rm]2+σ2(i1)[R12]22{σ2(i1)R12+σ2(i2)R12}=R12+R1Rm+12Rm2 Thus, the resistance of the identical electrodes can be estimated by solving the following equation(15)R12+R1Rm+12Rm2-Rn,Ave2=0where R1 is now treated as the unknown, Rn,Ave2 is a known parameter that can be measured and physically depends both on R1 and Rm, and Rm is the known value of the measuring resistor used.
The physically acceptable solution is given by(16)R1=-Rm+4Rn,Ave2-Rm22 It should be noted that, although Rm appears explicitly in Eq.
(16), the actual value of the estimated R1 does not depend on the Rm used because, for a given R1, the value of Rn,Ave2 is determined by the value of Rm.
This can be easily verified by inserting any numerical values in Eqs.
(12)–(16), provided that identical values of R1 and R2 are used.
In summary, it can be concluded that the addition of an external resistor does not introduce a perturbation to the fundamental corroding system.
By using this approach, two values of apparent noise resistance, instead of one, can be extracted and, under the assumption that nominally identical electrodes have the same resistance, the noise resistance can be estimated.
The difference between the two apparent noise resistances provides an indication of the electrode asymmetry and a qualitative validation of the assumption of identical behavior.
Further, by recording the current using an external resistor rather than a zero-resistance ammeter, the possibility that instrumental noise and DC biasing is fed back to the corroding system is excluded.
As previously discussed, the electrochemical noise signal is generated by an asymmetry of the electrode behavior on a short timescale (single events) but, on a longer timescale, two electrodes can be considered to be practically identically noisy if, on average, the intensity, number and frequency of anodic and cathodic events on the two electrodes are closely similar.From the viewpoint of the equivalent circuits (Fig.
2a and b), the two electrodes can be considered identical if, over a defined timeframe, R1 is close to R2 and the variance of i1 is close to the variance of i2.
It is worth noting that under the usual assumption that R1=R2, it follows from Eq.
(14) that even a strong asymmetry in the values of i1 and i2 does not affect the value of the estimated noise resistances.
On the other hand, if R1≠R2, then an asymmetry in the magnitude of the noise sources might have significant impact on the value of the estimated noise resistance.In order to partially quantify electrode asymmetry from the equivalent circuit viewpoint, it is useful to evaluate the difference between Rn,1 and Rn,2 in the following two cases; (i) when the magnitude of the noise source on one electrode largely exceeds the magnitude of the noise source on the other electrode, but the electrode resistances have comparable values and (ii) when the resistance of one electrode largely exceeds the resistance of the other electrode, but the noise sources have comparable values.In the first case, it is assumed that σ2(i1)≫σ2(i2), and R1≈R2 ; therefore(17)Rn,1-Rn,2≈σ2(i1)[R1R2+R1Rm]2σ2(i1)R12-σ2(i1)[R1R2]2σ2(i1)R12≈Rm In the second case, R1≫R2, σ2(i1)–σ2(i2) and Rm is comparable to R1 and(18)Rn,1-Rn,2≈σ2(i1)[R1Rm]2σ2(i1)R12≈Rm A similar argument can be applied in the opposite cases (σ2(i2)≫σ2(i1), or R2≫R1), with the result being −Rm in both cases.
Based on the previous, a parameter (S) related to the electrode symmetry can be defined as the difference between the apparent noise resistances divided by the measuring resistor:(19)S=Rn,1-Rn,2RmS provides information on electrode asymmetry and, in particular, if σ2(i1)≫σ2(i2) and simultaneously R1≈R2, or R1≫R2 and simultaneously σ2(i1)≈σ2(i2)(20)S=Rn,1-Rn,2Rm→1If σ2(i1)≪σ2(i2) or R1≪R2(21)S=Rn,1-Rn,2Rm→-1While if σ2(i1)=σ2(i2) and R1=R2(22)S=Rn,1-Rn,2Rm=0Having clarified the qualitative behavior of S in the above limiting cases, it is useful to consider the values of S in intermediate cases, i.e.
when the resistance of the electrodes and the magnitude of the noise sources vary independently and the assumptions behind Eqs.
(20) and (21) do not hold.
Fig.
4 presents the calculated values of S as a function of the logarithm of the ratio between the electrode resistances and of the logarithm of the ratio of the standard deviation of the noise sources.
The Figure has been calculated for a value of Rm of 10,000Ω, and constant values of R1 (10,000Ω) and σ2(i1) (10−12A2) by varying R2 and σ2(i2) by one order of magnitude above and below the values of R1 and σ2(i1).
In video 1, available as additional material, the effect of the variation of the relative magnitude of R1 and Rm is disclosed.
As it is evident, the variation of the magnitude of the noise source has no effect on the values of S if R1=Rm, while a variation in the relative values of R1 and Rm has only little effect on the shapes of the curves.
With reference to Fig.
4, in quadrants 1 and 2, the resistance of electrode one significantly exceeds the resistance of electrode two; in quadrants 1 and 4, the magnitude of the noise source of electrode one exceeds significantly the magnitude of the noise source of electrode two.
In most practical cases, it is most likely that the electrode that corrodes more has simultaneously a lower resistance and produces more noise, thus quadrants 2 and 4 are probably representative of real corroding systems.
If the two electrodes are perfectly symmetrical (they have the same resistance and the same magnitude of the noise sources), the corroding system can be represented by point A (S=0).
If a value of S different from 0 is measured, then an asymmetry is inevitably present in the corroding system for the timescale inspected.
For example, if a value of S=0.6 is measured, the corroding system can be represented by point B (electrode 2 more noisy and with lower resistance compared with electrode 1, thus corroding preferentially) or point C (electrode 1 more noisy and with lower resistance compared with electrode 2, thus corroding preferentially) or any other point in the B–C line in the second and fourth quadrant in Fig.
4.
From the value of S, it is not possible to obtain direct information on which electrode is corroding preferentially, but it provides an indication of electrode asymmetry.
Finally, it should be noted that, while a value of the parameter S different from zero undoubtedly indicates some electrode asymmetry, a zero value does not guarantee that the electrodes behave perfectly symmetrically as considered later.
In order to validate the presented method, the circuit of Fig.
5 has been used.
Here, the current is measured continuously by a zero-resistance ammeter, simultaneously with the potential of the two electrodes.
A measuring resistor of 4.7kΩ is used to couple the specimens and a switch is added in parallel to the measuring resistor.
By using this configuration, the noise resistance of the two corroding electrodes can be evaluated alternatively with the traditional method (when the switch is closed) or by the proposed method (when the switch is open).
The switch position was changed approximately every four hours.
For the data acquisition, a Concerto (Capcis-Intertek) multichannel potentiostat was used.
The Concerto multichannel potentiostat physically acquires the potential or current signal at a nominal frequency of 5Hz and records the data at 1±0.05Hz, by averaging 5 readings.
Unreported numerical simulations show that, for corroding systems that produce 1/F2 type noise, this sampling method does not introduce significant errors due to aliasing above the Nyquist frequency of the recorded data (1Hz).
The corroding electrodes were made from AA2024T3 aluminum alloy of exposed area of 4.5cm2.
The test electrolyte was naturally aerated 3.5% NaCl.
The data were analyzed by using the software ECN Analysis 1.0, implementing a procedure briefly summarized as follows and described in detail in and [31,32]: (i) a segment of 2048 points from the potential and current noise record is extracted (t=0), (ii) the variances of potential and current are calculated for the extracted segment, (iii) the apparent noise resistance associated with that segment is calculated by taking the square root of the potential variance divided by the current variance, (iv) the calculated value of apparent noise resistance is assigned to the time corresponding to the first point of the extracted segment, (v) a new segment, displaced of 150 points (1898 points of overlap between consequent segments), is extracted and operation ii–iv are repeated and (vi) operations ii–v are repeated until the complete dataset is analyzed.
Subsequently, an in-house developed software was applied to the calculated apparent noise resistance to implement the method described in this work.
Fig.
6 presents the time evolution of the potential across the measuring resistor (Fig.
6a), and the electrode potentials and the coupling current (Fig.
6b and c, respectively).
Evidently, when the switch in parallel to the measuring resistor was closed, the potential difference across the resistor was approximately 0; conversely, when the switch was open, the potential across the resistor was proportional to the coupling current.
The electrode potentials were similar for the duration of the experiment and, generally, did not display particularly important spikes associated with the opening or closing of the switch, except for the first time when the switch was opened.
Similar considerations apply to the current that was generally fluctuating around 0 for the duration of the experiment.
Also, for the current, a significant drop was observed only the first time that the switch was closed.
In Fig.
7, the time evolution of the apparent noise resistances is presented.
When the switch was closed (Rm=0), the two values of apparent noise resistances Rn1,2 were identical, as predicted by Eqs.
(12) and (13) for Rm=0.
Conversely, when the switch was opened (Rm=4.7kΩ), the values of Rn1,2 were generally higher and the two apparent noise resistances were similar, but did not coincide perfectly.
In order to estimate the electrode resistances by Eq.
(16), the average value between the two apparent noise resistances was taken, with the results of this procedure illustrated in Fig.
8.
Here, the noise resistance calculated with the traditional method (switch closed, Rm=0) is plotted in black, while the estimated noise resistance calculated with the proposed method (switch open, Rm=4.7kΩ) is plotted in gray.
Satisfactory agreement between the two methods is evident, although larger fluctuations in the values of noise resistance are observed when the measuring resistor is used.
Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance.
In order to validate this conclusion, the experiment of Fig.
9 was performed.
Specifically, a pair of nominally identical specimens was initially coupled by a 4.7kΩ resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter.
The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration.
Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing.
The final dataset comprised potential values spaced 1±0.05s in time.
Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.
After 15h, the coupling resistor was replaced with a 2kΩ resistor and a similar operation was performed after 20h (10kΩ) and 24h (100kΩ).
The effect of using a higher value of coupling resistance was an increase of the potential difference between the two electrodes.
Such increase in potential difference enables a more accurate current estimation, because the signal-to-noise ratio in the analog to digital converter is improved.
However, the quality of the current reading was satisfactory for all the resistances tested.
Concerning the value of the estimated noise resistance, it did not appear to be dependent on the measuring resistor, as predicted by the theory; however, the fluctuations in the estimated value were higher when the measuring resistor was used instead of the ZRA to measure the current.
Further, when the measuring resistor method was used, fluctuations in the value of the estimated noise resistance where larger for the 100kΩ resistor and smaller for lower values of measuring resistors.
The reason for the fluctuations are not analyzed in detail here, but they are likely to be related to errors connected to electrode asymmetry, becoming more significant if the coupling resistor exceeds largely the value of the electrode resistances.
This can be qualitatively understood by considering Eqs.
(14)–(16); when the value of the measuring resistor is 0Ω (measurement by ZRA), Eq.
(16) reduces to the usual case, and a positive value of noise resistance is always obtained, regardless of electrode asymmetry.
Conversely, when the value of the measuring resistor is high and the resistance of one electrode is substantially different from the resistance of the other, the simplification introduced in Eq.
(14) is not correct and the estimated noise resistance can be a negative number.
In intermediate cases, when the measuring resistor is comparable with the resistance of the electrodes and the electrode asymmetry is present but not particularly severe, fluctuations in the value of the estimated noise resistance are observed.
In practice, if the value of the measuring resistor is similar to the value of the resistance of the electrodes, the current exchanged between the two corroding electrodes will be approximately one-half of the current that would be exchanged if the electrodes were connected by a zero resistance ammeter.
This is evident from Fig.
3, where the superimposition theorem is graphically illustrated.
When only one current source is considered (for example i1), the measuring resistor and the resistance of electrode 2 are in series.
If the measuring resistor and the resistance of electrode 2 have similar values, the current is approximately one-half of the current that would flow in the absence of a measuring resistor.
A similar argument applies when only i2 is considered.
Thus, the selection of a measuring resistor of value similar to the value of electrode resistance appears to be an appropriate choice.
In order to verify the effectiveness of the parameter S in evaluating electrode asymmetry, an experiment introducing deliberately asymmetrical electrodes was performed.
An electrochemical cell with two cylindrical compartments connected by a porous glass frit (split cell) was used to enable the modification of the environment in one compartment without affecting the environment in the other compartment.
As a result, the corrosion of one of the two electrodes can be significantly increased by, for example, alkalinization or acidification of the solution in its compartment.
The two compartments were filled with 3.5% NaCl solution and one AA2024T3 electrode was immersed in each compartment.
The two electrodes were coupled by a 10kΩ resistor, and the potential of each electrode with respect to a single saturated calomel electrode was acquired with a NI-USB 6009 analog-to-digital converter.
The electrochemical noise signal was acquired with the procedure described above.
Simultaneously with the electrochemical noise signal, images of the corroding surface were acquired at 150s interval.
After 4h of immersion in 3.5% NaCl, 5ml of 1M NaOH were introduced to one compartment of the cell, promoting increased corrosion in that compartment (electrode 2 in Fig.
10 and images a and c in Fig.
11).
The video 2 in the additional material displays the time evolution of potential, current, noise resistance, parameter S and surface appearance of the electrodes during the split cell experiment.
Due to the difference in environments, the hypothesis of identical resistances necessary to obtain Eq.
(14) is substantially violated and, as a result, the noise resistance calculated with the method presented here assumed negative values after 5h.
Correspondingly, after 5h, the parameter S approached 1.
Fig.
10 presents the potential and current time records recorded during the experiment.
Initially, the potentials were close to −0.6V for both electrodes, and current fluctuations across the 0 mean value were observed.
After the addition of the sodium hydroxide in one compartment of the cell, anodic dissolution was significantly accelerated on electrode 2 and vigorous hydrogen evolution was initiated (Fig 11a and video 2).
As a result, its potential decreased rapidly to −1.2V and, subsequently, partially recovered to −1.1V.
Due to the connection through the 10kΩ resistor, the potential of electrode 1 also experienced a drop to about −1V.
Accordingly, an average current of about 20 microamps was recorded.
Overlapped with the DC component, fluctuations were revealed.
The plot of the apparent noise resistances showed very similar values initially; after addition, for a brief period, Rn,2 exceeded Rn,1 and subsequently decreased to about 10kΩ.
After the addition, Rn,1 was stable and close to 10kΩ, a value similar to the Rm (10kΩ).
Subsequently, uniform attack was observed on the specimen immersed in the presence of sodium hydroxide, the surface of which turned black as a consequence of the accumulation of copper.
On both specimens, hydrogen evolution was observed, but it was more abundant on the specimen immersed in the presence of sodium hydroxide.
Initially, the parameter S fluctuated around the 0 value, but after addition of sodium hydroxide it rapidly decreased to approach −1 and, subsequently, changed sign, approaching 1.
The latter value was approximately maintained for the duration of the experiment.
The observed behavior can be rationalized by considering Fig.
4 and video 2.
Initially, the two electrodes were relatively symmetrical and the value of S fluctuated around the 0 value; following the addition of sodium hydroxide in one compartment, the electrode in that compartment immediately become more noisy compared with the electrode in the other compartments due to the onset of hydrogen evolution on its surface.
As the alkaline attack proceeded, copper from second phase material accumulated on the surface and the corrosion attack became more generalized.
At this stage, the decrease in electrode resistance became more important.
Consequently, the change in the sign of S can be rationalized considering that initially the electrode asymmetry was mainly due to a difference in the noise levels between the two electrodes but, subsequently, the reduction in electrode resistance due to alkalinization dominated.
The observed behavior can be qualitatively described by the trajectory identified by the arrows in Fig.
4.
From the previous findings, it can be confirmed that the parameter S is useful in estimating electrode asymmetry.
Large values of S necessarily indicate asymmetry between the two electrodes, while a 0 value is not, per se, sufficient to ensure that the electrodes behave symmetrically.
However, it should be noted that it appears unlikely that significantly dissimilar electrodes show very low values of S for prolonged times.
In order to evaluate asymmetry under practical conditions, electrochemical noise data were acquired from two pairs of nominally identical AA2024T3 electrodes connected in one case through a 4.7kΩ resistor (experiment A) and, in the other case through a 10kΩ resistor (experiment B), with the potential acquired by the Ni-USB 6009, as described in Section 3.2.
The values of the two measuring resistors were selected to be of the same order of magnitude as the resistance of the electrodes.
During the electrochemical noise measurement, images of the corroding surface were acquired at 10min intervals.
Fig.
12 and video 3 display the time evolution of the noise resistances and of the parameter S for both experiments (lines have been smoothed in the figure compared to the video to improve readability).
The values of the estimated noise resistances are closely similar (confirming that the value of the measuring resistor does not affect the estimation of the noise resistance) but, interestingly, the first pair of specimens showed a value of S displaced towards negative values for most of the duration of the experiment, while the second pair of nominally identical specimens displayed a value of S less biased for most of the duration of the experiment.
Fig.
13 and video 3 display the images of the surface after 80h of immersion.
It is evident for experiment A, that the specimen on the right was significantly more corroded than the specimen on the left.
Conversely, the corrosion on the specimens of experiment B appeared more equally distributed over the two specimens.
Scrutiny of video 3 provides further insight into the relationship between the electrochemical noise data and the corrosion process occurring on the surface.
It is evident for the experiment A (4.7kΩ measuring resistor) that corrosion was more intense on the right specimen from a relatively early stage, as suggested by the formation of dark regions on the right specimen after approximately 15h of immersion and by the larger number of clearly active corrosion sites.
This correlates with the time-evolution of the parameter S, generally biased towards negative values approaching −1 for most of the duration of the experiment.
Concerning experiment B (10kΩ measuring resistor), the value of S was less biased for the first 30h of the experiment.
At approximately 30h, a significant increase to approach 1 was observed, and such an increase could be related to the initiation and propagation of stable corrosion sites on the left specimen, as highlighted in video 3.
Based on the image assisted analysis for the system considered, it appears that values of S between +0.5 and −0.5 are an indication of acceptable electrode similarity and the estimated value of noise resistance can be considered to be reliable.
On the other hand, if the value of S approaches 1 or −1 for a prolonged period of time, the assumption of identical behavior between the two electrodes is not verified, and the value of the estimated noise resistance is less reliable and probably approaches the resistance of the electrode that is corroding less.
An alternative to the use of a zero resistance ammeter for acquiring the current during electrochemical noise measurement has been presented.
The method is based on the coupling of two nominally identical electrodes by a measuring resistor, and the potential across the measuring resistor can be used to calculate the coupling current.
The physical implications of coupling two electrodes by a measuring resistor have been discussed, suggesting that the operation does not introduce a perturbation to the corrosion process per se.
From the analysis of the relevant equivalent circuit, it has been pointed out that with the proposed method two values of apparent noise resistances can be obtained and the difference in such values, normalized by the value of the measuring resistor, provides an indication on the asymmetry of the electrodes.
It has been shown mathematically that, for identical electrodes, it is possible to estimate the electrode resistance based on the values of apparent noise resistance.
The procedure has been experimentally validated and satisfactory agreement with the usual approach that relies on the use of a zero resistance ammeter was found.
Importantly, the use of a measuring resistor instead of a zero-resistance ammeter eliminates the possibility that the noise produced by the active electronic within the measuring system is fed back to the corroding electrodes, thereby altering the corrosion process.
Further, the proposed method has the advantage of enabling the calculation of a parameter S relating to electrode asymmetry.
It has been demonstrated that values of S approaching 1 are a strong indication of electrode asymmetry; therefore, the reliability of the value of noise resistance should be questioned in such cases.
EPSRC is acknowledged for provision of financial support through the LATEST2 Programme Grant.
Intertek-CAPCIS™ is acknowledged for providing a Concerto™ multichannel potentiostat.
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.corsci.2013.08.014.
Supplementary video 1Variation of the value of the calculated parameter S as a function of the noise level and resistance of electrode 1 and of the value of the measuring resistor.Supplementary video 2Time evolution of potentials and current noise, noise resistance and parameter S, synchronized with the images of the corroding surface during the split-cell experiment.
Top image: electrode immersed in the compartment where sodium hydroxide was added.Supplementary video 3Time evolution of the noise resistances and parameter S for two experiments on nominally identical electrodes, synchronized with the images of the corroding surfaces.
Top image: experiment A, using a measuring resistor of 4.7kΩ, bottom image experiment B, using a measuring resistor of 10kΩ.
SKPFM measured Volta potential correlated with strain localisation in microstructure to understand corrosion susceptibility of cold-rolled grade 2205 duplex stainless steel Scanning Kelvin probe force microscopy (SKPFM) of annealed and cold-rolled grade 2205 duplex stainless steel has been correlated with microstructure analysis using electron back-scattered diffraction (EBSD).
In annealed microstructure Volta potential differences indicated micro-galvanic coupling between ferrite and austenite reasoning selective dissolution of ferrite.
The introduction of cold work reduced the difference between both phases, but the development of local extremes in Volta potential was observed.
Microstructure analysis revealed the presence of larger misorientation concentrations at these sites, which can explain the changes in observed corrosion behaviour, from selective dissolution in the annealed condition to localised corrosion attack after cold-rolling.
Duplex stainless steels (DSS) are frequently used for engineering applications, with complex shapes of components obtained via cold-rolling or shaping of plate and sheet material.
Plastic deformation in polycrystalline material is generally heterogeneous, and leads to the development of misorientation variations and gradients in the microstructure, typically associated with strain localisation [1] .
In duplex stainless steels, strain localisation is complex due to the mismatch of mechanical properties between body-centred-cubic ferrite (δ—bcc) and face-centred-cubic austenite (γ—fcc), which manifests itself during plastic deformation.
Load sharing between both crystallographic phases is observed leading to heterogeneous deformation within the microstructure, with micro-yielding near grain boundaries in the ferrite, augmented by plastic deformation and dislocation pile-ups in the austenite [2,3].
Pitting corrosion [4,5] and crack nucleation [6] is often triggered at such sites, but cracks have also been observed to nucleate at slip bands due to the presence of local differences in elastic anisotropy [7].
The magnitude of misorientation within grains increases with the introduction of plastic strain, which is often used as a quantitative measure to describe the degree of microstructure deformation [1].
However, the introduction of plastic strain can also result in sub-structure formation associated with the re- arrangement of dislocations.
Free dislocations generated during plastic deformation can readily re-arrange themselves, leading to the development of low-angle grain boundaries (LAGB) [1,8,9].
Plastic deformation results in an increase of the dislocation density which also affects local surface work function and hence the electrochemical potential [10].
The Electron Work Function (EWF) reflects the electron activity [10,11] and is the minimum energy required to remove electrons from inside a metal to a position far away from the surface on the atomic scale, but still close to the metal to be influenced by ambient electric fields [10–17].
It is not a material property but characteristic of the surface and is sensitive to pre-existing deformation, surface oxide layers, roughness, temperature, and adsorbents [15,17,18] .
The EWF has been reported to decay with the induction of plastic deformation up to about 40% plastic strain after saturation is reached which was observed to be independent from the type of the material and was enhanced by the rate of deformation [10] .
An increase of plastic strain and strain rate decreased the EWF hence the change in EWF could be attributed to the dislocation density in the microstructure.
Furthermore, it was demonstrated that at grain boundaries the EWF decreased, in contrast to the grain interior of nanostructure materials, indicating that at such sites the electron activity is high with the result that the surface became more electrochemically reactive [11] .
Such an increase in electrochemical reactivity affects local and overall corrosion performance of the material [11].
The related Volta potential (Ψ) is the potential difference between a position infinitely far away from the surface and a position just outside the surface, and is the measureable quantity characterising electrochemical behaviour of a metal [12,17].
The scanning Kelvin probe force microscopy (SKPFM) technique allows detection of local EWF (if the EWF of the tip is known), or Volta potential differences (ΔΨ) between an atomic force microscopy tip (usually Pt coated) and the metal surface [14,15,19].
The lateral resolution of SKPFM can be as high as 10’s of nm in ambient air, with a sensitivity up to 10–20meV [19].
Volta potential is a characteristic property of a metal surface and can be used to understand electrochemical processes [16] .
It is sensitive to any kind of surface defects, chemical variations, and residual stress [13,17].
Volta potential differences in microstructure have been used to predict corrosion behaviour [10,15,18,20–22].
Regions with larger (ΔΨ) indicate increased surface reactivity [11,15,18], and even a correlation between Volta potential differences measured in nominally dry air and their free corrosion potential (Ecorr) pre-determined under immersed conditions has been reported [18].
Furthermore, a relationship between Ecorr measured under thin electrolyte and the Volta potential difference was demonstrated [23].
Linear correlation of Volta potential measured via the Kelvin probe technique measured in air with the free corrosion behaviour (Ecorr) upon immersed specimens [24], and this relationship was demonstrated to hold true for potentials measured via SKPFM [23] .
However, SKPFM Volta potential differences in air may not always correlate with corrosion potentials measured in aqueous solution, and interpretation of SKPFM data regarding corrosion phenomena need to be treated with caution [15].
This shortfall can be minimised by conducting SKPFM measurements in humid air (>60% relative humidity), where several mono-layers of water are present on the metal surface, mimicking “solution- like” behaviour [25].
The purpose of the work reported in this study was to investigate the effect of plastic deformation on local microstructure Volta potential differences (ΔΨ) and compare these results with atmospheric corrosion observations.
The idea is that through the introduction of plastic deformation steep gradients of ΔΨ on confined regions can be formed in the microstructure which may correlate with local strain heterogeneities and act as susceptible sites for localised corrosion and also for stress corrosion cracking.
A duplex microstructure was chosen to maximise the effect of plastic strain on microstructure behaviour.
Grade 2205 duplex stainless steel is considered by the UK’s nuclear industry as a candidate container material for storage of intermediate-level radioactive waste (ILW) which undergoes various deformations [26].
Localised corrosion and, in particular, environment-assisted cracking has been considered as the main material degradation concern that may limit the integrity of ILW containers, which mechanistic understanding is required to support container lifetime predictions.
A solution-annealed (mill annealed) grade 2205 DSS plate with a composition (in wt.-%) of 22.4Cr, 5.8Ni, 3.2Mo, 1.5Mn, 0.4Si, 0.016C, 0.18N, and Fe (bal.)
was used in this study.
The chemical composition of ferrite and austenite was determined by energy dispersive X-ray (EDX) technique interfaced to a scanning electron microscopy (SEM) as 25.0Cr, 4.3Ni, 4.3Mo, 1.7Mn, 0.5Si, and Fe (normalised) for ferrite, and 22.1Cr, 6.9Ni, 2.6Mo, 1.9Mn, 0.4Si, and Fe (normalised) for austenite, all in wt.-%.
Cold rolling of the plate was performed with a thickness reduction of 40%, and rectangular coupon specimens (8×8×8mm3) were cut from both the as-received and cold- rolled material.
The surface of these coupons was prepared by grinding to 4000-grit, followed by mirror-finish polishing using 3, 1, ¼, and 0.1μm diamond paste, and finalised with an OP-S polishing treatment.
Electron Back-Scattered Diffraction (EBSD) was used for material characterisation, by extracting grain size, crystallographic phase fraction, and local misorientation (LMO), with the latter indicative of the distribution of plastic strain in the microstructure.
A FEI Quanta 650 scanning electron microscope (SEM) interfaced with a Nordlys EBSD detector from Oxford Instruments with Aztec V2.2 software was used for data acquisition.
A step size of 0.15μm over an area of 856×746μm2 with an accelerating voltage of 20kV was used for data acquisition.
Data post processing was carried out with HKL Channel 5 software.
High-Angle Grain Boundaries (HAGB's) were defined with misorientation ≥15° and Low Angle Grain Boundaries (LAGB's) between >1° and <15°.
The grain size was determined by the mean linear intercept method as the mean of vertical and horizontal directions (twins disregarded).
LMO maps were generated by using a 3×3 binning and a 5° threshold for the sub-grain angle threshold.
This analysis gives the average LMO for a misorientation below the pre-determined sub-grain angle threshold, and this method was used to locate regions with higher concentrations of misorientation in the microstructure.
The latter is typically associated with local micro-deformation in the form of plastic strain, due to the presence of dislocations [1].
All SKPFM measurements were performed prior to EBSD analysis of the same area to allow correlation of the results.
A MultiMode 8 atomic force microscope from Veeco Instruments (AFM) with Nanoscope V8.15 acquisition programme from Bruker was used.
OSCM-PT probe was employed for Volta potential (ΔΨ) measurements.
The interleave lift height was 50nm.
The SKPFM scan size was 50×50μm2 for the mill-annealed and 55×55μm2 for the 40% cold-rolled sample.
The tip was biased in order to zero the contact potential difference; therefore, positive potential differences measured indicate larger electronic activity of the microstructure.
The reliability of the probe used was assessed by measuring the Volta potential difference of high quality HOPG (Highly Oriented Pyrolytic Graphite) which is the highest-quality synthetic form of graphite and commonly used to calibrate AFM probes [27].
Maximum potential differences of 1–2mV were measured indicating good quality of the tip and maximum sensitivity of the system.
The potential difference measured prior to and after the test was identical supporting the stability of the tip.
SKPFM tests were carried out in ambient air at a temperature of 30°C with 38±1% Relative Humidity (RH) or in a controlled environment of 33±1°C and 86±1% RH.
These humidity values were chosen to obtain information about the effect of water multi-layer coverage on Volta-potential differences, invoking “solution-like” behaviour [25].
The humidity was controlled in a sealed AFM chamber.
Air was bubbled through a 5l flask containing 5M NaCl solution, heated by a water bath to 60°C, creating a humid gas atmosphere which was directed to the chamber (a Perspex enclosure) housing the entire AFM setup.
According to Leygraf et al.
[25], at least six to ten monolayers of water with chloride species should form at 86% RH supporting a correlation of Volta potential measurements with local electrochemical activity.
Temperature and relative humidity (RH) were recorded during the measurement with an EL-USB-2-LCD temperature and humidity data logger.
SKPFM data analysis was performed with the software NanoScope Analysis 1.5.
Topography maps were flattened using 1st order flattening.
Data acquisition was carried out with 512×512 pixel resolution, with all values reported relative to the work function of the AFM tip (Pt-tip).
Potentials measured with higher and lower values with respect to the AFM tip indicate a net anodic and net cathodic activity, respectively.
Large Volta potential differences measured with respect to Pt mean lower absolute Volta potentials of the measured region/feature in the microstructure and, hence, higher corrosion activity.
Volta potential maps of cold-rolled specimen measured in 86% RH environment were flattened and inverted in order to enhance potential contrast of local potential “hot-spots”.
Higher and lower potential values indicate net cathodic and net anodic activity, respectively, in this case only.
Atmospheric corrosion tests were performed on 4000-grit surface finished coupons.
Tests were performed by exposing samples to 80°C and 27% RH for 212hrs.
Water droplets containing MgCl2 were applied onto the surface, yielding chloride deposition densities ranging from 145μg/cm2 to 1450μg/cm2.
An Eppendorf micropipette was used to dispense the liquid with the volume of the droplet 0.5μl producing an overall droplet radius of 1.7mm (the effect of secondary spreading of the droplet was not considered).
This exposure regime is close to the deliquescence point of MgCl2, resulting in a concentrated, very aggressive MgCl2 solution.
Post exposure, the corrosion morphology was analysed with a Keyence VHX-200 optical microscope and the FEI Quanta 650 SEM.
To identify whether the ferrite or the austenite phase had corroded, the mill-annealed sample was additionally etched in aqueous 40wt.-% KOH solution, using 5V for 5s.
This electrolytic etch stains the ferrite [28].
The cold-rolled specimen was analysed in the SEM in the backscattered electron imaging mode, to distinguish between ferrite and austenite.
To demonstrate the effect of cold-rolling on microstructure development, EBSD phase maps of all three process orientations (ND, RD, and TD) of the annealed and 40% cold-rolled microstructures are shown in Fig.
1.
Cold-rolling induced changes in grain shape and grain morphology, particularly in the austenite.
The average grain size of ferrite as a sum of all process directions decreased from 7.3±0.3μm in the annealed condition to 5.1±1.9μm after 40% deformation, whilst the austenite grain size decreased from 7.1±0.9μm to 4.2±1.3μm.
The average aspect ratio (vertical to horizontal mean length) in all process directions decreased from 1.8 to 1.2 in ferrite, and from 1.6 to 1.1 in austenite.
This decrease in grain size can possibly be attributed to severely deformed and broken-up grains after cold deformation.
Other phases such as sigma (σ) or chi (χ) phase were not present in the microstructure.
The austenite to ferrite phase ratio as an average of all three processing orientations was 44:56 and 47:53 for the solution annealed and 40% cold-rolled condition, respectively.
The small increase in ferrite content with cold rolling is within typical microstructure variations observed in duplex stainless steel.
There are also reports about the formation of deformation-induced bcc-martensite in duplex stainless steels, which would be co- indexed as ferrite since the Kikuchi patterns of martensite overlap with the Kikuchi patterns of ferrite in EBSD analysis.
The martensite is typically needle-shaped and forms within the austenitic phase [29].
However, EBSD analysis of the 40% cold-rolled sample in Fig.
1 did not show any needle-shaped features.
Topography and Volta potential maps in air (38% RH) of the annealed sample with corresponding EBSD phase and LMO maps of the same region are shown in Fig.
2.
The topography and Volta potential maps showed good correlation with the EBSD phase map, facilitating observation of both crystallographic phases to the corresponding ΔΨ values.
Austenite and ferrite had distinctively different potential values, with ferrite showing higher potential values than austenite, meaning expected higher electrochemical activity.
Mean ΔΨ values of 408±16mV and 320±11mV were measured for ferrite and austenite, respectively.
The mean potential difference between both phases is about 70–90mV, with steep potential gradients across interphases.
Similar potential differences of 50–60mV between ferrite and austenite, with ferrite indicating the less noble phase (anode) and austenite the nobler phase (cathode) has been reported by Sathirachinda et al.
who investigated solution-annealed and sensitised microstructure and correlated local elemental depletion with local ΔΨ assessments [30].
The probe used was SCM-PIT with Pt-Ir coating and slightly different to that used in this study (OSCM-Pt probe with Pt coating).
Small differences in chemical compositions and heat treatment history may also have contributed to different potentials measured.
However, similar interphase potential gradients to our study were measured.
The potential variation within each phase is small (Fig.
2b), and no obvious potential hot-spots (individual sites with high local ΔΨ at confined regions) are detected.
This indicates existence of a driving force for galvanic interaction between both phases, with expected onset of electrochemical activity at interphase boundaries and dissolution towards the ferrite.
Differences in ΔΨ generally indicate a driving force for galvanic interaction, whereas local potential hot-spots or regions with steep potential changes (gradients) indicate susceptible sites for localised corrosion [31].
Selective corrosion attack in duplex stainless steel has frequently been observed on ferrite to occur in mild chloride-bearing environments, with nucleation at or in the vicinity of ferrite-austenite interphase boundaries, whilst austenite showed less corrosion and stress corrosion cracking susceptibility [32–41].
The corrosion potential (Ecorr) of austenite in chloride media is lower than that of ferrite despite its lower chromium and molybdenum content.
Preferred nitrogen and nickel partitioning in the austenite renders enhanced passivation behaviour and, hence, electrochemical nobility in chloride environments [38,39,42,43].
These data are in close agreement with the studies conducted by Sathirachinda et al.
who also observed smooth potential distribution in the solution-annealed microstructure which seemed to have disrupted after sensitisation treatments showing local Volta potential extremes associated with enhanced susceptibility to localised corrosion which was confirmed by corrosion immersion tests using aggressive acidic chloride electrolyte [30].
The ferrite phase was seen to corrode primarily while the austenite seemed to remain unaffected which clearly demonstrated the galvanic interaction between ferrite and austenite where ferrite formed the net anode and austenite the net cathode [30].
The LMO map (Fig.
2d) indicates strain localisation mainly concentrated within ferrite and at small grain clusters with both ferritic and austenitic grains.
The mean LMO has a maximum value of 2°, confirming the presence of only small local strain variations.
LMO was similar in both phases, meaning that selective attack cannot be associated with LMO values.
The topography and corresponding Volta potential maps for the 40% cold-rolled condition in air (38% RH) are shown in Fig.
3.
The mean potential of the entire region showed an overall ΔΨ rise of 170–190mV with respect to the annealed microstructure condition.
This large potential shift is expected to also result in a more corrosion susceptible microstructure (less noble).
The driving force for a corrosion attack seemed to have increased after cold rolling possibly due to a strain-induced alteration in the electron band structure of the passive oxide layer caused by an enhancement of defects such as dislocation density (and other defects), where the amount of LAGB’s and HAGB’s increased.
Lower work functions can result from defect-induced alteration in the electron band structure of passive oxide layers due to shifts in the Fermi level position caused by a change in the electron band gap of the oxide [44].
It has been shown in literature that the passivating layer is typically weakened by the introduction of large amounts of cold deformation, so that the corrosion current density and passive current density significantly increases upon immersion in chloride containing media [45].
It is likely that dislocation multiplication in the microstructure induced by cold deformation can affect the passivating characteristics of both crystallographic phases, which in turn resulted in an increase of the mean ΔΨ in the duplex microstructure.
Since the Volta potential depends strongly on the electronic structure of the surface oxide layer of the metal (and other adsorbents) and due to the fact that substantially larger ΔΨ was measured, the reduction of the practical nobility of the cold-rolled microstructure must be related to defect-caused degradation of the surface oxide layer which seemed show to local heterogeneities across the surface [44].
In addition, the ΔΨ difference measured between ferrite and austenite decreased to only about 5–10mV difference, with ferrite assuming Volta potential values of 593±5mV, whilst austenite showed 589±5mV.
Hence, the driving force for micro-galvanic interaction between both phases is expected to have decreased significantly by cold deformation.
However, local (ΔΨ) potential hot-spots within both phases have become visible which were not present prior to cold work, indicating preferential sites of localised electrochemical activity (arrows in Fig.
3b).
There were numerous local hot-spots, indicating enhanced selective net anodic activity in the ferrite with typical (ΔΨ) values of 598±2mV.
This indicates a higher local electronic activity at these confined regions.
In addition, hot-spots with lower ΔΨ were formed in austenite with values of 580±3mV.
Such sites would indicate lower electronic activity i.e., enhanced selective net cathodic behaviour but potential gradients surrounding hot-spot zones can trigger local galvanic activity, hence enhance the surface reactivity at these confined regions and so facilitate preferential electrochemical attack.
Large potential gradients at these confined regions, measured over short distances are expected to trigger localised attack in contrast to regions with smaller potential gradients.
In general, large ΔΨ gradients can affect the galvanic interaction between individual phases.
After cold deformation, the overall potential gradient at the interphases decreased, which indicates a reduction of galvanic interaction between ferrite and austenite.
Such sites would still be expected to behave preferentially active, but a selective attack on ferrite only, as observed in the annealed condition, would not be expected.
Furthermore, the region over which potential changes occurred in the cold-rolled condition became larger, and sites of local potential gradients (hot-spots) were observed in both phases meaning that both ferrite and austenite would be expected to be electrochemically active, but ferrite obviously more due to the nobler potential.
The variation of ΔΨ within each crystallographic phase is most likely associated with the presence of dislocation-related strain fields adjacent to interphases, due to the inherent crystallographic mismatch between ferrite and austenite.
The EBSD phase map (Fig.
3c) shows LAGB's concentrated in both ferrite and austenite, with highly deformed regions consisting of accumulated LAGB’s particularly visible at the interphase (Fig.
3d).
The observed SKPFM hot-spots in Fig.
3b seemed to be associated with some of the misorientation hot-spots observed in Fig.
3d.
Also, the mean LMO shows a broader distribution with a maximum value of nearly 5°, indicating the presence of large local strain variations in the microstructure.
It should be noted that during plastic deformation local chemical variations and surface roughness can be altered, which would also contribute to the measured ΔΨ.
Further SKPFM analysis was performed in humid air (86% RH) to assess whether the observed local (ΔΨ) differences at low RH were also present.
The surface topography map with corresponding Volta potential maps at 38% RH and 86% RH of the 40% cold-rolled specimen are shown in Fig.
4.
Both Volta potential maps were flattened using 1st flattening order and mathematically inverted to obtain an enhanced contrast of local potential hot-spots.
The latter produces a better contrast but reverses the potential ranking order (i.e., austenite has higher potential difference).
Several local ΔΨ hot-pots were observed in both the ferrite and austenite, indicating local corrosion-activity.
These local potential variations with discrete potential extremes causing potential contrasts indicate micro-galvanic coupling within both phases and at interphase boundaries.
Some larger, trench-like regions in austenite are also present, indicating similar (ΔΨ) differences.
These regions are currently subject to further investigations.
In dry air, the passivating layer of stainless steels consists of a thin film oxide/oxyhydroxide layer, usually 1–3nm [25,46,47].
In humid air above 60% RH, at least ten monolayers of water will be adsorbed resulting in thicker passivating layers which decreases the Volta potential difference [25,48].
It has been reported that this potential drop is in the order of 50–100mV [48].
However, the measurement in Fig.
4 gives a potential drop of 200mV in 86% RH compared to 38% RH.
Water molecules could have adsorbed onto the Pt probe which may have resulted in a minor change in the Volta potential of the Pt probe.
The corrosion behaviour of the mill-annealed and 40% cold- rolled microstructures after exposure for 212h to 86% RH atmospheric environment is shown in Fig.
5.
The ferrite selectively corrodes in this environment [32–35], supported by observations in Fig.
5(a and b).
The corrosion attack showed a filiform-like appearance, percolating through the microstructure.
Corrosion initiated on interphase boundaries and propagated preferentially over ferrite regions indicating net anodic behaviour while austenite was the net cathode.
Slight attack on some austenite parts, however, has been observed indicating less favoured corrosion attack occurring from the ferrite towards the austenite phase.
After cold rolling almost no selective corrosion of the ferrite was observed, but localised corrosion attack occurred at discrete sites primarily located in the austenite (Fig.
5c and d).
Local initiation sites of pitting corrosion in austenite were primarily located in the grain interior, possibly related to regions containing local plastic strains.
Alternatively, some of these discrete sites may also be associated with strain-induced martensite, which has been reported for duplex stainless steels [49,50].
These observations are in line with data shown in Fig.
3 and Fig.
4, supporting the notion that local microstructure can affect macroscopic corrosion behaviour.
The work also highlights how the degradation mechanisms of a material can change as a function of microstructure strain condition.
The change from selective corrosion to localised attack could be a potential concern for components with thin wall thickness, and local susceptible sites that follow from cold working may also facilitate stress corrosion crack initiation.
1)Volta potential differences of the solution-annealed microstructure measured over austenite and ferrite differed by 70–90mV, with ferrite indicating a net anodic potential.
The latter is expected to facilitate micro-galvanic coupling between ferrite and austenite, resulting in selective dissolution of the ferrite with atmospheric exposure to MgCl2 containing solutions.
The introduction of 40% cold-roll reduction significantly reduced the Volta potential difference between ferrite and austenite, but in parallel increased the mean Volta potential difference of the entire microstructure.
Volta potential differences maps of the cold-rolled microstructure showed local potential hot-spots (highly active sites at confined regions) with exposure to both 38% and 86% RH environment, indicating corrosion-active sites.
After 40% cold rolling, localised pitting corrosion in austenite and only little attack on ferrite was observed, supporting changes to the overall corrosion response of the duplex stainless steel microstructure.
The authors acknowledge Radioactive Waste Management (RWM) grant number NPO004411A-EPS02 and EPSRC grant number EP/I036397/1 for financial support.
The authors are grateful for the kind provision of Grade 2205 Duplex Stainless Steel plate by Rolled Alloys.
The authors also appreciate the valuable comments provided by Dr Robert Winsley, Radioactive Waste Management Ltd. and Dr Anthony Cook, The University of Manchester.
Coarse grained force field for the molecular simulation of natural gases and condensates The atomistically-detailed molecular modelling of petroleum fluids is challenging, amongst other aspects, due to the very diverse multicomponent and asymmetric nature of the mixtures in question.
Complicating matters further, the time scales for many important processes can be much larger than the current and foreseeable capacity of modern computers running fully-atomistic models.
To overcome these limitations, a coarse grained (CG) model is proposed where some of the less-important degrees of freedom are safely integrated out, leaving as key parameters the average energy levels, the molecular conformations and the range of the Mie intermolecular potentials employed as the basis of the model.
The parametrization is performed by using an analytical equation of state of the statistical associating fluid theory (SAFT) family to link the potential parameters to macroscopically observed thermophysical properties.
The parameters found through this top-down approach are used directly in molecular dynamics simulations of multi-component multi-phase systems.
The procedure is exemplified by calculating the phase envelope of the methane–decane binary and of two synthetic light condensate mixtures.
A methodology based on the discrete expansion of a mixture is used to determine the bubble points of these latter mixtures, with an excellent agreement to experimental data.
The model presented is entirely predictive and an abridged table of parameters for some fluids of interest is provided.
A typical crude oil consists of several thousands of distinct chemical species, all of them roughly similar in chemical nature, but with an important spread in terms of their molecular size, morphology, and thermophysical behaviour.
Furthermore, being a natural product, the particular properties of the mixture vary widely from reservoir to reservoir and can even change with time and during the extraction and processing stages [1].
While the lighter ends can be characterized individually, e.g.
by gas chromatography or mass spectrometry, as the molecular weight increases the number of closely-related structures and their complexity increase combinatorially as their number or mass fraction decreases.
As the heavier end of the spectrum is approached, only very general descriptions of these fractions can be obtained, usually expressed in terms of some general characteristics as the aromatic character, the percentage of heteroatoms, etc.
It is ludicrous to postulate that one could model these highly complex systems by explicitly taking into account each and every distinct molecule present in the system, even if such information could ever be obtained.
As a consequence, in the pursuit of the theoretical modelling of these mixtures, historically two schemes have become the mainstream tools of petroleum engineering; either the description as a continuum distribution [2] or the description as a discrete but finite set of pseudo-components [3].
Pseudo-components are artificial assignments of a cut or fraction of the mixture to values of critical properties, densities and acentric factors which on average represent the bulk behaviour, obtained from measured oil bulk properties, light ends analysis, distillation, or other characterization methods.
The concept brings back simplicity into the description of a mixture and the number of pseudo-components usually employed to describe a crude is in the order of dozens.
There are a number of empirical ways to perform this mapping, and no consensus of an optimal procedure exits [4].
Related approaches map the behaviour of a crude to a mixture of real components [5] or characterize pseudo-components based on 13C NMR and other analytical data subsequently applying group contribution methods [6,7] to obtain the corresponding equation of state (EoS) parameters.
Whichever the procedure employed, the mapping of the mixture to a finite set of constituents allows the use of analytical EoS to be used as fitting tools, as they are mostly built with a discrete mixture in mind.
The number and diversity of the EoS available for this purpose is staggering and their review is removed from the scope of this manuscript.
The reader is referred to recent monographs [8,9] for further details.
A more modern approach to study thermophysical properties of fluid mixtures is by means of classical molecular simulations.
The recent perspectives by Maginn [10,11] and Palmer and Debenedetti [12] give the reader some insight on the currently accepted views.
However, it is important to not to raise false expectations on the capabilities of computer simulations and particularly to understand the present and future limitations.
Fully atomistic modelling, where the individual molecules are described in terms of their constituent atoms and the bonds between them, cannot currently be used to explore more than several nanoseconds of time (in the case of Molecular Dynamics) and a few thousands of individual molecules.
Even with modern advances in parallel processing, the use of graphical processing units and the reduction in the costs of hardware, these limits are bound to remain essentially unchanged (see for example the comments made during a recent Faraday Discussion [13] on the topic).
This is not to say that both massively large systems [14,15] and/or long time frames [16] have not been explored, but they are far from the norm; furthermore one extreme usually precludes the other.
Unfortunately, this scenario is incompatible with the apparent need for modelling crudes with hundreds or thousands of different species, each in a discrete composition and for reasonably long times (e.g.
to study asphaltene aggregation; freezing of waxes; solubility of gases, etc.).
An immediate corollary of the above comments is that in the present and immediate future a detailed atomistic description of a crude oil is essentially unfeasible.
It is thus natural to consider that the atomistic modelling will follow the EoS modelling approach, i.e.
it is compulsory to describe a crude oil as a mixture of a relatively small number of prototypical species or surrogate real molecules [17].
Some key questions still remain to be answered as to the number and nature of discrete elements necessary for a trustworthy representation, the level of fidelity required from the models and the strategies employed to represent the more “unknown” fractions present in heavier crudes.
Notwithstanding some of the above warning signs, some “heroic” efforts have been made to atomistically describe complex oil mixtures by simulations.
Notable is the seminal work of Lagache et al.
[18] captured in extenso in a book [19], that a decade ago described the modelling of naturally occurring high-pressure high-temperature hydrocarbon gas mixtures using 18 discrete representative molecules.
Other examples are the work of Maldonado et al.
[20] who have presented a molecular dynamics (MD) simulation of 25 discrete n-alkanes from C6 to C30 using atomistic models.
They considered a very low density system and the adsorption behaviour of this mixture onto graphite surfaces.
Other authors have considered discrete mixtures of a handful of small molecular weight hydrocarbons to mimic a crude, analysing, for example the accumulation of aromatics at the oil–water interface [21] the interfacial properties of gases and brine [22] and the diffusion of gases [23].
Recently, Li and Greenfield [24] employed a system composed of a dozen representative molecules to describe asphalt systems.
The use of proxy molecules to represent a cut or family of homologous molecules is a natural progression in the simplification of the problem.
Mixtures of a small number of discrete model molecules, each representing a family of molecules (e.g.
resins, asphaltenes) have been employed to discuss bitumens [25,26] while binary mixtures of heptane (or toluene) plus a model molecule are routinely employed to study the effects of asphaltene aggregation [27–30] in spite the fact that pure heptane (or toluene) are clearly deficient models of a complex crude.
Wax deposition is also commonly studied using representative few-component alkane mixtures [31–36].
All of the above approaches encounter the technical problem associated with the fact that molecular simulations are based on the a priori specification of pairwise potentials amongst the N atoms that constitute the mixture and that the time required to solve the problem scales in principle as N2.
The recently observed increased proliferation of atomistically-based studies is a reflection of both reduction in the cost of high performance computer hardware and the increased confidence in the quality of the predictive power of classical atomistic force-fields.
However, the crux of the matter is that even the speedup provided by advanced algorithms which decrease the scaling of the problem and/or the steady historical increase in computational power implicit in Moore's law [37] are not enough to provide the baseline for fully atomistic modelling of crudes.
A way forward is to recognize that the level of detail incorporated into the existing atomistic models is far too great for the needs of this problem, more so if one recognizes the large uncertainties surrounding the detailed characterization of the actual crudes.
The use of simplified versions of the potentials, generically called coarse grained (CG) models becomes the immediately obvious route.
Coarse graining is a term that refers to the use of simplified molecular models, where the atomistic detail is removed and substituted by the description of molecules in terms of “super-atoms” which represent, typically, a small number of heavy atoms.
For example, in a standard CG representation, a propane molecule could be modelled as an isotropic spherical bead where all the electronic details, the intramolecular vibrations, bond bendings and molecular topology are incorporated within a point pair-wise interaction model.
Coarse graining techniques have been extensively used in computational biology [38,39] where the self-assembly of large molecules is the main point of interest, and have become a mainstream technique for the study of complex fluids, materials and soft matter.
One of the key issues in developing CG force fields is the methodology used to parameterize the intermolecular potential.
Although not uniquely, most CG approaches start with an atomistically detailed model and integrate out the degrees of freedom not deemed to be relevant [40].
This procedure, by its own nature, removes information and the resulting force field is inherently deficient, especially in terms of transferability.
In the case of interest here, a bottom up coarse graining makes no sense, as the initial components are not well defined to start with.
More aggressive CG of this type inevitably ends up losing the link to the parent models, with the corresponding loss in robustness.
Dissipative Particle Dynamics (DPD), for example has been employed to model crude oil systems [41–44], borrowing the idea that the properties of soft repulsive beads may mimic “lumps” of fluid.
DPD is appropriate for qualitative studies, but is challenging to use as a predictive tool [45].
A fundamentally different “top-down” approach is used herein, where the CG potential parameters are optimized to reproduce the macroscopically observed thermophysical properties (instead of integrating high fidelity atomistic models).
This change in paradigm is achieved by employing an equation of state (an analytical representation of the free energy) as the link between the molecular-level interaction potential and the macroscopic experimental data that relates to it.
We seek to perform the search for effective potential parameters in an average sense capturing the thermophysical properties of a molecule, e.g.
its density over a wide temperature range, its vapour pressure, etc.
with a single set of parameters.
The idea of using an EoS to obtain parameters to be used in molecular simulations is not new; for example Cuadros et al.
[46] used a cubic equation of state to fit Lennard–Jones (LJ) spherical parameters to a series of fluids.
The fact that the LJ model does not have the required flexibility to model a wide range of fluids, its inability to model non-spherical geometries and in some cases, the weak link between the equation of state and the intermolecular potential have hampered the popularity of these approaches.
These limitations are removed if one employs a molecular-based equation of state; e.g.
Müller and Gubbins [47] used a decorated LJ sphere with association sites to obtain an intermolecular potential for water by linking it to an appropriate EoS while Vrabec et al.
[48] used an accurate equation of state to successfully parameterize a two center LJ bead model with central dipoles and quadrupoles and used the approach to develop force fields for a large range of small molecules with an accuracy that rivals experimental measurements.
This is the essence of our approach [49]: to employ a molecular-based EoS to parameterize a force field that can be employed directly in molecular simulations, details provided in the next section.
The Statistical Associating Fluid Theory (SAFT) is a well-developed perturbation theory used to describe quantitatively the volumetric properties of fluids.
The reader is referred to several reviews on the topic which describe the various stages of its development and the multiple versions available [50–53].
The fundamental difference between the versions is in the underlying intermolecular potential employed to describe the unbounded constituent particles.
Hard spheres, square well fluids, LJ fluids, argon, alkanes have all been employed as reference fluids in the different incarnations of SAFT.
For the purpose of this work we will center on a particular version of the SAFT EoS, i.e.
the SAFT-VR Mie recently proposed by Laffitte et al.
[54] and expanded into a group contribution approach, SAFT-γ, by Papaioannou et al.
[55].
This particular version of SAFT provides a closed form EoS that describes the macroscopical properties of the Mie potential [56], also known as the (m,n) potential; a generalized form of the LJ potential (albeit predating it by decades).
The Mie potential has the form(1)ϕ(r)=Cεσrλr−σrλawhere C is an analytical function of the repulsive and attractive exponents, λa and λr, respectively, σ is a parameter that defines the length scale and is loosely related to the average diameter of a Mie bead; ɛ defines the energy scale and corresponds to the minimum potential energy between two isolated beads; expressed here as a ratio to the Boltzmann constant, kB.
The Mie function, as written above, deceivingly suggests that four parameters are needed to characterize the behaviour of an isotropic molecule, however the exponents λa and λr are intimately related, and for fluid phase equilibria, one needs not consider them as independent parameters [57].
Accordingly, we choose herein to fix the attractive exponent to λa=6 which would be expected to be representative of the dispersion scaling of most simple fluids and refer from here on to the repulsive parameter as λ=λr.
The potential simplifies to(2)ϕ(r)=λλ−6λ66/(λ−6)εσrλ−σr6 In the CG application of the SAFT models one considers spherical elements that correspond to a chemical moiety comprised of several heavy atoms; i.e.
“super-atom” beads.
Furthermore, the SAFT theory lends itself naturally to consider chain molecules made of tangentially-bonded beads.
This adds to the model an additional parameter, m, which quantifies the number of elements in a chain molecule.
SAFT also has a built-in provision for embedding associating sites unto the models, which has not yet been employed in CG models, although there is no fundamental limitation for this.
In summary, the CG model for an arbitrary molecule is sketched in Fig.
1 and corresponds to a chain of m tangent spherical segments, each of them characterized by a triad of parameters, (ɛ, σ, λ).
The key requirement for an EoS model to be used in a top-down CG approach is its accuracy in representing the underlying Hamiltonian, e.g.
the question is: how well do the simulations of the potential agree with the description made by the EoS?
Fig.
2 shows an example of such a fit for the SAFT-VR-Mie EoS, where the properties of the (34.29, 6) potential (a model of propane) obtained both by simulations and theory are compared.
The same set of parameters are used in both the theory and the simulations.
The agreement of the two routes is excellent.
The correspondence between theory and simulations makes it possible to invert the procedure, i.e.
to use the EoS to fit the parameters (ɛ, σ, λ) to match experimental data and then to use the same parameters obtained with the theory in a simulation.
The agreement shown in Fig.
2 is not fortuitous, it is seen for a wide range of fluids, including, but not limited, to small polar molecules, refrigerants, chain-like fluids, etc.
[49].
Having established that the EoS is capable of representing the underlying potential accurately, there are several plausible alternatives for obtaining the parameter sets for pure components.
The obvious one is to perform a least square minimization between target experimental data sets and those predicted by the EoS.
Using as a target both the saturated liquid densities and vapour pressures along the extent of the fluid region is a classical approach which leads to the most consistently robust parameters.
Arguably it does require coding of the EoS and an appropriate optimization routine.
While tedious, the process is aided by the fact that commercial software packages [58] are starting to include the SAFT-γ models alongside optimization tools.
To circumvent and streamline the fitting procedure, Mejia et al.
[59] expressed the SAFT-VR-Mie EoS in terms of reduced units and found there was a direct association between the value of the repulsive exponent in the Mie potential and slope of the vapour pressure curve in a pressure–temperature diagram.
This observation suggested that an empirical correlation could be made between the Pitzer acentric factor, ω, which for a spherical molecule is related to said slope, and the repulsive exponent, λ.
A similar, and possibly more obvious link can be made between the value of the critical temperature and the energy scale of the model, ɛ, and between the size parameter σ, and a characteristic liquid density.
The resulting correlation, known light-heartedly as the M&M correlation, allows the determination of parameters for the SAFT CG force fields solely from the knowledge (or estimation of) critical properties.
Table 1 shows a very abridged collection of parameters of interest in the oil and gas industry obtained using this methodology.
As an example, the experimental densities and pressures for propane (m=1) are plotted in Fig.
2 alongside the EoS and the simulation results.
The agreement between theory, simulations and experiments for the densities, critical temperature and pressures are all excellent and typical of what is seen for all other compounds studied.
It underpins the idea that one can use the theory to fit parameters for the equation of state, with the understanding that the theory has the required degrees of freedom to appropriately reproduce the data, but even more importantly, that the molecular simulations that are performed with these parameters will reproduce the theoretical results and, by extension, the experimental data.
This three-pronged agreement is not always possible; e.g.
most EoS will not faithfully reproduce the properties of the underlying potential due to inherent approximations made throughout the theoretical derivation.
Similarly, not all potential functions have the flexibility to reproduce the properties of real fluids as a consequence of the reduced degrees of freedom within their functional form, e.g.
the LJ model shown in Fig.
2 will be incapable of fitting simultaneously the densities and vapour pressures of propane regardless of the choice of parameters (ɛ, σ) employed.
Inherent in the use of a multi-parameter force field such as the Mie potential is the fact that there is the need to simultaneously fit several parameters which can, in principle have some degree of degeneracy.
If one is not careful to include a wide range of experimental data, multiple solutions can be found to reproduce the same data with the same quality of fit.
As an example, Gordon [60] showed how the temperature–density diagram of methane could be predicted with accuracy with a wide range of potential parameters.
It is by taking a look at other properties (in the case presented by Gordon, at viscosity) that one could discern between the transferability of the potentials found.
In our case, we have taken to use simultaneously liquid phase density, vapour pressure and critical temperature to bracket the parameter region.
In spite of the above, it is clear, from an analysis of the behaviour of the parameters, that multiple parameters can be found all with a similar performance.
While this is an indicative of the robustness of the model, it also implies the need for some care to be taken when selecting the particular parameter values.
Take for example the case of butane.
If one were to fit the SAFT-VR-Mie equation of state to the experimental vapour–liquid phase equilibrium properties, one obtains [54] (m=1.8514, λ=13.65, ɛ/kB=273.64K, σ=0.40887nm).
The non-integer value of m precludes the use of these parameters in CG simulations (i.e.
what is a fraction of a bead?).
This leaves us the choice to arbitrarily choose to model butane as either a single sphere (m=1) or as a dimer (m=2).
The resulting parameters, obtained through the M&M correlation (or through direct fitting to the EoS) are (m=1, λ=40.81, ɛ/kB=510.63K, σ=0.5303nm) and (m=2, λ=13.29, ɛ/kB=256.36K, σ=0.3961nm), respectively.
Ramrattan et al.
[57] have noted that the value of the repulsive exponent λ has a direct relation to the fluid range, i.e.
the ratio between the critical and triple point of a fluid; and that this metric is a valuable tool to bracket the possible parameter space.
For the attractive exponent used here, “hard” repulsive exponents, e.g.
values larger than λ=12 reduce the fluid range and after a value of λ=43 the fluid phase is no longer stable being suppressed by the presence of the solid phase [57].
The upshot of this is that hard potentials might exhibit premature freezing as compared to the experimental results.
In the example above, Ramrattan [61] predicts a triple point of 331K for the single sphere model of butane and 139.8K for the dimer model, the latter comparing much better to the experimental value of 134.6K [62].
For the case of mixtures, a new set of unknown parameters come into play, namely the cross-parameters corresponding to the binary interactions.
The best course of action is to obtain these parameters by fitting them to reproduce the properties of selected mixtures, however this is seldom possible.
Lafitte et al.
[54] suggested the following combination rules that can be used as first approximation to describe the interaction between two different Mie fluids, labelled with subscripts ii and jj.
(3)σij=σii+σjj2; εij=σii3σjj3σij3εiiεjj;(λij−3)=(λii−3)(λjj−3) The SAFT coarse grained models do not provide information on the intramolecular interactions, as these are all averaged out during the fitting procedure.
However, one can recognize that both overall shape, intersegment connectivity and rigidity are crucial to preserve the quality of the structure prediction [64].
A limitation of the theory is that the CG segments be rigidly bonded at a distance corresponding to that used to evaluate the reference radial distribution function.
In this work, this distance is taken to be the characteristic size, σ, i.e.
the CG spheres are bonded at a distance of σ.
With respect to the bending of longer chains, the underlying theory only specifies that on average, the molecules should remain extended [65].
This is a natural configuration for alkanes and similar molecules present in crude oils.
Within these models, this elongation is biased by adding a bond angle bending potential [66], ϕangle, between three consecutive beads, ϕangle=kangle(θ−θ0)2, where θ is the angle subtended by three consecutively bonded spheres.
The particular values of the angle, θ0=157.6°, and the constant that restricts the distribution, kangle=3.38Jmol−1deg−2 (2.65kcalmol−1rad−2), are obtained by averaging over all-atom models of short-length alkanes.
As an example of the predictive capability of the methodology, we use here the binary mixture of methane and n-decane at 363.15K.
This is a particularly asymmetric mixture at a temperature which is supercritical for the light component.
Methane is modelled as a single spherical molecule and decane as a chain of three beads, c.f.
Table 1.
Cross interactions are not fitted to the mixture properties in order to explore the robustness of the force field parameters, although it is clear that a binary fit could, in principle produce a better match at the expense of predictability.
Molecular simulations ran for this mixture in the standard canonical (NVT) ensemble, where the number of molecules, N, the temperature, T, and the system volume V are kept constant.
All simulations were run using GROMACS [69] software suite and correspond to classical molecular dynamics (MD) simulations.
Visualizations are rendered using VMD [70].
Reported properties are averaged over at least 2×106 steps (Δt=0.01ps) after the equilibration of the simulated system as determined by monitoring its total energy, and output pressure.
The Nose–Hoover thermostat was chosen for the NVT simulations.
Periodic boundary conditions and a potential cut-off of 2.0nm is applied in all simulations.
For a binary system, calculating the phase behavior of the mixture is a reasonably simple affair, as it amounts to preforming a phase split (isothermal flash) and evaluating the resulting pressure.
In MD, this is frequently done by quenching isochorically to the desired temperature an otherwise well mixed mixture [71].
We employ a simulation cell composed of 3750 decane molecules and 15,000 methane molecules corresponding to an overall mol fraction of methane of xmethane=0.8.
After equilibration, the system will present a liquid slab surrounded by a vapour phase.
Analysis of the density distributions allows the calculation of the molar phase compositions.
The pressure is obtained by inspecting the component of the pressure tensor which is normal to the interface (z direction).
Fig.
3 compares the predicted results of the model to the available experimental data.
Spanning a large range of pressures and skewed compositions, the results are well within what one could expect for a purely predictive model.
Since the simulations describe the two-phase region, one can extract from them information regarding structural transport and/or interfacial properties.
Particularly, the interfacial tension of the mixture is calculated both through the mechanical route and thermodynamic route [73], and compares well with the available experimental data, as shown in Fig.
4.
This information would not be directly available from an EoS and hints at the extent of the transferability and representability of the CG models used.
In a light crude oil, the modelling of this binary pair is often the most sensitive one, as it comprises typically the most abundant compound (methane) with one of the most dissimilar ones (decane) in terms of phase behaviour.
The accurate and predictive capacity of the simulations based on the SAFT CG force field suggests its potential for the description of multicomponent mixtures as considered below.
Yarborough [75] documented a selection of synthetic mixtures of light condensates with compositions and phase equilibria in a range of conditions of interest to the reservoir engineering community.
In particular and with no prejudice we study mixtures labeled M4 and M8 which are composed of light alkanes up to n-decane with and without toluene.
The overall compositions of the mixtures are given in Table 2.
A system was set up with 10,000 molecules, corresponding to the compositions given in Table 2.
An NVT calculation from an initial well-mixed system, quenched to 366.48K (200°F) into a rather expanded system (11×11×100nm3) provides a two-phase liquid vapour split, from which the surface tension is calculated as before.
Similarly, the vapour pressure is determined from the analysis of the z-component of the pressure tensor.
Other simulation details mirror the conditions used for the methane–decane binary.
Fig.
5 presents a snapshot of an equilibrium condition and an average density profile for each component along the z-axis.
A further analysis over these profiles was used to calculate the average molar fractions in the gas and liquid phases, yi, xi, and their ratio, the distribution factors Ki=yi/xi.
The agreement (see Table 3) between the experimental data and the simulations is very good, considering there are no adjustable parameters.
The equilibrium pressure is calculated as 29.97±0.06bar and compares well with the experimental [75] value of 31.85bar.
The interfacial tension of the mixture at this point is calculated as 13.12±0.46mN/m.
Fig.
5 shows that the light components (C1 to C5) exhibits excess adsorption at the liquid–vapour interface, i.e.
they accumulate at the interface, with the most noticeable adsorption by methane.
The interfacial thickness is seen to be considerable (∼5nm) suggesting that rather large system sizes are needed to include interfacial effects.
The rather elongated length in the z direction, corresponding to 0.1μm, strengthens the idea that for explicit simulations of multicomponent multiphase systems, a speed up in the calculations is needed, in this case resulting from the reduced number of interactions required from the CG model.
The determination of the bubble and dew points of a mixture is a staple of petroleum engineering thermodynamics.
The bubble (dew) point of a mixture is defined as the condition of pressure, P, and temperature, T, where a liquid (vapour) mixture is in equilibrium with an incipient second phase, i.e.
a coexisting vapour (liquid).
In practice, the overall composition of a single phase mixture is specified and either the temperature and pressure boundary at which the second fluid phase becomes stable is the required output.
The results are usually expressed in terms of a P–T diagram where the curves describe said phase boundaries and may include other curves describing other existing phase boundaries, such as fluid–fluid or solid–fluid and/or similar curves at other compositions.
The practical importance of determining the phase envelope of a gas or crude oil cannot be underestimated, as it is crucial knowledge in many aspects of reservoir production and transport.
Experimentally, the bubble (or dew) point is obtained by a slow depressurization (expansion) of a mixture in a pure state, usually employing a mercury displacement pump.
At different points during the process, the volume of the mixture is monitored.
A change in slope of the pressure–volume diagram is indicative of the appearance of a second fluid phase, as the compressibilities of the gas and the liquid are often significantly different.
The phase change point is not normally found experimentally, but rather found by the intersection of two lines fitted to the pure phase and mixed phase compressibilities.
A comprehensive review of the method and its relation with other phase equilibria methods is given in standard textbooks [76–78] and detailed in recent review [79].
In the oil and gas industry, this procedure is a rather standard part of the PVT characterization of a crude, however it is expensive and time consuming.
For mixtures described by an equation of state, this calculation amounts to simultaneously solving the condition of thermal, mechanical and diffusive equilibria (equality of chemical potential) amongst two fluid phases for each component of the mixture.
The analytical nature of this calculation lends itself to a reasonably rapid solution by numerical methods.
In its most common form, the composition and temperature are fixed and the pressures at either the bubble or the dew point are recursively calculated.
The reader is referred to the excellent textbooks that describe the common algorithms employed [80–82].
The quality of the result is obviously limited by the accuracy of the EoS to faithfully represent fluid mixtures.
Furthermore, the fact that some of the more interesting features of the phase diagram are close to the critical points of the mixture, make these calculations particularly challenging for all but the most optimized and force-fitted of models.
From the point of view of performing a canonical (NVT) simulation, the determination of the bubble (or dew) point is far from trivial.
Quenching a one-phase mixture to a temperature at which phase separation occurs (flashing) produces two phases with usually very distinct compositions.
More importantly, the liquid (or vapour) phase composition is an output of the simulation and cannot be fixed a priori.
Some algorithms have been published for the purpose of obtaining the bubble point calculations from simulations using pseudo ensembles [83,84] although they are tailored for Monte Carlo and Gibbs Ensemble-based simulations.
For the bubble point determination of a mixture, we consider a one phase state point similar to the one described above, but at a much higher pressure (a much lower total volume) than that expected for the bubble point.
The precise location of this point is irrelevant to the outcome of the calculation.
In this state we record both the pressure and the density.
We use this state point as the initial condition for an NPzzT simulation, where the pressure, P, coupling is isotropic in the x and y direction, but different in the z direction.
This latter ensemble is useful to achieve different pressure levels (with the longest length of the simulation cell properly oriented in the z-axis) all at constant temperature and overall composition.
The Berendsen thermostat and barostat were selected as the coupling algorithms for the NPzzT simulations.
After equilibration, the system density is recorded and further decompression is applied (the pressure is set at a lower value), mimicking the experimental procedure.
Eventually the system will cross the bubble point and a two phase system will evolve.
A plot of the pressure as a function of the mass density, ρ, for all the equilibrated states shows a kink in the slope, corresponding to a change in the compressibility, (∂P/∂ρ)T, associated with a change in the nature of the phases that compose the system.
As expected, the pure liquid phases have a higher compressibility and a steeper slope.
The bubble point, instead of being simulated is obtained by the intercept of the slopes in the pressure–density diagram (Fig.
6).
The results for mixture M4 are plotted alongside the experimental results [75], the pressure and the bubble point predictions are excellent; 215.1bar which implies a 0.92% error above the experimental value.
The densities seem slightly overpredicted by about 2%; the density at the bubble point is found to be 401.5kgm−3, again, slightly above the experimental value.
Following the above-mentioned methodology, Fig.
7 shows the results of the bubble point determination for mixture M8.
Here, only one experimental point (in red) is reported and is compared with several simulations points and a standard fit using an optimized cubic equation of state; the Peng–Robinson EoS [85] with the Peneloux volume translation [86].
The EoS calculations are based on the use of binary interactions parameters which allow the theory to match, in as much as possible, the available experimental point.
In addition, we plot the prediction from the multi-parameter GERG-2008 reference EoS [87]; an engineering EoS tailored specifically for these type of systems.
Although no clear conclusion can be obtained, as the EoS results are conflicting and there is only one data point to compare to, the trends of the simulations seem in reasonable agreement with the expected results.
The simulations allow for the prediction of the full phase envelope including the retrograde condensation and the low pressure dew point with no mixture adjustable parameter whatsoever; i.e.
it is a full prediction of the experimental curve.
Fig.
8 shows a typical configuration at conditions of an impending appearance of the bubble point.
An interesting observation is even at such conditions is not visually evident that a phase separation will occur.
The emerging vapour phase is predominantly methane (white–gray beads in Fig.
8) and there is no qualitative indication of a nucleating phase or clustering.
Configurations very close, but below the bubble point show roughly the same characteristics.
The bubble point is seen to be a macroscopic property which even at these very large system sizes would be hard to detect directly.
No level of foreseeable technological prowess will suffice to allow the commonplace atomistic modelling of complex crudes.
On the other hand, an appropriate coarse graining model can allow for the quantitative calculation of fluid phase properties of fluid mixtures of interest to the oil and gas sectors.
The correspondence between the theory and simulations makes it possible to use the SAFT-γ EoS to fit the parameters (ɛ, σ, λ) to match experimental data and use these same calculated parameters in a molecular simulation.
This apparently cyclic argument becomes useful when the simulations are employed to gain information otherwise inaccessible from EoS.
The robustness of the force fields allows the predictions of adsorption [88], transport and interfacial properties [89] which are not part of the original fit.
The level of CG described here is different from that understood in the oil and gas industry when approximations are made to reduce the degrees of freedom by considering solvent-free models [90] and effective averaging of potentials [91].
The Mejia et al.
M&M correlations [59] have a real potential for developing models in this field, as they are particularly well suited for calculating intermolecular potentials of effective pseudo-components and similarly undefined fractions, which can then be modelled in classical MD programs without loss of fidelity.
All that is needed as an input is the characterization by means of an acentric factor, a critical temperature and a density.
This is a key aspect of the methodology which can be exploited to model poorly defined crude mixtures.
The SAFT CG simulations are able to predict the phase behaviour of light crude oil mixtures and allow the simulation of reasonably large systems (we have explored elsewhere systems with up to 300,000 particles, corresponding to millions of atoms).
This is enough to observe complex dynamics, including, but not limited to cluster formation and phase segregation.
While we used here a seven-component mixture, there are no real limitations to expand this number.
Similarly, we have spanned several million time steps.
A point to note is that for these coarse grained models, the time scale changes in an unclear way.
In an all-atom simulation, these timesteps have a direct relationship with a well-defined time scale, as they link atomic masses and the distance parameters with time.
In a coarse grained simulation, both the masses and the energy and distance parameters are changed, and each step represents a different “time”.
More crucially, however, by eliminating the details and “roughness” of the molecules, their diffusion and mobility is significantly enhanced.
The molecules explore a larger part of phase space, reaching equilibrium states and overcoming energy barriers much before they would in an atomistic model.
Unfortunately, there is no clear recipe for this scale up [38].
In fact, it has been suggested [92] that the distribution of, for example, characteristic timescales, should correspond to appropriately weighted average of distributions from the different dynamics under consideration.
If one compares the self-diffusion of small molecules, e.g.
alkanes, in a liquid state from both an atomistic and a CG model, one sees [66] a speedup in the latter of at least an order of magnitude.
Using this rough guide, the simulations presented correspond to effective times of up to 10×20ns=0.2μs, which are enough to observe phase separation and clustering of even the most complex systems.
The model presented here corresponds to homonuclear molecular models.
An improvement can clearly be made if one employs the theory to its fullest, and considers heteronuclear models, i.e.
chains made of different type beads.
An expanded version of the theory [93] allows for this to be done, which is most useful when considering complex polyphilic molecules such as surfactants [94,95], transferrable models for paraffins and waxes [66], and larger hetronuclear molecules and will be valuable when extending the model to the heavier fractions, including resins and asphaltenes [96].
Work is under progress in this area.
We thank BP Group Technology's Distributed Research Laboratory for financial support through its molecular computation network, the U.K. Engineering and Physical Sciences Research Council (EPSRC) for financial support through research grants EP/E016340, EP/J014958 and EP/I018212 and the Thomas Young Centre for Theory and Simulation of materials for support through grant TYC-101.
Simulations described herein were performed using the facilities of the Imperial College High Performance Computing Service and the BP High Performance Computer centre.
Bayesian calibration of the constants of the k–ε turbulence model for a CFD model of street canyon flow In this paper we carry out a Bayesian calibration for uncertainty analysis in Computational Fluid Dynamics modelling of urban flows.
Taking the case of airflow in a regular street canyon, and choosing turbulent kinetic energy (TKE) as our quantity of interest, we calibrate 3-D CFD simulations against wind tunnel observations.
We focus our calibration on the model constants contained within the standard RANS k–ε turbulence model and the uncertainties relating to these values.
Thus we are able to narrow down the space of k–ε model constants which provide the best match with experimental data and quantify the uncertainty relating to both the k–ε model constants in the case of street canyon flow and the TKE outputs of the CFD simulation.
Furthermore, we are able to construct a statistical emulator of the CFD model.
Finally, we provide predictions of TKE based on the emulator and the estimated bias between model and observations, accompanied with uncertainties in these predictions.
The use of Computational Fluid Dynamics modelling in the built environment is widespread for indoor applications; however the successful and systematic application of CFD for the outdoor environment is still hindered by the prohibitive cost of field measurements and lack of validation of turbulent characteristics and boundary conditions for a variety of urban atmospheric boundary layer flows.
The problem is exacerbated further by the geometric complexity of the built environment and the difficulties in modelling flow around arrays of building obstacles.
An in-depth understanding of the airflow processes within street canyons is important to fully understand the pollutant dispersion within these spaces as well as issues relating to pedestrian comfort and energy use.
CFD has had a larger role to play in modelling this process in recent years as computer power and quality of commercial software have increased.
Built environment researchers and practitioners commonly use commercial CFD software for their simulations and employ default settings following simple guidelines.
Thus the k–ε model is a popular choice as it is simple and fast to run.
However it has not been extensively validated for urban boundary layers and is not entirely suitable in regions of high streamline curvature.
By understanding and quantifying some of the uncertainties relating to these CFD models of the outdoor environment, we can move towards improved modelling guidelines and better models for urban planners and street designers.
Better models will ultimately help to assess and mitigate risk to human health from sources such as pollution and the release of toxic gases, in order to inform policy and decision making in the urban environment, which now houses over half of the global population.
Here we follow the full Bayesian calibration framework of Kennedy and O’Hagan  [1], which involves representing bias and computer model outputs as Gaussian processes, to investigate how the sophisticated tuning of the empirical constants contained within the chosen turbulence model can improve the prediction of the turbulent quantities within a regular street canyon.
Thus, we attempt a calibration of the constants contained in the Standard k–ε turbulence model for the particular case study of flow over a symmetrical, homogeneous street canyon at aspect ratio (height to width of the street) of H/W=1 and a Reynolds number of 5×104.
ANSYS CFX is used to create the CFD model, which is then calibrated against measurements of turbulent kinetic energy, TKE, obtained in wind tunnel laboratory experiments  [2].
It was decided to calibrate against the TKE outputs due to the noted difficulties the k–ε model has at predicting TKE levels within street canyons.
The accurate prediction of these values is particularly important for modelling pollution dispersion.
Under low wind conditions the turbulence levels can have a large impact on how the pollutants are distributed throughout the street.
This calibration process allows us to: (1) evaluate a small systematic bias of the CFD model; (2) narrow down the set of parameter values that provides the best match between the CFD model outputs and the observations; (3) construct a fast statistical surrogate (also called emulator) of the CFD model; (4) use the emulator to quantify the uncertainty of the turbulent kinetic energy outputs resulting from both uncertainties in the CFD parameterization, the numerical code itself and measurement errors: this propagation of uncertainties would be an extremely computationally demanding task without emulation.
This paper starts with a discussion on uncertainty quantification and model verification for CFD models.
Then we give a brief introduction to the standard k–ε model followed by a summary of its use for urban atmospheric boundary layer flow.
A description of the experimental and CFD set up is then given, for this particular case study of a symmetrical street canyon, followed by an explanation of the Bayesian Calibration process.
Finally, we discuss the results and the conclusions gained from them.
There are several sources of uncertainty present within CFD models.
These can come from the model inputs, the form of the model, the assumptions made in the mathematical model, and numerical approximation errors which are due to the differential equations being approximated rather than solved explicitly (an example of this would be discretization error).
Different techniques can be applied to tackle each type of uncertainty.
Model verification deals with the uncertainties relating to numerical approximation.
It is a way of providing evidence that the computer model accurately represents the conceptual model of the system.
Model validation is used to ensure the computer model is an accurate representation of the physical system of interest and deals with the uncertainty related to model form.
This involves comparing the CFD output against experimental data.
A full description of these types of uncertainties and statistical methods for quantifying them can be found in Roy and Oberkampf  [3].
Uncertainty quantification in CFD and in numerical models in general can be done using a variety of statistical methods.
One such method, Polynomial Chaos, is discussed in detail in Najm  [4] with examples of applications of this technique to different types of flows such as incompressible and reacting flows.
Le Maitre et al.
[5] applied this technique to incompressible flow in a micro channel with low Reynolds Number.
Although uncertainty quantification is an important topic within CFD particularly within the field of turbulence models, it is not routinely addressed in CFD-based studies.
However there are a number of studies that have demonstrated different statistical techniques to address the issue of uncertainty within RANS turbulence models.
One such study conducted by Dunn et al.
[6] looked at the uncertainty contained within the model coefficients of the k–ε model.
They used the Latin Hypercube Sampling method rather than Bayesian Calibration techniques to investigate these uncertainties in the case of flow over a backward step and found that model coefficients had a significant effect on stream wise mean velocity, turbulence intensity, reattachment point location, pressure and wall shear stress.
An example of Bayesian Calibration of RANS turbulence models is given by Oliver and Moser  [7].
The Bayesian approach was used to analyse both model and parameter uncertainty within several types of turbulence models and was then applied to the case of incompressible channel flow.
Oliver and Moser  [7] were able to make a comparison between different uncertainty quantification models as well as different turbulence models and found both to impact the quantity of interest (QoI’s).
They found that after calibration the Chien k–ε model gave a prediction of the QoI’s with an uncertainty of approximately ±4%.
Another example of a study which used Bayesian Calibration to assess uncertainty within RANS models is Cheung et al.
[8].
The Spalart–Allmaras turbulence model was used to model the case of an incompressible boundary layer flow.
Seven constants within the SA model were calibrated against experimental data, and predictions of the quantities of interest were produced.
In this work the plausibility of competing models for uncertainty evaluation was investigated.
Edeling et al.
[9] recently calibrated four constants in the Launder–Sharma k–ε model for 13 wall-bounded turbulent boundary layer flows at a variety of pressure gradients.
They analysed uncertainties relating to the k–ε model constants and their propagation throughout the model to the QoI of velocity profiles.
The 13 separate Bayesian calibrations were carried out on the boundary layer velocity profiles generated by a specialized 2-D, compressible boundary-layer approximation mode applied to flow over a curved aerofoil-shaped surface.
In our study the Standard k–ε turbulence model will be applied for the particular case study of flow over a symmetrical, homogeneous street canyon at aspect ratio (height to width of the street) of H/W=1 and a Reynolds number of 5×104.
We will be focusing on the uncertainties relating to the model inputs.
In the scenario of an urban boundary layer flow, these are a large source of uncertainty, as the standard k–ε model has not been extensively validated for this complex case.
Although these uncertainties are well acknowledged among the community of CFD practitioners, it is not a standard procedure to account for them in either the model set up or when presenting the output of the CFD model.
Here we will look at how we can address such uncertainties and those relating to the k–ε model constants and how they can be propagated throughout the model to the quantities of interest (QoI’s), which are mainly the TKE profiles within the street canyon.
Cheung et al.
[8], having made the first step in their calibration of a CFD model, highlighted for future research the areas of comparative uncertainty analysis of RANS turbulence models, as well as the use of statistical emulators to save computing time on computationally expensive CFD models.
We construct here such an emulator of the RANS model.
Turbulent flow is characterized by random fluctuations of velocity.
It is possible to model turbulent flow within CFD without any adjustments to the Navier–Stokes equations.
This type of simulation is known as direct numerical simulation (DNS) and is prohibitively computationally expensive.
Turbulence models used within CFD simulations enable the capture of the main features of the flow without having to explicitly model all the details of the turbulence, thus saving on computer costs.
Large eddy simulations (LES) save computing time compared with DNS by explicitly resolving only the large and most important turbulent eddies in the flow and approximating the smaller scale turbulence.
However this technique is either still out of reach for simulation of the outdoor environment, or when computational power is available, is very difficult to tune, which may make it imprecise, see Blocken et al.
[10] and references therein.
We can reduce the amount of computing power needed by focusing on the mean properties of the flow.
This results in the Reynolds-Averaged Navier–Stokes (RANS) equations.
These equations contain correlations of the fluctuating velocity components ui′uj′¯ which are known as the Reynolds stresses.
The turbulence model is a way of closing the RANS equations by approximating the Reynolds stress.
The Standard k–ε turbulence model is a popular choice of RANS model.
The formulation of this model is as follows: The Reynolds stresses ui′uj′¯ are related to the shear stress of the flow τij by the following equation: (1)τij=−ρui′uj′¯ where ρ is the density of the fluid.
We can find the value for τ, and hence ui′uj′¯, from the following equation: (2)τij=μt(∂ūi∂xj+∂ūj∂xi)−23ρkδij where μt is the turbulent viscosity.
In the case of the standard k–ε turbulence model, turbulent viscosity is defined as (3)μt=ρCμk2ε where Cμ is a model constant  [11].
By solving the following differential equations for the turbulent kinetic energy, k, and the turbulent dissipation, ε we can find a value for μt   [11]: (4)D(ρk)Dt=∂∂xj((μ+μtσk)∂k∂xj)+τij∂ūi∂xj−ρε(5)D(ρε)Dt=∂∂xj((μ+μtσε)∂ε∂xj)+Cε1εkτij∂ūi∂xj−Cε2ρε2k where σk, σε, Cε1, Cε2 and Cμ are all empirical model constants.
The default values for these constants in most commercial CFD softwares, including ANSYS CFX tested here, are shown in Table 1.
These values follow the original formulation of Launder and Spalding  [12], who obtained them through data fitting for a wide range of flows.
When using CFD to simulate flow within the built environment it is important to generate the correct boundary layer flow at the area of interest.
A common problem of creating a neutral equilibrium atmospheric boundary layer is an acceleration in the velocity at ground level and a decay in turbulence profile as we move downstream.
This occurs when the boundary conditions, the turbulence model and its associated constants are not consistent with each other  [13], for example when the velocity, turbulent kinetic energy and turbulent dissipation profiles specified at the inlet do not satisfy the transportation equations of the Standard k–ε model.
The most common method to deal with this is to derive the inlet profiles for turbulent kinetic energy, k, and turbulent dissipation rate, ε, directly from the k–ε model formulation.
This was the approach of Richards and Hoxey  [14] who recommended the following formulation for the inlet profiles, which is consistent with the Standard k–ε model: 1.Inlet Velocity Profile: (6)u=u∗κln(z+z0z0) where u∗ is the frictional velocity, κ is the von Karman constant and z0 is the aerodynamic roughness length.
Turbulent Kinetic Energy profile: (7)k=u∗2Cμ.
Turbulent Dissipation profile: (8)ε=u∗3κ(z+z0).
The Standard k–ε Model does not just under-predict TKE levels for boundary layer flows but also has a tendency to under-predict TKE within street canyon flows.
Solazzo  [15] found that reducing the values for parameters σk and σε from the default values resulted in an improved prediction of TKE within the street canyon due to an increased spread of TKE in the shear layer above the street canyon.
It is clear from previous research that alteration of the k–ε constants in some instances can lead to an improvement in predictions of TKE within street canyon and urban boundary layer flows.
There is a need to better understand the uncertainties relating to these constants and find a way of reducing the range of possible values they could take.
This provides the motivation for this paper, which will perform a Bayesian calibration of four out of the five Standard k–ε constants for the case of street canyon flow.
Due to the interconnected nature of the boundary conditions, turbulence model and the constants it is important to note that by calibrating the constants we are examining the uncertainties connected to both the turbulence model and the boundary conditions at the inlet.
The data used to calibrate the CFD model were Turbulent Kinetic Energy values taken from a wind tunnel experiment carried out by Kastner-Klein et al.
[2] in the wind tunnel at the University of Karlsruhe.
The buildings on either side of the street were represented by two rectangular blocks.
The wind direction was perpendicular to the street axis.
The model of an atmospheric boundary layer was obtained through the use of small blocks placed on the wind tunnel floor.
Measurements were taken using a laser velocimetry system.
Velocity measurements were taken on a vertical cross section of the street at the centre of the street canyon.
Fig.
1 shows a cross section of the street canyon and the measurement locations within the street.
A fully 3-dimensional CFD simulation of the Kastner-Klein wind tunnel experiment  [2] was carried out using ANSYS CFX.
The dimensions of this model are given in Fig.
2.
An unstructured hexahedral mesh was used for this study.
A mesh refinement zone was created in between and around the buildings allowing for the capture of the details in this region.
In order to ensure only a small discretization error within the CFD model, a mesh sensitivity test was carried out for the street domain.
The mesh inside the street canyon was systematically varied until a mesh independent solution was reached.
We tested recommendations in best practice guidelines for urban CFD simulations, that a mesh size of at least H/10 (where H is the height of the building) should be used if the correct flow features are to be captured surrounding the building  [16].
Therefore the maximum cell size close to the ground and within the area of interest was taken to be 0.012 m. Five mesh sizes were tested.
The different mesh sizes are described in Table 2.
The minimum mesh size is the mesh size within the area of interest, which is here the centre portion of the street canyon.
The mesh outside this region is increased with an expansion ratio of 1.2 to a maximum size of 0.03 m. Velocity, TKE and dissipation profiles were taken from within the centre of the street canyon for all five meshes and are compared in Figs.
3–5.
The results show a significant sensitivity of TKE to mesh size.
At the roof top level reducing the mesh size from 0.012 m (H/10) to 0.003 m (H/30) results in 70% increase in TKE.
It appears that the mesh within the canyon and immediately above the canyon needs to be fine enough to capture the production of TKE at the leading edge of the upwind building, without this, gross underestimation of turbulence levels may occur.
The often quoted guideline of a cell size of H/10 within and immediately above the canyon is reasonable if only a qualitative prediction of flow patterns is needed.
If accurate predictions of turbulence quantities are required, then a much finer mesh is recommended.
The software code CFX places certain restrictions on the size of the mesh closest to the wall based on the size of the sand grain roughness used, therefore it would not be possible to reduce the mesh size beyond mesh size 5 without an alteration to the sand grain roughness height.
There was little to no difference in velocity profiles between meshes 4 and 5 and a maximum difference of 5% was found for the TKE values.
Therefore it was decided that mesh 4 could provide a reasonable level of accuracy for this study for a computationally reasonable cost.
To specify the boundary conditions at the inlet, Eqs.
(6)–(8) following Richards and Hoxey  [14] were chosen, due to the fact they are consistent with the k–ε model and easy to adapt to other situations.
The outlet boundary was set to outflow with static pressure 0 Pa.
The top of the domain boundary was set to a wall boundary with a specified shear stress of τ=0.36Pa, based on the wind tunnel data measurements, which determined a value of 0.55 m/s for the frictional velocity u∗.
In order to maintain a homogeneous atmospheric boundary layer flow, a sand grain roughness was applied to the wall boundaries in CFX.
This had to match the roughness blocks that had been placed on the floor of the experimental wind tunnel.
The recommended sand grain roughness in the literature  [9] for other cases is 30 times the roughness length which would give a value of 0.024 m for this experiment.
However, CFX specifies that a cell height of twice the size of the sand grain roughness must be used at the wall boundary; this would have resulted in a coarse mesh close to the wall, thus on the basis of the mesh sensitivity analysis of the empty domain, a sand grain roughness of 0.006 m was chosen.
It was then necessary to use a wall function to model the flow in the region closest to the wall.
Within this region the law of the wall was applied: (10)u+=1kln(y+)+C where u+ and y+ are dimensionless variables given by: (11)u+=Utu∗(12)y+=u∗Δyv with Ut the known tangential velocity at a distance Δy from the wall, u∗ the frictional velocity in the wall region and v the kinematic viscosity.
This formula was applied to the region containing the street geometry.
Outside this region the formula must be adapted to account for roughness, which causes turbulence in the shear layer and shifts the logarithmic profile downwards.
Thus, the law of the wall is modified in the following way: (13)u+=1kln(y+)+B−ΔB where B=5.2.
The downward shift ΔB can be expressed as: (14)ΔB=1kln(1+0.3hs+) where hs+ is defined as (15)hs+=huτv.
The calibration process consists of putting distributional assumptions (prior distributions or simply priors) on the calibration (also called tuning) parameters Cε1,Cε2,σk and Cμ before comparing with observations, and letting the information contained in the data update this a priori assumption to get as a result a posterior distribution of the calibration parameters.
The advantage of such a Bayesian analysis  [1] over standard estimation of parameters (e.g.
by minimizing the differences between observations and simulator outputs) lies mainly in the ability to retrieve a full description of the uncertainties about the parameters and consequently about the simulator outputs.
Moreover, the possibility for the modellers to express their–uncertain–scientific beliefs in terms of priors on the parameters enables a natural integration of scientific knowledge and evidence given by measurements.
It was decided to calibrate four out of the five k–ε model constants and not σε due to its strong interdependence on the other model constants.
Formula (9) was used to set the value of σε.
For simplicity we now denote Cε1 and Cε2 as C1 and C2.
The intervals explored for the calibration constants were chosen based on typical values suggested for the model constants and how these had been changed in the past, see Table 1.
We put uniform priors on these parameters, allowing for equal initial probability of being at any location in these intervals.
The convergence criteria for the CFD simulations was set to 1×10−4 with an imbalance of less than 0.1% as well as velocity and turbulence being monitored at several points to ensure stable conditions were reached.
We performed a total of 150 CFD runs.
Of these 150 runs 135 converged successfully.
Fig.
6 illustrates which of the calibration parameters did not achieve convergence.
It appears that these failures occur when C1 is relatively close to C2 and thus leads to numerical stability issues in (9) as σε becomes too close to zero.
C2>C1, which is not allowed by our choice of prior, would make σε negative and provoke an even more serious numerical problem.
One could have optimized better the design for these CFD runs, described in the following section, by not allowing close values of C2 and C1 and thus saved some computational runtime.
The complete set of inputs x=(h,θ) consists of parameters divided into two categories: the known parameters (normalized height h in [0,2]) and the unknown calibration parameters θ=(C1,C2,Cμ,σk).
We denote by yM(x) the empirical output of the computer model as a function of x=(h,θ) and η(x) the expected output of the computer model as a function of x=(h,θ).
The difference between yM(x) and η(x) is the numerical intrinsic error.
The computer code output η(x) is an approximation of the reality yR(h).
The notation used emphasizes that physical observations are only made at values of the observable parameter, h. To learn about the values of the calibration parameters, the CFD model is run at inputs x in a design (i.e.
choice of values) DM.
Field data (i.e.
TKE observations taken from the wind tunnel) yF(h) are collected at a number of inputs heights h. The design DF (the locations at which observations are collected) are given by Kastner-Klein et al.
[2].
For our design of experiments DM corresponding to the calibration parameters, we use a maximin Latin Hypercube (LHS) design  [17], whose 1-D and 2-D projections are seen in Fig.
6.
With this design we try to cover as much space as possible in the four-dimensional space of the calibration parameters with n=135 runs (after excluding the runs that did not converge).
For the spatial coordinates of the computer design DM, we choose the same locations as the experimental design.
Fig.
7 shows the CFD computed TKE values at these heights.
This is the first step in our study.
The following equations constitute an extension of Kennedy and O’Hagan  [1] as they specify the intrinsic CFD model numerical error.
They describe the relationships between the CFD model and the observations at the design points, using bias δ(h), intrinsic CFD model numerical error νη and observation error ν (both assumed constant across heights h): (16)yM(h,θ)=η(h,θ)+νη(17)yR(h)=η(h,θ∗)+δ(h)(18)yF(h)=yR(h)+ν.
Here, θ∗ is used to represent the true (unknown) values of the calibration parameters.
These equations suggest that even if the CFD model were run at the true values of the calibration parameters, it would still be a biased representation of reality.
Thus the model can never perfectly match observations without some additional process of adjusting for model errors.
It is crucial that such a discrepancy (or bias) term be introduced in order to avoid overfitting the data with an unrealistic choice of parameters.
Furthermore, the bias must vary according to height, as the CFD model shortcomings are strongly dependent on height.
This is unnecessary for the observation error and the numerical error in the CFD model as these may vary only very slightly according to height and are much smaller than the model bias.
Because the simulator output η(⋅) is unknown except at the design points DM, we assume that the unknown function follows a Gaussian stochastic process (GP) distribution.
That is, we model the simulator responses η(x),x∈Rp (here p=5 since DM is over a range of C1, C2, Cμ, σk and h, values), as coming from a multivariate normal distribution with mean μ and N×N variance–covariance function Σ conditional on the observed outputs from the N runs.
Since we initially standardize the entire set of responses (CFD model and observed) by the mean and standard deviation of the CFD responses, μ=0 above and the variability in the simulator (1/λη) below is approximately 1.
Thus, we approximate the CFD model by specifying a distribution of functions that interpolate the response η(x) in between the design points x in DM.
The random function is certain at the design points, and uncertain at untried points.
To specify Σ according to the calibration parameters we use a product Gaussian variance–covariance.
Thus, the (i,j)th element of Σ, cov(η(xi),η(xj)), is (conveniently using the notation θ4 for h): 1ληexp(−∑k=14βk|θik−θjk|2).
The notation θik denotes the ith design point in DM for θk.
The hyperparameters λη (the precision of the GP model), βk (which we call “correlation hyperparameters”) are to be estimated from the model output and the observations as described below.
The unknown bias function δ(h) is also modelled as a GP random function with mean 0 and correlation matrix with precision λδ and correlation parameter β5.
Finally, the random error and intrinsic error components are modelled as independent ν∼N(0,1/λν) and νη∼N(0,1/λνη).
Only the θ’s are deemed to be calibration parameters, or quantities to be tuned for the CFD model to perform better.
All the other quantities, such as observation and numerical errors, as well as the hyperparameters, are only auxiliary quantities for the analysis (but are of interest nevertheless).
The likelihood can be written, for a joint vector of given observations yF and CFD model outputs yM, (19)[yFyM]∼MVN(0,Σy) where (20)Σy=Ση+[Σb000]+[Σν00Σνη.].
For the estimation of the calibration parameters, and all the other quantities, we employ the Markov Chain Monte Carlo (MCMC) algorithm, see e.g.
Chib and Greenberg  [18].
The code we are using is publicly available as supplementary material from Han et al.
[19].
The chains are dependent random samples that ought to be distributed in the long run as the so-called posterior distributions of the parameters of interest, which are combinations of prior information about the values of these parameters and the information about the parameters provided by the measurements.
We then retrieve the posterior distributions of the various calibration parameters, which allow us to make inferences and quantify our uncertainty about the true values of these unknown quantities.
All the unknowns in the model (i.e.
the calibration parameters and the hyperparameters such as variances–here actually precisions–and correlation parameters β’s) require specified prior distributions which represent uncertainty about the values of these parameters before any data is collected.
The following choices are made for the priors: •To represent vague prior information about the true calibration parameter values, we specify a uniform prior distribution over the interval on which they were sampled for simulator runs.
To model the correlation hyperparameters in Σ, we conservatively place most of its prior mass on values for the corresponding correlations near 1 (indicating an insignificant effect).
Similarly, conservative priors were used for the hyperparameters associated with the correlations in the bias function.
Gamma prior distributions were used for each of the precision (i.e.
inverse of the variance) hyperparameters λη, λδ and λν.
Specifically, we use priors λη∼GAM(10,10) (with expectation 1 due to standardization of the responses), λδ∼GAM(10,.3) (with expectation around 3% of standard deviation of the standardized responses), and λν∼GAM(10,.03) 0.3% and λνη∼GAM(10,.001) 0.01%.
These prior assumptions are based on our original scientific understanding of the problem: we want the CFD model to capture most of the variations in order to tune our calibration parameters, and the bias ought to be small as a result.
Observation errors must be an order of magnitude smaller, and numerical errors are usually more than an order of magnitude smaller than observation errors.
Obviously data, through our Bayesian analysis, can sway these priors, as it does for the bias that is much larger than 3% of the TKE variations in the CFD in the lower part of the street canyon (see Fig.
10 in the next section).
The results of all 135 CFD runs are shown in Fig.
7.
The normalized TKE values are plotted against the normalized height.
We can see the wide spread in TKE results produced by varying the k–ε model constants.
This figure describes the sensitivity of the model to the values used for the calibration parameters.
It highlights the importance of choosing the most appropriate values for these constants and the need for calibration.
The posterior for the numerical errors and observation errors, are respectively small.
Figs.
8 and 9 respectively show the hyperparameters λν and λνη.
Their inverses are respectively smaller than 1/28 and 1/1100 compared to TKE’s variations (normalized to 1).
Hence, they are unlikely to have a big impact on the calibration results.
Fig.
10 displays the histograms for the posterior draws for all four calibration parameters.
From the distribution of the histograms we can infer information about the preferred values for each parameter and uncertainty related to that parameter.
The posterior for Cμ shows values at the lower end of the interval are given higher probability of providing a better match with the experimental data.
In particular we can see that the default value of 0.09 is not deemed a suitable value in this context, as there is low probability that this would be the best fit.
As noted earlier, using the boundary conditions suggested by Richards and Hoxey  [14] the TKE value at the inlet tends to be under-predicted when using the default value of Cμ.
To avoid this under-prediction, researchers in the past have decreased the values of Cμ thereby increasing the TKE value (see Eq.
(7)).
This is reflected by the posterior distribution.
The posterior for parameter σk is highly skewed towards the lower values of the interval which suggests the chosen interval of 0.5–2.5 was not wide enough.
The posterior shows values closer to 0.5 are more favourable.
This is consistent with findings of Solazzo  [15], who changed the values for parameters σk and σε for a similar street canyon configuration and found that decreasing the value of σk from the default value of 1 to a value of 0.53 had the effect of spreading the width of the TKE profile in the shear layer.
This lead to a greater proportion of the TKE being advected down into the street canyon, reducing the under-estimation of TKE within this area.
The preferred values we find for C1 and C2 are similar to the default values suggested by Launder and Spalding  [12].
By plotting the joint posterior distributions we can see how the probability distributions of the parameters are connected (see Fig.
10).
From these figures we can see how much the space of plausible model constants is reduced through the process of the calibration.
In order to utilize this result a suggestion would be to take a sample from this posterior distribution of parameter combinations and run the CFD models again with these new calibration values.
Thus rather than giving a single TKE output at each location we would have a range of TKE values that would better represent the uncertainty present within the CFD model and by sampling from the posterior we are able to narrow down the uncertainty of our output.
The final stage in the calibration process is to produce an emulator of the CFD model.
This extracts all the information contained within the CFD runs regarding the calibration parameters and their uncertainties and uses it to produce predictions of the TKE values at specified heights.
The mean posterior bias can be seen in Fig.
8, as the difference between the mean posterior for the CFD model, i.e.
the emulator, and the mean posterior for the predictions, i.e.
emulator + bias.
As expected typically the bias is slightly larger within the street canyon (normalized heights less than 1) due to the CFD model’s tendency to under-predict the TKE in this area as is illustrated by Fig.
4.
This would suggest that accurate prediction of the TKE values within the street canyon cannot be obtained purely by tuning of the k–ε model constants.
The bands about the predictions (shown by the red dashed line in Fig.
11) reflect all sources of uncertainties: in the unknown calibration parameters, in the computer model GP parameters (variance and correlation length), in the bias’ GP parameters (variance and correlation length), and in the errors (numerical and observation).
The bands are wider where more uncertainty can arise, especially at the upper and lower boundaries of the canyon, where no information is available on the other side to constrain the uncertainties further.
We expect to observe more or less 95% of the observations to lie in these uncertainty bands based on this calibration: a formal way to verify this would be to carry out a leave-one-out diagnostic where we would repeatedly remove one observation from the analysis and confirm that, 95% of the time, the new prediction would include this value.
This has been done for this calibration code in other modelling situations for a thermosphere–ionosphere electrodynamics general circulation model  [20] and for tsunami simulations  [21].
Overall, the plot in Fig.
11 (except for the boundary issue) seems to reflect that the confidence bands across height are reasonable.
Hence, we chose to not do a leave-one-out analysis here.
The value of the von Karman constant κ may not be considered known or universal, see McKeon et al.
[22], Oliver and Moser  [23], and Edeling et al.
[9].
Hence, we carried out another calibration using the exact same conditions as in the previous calibration with four parameters, but with the addition of a fifth parameter κ.
A uniform distribution over the interval [0.38,0.45] was chosen as prior distribution for κ.
We collected n=150 runs of the CFD model based on a maximin Latin Hypercube (LHS) design.
Fig.
14 shows the failed convergence of the sample paths for κ and for σk (the one parameter which converges and changed value in the main successful calibration).
Each includes 3 independent Markov chains for this investigation, and required around 3 days of CPU time on a 4 core machine as the complexity of the calculations increases dramatically with the size of the problem.
The calibration fails to calibrate κ as well as the other parameters.
Indeed, by adding the extra parameter κ, the calibration becomes a much more difficult exercise.
This is due to the fact that there is too little information in the observations to tune κ.
Overall the failure of the calibration of the set of parameters can be explained in two ways.
Either κ adds another level of uncertainty to the analysis or the added computational complexity does not allow the calibration to succeed (but might with more observations and model runs).
Our prior interval [0.38,0.45] is already narrow compared to Edeling et al.
[9] who let κ vary within [0.30,0.60] and obtained wide ranges of calibrated κ values fully covering the [0.30,0.50] range.
One could potentially resolve the convergence problem by constructing a more informative prior (e.g.
a narrower range or a sharper distribution for κ within [0.38,0.45]).
But then, the range would be too small to allow enough sensitivity in model outputs in order to make use of the information contained in the observations; the prior would overwhelm the signal from the data.
Both our attempt and the ones from Edeling et al.
[9] seemingly point to a need for much more precise and helpful observations, combined with more runs for many values of κ to confidently infer appropriate values of κ.
The results presented here are found to apply very specifically to the case study under examination, and cannot readily be applied to other fluid flow models, or even to other models of street canyon flows.
We calibrate the constants for the specific case being examined.
The nature of the general k–ε model constants first found by Launder and Spalding  [12] is that they were determined on the basis of being the best fit for a wide range of fluid flows.
This makes them a suitable first step in setting up a k–ε model, but when higher accuracy is desired they are found to be inadequate for many case studies and have to be modified; there is often no argument put forward for the modification and they are simply chosen on the basis of providing the best fit to a data set.
By performing a bespoke, sophisticated calibration for a street canyon model we obtain parameters that are the best possible fit for this case.
Inevitably, this will make them less transferable to other types of flows.
This is seen quite convincingly in examining the flow velocities.
For the case presented above, the TKE calibration resulted in improved representation of TKE profiles within the street canyon, but the impact on flow velocities was minimal and in fact the modelled vortex flow within the street both before and after calibration was just as expected from experimental results.
However, in our previous research  [24], a fast, 2.5 dimensional simulation was performed using ANSYS CFX.
Logarithmic profiles were specified for the inlet boundary conditions on the basis of providing a best fit to experimental data, following Kastner-Klein et al.
[2], Di-Sabatino et al.
[25].
The CFD model was then run and calibrated for only 3 parameters: C1,C2,Cμ.
Based on the posterior histograms, values of C1=1, C2=2.1, Cμ=0.12 were selected as providing the most improved model performance.
Thus, using the relationships mentioned above, the values of  σε=0.42 and σk=0.462 were found (some of these values are significantly different from those from our model presented above).
The CFD model was run again with the new choice of calibration parameters.
The results for the modified CFD model, as shown in Glover et al.
[24], are presented here again for comparison with the model presented here.
Qualitative comparison of the CFD flow patterns against the wind tunnel observations shows the improvement produced by the modification of the parameters.
The size of the flow separation and vortex produced above the upstream building in that study was reduced (Fig.
12), and the centre of vortex contained within the street canyon moved down towards the centre of the street.
A quantitative comparison is shown in Fig.
13 which shows the TKE profiles taken from the centre of the street canyon, demonstrating improved TKE predictions after modifying the CFD model.
These results are in line with results from other research (e.g.
Dunn et al.
[6]) that found that model coefficients had a significant effect on stream wise mean velocity, turbulence intensity, and the location of the reattachment point.
As discussed in Section  1.4, a lack of consistency between the inlet boundary conditions, the turbulence model chosen and its constants, creates problems of decay in the turbulence profiles downstream and makes it difficult to generate the correct Atmospheric boundary layer flow at the area of interest in the numerical domain.
In Glover et al.
[24] these modelling problems were in effect overcome by the calibration, which caused a greater improvement to the results of the model than the calibration presented here, because the model was not as tightly specified.
Thus, it becomes clear that we create here a bespoke calibration, producing case specific constants.
The results can be transferred to other similar case studies on the basis that we provide an estimate of upper and lower bounds for the values that these constants could reasonably be assumed to take for models of similar flows.
It would be of interest to attempt a series of case studies with slightly varying geometry and test how different the calibrated constants would be for the various cases.
This is beyond the scope of the present paper, due to the high computational efforts required in simulation of a full Atmospheric boundary layer flow, and is recommended for future research.
In this paper we demonstrated how Bayesian calibration can be used to assess the uncertainties relating to the constants contained within the standard k–ε model, when used to model street canyon flow.
We have demonstrated that there is a bias in the k-ε model which leads to the under-prediction of TKE within the street canyon.
We have shown that the default values for these constants often used in simulations do not necessarily provide the best match with experimental data.
We have also seen that purely tuning these constants cannot provide a perfect match with experimental data.
Posterior distributions showed lower values for Cμ and σk are more likely to give improved prediction of TKE when compared with experimental data.
We have also suggested some useful applications for the Bayesian calibration process for CFD simulation for various types of flows, even complicated ones with complex boundary conditions and large uncertainties.
For flows with limited experimental data or various physical unknowns, by performing the calibration we are able to sample from a smaller parameter space thus reduce the uncertainty in the CFD output that arises from our lack of knowledge about the parameterization.
We highlighted the ability of the calibration of TKE to improve flows even with large errors, such as when boundary conditions are not entirely consistent for the required flow, so that the use of calibration for the observations of one variable can trickle down to other variables.
We finally showed with the additional calibration of κ that one needs to be careful in the selection of parameters to be tuned.
Only the parameters that are likely to influence outputs should be used, and an iterative procedure might be used to screen the most important ones.
Moreover, the emulation enables this sampling procedure to be employed in order to efficiently and accurately quantify the uncertainties in the CFD output.
Our procedure does not address the issue of how parameterizations can vary for different flow types.
However, Edeling et al.
[9] carried out separate calibrations for a set of 13 boundary-layer flows.
They summarized this information across calibrations by computing Highest Posterior-Density (HPD) intervals, and subsequently represent the total solution uncertainty with a probability-box (p-box).
This p-box represents both parameter variability across flows, and epistemic uncertainty within each calibration.
A prediction of a new boundary-layer flow is made with uncertainty bars generated from this uncertainty information, and the resulting error estimate is shown to be consistent with measurement data.
This approach is helpful, but it might be extended further by modelling proximity across flows through a distance that would relate to the flow characteristics in order to borrow strength across calibrations instead of splitting the calibrations and then merging the outcomes afterwards.
This is a challenging but attractive venue for future research.
Here we have laid out the first step in our calibration process, but there are still many important questions that need to be answered.
For example the Bayesian calibration could be used on several slightly different turbulence models for a particular flow, to give quantitative estimates of uncertainty to aid in the selection of the best model for the case.
Unfortunately due to restraints on computer resources we were only able to calibrate a single 1-D profile with a single QoI but, with ever increasing computing capabilities and statistical approximations, calibrations which use experimental data taken from two and even three-dimensional space and calibrate against multiple QoI’s are not far out of reach.
However the calibration process is likely to be most useful for applications where experimental data is sparse.
The question of whether we are able to apply the calibration results to other locations within our model, other quantities of interest which we did not calibrate against (and even different flow scenarios as well) is still open ended.
This question is an important topic for future research for the use of calibration in CFD applications.
We would like to acknowledge EPSRC for funding this research as part of the Bridging the Gaps program.
The UC2−x – Carbon eutectic: A laser heating study The UC2−x – carbon eutectic has been studied by laser heating and fast multi-wavelength pyrometry under inert atmosphere.The study has been carried out on three compositions, two of which close to the phase boundary of the UC2−x – C miscibility gap (with C/U atomic ratios 2 and 2.1), and one, more crucial, with a large excess of carbon (C/U = 2.82).
The first two compositions were synthesised by arc-melting.
This synthesis method could not be applied to the last composition, which was therefore completed directly by laser irradiation.
The U – C – O composition of the samples was checked by using a combustion method in an ELTRA® analyser.
The eutectic temperature, established to be 2737 K ± 20 K, was used as a radiance reference together with the cubic – tetragonal (α → β) solid state transition, fixed at 2050 K ± 20 K. The normal spectral emissivity of the carbon-richer compounds increases up to 0.7, whereas the value 0.53 was established for pure hypostoichiometric uranium dicarbide at the limit of the eutectic region.
This increase is analysed in the light of the demixing of excess carbon, and used for the determination of the liquidus temperature (3220 K ± 50 K for UC2.82).
Due to fast solid state diffusion, also fostered by the cubic – tetragonal transition, no obvious signs of a lamellar eutectic structure could be observed after quenching to room temperature.
The eutectic surface C/UC2−x composition could be qualitatively, but consistently, followed during the cooling process with the help of the recorded radiance spectra.
Whereas the external liquid surface is almost entirely constituted by uranium dicarbide, it gets rapidly enriched in demixed carbon upon freezing.
Demixed carbon seems to quickly migrate towards the inner bulk during further cooling.
At the α → β transition, uranium dicarbide covers again the almost entire external surface.
If the higher fissile density constitutes a big advantage of actinide carbides as an alternative nuclear fuel to oxides, the uncertainties mostly linked to metastability and uncontrollable oxygen and nitrogen impurities still represent an obstacle to the fabrication and employment of these materials [1].
Moreover, the metallic thermal conductivity and high melting temperature of actinide carbides ensures a higher conductivity integral margin to melting for these materials with respect to the traditional UO2, UO2–PuO2 and ThO2 fuels.
This feature, together with the better compatibility with liquid metal coolants of carbides compared to oxides, are further reasons making of them good alternative candidates for high burnup and/or high temperature nuclear fuel.
Uranium carbide was traditionally used as fuel kernel for the US version of pebble bed reactors as opposed to the German version based on uranium dioxide.
For the Generation IV nuclear systems, mixed uranium–plutonium carbides (U, Pu) C constitute the primary option for the gas fast reactors (GFR) and UCO is the first candidate for the very high temperature reactor (VHTR).
In the former case the fuel high actinide density and thermal conductivity are exploited in view of high burnup performance.
In the latter, UCO is a good compromise between oxides and carbides both in terms of thermal conductivity and fissile density.
However, in the American VHTR design, the fuel is a 3:1 ratio of UO2:UC2 for one essential reason, well explained by Olander [2] in a recent publication.
During burnup, pure UO2 fuel tends to oxidize to UO2+x.
UO2+x reacts with the pyrocarbon coating layer according to the equilibrium:(1)UO2+x + xC → UO2 + xCO The production of CO constitutes an issue in the VHTR because the carbon monoxide accumulates in the porosity of the buffer layer.
The CO pressure in this volume can attain large values and, along with the released fission gas pressure, it can compromise the integrity of the coating layers and contribute to the kernel migration in the fuel particle (“amoeba effect”).
In the presence of UC2, the following reaction occurs rather than reaction (1) in the hyperstoichiometric oxide fuel:(2)2UO2+x + xUC2 → (2 + x) UO2 + 2xC Because no CO is produced in reaction (2), this latter is more desirable than (1) in view of the fuel integrity.
The simultaneous presence of UC2 and pure carbon in the fuel assembly, however, creates the possibility of further reaction between the dicarbide and pure carbon, conceivably resulting in the formation of a eutectic in case of thermal excursion.
The existence of metal carbide-carbon eutectics in metal-carbon binary systems is well known for decades.
The U–C system, in particular, has been studied both theoretically and experimentally [3–7].
However, the existence of a UC2−x – C eutectic has been more postulated on the basis of experimental information collected in the immediate vicinity of the C/U = 2.00 composition [1,8] and thermodynamic optimisation of the binary phase diagram [5] than observed in compositions more abundantly enriched in carbon.
A possible reason for such a lack of experimental information can be sought in the phase demixing, hindering homogeneous synthesis of carbides with a large excess of graphite with traditional synthesis methods such as powder metallurgy or arc melting.
In the current work, uranium dicarbide with a large excess of carbon (up to an atomic ratio C/U = 2.8) was obtained by melting uranium dicarbide in a graphite crucible in an induction furnace up to 2800 K under an inert atmosphere.
The thus obtained carbide–carbon mixture was then heated beyond 3500 K by laser irradiation, in order to obtain, upon rapid cooling, a homogeneous material.
Simultaneous detection of the sample's real temperature by a fast multi-wavelength spectro-pyrometer permitted, for the first time in our knowledge, sound identification of a liquidus temperature, followed by the eutectic/monotectic transition, and, at much lower temperature, the cubic–tetragonal solid–solid phase transition typical of uranium dicarbide.
As a comparison, similar thermograms (temperature versus time curves) are presented for the compositions UC2 and UC2.1.
In addition, analysis of the radiance spectra recorded during the thermal cycles yielded in situ estimation of the sample surface composition across the various phase transitions.
Uranium dicarbide buttons were prepared by cutting arc melted and quenched drops without major difficulties [3].
In contrast, homogeneous samples with larger carbon content could hardly be prepared by the same technique due to the large and uncontrollable separation of pure, solid carbon from the melted phase.
Carbon-rich samples were rather produced by directly melting up to 2800 K in an induction furnace under oxygen-free, inert gas flux (helium) UC1.9 fragments placed at the bottom of a graphite cylindrical crucible.
The crucible's bottom then resulted in a mixture of graphite and eutectic (Fig.
1a).
This part was then further homogenised by laser heating it under inert atmosphere (Ar at 0.3 MPa).
After fast quenching to room temperature (cooling rates of 104–105 K s−1), the bottom part of the crucible became a reasonably homogeneous eutectic mixture of carbon and uranium dicarbide, with a large excess of carbon with respect to the exact eutectic composition.
This latter has been established to be close to UC2.00 [5].
The melted and re-frozen eutectic surface was then cut away from the rest of the crucible (Fig.
1b).
Its exact chemical composition was obtained by crushing a small part of it into a fine powder successively measured in the infrared carbon dioxide analyser described below.
By repeating several tests, a reproducible composition C/U = 2.82 ± 0.08 was obtained with such an approach.
The melted and re-frozen eutectic disks cut from the cylindrical graphite crucibles were then mounted again in the laser heating sample holder, and laser shot a few more times beyond melting, in order to study more accurately phase transitions occurring during the thermal cycles, and their repeatability over successive thermal cycles.
Two commercial ELTRA® analysers combined with a controlled-atmosphere combustion setup were employed to measure, respectively, the carbon and oxygen content of the current samples.
The traditional approach is based on the combustion of a sample at high temperature that permits to form gaseous carbon dioxide (CO2) from either the carbon or the oxygen contained in the specimen, depending on the atmosphere and the crucible material in which the measurement is carried out (Fig.
2).
The measurement is then based on the measured IR absorption of the produced CO2 at a characteristic wavelength (2640 cm−1 or 3.79 μm) [9].
In order to measure the carbon content, the sample is placed in an Al2O3 crucible inside an electrical induction furnace and heated up to 3500 K under a flow of oxygen (99.99% pure).
It is assumed that all and only the carbon from the sample reacts with oxygen to produce CO and CO2.
These gases pass through a system of filters and catalysts to obtain only CO2 which is measured by IR-absorption spectroscopy.
The quantity of CO2 analysed directly yields the carbon content of the sample.
In the same way, using a graphite crucible inside a resistance furnace and a helium flow, it is possible to measure the oxygen contained in the sample.
In this case, it is assumed that all the oxygen contained in the sample reacts with the crucible during heating and forms CO and CO2.
The final amount of CO2 is quantitatively determined by IR spectroscopy and, in this case, all the measured CO2 corresponds to the oxygen content in the sample.
Both systems were calibrated with standard materials such as steel and U3O8.
Details of the laser-heating setup used in this research have been reported in a previous publication [10], although the technique has been partially modified in the present work.
During the shots, a carbide disk was mounted in a sealed autoclave under an inert atmosphere (slightly pressurised argon at 0.3 MPa), in order to minimise high-temperature oxygen contamination of the samples.
Thermograms were measured on samples laser heated beyond melting by a TRUMPF® Nd:YAG cw laser radiating at 1064.5 nm, the power vs. time profile of which is programmable with a resolution of 1 ms. Pulses of different duration (100 ms–1000 ms) and maximal power (180 W–675 W) were repeated on a 8 mm diameter spot on a single sample surface as well as on different samples of the same composition in order to obtain statistically significant datasets for each composition.
Excessive thermal shocks were minimised by starting each series of laser pulses from a pre-set temperature of about 1500 K. Each series consisted of three heating–cooling pulses on the same sample spot without cooling the material below an intermediate temperature of approximately 1500 K. The peak intensity and duration of the high-power pulses were increased from one heating–cooling cycle to the other, in order to check the result repeatability under slightly different experimental conditions (Fig.
3).
This approach constituted a step forward in the laser heating technique.
It ensured a better mechanical stability of the samples, on which several successive shots could be repeated to check the result repeatability and the eventual effects of non-congruent vaporisation or segregation phenomena.
The onset of melting was detected by the appearance of vibrations in the signal of a probe laser (Ar+ cw 750 mW–1.5 W) reflected by the sample surface (reflected Light Signal technique, or RLS) [10].
The sample cooled naturally when the laser beam was switched off during the thermal cycle.
Thermal arrests corresponding to exothermic phase transitions were then observed on the thermograms recorded by the fast pyrometers.
These operate in the visible–near infrared range between 488 nm and 900 nm.
The reference pyrometer wavelength was here 655 nm.
This was calibrated according to the procedure already reported elsewhere [6,10].
The normal spectral emissivity (NSE or ελ) of uranium carbides is necessary for the determination of the sample real temperature T from the radiance temperature Tλ directly measured by the calibrated pyrometers at the wavelength λ, according to the well-known pyrometer equation [11].
(3)1T=1Tλ+λc2lnελ(λ,T) Equation (3) is valid within Wien's approximation to Planck's law, i.e.
when exp(c2λ·T)>>1.
If λ is expressed in μm and T in K, then the second radiation constant c2takes the value of 14,388 μm⋅ K. NSE of uranium monocarbide UC and uranium dicarbide UC2−x have already been studied in details in recent publications [6].
The normal spectral emissivity of carbon-richer samples has been studied for the first time in this work employing the same multi-wavelength pyrometry approach.
Uncertainty of the temperature measurements was calculated according to the error propagation law [10], taking into account the uncertainty associated to pyrometer calibration, the emissivity, transmittance of the optical system and the accuracy in detecting the onset of vibrations in the RLS signal.
The estimated cumulative uncertainty is thus lower than ±1% of the reported temperatures in the worst cases, with a k = 2 coverage factor (corresponding to two standard deviations around the average value).
Characterization of the material composition and its evolution after laser heating/melting has been an essential part of this work.
The main techniques employed to this goal are Raman spectroscopy, scanning electron microscopy (SEM), and powder x-ray diffraction (PXRD).
SEM and energy dispersion X-ray spectroscopy (EDX) analyses were performed using a VEGA TESCAN® SEM operated at 30 kV and equipped with a SAMx EDS SD-Detector.
PXRD analyses were performed with a Bruker® D8 Advance diffractometer (Cu-Kα1 radiation) with a 2θ range of 10°–120° using 0.009° steps with 2 s of integration time per step at operating conditions of 40 kV and 40 mA.
Rietveld analysis of the recorded XRD patterns was performed with the help of the Powder Cell© software.
Raman spectra were measured with a Jobin-Yvon® T64000 spectrometer used in the single spectrograph configuration.
The 647 nm line of a Kr+ Coherent® laser was used as excitation source, with a nominal power at the laser cavity of 40 mW.
This wavelength and power were chosen in order to optimise the signal/noise ratio and reducing undesirable oxidation/burning effects on the sample surface.
Spectra were measured in a confocal microscope with a 5·104-fold magnification and long (1 cm) focal distance.
With this configuration, the laser power reaching the sample surface was approximately 5 mW.
This configuration yielded a good spectral resolution (±1 cm−1) independently of the surface roughness, with a spatial resolution of 2 μm × 2 μm.
Eventual polarisation effects on the Raman spectra were neglected in the present investigation.
The spectrometer detector angle was calibrated daily with the T2g excitation of a silicon single crystal, set at 520.5 cm−1 [12].
Fig.
3a shows two repetitions of four successive heating–cooling cycles recorded on a UC2.82 sample.
This graph shows that the thermogram shape and thermal arrests are well repeatable from one pulse to the other and even from one set of pulses to the next, even by using laser heating pulses of different intensity and duration.
More detailed plots of the two highest power heating pulses well beyond the liquidus temperature are displayed in Fig.
3b.
Here some essential phase transitions are better visible on the cooling stage of the thermograms.
The same transitions are less clear on the heating flank of the temperature vs. time curves, because of the very fast laser-induced heating.
The thermograms reported in Fig.
3a and b are expressed in radiance temperature Tλ, corresponding to a virtual sample surface NSE = 1.
Phase transition temperatures reported in Fig.
3a and b will be considerably modified according to equation (3) after the emissivity NSE reassessment discussed in the next section.
Three thermal arrests are clearly visible in the thermograms recorded on UC2.82 heated well beyond 3000 K (Fig.
3b).
Following the U–C phase diagram recently established [5], these can be attributed to the following phase transitions: liquidus temperature, UC2–C monotectic, and cubic – tetragonal (α → β) solid transition.
The first two transitions can also be identified with the help of the Reflected Light Signal (RLS) curve, displayed in Fig.
3a and b, and zoomed in the inset of Fig.
3b.
In particular, six events occurring on the sample surface during a laser heating pulse can be identified in the RLS curve magnified in the inset of Fig.
3b.
They are marked in Fig.
3b with the letters from A to D. A reasonable attribution of these events is possible by reading, on the corresponding thermogram, the surface temperatures at which they occur.
A transition occurs slightly above 1400 K, where no phase boundaries are expected.
It can therefore be explained with the formation of a morphological defect (e.g.
: a crack) on the sample surface.
B marks the onset of slight oscillations in the RLS.
It can be attributed to the formation of the first liquid spots on the surface, i.e.
to the solidus transition.
C denotes the onset of large oscillations, and indicates complete melting of the surface, corresponding to the liquidus point.
Consistently, these large oscillations cease at point D corresponding, within the temperature uncertainty, to the liquidus transition on cooling (formation to the first crystallites in the liquid pool).
Likewise, point E (end of the weaker oscillations) indicates total solidification (solidus point).
For compositions with excess carbon, this solidus point also corresponds to the UC2−x – C eutectic temperature.
Finally, the event marked with F in the RLS curve occurs corresponds, in a repeatable way in the various pulses, to the cubic – tetragonal (α → β) solid state transition of uranium dicarbide.
It is interesting to notice that even this solid – solid phase transition implies a slight effect (vibration or morphological change) in the sample surface, visible in the RLS curve.
Of course this has an impact on the morphology of the solidified eutectic quenched to room temperature and successively observed by electron microscopy.
The cooling stages of single thermograms relative to the three carbide compositions investigated in this work are reported in Fig.
4.
Only the last two transitions (solidus – eutectic and α → β) are obvious in the thermograms relative to the samples UC2 and UC2.1, certainly because the liquidus is too close to the monotectic for the two thermal arrests to be clearly distinguished (at the proposed eutectic composition, UC2.00, the two temperatures should actually coincide).
It can also be seen that the same phase transitions are visible by thermal arrests at higher radiance temperatures in the sample UC2.82.
This effect can be attributed to an increase in the sample emissivity linked to the higher carbon content, as already recently observed, for example, in the zirconium – carbon system [14].
The NSE of uranium dicarbide has been established to be 0.53 at 0.655 μm [6].
If one substitutes this value in equation (3) to obtain real temperature thermograms for UC2 and UC2.1, the relative phase transitions do occur at the temperatures recommended in the literature (2737 K for the eutectic and 2050 K for the α → β transition).
Obviously, higher NSE values should be used for the UC2.82 sample.
This point will be discussed in section 4.1 of the present paper.
Thermal radiation (Planck) spectra were recorded by multi-wavelength pyrometry at several time points, identified by full circles on the UC2.82 thermogram plotted in Fig.
5a.
Fig.
5b shows four examples of these spectra recorded at different temperatures.
One of them was recorded in liquid UC2.82, the other three correspond to the thermal arrests visible in Fig.
5a.
Because of a poor signal-to-noise ratio at the tails of the recorded spectra, data were only considered for the current analysis between 0.550 μm and 0.900 μm.
Calling Si (λ, T) be each of these spectra, the following relation holds:(4)Si(λ,T)=ελi·Pi(λ,T).
Pi (λ,T) is the Planck (ideal black body) emission function at temperature T:(5)P(λ,T)=c1Ln2·λ5·[exp(c2n·λ·T)−1]−1.where c1L = 2·h·c02 is the first radiation constant and c2 = h·c0·k is the second radiation constant; c0 is the speed of light in vacuum, h is Planck's constant, and kB is the Boltzmann constant already considered in equation (3).
For the purposes of the present work, the refractive index was always taken to be equal to 1 (being the current medium air or an inert gas close to atmospheric pressure).
Because uranium carbides are metallic materials (their electronic density of states does not vanish at Fermi level) [1], their NSE cannot be assumed to be wavelength-independent in the investigated spectral range [11].
Under these conditions, it has been shown that direct fit of experimental radiance spectra Si (λ, T) with functions of the type P (λ, T) of equation (5) can yield very inaccurate results for ελ and T [13].
The problem is inversed by imposing, whenever possible, well-established temperatures for the observed thermal arrests.
Equations (4) and (5) permit then direct determination of the normal spectral emissivity ελi in the corresponding spectra.
This procedure is only possible for spectra Si (λ, T) recorded at the eutectic arrest (2737 K [5]) and at the cubic – tetragonal (α → β) solid state transition of uranium dicarbide, fixed at 2050 K [1].
In these two cases, the sample NSE for each spectrum Si (λ, T) can then be obtained by solving equation (4) for ελi, i.e.
respectively:(6)ελi(2737)=Si(λ,2737)P(λ,2737).(7)ελi(2050)=Si(λ,2050)P(λ,2050).
It should be noted, incidentally, that imposing the eutectic–monotectic temperature at any composition between UC1.9 and pure C implies the assumption of zero uranium dicarbide solubility in pure graphite.
Such assumption is certainly reasonable when one considers the very different structures of the two phases.
The trends thus obtained for ελ in UC2.82 at the eutectic and the cubic–tetragonal transition temperatures are reported in Fig.
6.
The reported uncertainty bands are calculated considering the intrinsic uncertainty of the radiance spectra measurements, and the uncertainty affecting the assumed reference temperatures (2737 K ± 20 K and 2050 K ± 20 K).
The difference between the two NSE average trends gives an idea of the temperature dependence of this parameter.
On the other hand, it was shown [6] that the temperature dependence of NSE in UC2 and pure carbon is smaller than the experimental uncertainty between 2050 K and the uranium carbide melting point.
Therefore, one should assume that the variation of NSE reported in Fig.
6 between 2050 K and 2737 K can be due to a real change in the surface composition, as mentioned at the end of subsection 3.1.
Obtaining the NSE (ελ) of the investigated carbide compositions, and in particular of the high-carbon content sample at different temperatures, becomes then a complex task, requiring a parallel analysis of the thermal radiance and surface composition during the heating/cooling process.
This analysis is possible with the current data.
It is reported in section 4 of this paper.
As a support to the described radiance spectroscopy analysis, the surface of the current uranium carbide samples has been studied by scanning electron microscopy (SEM) (Fig.
7), PXRD (Fig.
8) and Raman spectroscopy (Fig.
9) after quenching to room temperature.
SEM images in Fig.
7 show that the morphology of eutectic quenched samples is rather irregular.
Unlike other metal-carbide systems, no lamellar eutectic structure is recognizable here.
Instead, pure carbon is visible on the sample surface in the form of acicular spots.
This observation agrees with earlier ones, performed on uranium dicarbide only [8].
Although it is impossible to quantitatively estimate the corresponding carbon surface area fraction sC, from such tiny acicular spots of carbon, it seems reasonable to assume that it must be very small in these images recorded at room temperature.
PXRD patterns were recorded on powdered samples.
Therefore, this cannot be strictly considered as a surface analysis method.
Nonetheless, results are useful in clearly identifying two phases in the specimens quenched to room temperature: graphite and tetragonal uranium dicarbide.
The XRD pattern of UC2.82 is shown in Fig.
8.
Patterns of UC2 and UC2.1 are similar, with less intense graphite peaks.
This confirms that at lower temperatures the present samples follow the metastable phase boundaries recently assessed for the U–C binary system [1,4,5].
Because of its extremely slow kinetics [1], the formation of uranium sesquicarbide, U2C3, which would be thermodynamically favourable, is not observed with the current high cooling rates.
Raman spectra recorded on the quenched surfaces of samples laser melted in this work are reported in Fig.
9.
From the established emissivity values, and using Kirchhoff's theorem stating that emissivity equals absorptivity at the thermodynamic equilibrium [11], the penetration depth of the current Raman analysis can then be determined to be a few hundred nm.
Therefore, the current Raman analysis can be considered to be representative of the sample's external surface, plus a small volume into the bulk material.
No Raman active peaks are observed for cubic or tetragonal uranium carbides, by analogy with similar carbides more broadly investigated [16].
A very strong Raman peak is only visible at 1589 cm−1.
This is the well-known G peak of graphite.
Interestingly, in the quenched eutectic sample this peak is not accompanied by its homologue at 1350 cm−1 (the “D peak” of graphite), which is typical of defective and disordered graphite [17].
Both peaks are actually visible in a “clean” graphite crucible identical to those used for the in-furnace synthesis of the current UC2.82 samples.
Also the G peak appears more complex in the pure graphite sample, where a shoulder (sometimes called “G'” [17]) is visible at a slightly higher wavenumber than the main peak.
This observations lead to the conclusion that highly ordered graphite is produced in the acicular C-inclusions formed on the quenched surface of the present hyperstoichiometric carbide samples.
Similarly ordered graphite was produced by quenching ZrC–C eutectic samples from the liquid phase [14].
This conclusion justifies the choice of the carbon NSE value reported in equation (7).
The emissivity of highly ordered graphite has actually been assessed to be very high, approaching 1 [15].
The present results are the first reported on the melting behaviour of uranium carbides containing a large amount of excess carbon.
For their interpretation it is therefore reasonable to refer, by analogy, to similar transition metal-carbide systems the high temperature behaviour of which has been studied more extensively.
In particular, the zirconium – carbon eutectic system has been recently investigated with the same approach as the present uranium – carbon system [14].
Estimating the NSE of melting and solidifying UC2+x from the radiance spectra of the type of those reported in Fig.
5b constitutes a first, complex task.
It is necessary, however, for the determination, through equation (3), of the real liquidus temperature of the observed composition.
Two main difficulties need to be faced: 1) The investigated material surface is a composite one, mixture of two immiscible phases (uranium dicarbide and graphite) having different optical properties; 2) NSE cannot be rigorously assumed to be wavelength-independent in this type of carbides.
It has been observed in the Zr–C system [14], that the presence of excess carbon not soluble in the carbide matrix results, both in the liquid and upon eutectic solidification, in an increased NSE with respect to the metal carbide at the limit of the eutectic domain.
This effect has been explained on the basis of the definition of NSE and Planck's radiation law, as in equation (5), taking into account the area fraction of carbon with respect to the carbide matrix in the quenched solid eutectic surface.
The following equation can be easily derived for the total NSE of the composite surface:(8)ελTOT=ελUC2−x·sUC2−x+ελC·sC Here ελUC2−x is the NSE of uranium dicarbide, ελC the NSE of pure carbon, sUC2−x and sC are the surface area fractions of UC2−x and C, respectively (0 ≤ sUC2−x, sC ≤ 1, sUC2−x + sC = 1).
The total NSE of the composite surface, ελTOT, is obtained here from the current experimental results with the help of equations (4) and (5).
Obviously, here ελTOT coincides with ελi of equation (4).
It is worth specifying, at this point, that “surface composition” means here the average composition of the external layer of material, from which is mostly emitted the thermal radiation detected by the current multi-wavelength pyrometer.
The lack of refractive index data for cubic uranium dicarbide [1] and the fact that the present material is actually biphasic, make it impossible to provide any accurate determination of the thickness of such a layer.
However, taking into account the wavelength range investigated in the present analysis, the numerical aperture of the pyrometer objective and arbitrary values for the complex refractive index compatible with the current emissivity data, an approximate penetration depth of 100 nm–200 nm can be estimated for the present thermal radiance emission analysis.
Such penetration depth corresponds to the thickness of the surface layer investigated in this work.
The differences between the UC2.82 NSEs at the eutectic and α → β transition temperatures shown in Fig.
6 are likely to depend on the surface morphology and composition evolution during cooling, according to equation (8).
In fact, the NSE trends of Fig.
6 are successfully fitted with the help of equation (8), assuming the following expressions for ελUC2−x and ελC:(9)ελUC2−x=0.75−0.42·λ+0.18·λ2(10)ελCs=0.96±0.03(SOLID).
Equation (9) was proposed in an experimental assessment of optical properties of melting and solid uranium carbides [6], whereas equation (10) is deduced from a recent review [15], and is reasonable for highly ordered solid graphite.
The fit yields different values of the sample surface composition at the two temperatures: sC = 0.3 at the eutectic point and sC close to zero at the cubic–tetragonal transition point.
Of course, these results are affected by an uncertainty even larger than the one reported for the NSE values.
Nonetheless, this procedure, applied at particular temperatures for which the material behaviour is relatively well-established, shows that even for the carbon-rich compositions it is reasonable to use the NSE values and trends recommended for uranium dicarbide and specimens with little excess carbon.
With this assumption, the approach can then be extended to all temperatures, and spectra Si (λ,T) can be fitted using temperature and sC as free parameters, instead of T and ελ.
For radiance spectra measured in the liquid, the following value for the NSE of pure liquid carbon is taken:(11)ελCl=0.87±0.05(LIQUID),as recently extrapolated for the system ZrC–C [14].
By combining equations (4), (5) and (8) one obtains the following fitting equation:(12)P(λ,T)=c1Lλ5·1exp(c2Lλ·T)−1·(ελUC2+(ελC−ελUC2)sC) Using equations (9)–(11) for the various emissivity expressions, the fit yields the best values of the real temperature T and the carbon surface area fraction sC for each radiance spectrum recorded.
Fig.
10 reports, for sample UC2.82, the resulting temperature and total NSE given by equation (8) once sC is determined with the described procedure.
Finally, considering the uncertainty propagation through the whole fitting procedure, and the large cooling rate compared to the acquisition time of radiance spectra (4 ms), the UC2.82 liquidus temperature, corresponding to the first thermal arrest observed in the thermograms of Fig.
3, can be determined only with la large error band: Tliquidus (UC2.82) = 3220 K ± 50 K. In conclusion, the segregation of a pure carbon phase on the sample surface during the heating/cooling cycles plays an important role in the radiative heat transfer balance of this type of carbides.
The corresponding evolution of the surface composition resulting from current radiance spectra fit is discussed in the next section.
As a summary of the present study, Fig.
11 reports the trend of the C/U atomic ratio on the surface of a UC2.82 sample, between 2000 K and 3500 K. The C/U ratio has been obtained by assuming, according to the present room temperature PXRD results combined with literature data up to 2500 K [1], that the uranium dicarbide phase in equilibrium with pure carbon at the limit of the eutectic region has the composition recommended in Ref.
[1].
Then the C/U atomic ratio can be written:(13)C/U=f(T)+sC(T),Where sC (T) is obtained by fitting the current radiance spectra with equation (12).
The term f (T) represents, as a function of temperature, the C/U ratio at which uranium dicarbide is in equilibrium with pure graphite.
Using the phase boundaries reported in Refs.
[4] and [5], it can be approximated by the following linear relationship:(14)f(T)=1.89+4.4·10−5·Tfor 2050 K ≤ T ≤ 2737 K. Carbon enrichment on the sample surface is always significantly lower than the excess carbon content of the initial bulk composition.
This can be explained at all temperatures with a higher surface tension of uranium dicarbide compared with pure carbon.
Only at temperatures where liquid UC2+x and solid C coexist (between solidus and liquidus) and close to solidification a significant excess of carbon can be clearly observed.
Obviously, the highly non-congruent solid–liquid transition results in the segregation of solid carbon even on the sample surface.
It is not impossible, if the present system follows the conventional eutectic solidification dynamics [18–20], that such solid carbon forms some typical lamellar structures at temperatures immediately lower than the solidification point.
However, because of relatively fast carbon diffusion in uranium dicarbide [1], and the thermodynamic force represented by the higher surface tension of UC2−x, these lamellae rapidly dissolve into the bulk, leaving on the surface only those traces similar to acicular spots of highly ordered graphite shown by SEM analysis in Fig.
7 and analysed here by Raman spectroscopy.
Such approximate description of the surface morphology evolution should obviously be corroborated by further experimental characterisation and by calculations of solidification dynamics and solid-state diffusion, which are beyond the scopes of the present work.
Nonetheless it should be appreciated that the current high temperature radiance spectra measurements, combined with some more traditional characterisation techniques on materials quenched to room temperature, permit a consistent analysis of the sample phase transitions and surface features (composition and morphology).
At the temperatures studied here (T > 2500 K), such features would be hardly accessible, even very qualitatively, to most of the material characterisation techniques available up to date.
Finally, the present results give hints about the possible melting behaviour of uranium carbide nuclear fuel coated in graphite (cf.
[21]).
Although melting of such type of fuel is very unlikely, even in a reactor mishap, the formation of liquid might indeed result in the migration of the carbide fuel towards the outer surface of the coating.
Such information is certainly relevant to the safety analysis of this type of nuclear fuel in accidental conditions.
The eutectic system uranium dicarbide-carbon has been studied by laser heating, with particular focus on the composition UC2.82, synthesized by melting fragments of uranium dicarbide in a graphite crucible.
The following conclusions can be drawn:1.The normal spectral emissivity, and, therefore, the thermal radiance emitted by hyperstoichiometric uranium carbides depends on the surface area fraction of free carbon.
In particular, NSE of UC2.82 reaches a maximum value of 0.7 at the first solidification (liquidus) temperature, compared to the value NSE = 0.53 established for pure uranium dicarbide.
Accordingly, using as a reference temperature the uranium dicarbide-carbon eutectic, fixed at 2737 K, the liquidus temperature of UC2.82 has been measured to be 3220 K ± 50 K. The current radiance spectroscopy measurements permit a qualitative determination of the evolution of the sample surface composition during the heating/cooling process.
The surface fraction of carbon, in particular, has always been observed to be considerably lower than the nominal material composition, both in the solid and in the liquid phases.
This signifies that uranium dicarbide has probably a higher surface tension than carbon, and that carbon can quickly diffuse into uranium dicarbide, both liquid and solid.
As a result, no lamellar eutectic structure has been observed after quenching to room temperature.
Instead, a pure graphite phase is demixed in the form of tiny acicular inclusions.
Raman spectroscopy measurements have shown that these acicular inclusions consist of highly-ordered graphite.
These results are useful for determining the interaction between uranium dicarbide and carbon in certain so-called “advanced fuel” concepts.
The analysis can be extended to other C/U atomic ratios, or to other even less-studied analogous systems, such as, for example, the binary plutonium-carbon.
The Authors wish to acknowledge R.J.M.
Konings (TU Delft and JRC ITU) for useful scientific discussions, D. Bouexière and R. Eloirdi (JRC-ITU) for their help in XRD analysis and M. Ernstberger (JRC-ITU) for SEM imaging.
Assignment of the vibrational spectrum of l-cysteine Ab initio calculations of the complete unit cell of l-cysteine for both the orthorhombic and monoclinic polymorphs have been carried out.
The results suggest the presence of a previously unrecognised, weak dihydrogen bond of a novel type: S–H···N–H in the gauche conformer of the monoclinic polymorph.
Comparison of the calculated transition energies to those observed in the infrared, Raman and inelastic neutron scattering spectra of the orthorhombic form shows excellent agreement, as does the simulated INS spectra to that experimentally measured.
The assignments are in general agreement with those in the literature but differ in detail.
The strong intermolecular interactions present make the use of periodic-DFT essential in order to correctly assign the spectra.
The need for, and the complementarity of, all three types of vibrational spectra: infrared, Raman and INS is clearly demonstrated.
The amino acid l-cysteine (C3H7NO2S, see Fig.
1 for the structure) is a semi-essential (i.e.
it can be biosynthesized in vivo) nutrient in humans [1].
The sulfhydryl (S–H) group is important biologically in that condensation of nearby sulfhydryl groups to form covalent cystine disulfide linkages R–S–S–R is an often crucial stabilization mechanism in protein structure [2].
Depending on the preparation conditions an orthorhombic (space group P212121) [3–5] or a monoclinic (space group P21) [6] form are obtained.
In addition, several more phases are obtained with the application of pressure [7].
In the solid state, as with most amino acids, the compound exists as a zwitterion.
Several conformers are found; a gauche form in the orthorhombic phase, Fig.
1b, a different gauche conformer, Fig.
1c, and a trans conformer, Fig.
1d, in the monoclinic phase.
The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10–14].
Spectral assignments have been made using empirical force fields [15], Hartree–Fock calculations [10,16,17] based on the isolated molecule approximation.
For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory.
A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers.
In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19].
The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments.
The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.
Neutron vibrational (INS) spectra were those reported previously [21] recorded using the TOSCA [22] and MARI [23] spectrometers at ISIS [24].
The two spectrometers are complementary [25], TOSCA has excellent resolution in the 0–1800cm−1 range, while MARI can access the S–H/N–H/C–H stretch region because it can access lower momentum transfer values than TOSCA at these energies.
Infrared spectra (2cm−1 resolution, 256 scans) were recorded at room temperature with a Bruker Vertex 70 Fourier transform infrared spectrometer using a Pike single reflection attenuated total internal reflection accessory.
Room temperature Raman spectra were recorded with a Renishaw InVia system using 785nm excitation and have been corrected for the instrument response.
Periodic density functional theory (periodic-DFT) calculations were carried out using the plane wave pseudopotential method as implemented in the CASTEP [26,27] code.
Exchange and correlation were approximated using the PBE functional [28].
The plane-wave cut-off energy was 900eV.
Brillouin zone sampling of electronic states was performed on 4×3×5 Monkhorst–Pack grid.
The equilibrium structure, an essential prerequisite for lattice dynamics calculations was obtained by BFGS geometry optimization after which the residual forces were converged to zero within 0.0012eV/A.
Phonon frequencies were obtained by diagonalisation of dynamical matrices computed using density-functional perturbation theory [29] and also to compute the dielectric response and the Born effective charges, and from these the mode oscillator strength tensor and infrared absorptivity were calculated.
The atomic displacements in each mode that are part of the CASTEP output, enable visualisation of the modes to aid assignments and are also all that is required to generate the INS spectrum using the program ACLIMAX [30].
It is emphasised that all the spectra shown have not been scaled.
l-cysteine (97%, Aldrich) was used as received.
Powder X-ray diffraction confirmed that it was the orthorhombic form.
The observed and calculated structures are in good agreement as shown in Table 1, with bond distances generally with 0.05Å and bond angles within 1°.
The only noteworthy differences are the dihedral angles in the trans conformer in the monoclinic phase that is predicted to be somewhat more skew than is observed.
The calculations show the orthorhombic polymorph to be marginally the more stable, although the difference in total energy is only 0.086eV.
The structures clearly display the N–H⋅⋅⋅O and S–H⋅⋅⋅O hydrogen bonding motifs described previously [3–6].
However, there is an additional interaction in the gauche conformer of the monoclinic phase that has not been previously noted.
The experimental [6] intramolecular SH⋅⋅⋅HN distance is only 2.262Å, however, this is based on the S–H (1.342Å) and N–H (0.929Å) bond lengths determined by X-ray crystallography, which typically underestimates X–H distances by ∼0.1Å [31].
After geometry optimisation, the distances become: S–H (1.361Å) and N–H (1.045Å) and the SH⋅⋅⋅HN distance is 2.303Å.
Thus both experiment and theory show that the distance is less than twice the van der Waal’s radius of hydrogen (2.4Å), the essential prerequisite for a dihydrogen bond [32].
Other structural features are consistent with this assignment: ∠ N–H⋅⋅⋅H=128.6° (119.8°), ∠ S–H⋅⋅⋅H=113.2° (108.2°), (ab initio values in brackets) which fall in the expected ranges 117–171° and 95–120°, respectively [31].
The dihedral angle N–H⋅⋅⋅H–S=44.2° (34.2°) is small, thus the four atoms are nearly coplanar, as often found in hydrogen bonding systems.
The calculated Mulliken charges are: (S)H=+0.16 and (N)H=+0.43, thus the requirement for a difference in charge, δ−, δ+, is also met.
Overall, the evidence is consistent with the presence of a weak intramolecular dihydrogen bond SH⋅⋅⋅HN.
This is a type of dihydrogen bond that has not been previously recognised in any system, including amino acids.
This is clearly an unusual type of dihydrogen bond; those involving BH or AlH are the most well-known, BH3NH3 being the paradigm, however, there are many examples known involving other atoms.
Thus transition metal hydrides may be acceptors and examples of intramolecular C–H⋅⋅⋅H–O dihydrogen bonds are also known [32].
For both polymorphs, there are four formula units in the primitive cell.
In both cases, the site symmetry is C1, for the orthorhombic form the D2 factor group results in all modes being allowed in the Raman spectrum but only the B1, B2, B3 allowed in the infrared spectrum with the A1 modes inactive.
For the monoclinic form the combination of C1 site symmetry and C2 factor group means that all the modes are allowed in both the infrared and Raman spectrum.
Note that because a mode is allowed in the infrared or Raman, it does not necessarily have any significant intensity.
All modes are allowed in the INS spectrum, however, the dependence of the intensity on the cross section and the amplitude of vibration [33], means that there is a strong propensity that only modes with significant motion of the hydrogen atom(s) will have intensity, thus the carboxylate stretch modes are very weak or absent in the INS spectrum.
The infrared, Raman and INS spectra of l-cysteine are shown in Fig.
2.
As noted previously [21], similarities and differences are apparent between all three types of spectra and emphasise the necessity to have all three forms of spectra for a complete analysis.
Comparison of the experimental and calculated INS spectra is a stringent test of an ab initio calculation.
Fig.
3 compares the experimental spectrum of l-cysteine (middle) with that calculated for the orthorhombic (top) and monoclinic (bottom) phases.
As noted for the Raman spectra [14], the spectra of the two phases are obviously different, the experimental spectrum is a better match for the orthorhombic form, although it is noted that this is a qualitative judgement, albeit in agreement with the powder X-ray diffraction result that the sample is the orthorhombic form.
The only significant difference between the observed and calculated INS spectra of the orthorhombic form is that the [–NH3]+ torsion is calculated slightly higher than is observed (545cm−1 vs. 487cm−1), in contrast to isolated molecule calculations [10,16,17] that place it near 350cm−1.
From Fig.
2, it can be seen that this mode is only observable by INS spectroscopy.
Some of the difference may be a consequence that the calculated structure at the experimental lattice parameters is at a pressure of 1.5GPa and the modes are known to be somewhat pressure sensitive [14].
The transition energy of 487cm−1 is very close to that seen in 1,6-diaminoethane dihydrochloride, 468/489cm−1 and suggests that this may be a useful group frequency for INS, in the same way that the methyl torsion is commonly observed at 240±10cm−1 in aliphatic compounds [33].
As noted by other authors, [20], the modes at <200cm−1 are strongly mixed and it is not possible to distinguish the translational, librational and low energy skeletal modes.
At higher energies, the modes are usually an admixture, with very few that are pure, thus the description should be read as the leading term in the potential energy distribution rather than as a complete description.
This is illustrated in Fig.
4 that compares the calculated INS spectra (fundamentals only) of the orthorhombic polymorph, 4a, with those calculated for just the hydrogen attached to sulphur, 4b, and only the hydrogen atoms attached to the nitrogen atom.
For the sulfhydryl hydrogen, in addition to the expected in- and out-of-plane bends at ∼1060 and ∼340cm−1, respectively, there are several modes that involve motion of the atom.
This is even more so for the [–NH3]+ hydrogen atoms that participate in most of the modes across the entire spectral range.
For the monoclinic form, the gauche and trans conformers form two largely independent sets.
This leads to distinct differences in transition energies between the conformers, a striking example is that one of the N–H stretch modes of the gauche conformer is almost coincident with the S–H stretch at ∼2500cm−1, the corresponding mode in the trans conformer is almost 500cm−1 higher in energy.
The transition energies for the gauche conformers in the polymorphs are generally similar but not identical, emphasising the sensitivity of vibrational spectroscopy to minor structural differences.
The assignments given in Table 2 are in general agreement with those in the literature [10,15–17,20] but differ in detail.
As already noted for the [–NH3]+ torsion, the isolated molecule calculations underestimate the transition energies for deformation modes involved in hydrogen bonding, thus the S–H out-of-plane bend at ∼340cm−1 is predicted at 150cm−1, whereas it is predicted correctly by the present periodic-DFT calculations.
The S–H stretch of cysteine residues in proteins has been used as a probe of sulfhydryl hydrogen bonding [34–36].
In the tail spike protein [36] the range is 2530–2585cm−1, for strong to very weak hydrogen bonds, thus in l-cysteine, the value of ∼2550cm−1 indicates a strong S–H hydrogen bond.
This paper reports the first calculation of the complete unit cell of l-cysteine for the orthorhombic and monoclinic polymorphs.
The spectra of both polymorphs are clearly distinguishable.
The strong intermolecular interactions present make the use of periodic-DFT essential in order to correctly assign the spectra.
The need for, and the complementarity of, all three types of vibrational spectra: infrared, Raman and INS is clearly demonstrated.
We thank the STFC Rutherford Appleton Laboratory for access to neutron beam facilities.
Computing resources (time on the SCARF compute cluster for the CASTEP calculations) was provided by STFC’s e-Science facility.
Kelvin wave propagation along straight boundaries in C-grid finite-difference models Discrete solutions for the propagation of coastally-trapped Kelvin waves are studied, using a second-order finite-difference staggered grid formulation that is widely used in geophysical fluid dynamics (the Arakawa C-grid).
The fundamental problem of linear, inviscid wave propagation along a straight coastline is examined, in a fluid of constant depth with uniform background rotation, using the shallow-water equations which model either barotropic (surface) or baroclinic (internal) Kelvin waves.
When the coast is aligned with the grid, it is shown analytically that the Kelvin wave speed and horizontal structure are recovered to second-order in grid spacing h. When the coast is aligned at 45° to the grid, with the coastline approximated as a staircase following the grid, it is shown analytically that the wave speed is only recovered to first-order in h, and that the horizontal structure of the wave is infected by a thin computational boundary layer at the coastline.
It is shown numerically that such first-order convergence in h is attained for all other orientations of the grid and coastline, even when the two are almost aligned so that only occasional steps are present in the numerical coastline.
Such first-order convergence, despite the second-order finite differences used in the ocean interior, could degrade the accuracy of numerical simulations of dynamical phenomena in which Kelvin waves play an important role.
The degradation is shown to be particularly severe for a simple example of near-resonantly forced Kelvin waves in a channel, when the energy of the forced response can be incorrect by a factor of 2 or more, even with 25 grid points per wavelength.
The Arakawa C-grid is widely used as the basis for horizontal discretisations in geophysical fluid dynamics, over a range of scales from lakes to the global ocean [1–3].
Its simplest application is to shallow-water models with a horizontal velocity u=(u,v) and a pressure (or height) variable p, with the nodes arranged on a regular Cartesian grid as shown in Fig.
1(a).
So, when evaluating derivatives using simple second-order centred differences, ∇p is naturally evaluated at u nodes and ∇⋅u is naturally evaluated at p nodes, which is optimal for the discretised equations of motion.
It is easy to ensure that the resulting schemes conserve mass, and it is possible to conserve additional quantities such as energy and enstrophy [4].
Analogous staggered grids are widely used in the modelling of acoustic, seismic and electromagnetic waves [5–7].
Here we consider inviscid fluid dynamics, for which the normal component of the flow vanishes at the coastline with no restriction on the tangential flow:(1)u⋅n=0, where n is the normal vector at the coastline ∂Ω.
For rectangular domains, (1) may be implemented without approximation by aligning the C-grid so that nodes of the normal velocity lie on ∂Ω, as shown in Fig.
1(b).
However, the situation is not so simple for other domains.
Then, in the absence of cut-cell methods [8,9], the standard treatment introduces a numerical coastline ∂Ωs that differs from the continuum coastline ∂Ω and typically contains staircases, as shown in Fig.
1(c), (d).
The nature of the C-grid again means that nodes of the normal velocity may be chosen to lie on ∂Ωs, so that (1) is easy to apply, with n being (0,1) or (1,0).
However, it is not clear how the resulting numerical solution will converge to the continuum solution as the grid size h→0.
In particular, the normal vector of ∂Ωs (constrained to be (1,0) or (0,1)) nowhere approaches that of ∂Ω as h→0, so will (1) be applied correctly?
It seems plausible that (1) is somehow being applied correctly on-average, but does this ensure convergence to the continuum solution?
One expects O(1) errors in u close to the coastline, since u is directed along the coastline in the continuum solution but is constrained otherwise in the discrete solution, but is there convergence for p, and for u away from the coast?
And, if this is achieved, at what rate does the convergence occur?
Even if the equations of motion are discretised in the interior at second-order (or higher) in h, the distance between ∂Ω and ∂Ωs decreases at first-order in h. Does this geometrical approximation induce global errors at first-order in h?
The numerical errors induced by staircase boundaries have been investigated in various specific configurations, for both acoustic and electromagnetic waves (e.g., [10–13]) and shallow-water flows (e.g., [14,15]).
However, in some of these studies there is no quantification of the rate of convergence towards the continuum solution [10,11], and in others the quantification is insufficient to accurately determine the rate of convergence [12,15].
For example, for acoustic wave reflection problems (equivalent to non-rotating shallow-water dynamics), Tornberg and Engquist [12] estimate convergence rates between 0.5 and 1 for the pressure in the L2 norm, based on experiments at three different resolutions.
For a linear wind-driven ocean circulation in a circular domain (i.e., rotating shallow-water dynamics), Greenberg et al.
[15] estimate convergence rates between 1 and 2 for pressure in the L2 norm, again based on experiments at three different resolutions, even when fourth-order finite-differences are used in the interior.
Other studies have been more precise.
For a non-rotating shallow-water flow, Pedersen [14] showed analytically that a gravity wave reflected from a staircase boundary aligned at 45° to the grid inherits a phase-shift of O(h) (i.e., first-order accuracy).
For the related problem of acoustic wave reflection in three dimensions, Häggblad and Engquist [13] report errors of O(h) for pressure and velocity in the L2 norm.
Here, our main aim is to explicitly calculate the nature of the convergence of discrete solutions on the C-grid as h→0 for the Kelvin wave, which is a particularly important type of coastal motion.
As discussed by Gill [16] and Hutter et al.
[17], the Kelvin wave plays an important role in various barotropic and baroclinic phenomena in lakes and oceans, including tides.
Unlike the discrete doubly-periodic inertia-gravity waves studied by Arakawa and Lamb [18], the Kelvin wave relies upon a coastline for its existence, and we might anticipate that its interaction with staircase coastlines will be of special importance.
As such, and as reviewed in Section 4.2 of Greenberg et al.
[15], there have been several previous studies of Kelvin wave propagation on the C-grid.
Of most relevance here is that of Schwab and Beletsky [19], who estimated the phase speed of Kelvin waves by time-stepping the shallow-water equations in a square basin.
They showed that when the grid is aligned at 45° to the coastline, the phase speed of the discrete solution differs more from the continuum value than when the grid and coast are aligned.
Here, we are able to quantify the rate of convergence for these two angles analytically, and extend the results to other angles using carefully designed numerical calculations.
As described in Section 2, we treat the simplest problem of interest: linear inviscid motion, with uniform background rotation in a fluid of constant depth and with a straight coastline, for waves that are periodic in the along-shore direction.
We focus on the effects of spatial differencing, and seek solutions that are time-harmonic.
In Section 3, we start by examining the case where the grid and coastline are aligned, and confirm analytically that second-order convergence (in wave speed and horizontal structure) is achieved as h→0.
In Section 4, we then examine the case where the grid and coastline are aligned at 45°, and demonstrate analytically that the convergence in wave speed and pressure as h→0 is degraded to first-order.
In Section 5, we confirm these findings numerically, and extend them to other angles.
In Section 6, some numerical solutions of forced Kelvin waves in a channel are given, explicitly demonstrating the significant errors that can arise when the channel and grid are not aligned and the forcing is near-resonant.
In Section 7, the reasons for the first-order convergence are examined theoretically, and two examples of how the boundary conditions can be modified to obtain second-order convergence are given.
We conclude in Section 8.
We consider the shallow-water equations linearised about a state of rest, for a fluid of uniform depth (e.g., Section 14.9 of [20]):(2)∂u˜∂t˜−f˜v˜=−∂p˜∂x˜,∂v˜∂t˜+f˜u˜=−∂p˜∂y˜,∂p˜∂t˜+c˜2(∂u˜∂x˜+∂v˜∂y˜)=0.
Here, c˜ is the linear long wave speed and f˜>0 is the Coriolis parameter, from which a distinguished length scale L˜ (the Rossby radius of deformation) can be formed:(3)L˜=c˜f˜.
For surface wave (barotropic) dynamics, (u˜,v˜) is the depth-averaged horizontal flow, p˜ is gravity times the free-surface displacement, and c˜2 is gravity times undisturbed depth.
At mid-latitudes, we may have c˜≈200 ms−1 and L˜≈2000 km in the deep ocean, and c˜≈20 ms−1 and L˜≈200 km in a lake or shallow sea.
For internal wave (baroclinic) dynamics, the vertically-varying horizontal flow and pressure are given by (u˜,v˜)ϕ and ρ˜p˜ϕ respectively, where ϕ(z) is a vertical structure function and ρ˜ is a typical fluid density, and c˜2 scales like gravity × depth × fractional density change over the fluid layer depth.
At mid-latitudes, we may have c˜≈2 ms−1 and L˜≈20 km in the deep ocean, and c˜≈0.3 ms−1 and L˜≈3 km in a lake or shallow sea.
We consider time-harmonic waves with frequency ωf˜.
Introducing nondimensional coordinates(4)x=x˜L˜,y=y˜L˜,t=f˜t˜, we seek solutions in terms of nondimensional variables u=(u,v) and p, according to(5)u˜=u˜0Re(u(x,y)e−iωt),p˜=c˜u˜0Re(p(x,y)e−iωt), where u˜0 is a characteristic horizontal flow speed.
Then (2) becomes(6a)−iωu−v=−∂p∂x,(6b)−iωv+u=−∂p∂y,(6c)−iωp+∂u∂x+∂v∂y=0.
We use these nondimensional equations for the remainder of this study.
Seeking solutions in a semi-infinite domain y>0 with v=0 and (u,p)∝eikx, (6a), (6c) imply ω=±k and u=±p.
Then (6b) implies ∂p/∂y=∓p, so that coastally-trapped solutions in y>0 are given by(7a)u=e−yeikx,(7b)v=0,(7c)p=e−yeikx,(7d)ω=k.
This is the famous Kelvin wave [21].
In dimensional coordinates, the phase speed and group speed both equal c˜, and the cross-shore decay has a length scale L˜.
Note that the assumption of constant f˜ could be violated for sufficiently large L˜; see [22] for a discussion of how the phase speed changes in this case.
We examine how this wave is reproduced by discrete solutions on the C-grid, using the notation of Fig.
1(a) withxm=mh,yn=nh.
Using second-order centred finite-differences, (6a), (6b), (6c) are discretised as(8a)−iωum,n+12−vm+12,n+1+vm+12,n+vm−12,n+vm−12,n+14=−pm+12,n+12−pm−12,n+12h,(8b)−iωvm+12,n+um+1,n+12+um+1,n−12+um,n−12+um,n+124=−pm+12,n+12−pm+12,n−12h,(8c)−iωpm+12,n+12+um+1,n+12−um,n+12+vm+12,n+1−vm+12,nh=0, where m,n∈Z.
The discretisation of the Coriolis terms using a four-point stencil is standard, although other choices are possible (e.g., [23]).
We again suppose that the fluid occupies y>0, with the coastline at y=0 aligned with the gridline y=y0.
Thus, solutions of (8) are sought with n⩾0, and vm+1/2,0=0 for all m∈Z.
The easiest way to derive the Kelvin wave solution on the C-grid is to adapt the derivation leading to (7).
We seek solutions of the form(9)um,n+12=uˆn+12exp(ikxm),vm+12,n=0,pm+12,n+12=pˆn+12exp(ikxm+12).
Substituting into (8a), (8c) yields−iωuˆn+12=−2isin(kh/2)hpˆn+12,−iωpˆn+12+2isin(kh/2)huˆn+12=0, so that there is only a non-trivial solution when(10)ω=±(sin(kh/2)(kh/2))k,with uˆn+12=±pˆn+12.
With (9), the cross-shore momentum balance (8b) reads(11)cos(kh/2)2(uˆn+12+uˆn−12)=−1h(pˆn+12−pˆn−12)⇒pˆn+12=γpˆn−12,with γ=2∓hcos(kh/2)2±hcos(kh/2), using (10).
We need |γ|<1 for coastally-trapped solutions, which can only be attained by taking the upper root in (10) and (11).
The solutions are oscillatory in the cross-shore direction when hcos(kh/2)>2, and monotonically decaying when 0<hcos(kh/2)<2.
In either case, the full Kelvin wave solution is(12)um,n+12=pˆ12γnexp(ikxm),vm+12,n=0,pm+12,n+12=pˆ12γnexp(ikxm+12), where(13a)ω=(sin(kh/2)(kh/2))k,(13b)γ=2−hcos(kh/2)2+hcos(kh/2).
In the limit kh→0, (13a) and (13b) reduce to Eqs.
(3.7) and (3.11) of Hsieh et al.
[24], who derived similar discrete Kelvin wave solutions but ignored the effects of along-shore differencing.
We first compare the accuracy of ω for our discrete solution with that of the continuum solution (7d).
Note that (13a) implies |ω/k|<1, so that the phase speed is reduced from the continuum value of unity, and the waves become weakly dispersive.
For kh≪1, (13a) gives(14)ω=(1−(kh)224+O((kh)4))kas h→0, so that the expression for ω is second-order accurate in h. To obtain ω within 10% of the continuum value (7d), we require 4 or more grid points per wavelength (kh/2⩽π/4); to obtain ω within 1% of (7d), we require 13 or more grid points per wavelength (kh/2⩽π/13).
So, it is clear that the frequency and phase speed of this discrete Kelvin wave do depend on h. This in contrast to the conclusion of Hsieh et al.
[24], who implied that the phase speed of the inviscid Kelvin is exactly unity on the C-grid.
However, this is not the case when the effects of along-shore discretisation are taken into account, which is necessary to analyse discrete solutions of real numerical models.
To compare the spatial structure of (12) with (7a), (7b), (7c), we normalise (12) by setting p=exp(ikx) at y=0.
To O(h2), this is achieved by taking (3/2)pˆ12−(1/2)pˆ32=1, so that pˆ12=2/(3−γ).
We also write γn=γ−12γn+12=γ−12exp((n+12)logγ)=γ−1/2exp(−ryn+12), where(15)r=−1hlogγ=1hlog(2+hcos(kh/2)2−hcos(kh/2))=1+O(h2,(kh)2)as h→0, using (13b).
Thenpm+12,n+12=2γ−12(3−γ)−1exp(−ryn+12)exp(ikxm+12)⇒pm+12,n+12p(xm+12,yn+12)=γ−1/2(1−γ−12)−1exp((1−r)yn+12)=(1−h2+O(h2))(1+h2+O(h2))exp(O(h2,(kh)2)yn+12)=1+O(h2,(kh)2)as h→0.
Thus, the spatial structure of p is second-order accurate in h. The same conclusion applies to u, which is equal to p in (7) and (12).
So, two factors determine the overall accuracy of the solution: kh (a measure of how well the along-shore oscillations with wavenumber k are resolved), and h (a measure of how well the cross-shore structure with unit lengthscale is resolved).
From (14), the accuracy of the frequency is solely determined by kh; this is consistent with the derivation of (10), which only involved along-shore derivatives.
The cross-shore structure could be poorly resolved (and will be when h≳1) yet the frequency and wave speed will still be accurate for sufficiently small k. We now examine discrete Kelvin wave solutions for propagation along a coast aligned at 45° to the C-grid.
We take the discrete representation of the coast to be a staircase lying between y=x−h and y=x, as illustrated in Fig.
2(a).
The discrete boundary conditions are thus(16)uj,j−12=0,vj+12,j=0,j∈Z.
The corresponding continuum domain is y>x−xc, where xc could be positive or negative according to whether or not cut grid cells are retained in the computational domain.
In our detailed calculations, we suppose that xc=0 so that the coastline is at y=x.
Then, using along-shore and cross-shore coordinates X=(x+y)/2 and Y=(y−x)/2, the continuum Kelvin wave solution (7a), (7b), (7c) becomes(17)p=u(t)=e−YeikX,u(n)=0,where u(t)=v+u2,u(n)=v−u2.
Here, u(t) and u(n) are the components of the velocity that are tangential and normal to the coastline, respectively.
To construct discrete solutions, we introduce an along-shore index M and a cross-shore index N, defined via(18)M=m+n,N=n−m.
Using these new indices, we write any variable w as(19)wm,n=WM,N, with (18) giving (M,N) in terms of (m,n).
On u and v nodes, M+1/2 and N+1/2 are both integers, with UM,N only defined when M+N is odd and VM,N only defined when M+N is even.
On p nodes, M and N are both integers, with PM,N only defined when M+N is odd.
Then, the discretised equations of motion (8) become(20a)−iωUM,N−VM+1,N+VM,N−1+VM−1,N+VM,N+14=−PM+12,N−12−PM−12,N+12h,N⩾12,(20b)−iωVM,N+UM+1,N+UM,N−1+UM−1,N+UM,N+14=−PM+12,N+12−PM−12,N−12h,N⩾12,(20c)−iωPM,N+UM+12,N−12−UM−12,N+12+VM+12,N+12−VM−12,N−12h=0,N⩾0.
At the coastal nodes, which have N=−1/2 from (16) and (18), we instead apply (16):(21)UM,−12=VM,−12=0for all M. Since the geometry at fixed N is invariant under translations on the grid in the along-shore direction (i.e., the M index), we seek spatially periodic solutions of the form(22)UM,N=UˆNeiMθ,VM,N=VˆNeiMθ,PM,N=PˆNeiMθ.
If we suppose that there is a spatial wavenumber k in the along-shore direction, then increasing M by unity at fixed N (which corresponds to moving a distance h/2 ) increases the phase by kh/2; so we set(23)θ=kh2.
Substituting (22) into (20) gives−iωUˆN−14(VˆN−1+(eiθ+e−iθ)VˆN+VˆN+1)=−1h(eiθ/2PˆN−12−e−iθ/2PˆN+12),−iωVˆN+14(UˆN−1+(eiθ+e−iθ)UˆN+UˆN+1)=−1h(eiθ/2PˆN+12−e−iθ/2PˆN−12),−iωPˆN+1h(eiθ/2UˆN−12−e−iθ/2UˆN+12+eiθ/2VˆN+12−e−iθ/2VˆN−12)=0, which are simply coupled difference equations.
Seeking solutions of the form(24)(UˆN,VˆN,PˆN)=γjN(Aj,Bj,Cj), for some γj and (Aj,Bj,Cj) to be determined, we obtain(−iω−ajbjaj−iωcjbjcj−iω)(AjBjCj)=0, where(25a)aj=(γj+γj−1+2cosθ)/4,(25b)bj=(γj−12eiθ/2−γj12e−iθ/2)/h,(25c)cj=(γj12eiθ/2−γj−12e−iθ/2)/h.
For non-trivial solutions, the determinant of the 3×3 matrix must vanish, leading to ω=0 or ω2=aj2−bj2−cj2.
This latter condition may be written as(26)(γj+1γj)2+4cosθ(1−8h2)(γj+1γj)+4(cos2θ−4ω2+16h2)=0,⇒γj+1γj=16cosθh2(1−h28±1−(1+cos2θcos2θ)h24+ω2h416cos2θ).
The corresponding eigenvectors (24) have entries(27a)Aj=iωbj−ajcj,(27b)Bj=iωcj+ajbj,(27c)Cj=aj2−ω2.
At fixed ω, the most general solution would involve a sum of four eigensolutions (24) corresponding to the four values for γj implied by (26).
However, we seek Kelvin wave solutions, for which |γj|<1 (implying coastal trapping) and ω→k as h→0.
In this limit, the right-hand side of (26) is greater than 2 (whether the plus or minus sign is taken), so that there are four real roots for γj, only two of which have |γj|<1.
These two roots (γ1 and γ2) satisfy(28)γ1+γ1−1=2+μ2h2+O(h4)⇒γ1=1−μh+μ2h22+O(h3)(29)where μ=1+k2−ω22,(30)γ2+γ2−1=32h2(1+O(h2))⇒γ2=h232+O(h4), where (23) has been used to expand cosθ for h≪1.
Note that γ1 represents a physical mode, giving cross-shore decay on the scale of the Rossby radius of deformation, whilst γ2 represents a computational mode, with cross-shore decay determined by the grid spacing h. For the latter, it is simple to show from (24), (25), (27) and (30) that(31)(UˆN,VˆN,PˆN)∝h2N(1,1,2)as h→0, which thus corresponds to a computational boundary layer with a signature in both the velocity and pressure.
The possibility of such computational boundary layers along staircase coastlines was noted by Pedersen [14] and Cangellaris and Wright [10] in their calculations of (non-rotating) waves on the C-grid.
The general coastally-trapped solution is a sum of the physical and computational modes, and using (24) is(32)(UˆNVˆNPˆN)=α1γ1N(A1B1C1)+α2γ2N(A2B2C2), for some α1 and α2, where (Aj,Bj,Cj) are determined from (27) and (25).
Non-trivial solutions satisfying the boundary conditions (21), which imply Uˆ−1/2=Vˆ−1/2=0 from (22), can thus only be found when(33)|A1A2B1B2|=0.
Then, normalising the solution so that Pˆ0=1, which implies α1C1+α2C2=1, and combining with either boundary condition to find α1 and α2, (32) becomes(34)(UˆNVˆNPˆN)=1C1−(γ2/γ1)1/2A1C2/A2(γ1N(A1B1C1)−γ2N+1/2γ11/2(A1B1A1C2/A2)).
Note that N+1/2 should be a non-negative integer to evaluate UˆN and VˆN, and N should be a non-negative integer to evaluate PˆN.
At given k and h, the discrete Kelvin wave frequency ω is determined by (33), with A1,2 and B1,2 given in terms of ω using (27a), (27b), (25), and (26).
These coupled nonlinear equations are solved easily using a Newton–Raphson iteration, which is initialised with the continuum Kelvin wave frequency ω=k.
Solutions for ω determined in this way are shown in Fig.
3, for k=1/2, 1, 3/2 and 2, and various values of h. In each case, it is clear that ω is less than the continuum value k, and the errors in ω for this staircase coastline ∂Ωs are much greater than those for a straight coastline ∂Ω aligned with the grid.
For example, with eight grid points per wavelength, the errors with ∂Ωs are in the range 14–32% for these values of k, whereas (13a) implies those with ∂Ω are under 3%.
Further, the deviation of ω from k appears to scale with h as h→0 for ∂Ωs, rather than with h2 as implied by (14) for ∂Ω.
The same kind of behaviour can be made out in Fig.
3 of Schwab and Beletsky [19], although the accuracy of their results was compromised by their less precise calculations (involving time-stepping the equations of motion in a square basin).
To quantify the nature of the convergence, we measure the relative error in ω from the continuum value k via(35)Δ=k−ωk, which will be positive, since ω<k (from Fig.
3).
To confirm that Δ is proportional to h as h→0, we consider Δ/h as a function of h for h≪1.
As shown in Fig.
4 for k=1/2, 1, 3/2, and 2, Δ/h does approach a constant Δ1 as h→0, where Δ1 is independent of k. Its value may be estimated by fitting a curve of the form Δ/h=Δ1+Δ2h+Δ3h2 to each set of points in Fig.
4; for each value of k, we find Δ1=0.35355, to five decimal places.
However, the next-order correction Δ2 depends on k. The numerical results may be confirmed analytically by seeking an asymptotic solution for ω in the form(36)ω=k(1−Δ1h+O(h2)),as h→0.
Then, (29) implies μ=(1+k2Δ1h+O(h2))/2, so that (28) implies(37)γ1=1−h2+(1−22k2Δ14)h2+O(h3).
Substituting this and (30) in (25a), (25c), straightforward but lengthy calculations yield asymptotic expansions for a1,2 and c1,2, and then for A1,2 from (27a):(38)a1=1+O(h2),c1=ik−12−k2Δ12h+O(h2),A1=1−k22+kΔ12(2k+i(k2−1))h+O(h2),(39)a2=8h2(1+O(h2)),c2=−42h2(1−ikh22+O(h2)),A2=322h4(1−ikh22+O(h2)).
Note that b1,2 and B1,2 can be inferred: when h≪1, so that γ1,2 are real, it follows from (25) and (27) that a1,2 are real and bj=−cj⁎, so that Bj=Aj⁎.
Then the solvability condition (33) becomes Im(A1A2⁎)=0, and (38) and (39) implyk(1−k2)(1−22Δ1)+O(h)=0.
Thus, in the general case with h≪k(1−k2), it follows that Δ1=(22)−1, so that (36) gives(40)ω=k(1−h22+O(h2))as h→0.
This is the desired result.
It shows that the frequency and phase speed of the Kelvin wave are only predicted to first-order accuracy in h as h→0, in contrast to the solution when the coast and grid are aligned which is accurate to second-order in h. Further, the relative error Δ=(22)−1h+O(h2) is independent of k as h→0.
The asymptotic results agree with the numerical solutions shown in Fig.
4, where we found Δ∼0.35355h as h→0.
The numerical solutions show that the same relative error is obtained when k=1, where a modified asymptotic analysis would be required.
Having determined ω, the horizontal structure (34) may be deduced as h→0.
For γ1,2, we substitute Δ1=(22)−1 in (37), and use the leading-order term of (30):(41)γ1=1−h2+(1−k24)h2+O(h3),γ2=h232+O(h4).
For A1,2, we substitute Δ1=(22)−1 in (38), and use (39).
Then B1,2 may be deduced, since Bj=Aj⁎ in this limit.
Finally, for C1,2 we calculate the first two terms of (27c) using (38), (39) and (40):(42)C1=1−k2+k2h2+O(h2),C2=64h4(1+O(h2)).
Then A1C2/A2=(1−k2)+k2h/2+O(h2)=C1+O(h2) (so that the two column vectors in (34) agree to O(h2)), and C1−(γ2/γ1)1/2A1C2/A2=(1−k2)+2(5k2−1)h/8+O(h2).
So (34) becomes(43)(UˆNVˆNPˆN)=(γ1N−γ2N+1/2γ11/2)(1/2+(1−2ik)h/81/2+(1+2ik)h/81+2h/8)+O(h2), with γ1,2 given by (41).
The term involving γ1N (the physical mode) dominates when N≫1 or when N⩾0 and h≪1, since the term involving γ2N+1/2 (the computational mode) is O(h2N+1).
However, the two terms cancel out when N=−1/2, in order to satisfy the boundary conditions (21).
For small positive values of N and moderate values of h, there is a transition between these two regimes – i.e., a computational boundary layer (cf.
[14,10]).
The accuracy of the spatial structure can be assessed by comparing (43) with the continuum solution (17).
For the pressure, the accuracy is largely determined by the rate of decay away from the coast of the physical mode.
For N⩾1, PˆN=γ1N+O(h)=exp(Nlogγ1)+O(h)=exp(−rYN)+O(h), where YN=Nh/2 is the distance perpendicular to the coastline, andr=−2hlogγ1=1+k2h22+O(h2)as h→0, using (41).
Since the continuum solution (17) has a decay rate r=1, the decay rate of the discrete solution is thus only accurate to first-order in h, so that the spatial structure of the pressure is also only accurate to first-order in h. This is to be contrasted with the behaviour (15) when the coast and grid are aligned, with r=1+O(h2) as h→0.
If the coastal u and v nodes lay within the continuum domain, then there would be O(1) errors in the velocity at those nodes (since the continuum flow is alongshore, whereas either u or v is zero in the discrete solution).
However, as we have formulated the problem, with the coastal u and v nodes outside of the continuum domain as shown in Fig.
2(a), such O(1) errors do not arise.
Then, for the interior velocity nodes (with N⩾1), (43) matches the continuum solution (17) to leading order.
However, there is an error at O(h), which is clearly illustrated by calculating the normal velocity u(n)=(v−u)/2 implied by (43) on the pressure nodes:(44)UM,N(n)=12(VM−12,N−12+VM+12,N+12−UM−12,N+12−UM+12,N−12)=(ikh22γ1N+O(h2))eiMθ.
Thus, the normal velocity is not identically zero, as in the continuum solution (17) or the discrete solution (12) when the coast and grid are aligned.
Instead, there is a weak cross-shore flow of O(h).
We conclude that the discrete solution (43) for u and v is (at best) first-order accurate in h. We have shown that the frequency and horizontal structure of Kelvin waves are obtained at second-order in h when the grid and coast are aligned, but only at first-order in h when the grid and coastline are aligned at 45°.
We now confirm these results and investigate other alignments numerically.
The numerical experiments are performed in a periodic channel of length L and (finite) width D. The channel is discretised on the C-grid as shown in Fig.
5, with the left-most and right-most u nodes mapping to each other.
The number of grid cells in the x-direction and across the channel in the y-direction are nx and ny, respectively, and the vertical offset between the ends of the channel is ns grid cells.
So, in Fig.
5, nx=6, ny=4, and ns=3.
Denoting the angle between the channel walls and x-axis by θ, it follows that the geometry must be chosen so that(45a)nyhcosθ=D,(45b)Lcosθ=nxh,(45c)Lsinθ=nsh.
In particular, (45b), (45c) imply that tanθ=ns/nx must be chosen to be a rational number.
The nondimensional discretised equations of motion (8) and boundary conditions are written as an N×N linear system (L−iωI)a=0, where a is a column vector containing all of the unknowns, and N is the total number of u, v, and p nodes within the domain.
This is an eigenvalue problem for the unknown frequency ω; starting from an initial guess, particular eigenvalues are found iteratively using a routine from ARPACK [25].
At fixed θ, an eigenvalue ω is first found at large nx starting from an initial guess ω=k=2π/L, which is the frequency for a continuum Kelvin wave of wavelength L. The discrete solution obtained is then tracked to smaller values of nx using initial estimates for ω based on eigenvalues already obtained for larger values of nx.
Although we have formally considered Kelvin waves in a half-space in our analysis, the continuum solution (7) and the discrete solution of Section 3 remain valid in a channel of width D, since the cross-shore flow is everywhere equal to zero.
The discrete solution of Section 4 is not valid in a channel of finite width, since the condition of no normal flow has not been applied at the far coastline.
However, since solutions decay exponentially in the cross-shore direction, the resulting discrepancy between theory and numerics is expected to be exponentially small in D. We take D close to 4π; in test runs, the values of ω thus obtained change by less than 10−12 when the solutions are recalculated with D close to 8π.
We measure the relative error in the frequency from the continuum value (7d) using Δ, defined in (35).
Shown in Fig.
6 are calculations of Δ when θ=0 and θ=π/4, for along-shore wavenumbers k=1/2, 1 and 2 in a channel of width D=4π.
It is clear that there is excellent agreement between the numerical results and the analytical predictions, even for relatively large values of h. The more rapid convergence when θ=0 is evident, with (13a) giving Δ∼(kh)2/24 as h→0, thus implying greater errors for short waves.
In contrast, when θ=π/4, the theoretical prediction based on (40), which implies Δ∼(22)−1h as h→0, is confirmed, with Δ independent of k for h≪1.
The horizontal structure of the solutions is shown in Fig.
7, for calculations with nx=16.
In the bottom row, for θ=0, one can see that the pressure p and tangential velocity u(t) are equal, and the normal velocity u(n)=0, in agreement with the continuum solution.
In the top row, for θ=π/4, one can see that the pressure p and tangential velocity u(t) are close to being equal outside the computational boundary layer, and that there is a weak cross-shore flow u(n), in agreement with (44).
We have no analytical predictions for the accuracy of the solution when 0<θ<π/4.
However, numerical results show that first-order convergence in h is attained, as when θ=π/4.
Shown in Fig.
8 is the behaviour of Δ as a function of h when k=1, for four different values of θ in (0,π/4), along with that for θ=π/4.
For each value of θ, also shown is a line Δ=Δ1h, determined numerically by fitting each set of points via Δ/h=Δ1+Δ2h+Δ3h2.
One can see that these lines give excellent fits, showing that first-order convergence in h is obtained as h→0 for each value of θ, even for θ≪1.
The corresponding values of Δ1 are shown in Table 1; they decrease from the analytical prediction of (22)−1 at θ=π/4 to much smaller values as θ→0.
(The same values of Δ1 are obtained when the analysis is repeated at k=1/2 and k=2, suggesting that Δ1 is independent of k, as was proved to be the case when θ=π/4.)
However, even when θ≪1, so that the channel may have just a single step, the convergence in the frequency is still reduced to first-order in h. The horizontal structure of solutions with tanθ=1/2 and tanθ=1/4 is shown in the second and third rows of Fig.
7.
In both cases, the Kelvin wave structure is clearly visible in p and the along-shore flow u(t).
However, in both cases the staircase boundaries induce a non-zero cross-shore flow u(n), which is inconsistent with the continuum solution.
Although the free Kelvin wave problem is of considerable theoretical importance, problems with forcing and damping have greater practical importance.
In nature, the forcing could be due to a wind stress at the free surface or an astronomical tidal potential, and the damping could be due to the turbulent stress of a bottom boundary layer.
Regardless of the details, the forced response is composed of shallow-water waves, possibly including Kelvin waves, with the largest amplitudes in waves with a natural frequency ωf close to that of the forcing frequency ω; various examples of this sort are given in Chapters 9 and 10 of Gill [16].
When ω≈ωf, there is a large amplitude near-resonant response, the size of which is sensitive to the weak damping and |ω−ωf|.
Thus, in numerical solutions of near-resonantly forced waves, we anticipate that errors in ωf (associated with the spatial discretisation) could lead to non-trivial errors in the forced response.
We investigate this in an idealised setting, by calculating the response to a prescribed time-harmonic forcing and linear damping, in our existing geometry of a periodic channel of length L and width D, as illustrated in Fig.
5.
We again consider the nondimensional equations (6) for motions of the form (5) with frequency ω, but with forcing and damping terms ∇Φ−ϵu added to the right-hand side of (6a), (6b).
We take Φ(x,y)=(1+y)/(5/4+cos(2πx/L)), with x the distance along the channel and y the distance across the channel; this particular forcing is not chosen with any particular application in mind, but rather to ensure the excitation of a wide spectrum of shallow-water waves.
For simplicity, the damping is taken to be linear, with timescale ϵ−1.
We take ϵ=0.05, corresponding to weak damping (ϵ≪ω), as would typically be the case in lakes and oceans.
Solutions are calculated numerically by writing the discretised equations of motion and boundary conditions as a linear system (L−iωI)a=b, where a is a column vector containing all of the unknowns, and b is a column vector containing the forcing.
The system is solved by a direct matrix inversion for two cases: (i) where the channel and grid are aligned; (ii) where the channel and grid are aligned at 45° (i.e., the coasts are perfect staircases).
The nature of the response thus obtained is characterised by the time-averaged and domain integrated energyE(ω)=h24∑m,n(|uˆm,n+12|2+|vˆm+12,n|2+|pˆm+12,n+12|2).
Solutions have been calculated for the parameters D=π/2 and L=4π, implying that along-channel wavenumbers k={1/2,1,3/2,2,…} are permitted.
Since our focus here is on Kelvin waves (with frequencies ω=k={1/2,1,3/2,2,…}) rather than Poincaré waves (e.g., [16], with frequencies ω⩾1+k2+(π/D)2⩾21/2), we restrict attention to the frequency range 0.2⩽ω⩽1.8, thus allowing resonances for Kelvin waves at ω=1/2, 1 and 3/2.
Note that, at mid-latitudes with f˜=10−4 s−1, the corresponding dimensional frequency range of 0.2×10−4 s−1 to 1.8×10−4 s−1 includes both diurnal and semi-diurnal (tidal) frequencies of 0.7×10−4 s−1 and 1.4×10−4 s−1.
The energy E of the forced response is shown as a function of forcing frequency ω in Fig.
9(a), for the case where the channel and grid are aligned, at relatively low resolution (h=π/6, or 24 points along the channel) and high resolution (h=π/32, or 128 points along the channel).
As expected, the response is largest when ω is close to 1/2, 1 or 3/2, corresponding to the near-resonant excitation of a Kelvin wave.
However, the maxima in E are displaced a little to the left of the continuum resonant frequencies, since the frequency of discrete Kelvin waves (14) is reduced by O(kh)2 in this orientation.
The displacement is most pronounced at h=π/6 and for the shortest resonant wave (with k=ω=3/2), which is then resolved by just 8 points per wavelength.
A corresponding plot is shown in Fig.
9(b) for the case where the channel and grid are aligned at 45°.
The same overall pattern is obtained, but the maxima in E are now further to the left of the continuum resonant frequencies.
These greater displacements are due to the larger frequency reduction (40) of O(h) for discrete Kelvin waves travelling along staircase coastlines.
The errors are obvious when h=0.555, corresponding to about 22 grid points per wavelength for the longest mode with k=1/2, but are still evident when h=0.093, corresponding to 135 grid points per wavelength at k=1/2.
It is clear from Fig.
9(a), (b) that the amplitude of the forced response is sensitive to the spatial resolution and the orientation of the grid, especially for near-resonant forcing.
This is quantified in Fig.
9(c), (d), where E is shown as a function of h at five values of ω clustered around the continuum Kelvin wave resonance at ω=1.
In Fig.
9(c), for the case where the channel and grid are aligned, it can be seen that E converges rapidly as h→0, with errors of 10% or more in E only when the forcing is near resonant (|ω−1|≲0.1) and h≳0.5 (corresponding to less than 13 points per wavelength).
In Fig.
9(d), for the case where the channel and grid are aligned at 45°, it can be seen that the solutions do converge towards the same values of E as h→0, but at a much slower rate.
At resonance (ω=1), to obtain E to within 10% of the limiting value requires h≲0.05, corresponding to more than 125 grid points per wavelength.
Note that the energy of resonant modes will always be underestimated when the frequencies of free waves are reduced by the spatial discretisation, since the peak of the response will be shifted to the left in frequency space, as in Fig.
9(a), (b).
The same conclusion holds (at sufficiently small h) for waves just above resonance (e.g., at ω=1.1), but, by the same reasoning, E is overestimated for those modes just below resonance (e.g., at ω=0.9), consistent with the results shown in Fig.
9(c), (d).
Thus, staircase boundaries degrade the accuracy of forced shallow-water solutions involving Kelvin waves.
Even when the Kelvin waves are apparently well-resolved (e.g., with 125 grid points per wavelength), the errors in the amplitude of the response can be 10% or more, if near-resonant waves are involved (although note that the amplitude of the response is sensitive to the assumed value of the damping coefficient ϵ).
The degradation is likely to be non-trivial for tidal flows, which often involve the near-resonant forcing of Kelvin waves (e.g., [26,27]).
For example, barotropic Kelvin waves on continental shelves (which dominate the tidal response around the U.K., for example) have a wavelength of about 1000 km at semi-diurnal frequencies, so that a grid spacing of about 5 km (or 1/20th of a degree of latitude) would be required to obtain frequencies to within 1% (see Fig.
8).
Although such resolution is attainable by regional models, it is at the limit for present-day global tidal models.
Indeed, this may be one of the reasons for the strong resolution dependence of global tidal solutions [28,27], which are typically overly energetic at lower resolutions (cf.
the curve for ω=0.9 in Fig.
9(d)).
Having established in Sections 4 and 5 that staircase boundaries lead to discrete Kelvin wave solutions that are accurate to first-order in grid spacing h, we now consider how the boundary treatment may be modified to achieve second-order accuracy.
In this brief discussion, we restrict attention to the configuration of Fig.
2(a), with the grid aligned at 45° to the continuum coastline at y=x.
Then, the coast has a unit normal vector n and unit tangential vector t given byn=(1,−1)2,t=(1,1)2, and the coastal boundary condition is(46)u⋅n=0on y=x.
We consider how this is implemented on an arbitrary coastal grid cell [xn,xn+1], [yn,yn+1].
In the staircase approximation, the computational coastline is deformed from y=x to enclose the entire coastal grid cell, and one applies un+1,n+1/2=0 and vn+1/2,n=0.
Assuming that ∂u/∂x and ∂v/∂y are of order unity, these two boundary conditions imply that un+1/2,n+1/2=O(h) and vn+1/2,n+1/2=O(h) respectively, so that u⋅n=O(h) at (xn+1/2,yn+1/2) and (46) is satisfied to first-order.
It would thus be surprising if the resulting discrete solutions were more accurate than first-order in h. The same reasoning implies that u⋅t=O(h) at (xn+1/2,yn+1/2), i.e., that the tangential component of the flow is also restricted to be small, suggesting the possibility of O(1) errors.
However, as was illustrated in the top row of Fig.
7, whilst u⋅n is of O(h) in numerical solutions (as expected), u⋅t is of order unity at (xn+1/2,yn+1/2).
The disparity between this outcome and our reasoning arises because the discretised representations of ∂u/∂x and ∂v/∂y are not of order unity at the coastline as assumed – rather, they have magnitude h−1 because of the thin computational boundary layer.
The reason why the restriction on u⋅t is eased whilst that on u⋅n is maintained is because the computational mode (31) has (UˆN,VˆN)∝(1,1) as h→0 (whatever the frequency of the wave), so that to satisfy the staircase boundary conditions the physical mode is forced to have the same horizontal structure, with (UˆN,VˆN)∝(1,1) too, as h→0.
Then just one grid point away from the coast, where the amplitude of the computational mode has fallen from order unity to O(h2), the discrete solution is dominated by the physical mode and thus has (UˆN,VˆN)∝(1,1) to leading order, so that u⋅n is constrained to be small with no restriction on u⋅t.
Thus, the staircase boundary conditions have the desired effect of constraining u⋅n alone because of the particular horizontal structure of the computational mode, although it would have been hard to predict this outcome in advance.
These issues can be circumvented by abandoning the artificial deformation of the continuum domain to a staircase boundary, and instead taking the computational domain to match the continuum domain.
We retain the grid and nodes shown in Fig.
2(a), i.e., a regular Cartesian grid with no deformation or refinement near the coastline.
Then the velocity nodes un+1,n+1/2 and vn+1/2,n lie outside the computational domain and are regarded as ghost nodes, to be used to satisfy boundary conditions or dynamical conditions elsewhere in the continuum domain.
For example, these ghost nodes will be required in the calculation of the Coriolis terms in (8a) at (xn,yn+1/2) and in (8b) at (xn+1/2,yn+1), and in the calculation of ∇⋅u in (8c) at the coastal pressure node at (xn+1/2,yn+1/2).
However, these ghost nodes can also be used to discretise (46).
This can be achieved to second-order accuracy at the pressure node (xn+1/2,yn+1/2) via(47)un,n+12+un+1,n+1222=vn+12,n+vn+12,n+122, or at the vorticity node (xn,yn) via(48)un,n−12+un,n+1222=vn−12,n+vn+12,n22.
With the equations of motion (8) applied at all nodes within the computational domain (including the coastal pressure nodes on the boundary), we again consider the general discrete solution (32) for (unforced) Kelvin waves on the C-grid.
Then instead of applying the two staircase conditions un+1,n+1/2=0 and vn+1/2,n=0, which yielded the dispersion relation (33), we apply (47) and (48), which in terms of the rotated indices (18) implyUM+12,−12+UM−12,12=VM−12,−12+VM+12,−12,UM+12,12+UM−12,−12=VM+12,−12+VM−12,12, respectively.
For wavelike solutions of the form (22), we then have(49a)e+iθ/2Uˆ−12+e−iθ/2Uˆ12=e−iθ/2Vˆ−12+e+iθ/2Vˆ12,(49b)e−iθ/2Uˆ−12+e+iθ/2Uˆ12=e+iθ/2Vˆ−12+e−iθ/2Vˆ12, which imply Uˆ1/2=Vˆ−1/2 and Vˆ1/2=Uˆ−1/2 (cf.
Uˆ−1/2=Vˆ−1/2=0 for staircase boundaries).
Substituting from the general solution (32) for Kelvin waves, non-trivial solutions are only obtained when(50)|γ1+1/2A1−γ1−1/2B1γ2+1/2A2−γ2−1/2B2γ1−1/2A1−γ1+1/2B1γ2−1/2A2−γ2+1/2B2|=0, where Aj, Bj and γj are determined from (25), (26) and (27).
Like (33), which is the dispersion relation for discrete Kelvin waves with staircase boundary conditions, (50) is a dispersion relation for these modified boundary conditions.
It can be solved easily using a Newton–Raphson iteration, which is initialised with the continuum Kelvin wave frequency ω=k.
As expected, solutions for ω determined in this way are found to be second-order accurate in h. Writing the relative error Δ∼Δ2h2 as h→0 and making a fit to data determined numerically from (50), we find values for Δ2 as a function of k as shown in Fig.
10.
Also shown is the corresponding curve Δ2=k2/24 obtained when the coast and grid are aligned, from (14).
So, the errors from solutions on a grid aligned at 45° to the coastline need be no larger than those when the coast and grid are aligned, provided appropriate coastal conditions (such as (47) and (48)) are applied.
In addition to achieving second-order accuracy in the Kelvin wave frequency, these modified boundary conditions lead to a reduction in the amplitude of the computational mode in the solution.
This can be quantified by calculating(51)ϕu,v=|α2γ2−1/2A2α1γ1−1/2A1|andϕp=|α2C2α1C1|, which measure the ratio of the maximum amplitude of the computational mode to that of the physical mode, for the velocity (taking N=−1/2 in (32)) and pressure (taking N=0 in (32)), respectively.
As h→0, we can use the scalingsγ1∼1−h2,γ2∼h232,A1∼1−k22,A2∼322h4,C1∼1−k2,C2∼64h4, which are obtained from (41), (38), (39) and (42), to writeϕu,v∼2562|1−k2|h5|α2α1|andϕp∼64|1−k2|h4|α2α1|as h→0.
For the standard staircase boundary conditions, Uˆ−1/2=0 implies α1γ1−1/2A1+α2γ2−1/2A2=0, so that α2/α1∼−(2562)−1(1−k2)h5, giving ϕu,v∼1 and ϕp∼(42)−1h as h→0.
That is, the computational mode has the same amplitude as the physical mode in (u,v) at the boundary (where it must do so for the boundary conditions to be satisfied), but it only appears as a small correction of O(h) in p. For the modified boundary conditions (47) and (48) leading to (50), Uˆ1/2=Vˆ−1/2 implies (γ11/2A1−γ1−1/2B1)α1+(γ21/2A2−γ2−1/2B2)α2=0, so that α2/α1∼−(1−k2)h6/512, giving ϕu,v∼h/2 and ϕp∼h2/8 as h→0.
Thus, the amplitude of the computational mode is reduced by one order in h, and it will be hard to see the computational boundary layer in the horizontal structure when h≪1.
Further, the horizontal derivatives of u and v at the boundary associated with the computational mode are order unity, rather than of magnitude h−1 as for the staircase boundary conditions, suggesting that standard Taylor expansions leading to (47) and (48) are self-consistent.
Applying (47) and (48) is not the only way to obtain discrete Kelvin wave solutions to second-order accuracy.
Instead of applying u⋅n=0 twice for each coastal grid cell, one may apply either (47) or (48) along with a dynamical condition.
For example, taking the projection of the momentum equations (6a), (6b) in the along-shore direction gives−iωu⋅t=−t⋅∇pat y=x, where the Coriolis term, which is proportional to u⋅n, vanishes due to (46).
This equation may be discretised to second-order accuracy at the pressure node (xn+1/2,yn+1/2) via(52)−iω(un,n+12+un+1,n+1222+vn+12,n+vn+12,n+122)=−pn+32,n+32−pn−12,n−1222h, which for wavelike solutions (22) implieseiθ/2Uˆ−12+e−iθ/2Uˆ12+e−iθ/2Vˆ−12+eiθ/2Vˆ−12=2sin2θωhPˆ0.
When coupled with (49a), one may obtain another dispersion relation for discrete Kelvin waves in the form of a vanishing determinant, by substituting from (32) and demanding non-trivial solutions.
Frequencies obtained in this way are again accurate to second-order in h, and lead to values of Δ2 within 10−5 of those obtained from (50), i.e., they are indistinguishable to those shown in Fig.
10.
We conclude that it is possible to obtain discrete Kelvin wave solutions that are accurate to second-order in h (at least in wave frequency ω) using a regular grid identical to that of the standard staircase formulation, provided appropriate boundary conditions are implemented.
Such schemes have the advantage that existing grid generation algorithms may be used, with only minor changes to the discretised equations of motion being required at the coastal nodes.
This approach is similar to others used for modelling acoustic and electromagnetic waves (e.g., [29–32,13]), although the Coriolis terms of the rotating shallow-water equations place additional restrictions on the choice of discretisation.
On the Arakawa C-grid, the representation of a straight coastline as a staircase has a profound influence on the accuracy of discrete Kelvin wave solutions.
We have shown that the frequency and horizontal structure of the discrete Kelvin wave then converge towards the continuum values at first-order in grid spacing h, despite the second-order finite differences used within the domain.
This was proved analytically when the coast is aligned at 45° to the grid, but we have demonstrated numerically that the same result holds for other values of the angle θ between the coastline and the grid, even when θ≪1.
This is to be contrasted with solutions for which the coast and grid are aligned, where we proved that the frequency and horizontal structure are obtained at second-order in h. For example, when the along-coast wavenumber k=1, one may compare three cases: (i) grid and coast aligned, with relative error Δ≈h2/24 in the frequency and phase speed, (ii) grid and coast at an angle of tan−1(1/8)=7.1°, with Δ≈0.14h (from Table 1); (iii) grid and coast at an angle of 45°, with Δ≈(22)−1h≈0.35h.
To obtain the frequency and phase speed to within 10% (i.e., Δ<0.1), implies (i) h≲1.55, or 4 points per wavelength; (ii) h≲0.70, or 9 points per wavelength, (iii) h≲0.28, or 22 points per wavelength.
To obtain the frequency and phase speed to within 1% (i.e., Δ<0.01), implies (i) h≲0.49, or 13 points per wavelength; (ii) h⩽0.069, or 90 points per wavelength, (iii) h≲0.028, or 222 points per wavelength.
In practice, model coastlines would most likely be irregular and thus contain a range of angles, suggesting that scenario (iii) would give the most reasonable resolution estimates.
Then, high accuracy solutions (with frequency and phase speed correct to within 1%) may only be obtained at considerable computational expense (over 200 grid points per wavelength), perhaps making the computation infeasible in large domains.
In addition to recovering the frequency and phase speed, it is desirable for discrete solutions to recover two other important properties of the continuum Kelvin wave: no cross-shore flow, and no dispersion (d2ω/dk2=0).
When the coast and grid are aligned, the discrete Kelvin wave solution (9) has zero cross-shore flow, and d2ω/dk2 is O(h2).
When the coast and grid are not aligned, we have shown that staircase boundary conditions induce a weak cross-shore flow in the discrete solution, which was shown to be O(h) when the coast is aligned at 45°.
Such a flow is undesirable, physically.
There is better news for dispersion.
Since the O(h) correction to the phase speed is independent of k (which was proved analytically for θ=π/4, although it appears to generalise to 0<θ<π/4), it follows that d2ω/dk2 is O(h2).
So, the dispersive errors with staircase boundaries are of the same order of magnitude as those when the coast and grid are aligned.
Of course, here we have sought solutions that are time-harmonic and have thus only considered errors due to spatial differencing.
For time-dependent calculations, the time-stepping scheme could also introduce dispersive errors; some discussion of this for Kelvin waves (when the coast and grid are aligned) is given by Henry [33].
The degree to which solutions calculated using staircase boundaries are degraded will depend upon the nature of the problem under consideration.
As shown in Section 6, the degradation could be severe for problems involving the near-resonant forcing of Kelvin waves, since even small errors in the frequencies of the waves can lead to large errors in the forced response.
Such degradation is likely to be encountered when simulating tidal flows, which often involve a strong Kelvin wave component (e.g., [26,27]) and for which the response is dominated by waves coming into resonance with the tidal forcing.
Thus, Kelvin wave frequencies need to be calculated to high accuracy, and this is unlikely to be achieved with staircase boundaries for large-scale calculations.
Indeed, staircase boundary conditions may be one of the reasons for the strong resolution dependence of global tidal solutions at low spatial resolutions [28,27].
In addition to the errors in the frequency and wave speed, the horizontal structure of the discrete Kelvin wave solutions inherits a (non-physical) computational boundary layer when the coast and grid are not aligned.
This was examined in detail when θ=π/4, where it was shown that the discrete equations of motion admit two modes trapped against the coastline: a physical mode, and a computational mode (cf.
[14,10]).
The latter leads to a computational boundary layer with a width scaling like h2, which is a key part of the discrete Kelvin wave solution, since it is required to bring the velocity to zero at the coast.
Further, as discussed in Section 7.1, the horizontal structure of the computational mode plays a profound role in constraining the normal component of the flow to be small close to the coastline, whilst leaving the tangential component of the flow unconstrained.
Throughout this study, we have assumed a fluid of constant depth bounded by straight coastlines.
More generally, one must consider spatially varying bathymetry and curved coastlines.
In the shallow-water model, the alignment of any varying bathymetry with the grid should not influence the convergence of the numerical scheme, provided the bathymetry is smooth and well-resolved on the model grid – it is simply a matter of sufficiently accurate horizontal differentiation of the bathymetry in the pressure equation.
(Of course, for a fully three-dimensional model with staircase bathymetry, one would only expect first-order convergence in the vertical grid spacing, possibly leading to the kind of numerical errors reported elsewhere [8].)
For a smooth curved coastline represented as a staircase, the same first-order discretisation errors examined here will occur, degrading the accuracy of schemes with spatial differencing of second-order or higher in the ocean interior.
However, it is worth noting that real coastlines are not smooth; indeed, they are often regarded as having a fractal geometry, with features on arbitrarily small lengthscales [34].
In such cases, one might argue that a stepped coast is a better representation of reality than a smooth coast, and that the discrete Kelvin wave solutions (for a perfect staircase) could be more physically relevant than the continuum solutions (for a perfectly straight coastline).
The staircase-induced retardation of the Kelvin waves in the discrete solutions would then be regarded as the imprint of a real physical effect due to a rough coastline.
This is consistent with the results of Mysak and Tang [35], who showed analytically that the speed of (continuum) Kelvin waves at a straight coastline is reduced when small random deviations are added.
A detailed discussion of the interplay between grid size and the resolution of the bathymetry and coastline, and how this influences model accuracy, is well beyond the scope of this study, but has been addressed to some extent elsewhere (e.g., [36,37]).
Although our calculations have been restricted to Kelvin waves on the C-grid, it is likely that staircase boundaries also lead to first-order accuracy for similar types of motion on the C-grid, and perhaps on other types of finite-difference grid too, for the reasons given in Section 7.1.
As discussed in the Introduction, the results of Pedersen [14] and Greenberg et al.
[15] for other simple flows are consistent with this conclusion.
Similar first-order convergence is also expected for other systems of equations discretised using simple staircase boundary conditions, such as the viscous shallow-water equations.
Even in the case of no-slip boundaries, when both components of the velocity are set to zero which is apparently consistent with staircase boundary conditions, there is still a first-order geometrical error in doing so – although this could be improved to a second-order error using an extension of the ideas given in Section 7.2.
For stress-free boundaries, there is the additional complication of discretising the stress-free condition, which would have to be done with care to achieve second-order accuracy.
Indeed, in a study of nonlinear wind-forced solutions in a square basin (performed at fixed resolution of h=5/9, in our notation), Adcroft and Marshall [38] showed that a standard implementation of stress-free boundary conditions leads to an O(1) error when the coast and grid are not aligned.
There are of course alternative numerical methods that can be used for non-rectangular coastal geometries.
For annular domains, the use of polar coordinates allows boundary conditions to be implemented without approximation, so that the accuracy of the solutions is only limited by the temporal and spatial differencing of the interior.
A recent study of this type was made by Steinmoeller et al.
[39], who modelled internal waves in annular lakes using pseudospectral spatial differencing, thus obtaining exponential convergence.
For more complex geometries, one may consider cut-cell methods (e.g., [8,9]) or unstructured meshes (e.g., [40]) coupled with a finite-element or finite-volume treatment.
For example, discontinous Galerkin treatments of the shallow-water equations have become popular in recent years (e.g., [41–43] and references therein).
Indeed, Cotter et al.
[42] considered Kelvin wave propagation within such a framework, and used numerical simulations to demonstrate second-order convergence for propagation along a straight coastline, and the maintenance of a coherent Kelvin wave structure for propagation around a circular basin.
However, it is worth remembering that the convergence of finite-element methods is often less than second-order (e.g., [44,43]), and that care is required when discretising the boundary (e.g., [41]).
Nevertheless, many lake and ocean models are still based on Arakawa grids using second-order finite-differences in the interior, and for such models the first-order errors induced by staircase boundaries should be better recognised and understood, as should the possibility of introducing modified boundary conditions to obtain second-order accuracy.
We have given two examples of how this may be achieved for the simple case of a perfect staircase coastline, and it would be worthwhile to construct generalisations of these schemes to arbitrary coastal geometries, whilst conserving mass and energy.
This study was initiated during a visit to the Isaac Newton Institute at the University of Cambridge during December 2012, as part of the programme on Multiscale Numerics for the Atmosphere and Ocean.
The support of the Institute is acknowledged.
Subsequent work was supported through NERC grant NE/I013563/1 at the University of Leeds, where discussions with Vladimir Lapin and Mark Kelmanson were helpful.
Comments from two anonymous referees also helped to improve the manuscript.
Atomistic investigation of the structure and transport properties of tilt grain boundaries of UO2 We apply atomistic simulation techniques to address whether oxygen shows higher diffusivity at the grain boundary region compared to that in bulk UO2, and whether the relative diffusivity is affected by the choice of the grain boundary.
We consider coincident site lattice grain boundaries, Σ3, Σ5, Σ9, Σ11 and Σ19, expressing the {nn1}, {n11}, and {n10} surfaces, and evaluate the extent that the grain boundary structures affect the diffusion of oxygen.
We found that oxygen diffusion is enhanced at all boundaries and in the adjacent regions, with strong dependence on the temperature and local structure.
The performance of uranium dioxide (UO2) as a nuclear fuel material [1–3] is undermined by the corrosion of the material.
Oxidation is a concern during the fuel cycle, from fresh fuel fabrication to spent fuel storage, as it causes drastic changes in the physical and thermal properties of the material.
The fuel has a complex structure, with uranium exhibiting a range of oxidation states from II to VI, which results in a complex range of stoichiometries within the uranium–oxygen system, where 16 non-stoichiometric oxides have been identified between UO2 and UO3 [4].
An in depth discussion of the different phases of the U–O system can be found in Idriss [5].
The rate of oxidation of UO2 has been shown to be governed by the rate of oxygen diffusion through the oxide layers [6].
The diffusion of oxygen is influenced by the presence of point and extended defects, and interfaces between different phases, within the microstructure.
As a result, research has focused on many areas including structure [7], thermal conductivity [8–10], displacement cascades [11] and diffusional creep [12], in addition to transport properties covering both fission gas diffusion and nucleation [13–20], as well as self-diffusion of uranium and oxygen [15,21–25].
Grain boundaries influence many material properties of UO2; for example, the segregation of fission gas can lead to the formation of bubbles with consequences to the stability of the fuel [26].
The transport of oxygen in UO2 and its non-stoichiometric oxides, is an area of research that has drawn attention for many decades with many experimental [6,27–29] and theoretical [1,30,31] studies generally focussing on transport properties in the bulk material.
Marin and Contamin [32] investigated oxygen transport using 18O tracer diffusion in single crystal and polycrystalline UO2 specimens, finding similar diffusion coefficients for all samples with no enrichment of 18O at the grain boundaries of the polycrystalline sample.
Sabioni et al.
[33] found five orders of magnitude difference in the diffusion of uranium between the boundary region and the bulk, while oxygen diffusion appeared unaffected.
In contrast, Vincent-Aublant et al.
[21] studied stoichiometric UO2 grain boundaries using molecular dynamics, and found greatly enhanced diffusion of both uranium and oxygen in a region up to several nanometres from the boundary mismatch.
The idea of enhanced oxygen diffusion at grain boundaries was also supported by Govers and Verwerft [15] and Arima et al.
[22].
The latter suggested that oxygen diffusion in the boundary region was influenced by the structure and misorientation angle of the grain boundaries, and proposed the presence of three different regions of oxygen diffusion: oxygen vacancy diffusion at temperatures below 2500K, lattice (interstitial oxygen) diffusion at intermediate temperatures (2500–3000K), and fast ion diffusion (breakdown of the oxygen sub lattice) at temperatures above 3000K.
Studies on grain boundaries in UO2 are still scarce [7,15,22,34], but a number of papers have been published on grain boundaries in other fluorite structures including CeO2, HfO2, and doped ZrO2 [35–41].
As there are still conflicting reports as to whether the presence of grain boundaries affects the transport of oxygen in UO2, we present our investigation of coincident site lattice (CSL) [42] grain boundaries to determine whether any enhancement of oxygen diffusion is observed at the boundary, and the correlation between the enhancement and the grain boundary structure.
In order to sample different feasible interfaces, we focussed on grain boundaries with both low and high Σ values expressing the {nn1}, {n11} and {n10} surfaces.
It has been measured that the presence of CSL boundaries accounts for 17% in UO2 samples [7], and therefore their structures are of relevant to understanding the properties of the material.
A combination of potential-based simulations were used: GULP (General Utility Lattice Program) [43] to derive the potential parameters and calculate the point-defect energies, METADISE (Minimum Energy Techniques Applied to Dislocation, Interface and Surface Energies) [44] to generate and minimise the grain boundary structures, and DL_POLY [45] to apply molecular dynamics (MD) over a range of temperatures.
Initially, we describe the potential model employed in the calculations, followed by defect calculations, and finally with MD simulations of oxygen diffusion.
A partially charged, rigid ion potential model, based on a Morse potential with the addition of the repulsive term from a Lennard–Jones potential, as in Pedone et al.
[46], was employed.
The potential model is referred to as the “Morl potential” throughout the text.
The potential form is(1)U(rij)=Dij1-e-Bij(rij-r0)2-Dij+Aijrij12where Dij is the depth of the potential energy well, Bij is a function of the slope of the potential energy well, rij is the distance of separation and r0 is the equilibrium distance between species i and j, and Aij relates to the potential energy well and describes the repulsion at very short distance between species i and j.
Table 1 lists the potential parameters; the O–O interaction has been successfully used for fluorite structures as demonstrated by Sayle et al.
[47], while the U–O interaction was derived specifically to reproduce the experimental lattice parameter [48] and elastic constants [49] of UO2.
This rigid ion model has the major advantage of allowing us to compute large systems, although by assuming fixed charges, we neglect polaronic effects [50–52].
Point-defect energies were calculated for a range of defects at infinite dilution using the Mott–Littleton [53] method, in which atoms close to the defects (region 1) are allowed to relax to mechanical equilibrium, while atoms further away (region 2) are constrained to include only harmonic relaxation.
Calculations were performed with region sizes of 15Å for region 1 and 40Å for region 2.
Polycrystalline materials contain grain boundaries, which are defined as the interface between two crystalline grains of the same phase, with different orientations [54].
Grain boundaries are defined by the axes of the crystallographic directions of the two grains (hsksls), the rotation axis o=[hokolo], the normal axes to the grain boundary plane n, and the misorientation Θ, which defines the rotation needed to set both grains to an identical position.
Grain boundaries are defined by the relationship between n and o.
When o is perpendicular to n (o⊥n) or o is parallel to n (o∥n), the boundary is defined as tilt or twist respectively.
Grain boundaries that do not fit these relationships are classified as symmetrical, asymmetrical, twist and general (or random).
From the point of view of the actual atomic-level structure, grain boundaries can be distinguished in low- and high-angle.
When Θ is low enough, the misorientation can be seen as an array of dislocations, defining a low-angle grain boundary.
However, when the dislocations overlap, the boundary is formed by repeated structural units of a limited number of species, which defines a high-angle grain boundary.
High-angle boundaries can be further divided into subgroups depending on their energy.
Simple geometrical classification of high-angle grain boundaries have been attempted, but the representations are still not fully unambiguous.
Perhaps, the most known is the coincidence site lattice model, which assumes that the energy of the boundary is low when high coincidence of the atomic positions between the two grains is reached.
Σ is the reciprocal density of coincidence sites, according to the coincidence site lattice model.
In cubic systems, Σ can be evaluated as the sum of the squares of the Miller indices,(2)Σ=δ(h2+k2+l2)where δ=1 if (h2+k2+l2) is odd, while δ=0.5 when (h2+k2+l2) is even and hence in cubic systems Σ values are always expressed as an odd number.
It is widely accepted that low Σ values represent special boundaries (e.g.
Σ=3 is a singular boundary).
A more detailed review of grain boundaries can be found in Lejcek [55] and references therein.
The METADISE code [44] was used to construct the interfaces as previously described by Galmarini et al.
[56] and Harris et al.
[57].
The bulk crystal was cut along specified Miller indices to produce the desired surface.
Different surface terminations were minimised, and the ones used to create the grain boundaries are described in Section 3.3.
The grain boundary was created by reflecting the surface in order to generate a mirror image that was moved on a virtual mesh parallel to the boundary plane.
At each point of the mesh, the entire structure was relaxed to its energy minimum, thereby producing an energy surface with minima and maxima, representing more and less stable grain boundary configurations.
In the present study, we have focussed on stoichiometric CSL grain boundaries.
These boundaries can be defined as high-angle translated mirror tilt grain boundaries.
A mirror tilt boundary has the normal to the boundary plane (n) perpendicular to the rotation axes (o), but, as in our case the structure comes from a translation of the two grains with respect to each other, it is more appropriate to name the interface, “translated mirror tilt” grain boundary.
Six grain boundaries were investigated, Σ3(111), Σ9(221), Σ5(210), Σ5(310), Σ11(311), and Σ19(331), as they are observed in UO2 [21] and other fluorite-structured materials [35,36,58].
Formally, our interfaces should be written as Σn(hsksls)/[hokolo]−2Θ grain boundary; however for brevity we refer to them only as Σn(hsksls), as the rotation axes [hokolo], from which we calculate the misorientation angle Θ, is always the (001) (Fig.
1).
The formation and cleavage energies can be calculated to express the stability of a grain boundary.
The formation energy,(3)Ef=Egb-2EbAis the energy needed to form the grain boundary, and the cleavage energy(4)Ec=Egb-2EsAis the energy required to separate the boundary into two surfaces; A is the surface area, Egb is the energy of the structure containing the interface, Eb and Es are the energy of the bulk and surface containing half the number of atoms per unit area.
A further parameter that can be used to characterize the grain boundary is the width of relaxation, which refers to the influence of the interface (mismatch) on the surrounding bulk region.
It can be defined by measuring the distance from the interface, at which the atomic density returns to the bulk density, or the extent to which the diffusion behaviour is modified by the presence of the interface.
Molecular dynamics simulations were conducted on bulk UO2 and the most stable grain boundary configurations using the DL_POLY code [45].
The forces between atoms consisted of long-range Coulombic and short-range terms.
The electrostatic interactions of the system were evaluated using the Ewald method to a precision of 10−5 and the potential cut-off was 8Å.
All simulations were run with a timestep of 1fs and with the Nosé–Hoover thermostat and barostat.
The bulk simulation cell was comprised of 256 stoichiometric UO2 units.
Five different defective bulk systems were simulated containing between 0.4% and 1.9% Schottky defects.
We have introduced Schottky defects to allow for comparison with the results of Arima et al.
[22].
This introduces oxygen vacancies (Vo) maintaining the UO2 stoichiometry while anion Frenkel defects form spontaneously during simulation.
In the stoichiometric UO2 bulk simulation cell, the activation energy of oxygen migration is the sum of the formation energy of Vo and the migration energy of Vo.
In Schottky-defective UO2 simulation cells, the activation energy of oxygen migration is now purged of the formation energy of Vo, while keeping the simulation cells charge neutral.
The simulation cells of grain boundary configurations were comprised of two grain boundaries running in opposite directions, and equally spaced by a bulk region, so that the effect of the relaxation of one boundary, due to the mismatch, is negligible to the other boundary and the middle of the bulk region.
Therefore, to avoid any interaction between the two boundaries, the grain boundary configurations were comprised of 4320, 2880, 3840, 3930, 3600, and 3600 UO2 units for the Σ3(111), Σ9(221), Σ5(210), Σ5(310), Σ11(311), and Σ19(331), respectively.
In the simulation cells, the grain boundary plane is perpendicular to the x direction, and parallel to the yz plane, with the pipe of the boundary parallel to the y direction and perpendicular to the z direction.
All systems were equilibrated at 300K in the isothermal–isobaric ensemble (NPT), in which the N number of species, P pressure, and T temperature are conserved, for 1ns (following 100ps equilibration) until fluctuations of the configurational energy were negligible.
Annealing at high temperatures was performed for each configuration.
The temperature was increased up to 3000K for grain boundary systems and 3900K for the bulk systems, and then decreased back to 300K in the NPT ensemble for up to 1ns in order to reach stable grain boundary and bulk configurations.
The temperature was increased and decreased 1.5K/ps.
We chose temperature ranges that enabled us to compare the results of our bulk calculations with previous studies; furthermore, for the grain boundary configurations, the upper limit of 3000K was chosen to avoid grain boundary diffusion, and to provide a comparison with earlier modelling studies.
The stability of each boundary, with respect to temperature, is discussed in Section 3.
After annealing, simulations were run every 100K in the temperature range between 2000K and 3900K for bulk, and between 2000K and 3000K for grain boundary systems.
The NPT ensemble was employed to thermally equilibrate the systems, up to 1.1ns, until the fluctuations in the volume and the configurational energy at each temperature were negligible.
Oxygen diffusion data was then collected at each temperature in the canonical ensemble (NVT), in which the N number of species, V volume, and T temperature are conserved, over a time of 2ns for the bulk and 1ns for the grain boundary systems.
The data is reported for slightly different temperature ranges for each grain boundary because the grain boundaries themselves become mobile at different temperatures, thereby affecting the values of the oxygen diffusion coefficients.
Indeed, we are interested only in the oxygen diffusion and how it is affected by the presence of grain boundaries and their structures.
Grain boundary diffusion would alter and mask any effect of the crystalline grain boundary structure on the oxygen diffusion coefficient, thereby making the comparison between the influences of different grain boundary structures on the oxygen diffusion meaningless.
The time-average density profile and relative oxygen diffusion coefficient did not show a structured shape, as one would expect in crystalline materials, at the temperatures where the grain boundaries were diffusing.
Diffusion of grain boundaries was visually determined by inspection of the evolution of the system using VMD [59].
The temperatures, at which the grain boundaries diffuse, are discussed in Section 3 for each grain boundary system.
As we are interested in evaluating oxygen diffusion at the grain boundary, the oxygen diffusion coefficient (DO) cannot be calculated simply as an average over the whole system as large areas of bulk on either side of the interface would likely mask any contribution from the boundary.
Therefore, the simulation cell was divided into slabs parallel to the grain boundary plane (yz plane) with a width equal to the U–U distance in the direction parallel to the normal of the grain boundary plane (x direction).
The sizes of the slabs were 3.20, 0.93, 1.30, 0.90, 1.70, and 1.30Å for the Σ3(111), Σ9(221), Σ5(210), Σ5(310), Σ11(311), and Σ19(331), respectively.
In each slab, the three components of the oxygen diffusion coefficient (DO,x,s, DO,y,s and DO,z,s) were calculated with a correlation time of 25ps.
The components of the oxygen diffusion coefficient are evaluated in terms of the mean squared displacement (MSD) of the oxygen species in each slab, by using(5)DO,x,s=limt→∞〈|rO,x,s(t)-rO,x,s(t0)|2〉2twhere rO,x,s(t) is the position of the oxygen atom at time t in the direction x in the slab s. The MSD at time t corresponds to the average square distance travelled by an oxygen atom between the time t0 and the time t in the slab s in the direction x.
The components of the DO in each slab were then divided by the corresponding component of the DO,b in the bulk (DO,x,b, DO,y,b and DO,z,b), to highlight the increase of oxygen diffusion in the grain boundary region relative to the bulk region.
Therefore the three components of the relative oxygen diffusion coefficient (DO,x,rel, DO,y,rel and DO,z,rel), presented in Section 3, are dimensionless.
It is worth noting that the relative oxygen diffusion coefficients assume only positive values, greater than 1 if DO,x,s>DO,x,b and lower than 1 if DO,x,s<DO,x,b, and converge to 1 in the bulk region as DO,x,s=DO,x,b.
In a similar fashion to the method used to calculate oxygen diffusion coefficients (the width of the slabs was 0.1Å), the time-averaged densities of oxygen and uranium species were evaluated.
The time-average density of a species is the density of the species in each slab, over the time period of the simulation cell.
Details on calculating the relative oxygen diffusion coefficient and density profile can be found in Crabtree et al.
[60] and Kerisit and Parker [61].
Before describing the stable structures for the grain boundaries and their transport properties, we discuss the performance of the potential model, comparing it to available experimental and simulated data, and the structures of the surfaces used to build the grain boundary configurations.
We conclude with the comparison between the oxygen diffusion behaviour of grain boundaries and propose a simple way to evaluate their contribution to the oxygen diffusion in polycrystalline samples.
The Morl potential model was employed to calculate structural and diffusion properties of UO2 using energy minimisation and molecular dynamics, then compared with available literature data.
There are a large number of potential models in the literature, including rigid ion and shell potential models [2,62–65].
However, we required a potential model that not only accurately modelled the properties of UO2, but was also robust and stable at high temperatures, and was computationally efficient, thus enabling us to simulate large systems while reducing the computational cost.
As such, we chose a rigid ion potential model with partial charges.
Structural properties are well reproduced by all models (Table 2), but the significant improvement of our potential stands in the elastic constants which relate to how the system responds to stress.
Indeed, structure and elasticity are important parameters for elucidating grain boundary stability.
All potential models correctly predict the relative stability of the defect energies.
The Morelon potential model performed best as it was specifically derived to replicate defect formation energies, but it largely underestimates the bulk modulus.
The energies calculated with the Morl and the Arima potential models are overestimated; this is a known disadvantage of using rigid ion models as the ionic polarisability is not taken into account.
For completeness, we report two shell models with the best results given by the Catlow potential model.
The Morl, along with the Grimes shell potential model, accurately reproduce the activation energy of oxygen migration (the migration path was the lowest energy and most favourable diffusion mechanism observed in bulk UO2 [1]).
The major deficiency of the Morl potential is that the cation defect energies are high, and hence the number of cation defects will be underestimated.
However, this should not be an issue unless this model was applied to processes such as grain growth where cation mobility will contribute.
Bulk diffusion data obtained for stoichiometric and defective bulk systems (with Schottky defect concentrations between 0.4% and 1.9%), are comparable with that of Arima et al.
[22], Basak et al.
[66], and Yakub et al.
[67].
The oxygen diffusion coefficient of stoichiometric bulk UO2 is 1.7×10−12m2s−1 at 2500K and increases to 2.9×10−9m2s−1 at 3300K.
The oxygen diffusion coefficient of the defective bulk systems varies between 3.7 and 7.9×10−12m2s−1 at 2000K, 3.1 and 3.4×10−11m2s−1 3300K, and approximately 7.0×10−9m2s−1 at 3900K.
The direct comparison between calculated and experimental results is challenging as the sets of data are collected in different temperature ranges.
This discrepancy is due to the small time frame in which molecular dynamics is run which makes it necessary to perform simulations at high temperature to enable us to gain sufficient data to obtain meaningful statistics.
However, the trend is remarkably similar with both experimental and calculated data showing an increase in oxygen diffusion coefficient with temperature.
Bulk oxygen diffusion in the temperature range of 2000–3900K consists of three regions as a result of different diffusion mechanisms occurring in each, similar to observations by Arima et al.
[22].
The oxygen vacancy diffusion region is between 2000K and 2500K and is controlled by the formation of oxygen vacancies.
In this range, oxygen diffusion is seen in the stoichiometric system but not in the defective systems.
The lattice diffusion region is dominated by the formation of oxygen Frenkel defects at sub-lattice positions and is observed at intermediate temperatures (2500–3000K) due to the energy required to form these defects.
The fast ion diffusion region, where the oxygen sub-lattice breaks down is seen at temperatures above 3000K.
A more reasonable way of comparing the calculated and experimental data is by using the activation energies of oxygen migration, as displayed in Table 3.
The activation energy of oxygen migration in the lattice diffusion region can be determined in two ways.
The first (indirect) uses the expression(6)Ea=12ΔGFO+ΔHVomwhere ΔHm is the oxygen migration enthalpy and ΔGFO is the formation energy of the oxygen Frenkel pair, excluding any entropic effects [68].
The second (direct) directly calculates the Ea from the diffusion coefficients using the Arrhenius type relation(7)D=D0exp-EaRT All potentials overestimated the experimental value of the activation energy of oxygen migration, with the Arima model predicting it significantly higher (219%).
However, the lack of experimental diffusion data in a similar temperature makes the comparison quite difficult.
The structures of the surfaces employed to generate the grain boundaries are shown in Fig.
2.
The {111}, {311}, and {331} are type I surfaces, {210} and {221} are type III surfaces, and {310} is a type I surface according to Tasker [70].
Type III surfaces are reconstructed according to Oliver et al.
[71].
All type III (except for the {210}) and II surfaces are oxygen terminated while the type I {310} is oxygen and uranium terminated.
The surface energy is shown in Table 4.
There is no straightforward correlation between the surface energy and the type (Table 4).
However, there seems to be a correlation between the surface energy and the coordination of uranium surface atoms.
Uranium {111} and {311} surface atoms are 7- and 5-fold coordinated.
On the {331} and {221} surfaces, the coordination of U surface atoms is a mixture of 6, 7 and 8.
Uranium surface atoms are a mixture of 6-, 8- and 4-fold coordinated on the {210} and {310} surfaces.
Therefore, surfaces with high coordination numbers (e.g.
{111}, {221} and {331}) have lower surface energies, while surfaces with highly under-coordinated uranium surface atoms have higher surface energies ({210} and {310}), with the {311} surface in within the two groups.
The most stable structures of each grain boundary, as predicted by energy minimisation, are shown in Fig.
3; x is the direction perpendicular to the yz grain boundary plane with y parallel to the pipe.
Oxygen atoms are removed for clarity.
The six grain boundaries are all highly symmetric and the structural differences in the patterns are highlighted at the mismatch in Fig.
3.
Visually, the Σ3(111) [36,35] is the most bulk-like boundary while the Σ5(310) [72,39] and the Σ19(331) show quite large dislocation pipes along the y direction.
The Σ5(210) [73] and the Σ11(311) [36] have corner sharing diamond patterns and are somewhere between the two extremes.
Pipes may provide a less restricted path through the structure in a particular direction, possibly facilitating the diffusion of oxygen.
The Σ3(111), Σ5(210), Σ9(221) and Σ11(311) systems show strong resemblance to the experimental structures shown for ceria and doped zirconia [36,35,73].
The Σ5(310) structure consists of linked highly symmetric triangles similar to ones seen for yttria stabilized zirconia [58,38] and HfO2 [74].
Experimental structures are not available for the Σ19(331) grain boundary, but the strong resemblance of experimentally observed structures for the other grain boundaries gives confidence in the validity of the model (potential model as well) applied.
Table 4 lists the grain boundary formation and cleavage energies, along with the surface energy of the reflected surface, the misorientation angle and the half width of the boundary calculated as the distance from the boundary interface (mismatch) in which the interatomic distances do not match those of bulk UO2.
The Σ19(331) and Σ3(111) grain boundaries have the smallest grain boundary half widths while the Σ5(310) boundary has by far the largest.
There is no obvious correlation between all the quantities.
On average, the Σ3 system requires less energy to form than the Σ5 and Σ9 boundaries, which is likely the result of the low energies of the corresponding surfaces.
Similar correspondence cannot be seen for the high values of Σ.
Little variation in cleavage energies is seen within the Σ5, Σ3 and Σ9 systems, but it is higher for the higher values of Σ with the Σ11 showing the highest energy of 4.52Jm−2.
The annealed structures for each grain boundary using molecular dynamics are displayed in Fig.
4, along with the corresponding normalised density plot.
The structures are shown at 2000K and at the highest temperature at which the boundary was not seen to diffuse.
The diffusion of grain boundaries due to increased mobility is a mechanism of grain growth in polycrystalline structures [75,76], but for the purpose of this paper, grain boundary diffusion is not desired as it will introduce additional contributions to the oxygen diffusion, which are not related to the oxygen sub-lattice but to an extended defect.
The annealed structure of Σ3(111) over the temperature range studied (2000–3000K) is identical to the structure predicted by energy minimisation (EM).
At temperatures below 2400K, the structure of the Σ5(210) boundary is similar to the one seen in the EM; however, above 2400K a structural change is observed with the boundary showing shortened diamond shapes arranged end on end.
This new structure persists until temperatures exceed 2900K.
Above this temperature, grain boundary diffusion is observed.
When the temperature is lowered below 2400K, the system returns to its original structure.
The structure of the annealed Σ9(221) grain boundary resembles the structure predicted using EM calculations, and the experimental structure seen by Shibata et al.
[36].
The annealed Σ5(310) boundary differs slightly from the structure predicted using EM, consisting of a more distorted triangular pattern.
The Σ11(311) grain boundary displays no change in structure with temperature, although above 2700K grain boundary diffusion is seen.
The annealed Σ19(331) boundary is clearly different from the one determined using EM.
After annealing, the boundary consists of linked diamonds formed as a result of moving one of the uranium arrays into the pipe.
These diamond shapes resemble the high temperature structure seen for the Σ5(210) grain boundary, but unlike the Σ5(210), we do not see any grain boundary diffusion in the range of temperatures studied.
The description of oxygen diffusion at the grain boundaries has been divided into three parts depending on the diffusion behaviour of each group of interfaces.
Figs.
5–7 display the oxygen diffusion profiles of the three components of the relative oxygen diffusion coefficient (DO,x,rel, DO,y,rel and DO,z,rel as described in Section 2.3) as a function of the distance from the interface.
The mismatch is at 0Å and the positive and negative values signify the two sides of the interface which are symmetrical.
The direction perpendicular to the boundary plane is x, y is parallel to the pipe and z is perpendicular to the pipe and parallel to the boundary plane.
Whilst diffusion data was gathered at many temperatures, the oxygen diffusion data is shown only for significant temperatures, as the oxygen diffusion profile is similar in different temperature ranges.
Oxygen diffusion profiles of the {n11} grain boundary series, Σ3(111) and Σ11(311) boundaries, are shown in Fig.
5.
Across the range of temperatures, the oxygen diffusion profiles displayed only a single peak centred over the grain boundary interfaces (0Å), with enhanced oxygen diffusion seen at distances up to 6Å.
At 2000K, the oxygen diffusion is isotropic with only small variations in the relative oxygen diffusion coefficients (DO,x,rel, DO,y,rel and DO,z,rel).
At temperatures above 2600K, DO,x,rel decreases, indicating that the oxygen diffusion in the direction across the interface is less pronounced than that occurring parallel to the grain boundary plane.
At temperatures close to 3000K, the bulk region of the Σ3(111) grain boundary displays fast oxygen ion diffusion.
As the oxygen diffusion in the bulk and in the grain boundary regions becomes comparable, the relative oxygen diffusion coefficients in the boundary region are therefore highly reduced.
The Σ11(311) boundary showed grain boundary diffusion above 2700K, and therefore it is excluded from the comparison as DO,rel is affected by the diffusion of the grain boundary itself.
Oxygen diffusion profiles of the {n10} grain boundary series, Σ5(210) and the Σ5(310) boundaries, are shown in Fig.
6.
The oxygen diffusion at these two boundaries differs markedly from that of the {n11} series, but as for the {n11} series, the higher index grain boundary (Σ5(310)) shows grain boundary diffusion at a lower temperature compared to the lower index boundary (Σ5(210)).
The DO,rel of the Σ5(210) boundary is approximately 1.5 up to 2500K, suggesting that oxygen diffusion in the grain boundary region is only slightly enhanced (1.5times) compared to the one in the bulk region.
Above 2500K, there is a sudden increase in the DO,rel on either side of the interface, but not at the interface (0Å).
This enhancement coincides with the phase change described in Section 3.3.
DO,rel becomes fairly isotropic at 2900K, with oxygen diffusion only three times faster than that in the bulk, as fast oxygen ion diffusion started in the bulk region.
The Σ5(310) boundary shows a structured oxygen diffusion profile at lower temperatures with DO,rel modestly increased at the interface (0Å).
At 2600K, oxygen diffusion is up to 18 times faster at the interface (mismatch) compared to the bulk region and DO,rel becomes isotropic losing its marked structure.
Oxygen diffusion profiles of the {nn1} grain boundary series, Σ9(221) and Σ19(331) boundaries, are shown in Fig.
7.
Unlike the {n11} and {n10} series, the {nn1} shows the opposite behaviour: the higher index boundary shows grain boundary diffusion at higher temperatures compared to the lower index grain boundary (graph not shown).
In the whole range of temperatures, at both 2000K and 2500K, oxygen diffusion at the Σ9(221) grain boundary is enhanced at a distance of approximately 6Å from the interface.
DO,rel shows anisotropic behaviour with DO,y,rel enhanced compared to DO,x,rel and DO,y,rel.
The oxygen diffusion profiles of the Σ19(331) grain boundary at temperatures lower than 3000K are anisotropic, with enhanced DO,y,rel compared to DO,x,rel and DO,y,rel.
DO,y,rel is particularly high at 2500K where oxygen diffusion at the grain boundary mismatch is 30times greater than that in the bulk.
As the y direction is parallel to the pipe of the grain boundary, the increase in DO,y,rel suggests that the pipe is the source of the enhanced oxygen diffusion.
At 3000K, oxygen diffusion in the y direction is again faster than that in either the x or z directions, as DO,y,rel is greater than DO,x,rel and DO,z,rel.
The dramatic reduction in the values of DO,rel, compared to lower temperatures, arises from the onset of the fast oxygen ion diffusion in the bulk at high temperatures.
This behaviour is consistent with observations in all other grain boundary systems that did not show grain boundary diffusion at high temperatures.
Furthermore, at 3000K, the shape of the oxygen diffusion profile for the Σ19(331) resembles that seen at high temperature for the Σ5(210).
This may be the result of the similar structures of the two boundaries at high temperature (Fig.
4).
Grain boundary half widths listed in Table 5 were determined using the data from the relative oxygen diffusion coefficients (Figs.
5–7) and oxygen density profiles (Fig.
4).
The half width corresponds to the distance from the interface (0Å) at which oxygen diffusion and oxygen density, recover to the bulk values.
There is generally good agreement between the boundary half widths determined from MD (Table 5) and EM (Table 4) calculations with only the Σ3(111) and the Σ11(311) showing a relatively significant difference.
Generally, the values obtained from MD data predicted a shorter grain boundary half width compared to the EM data.
The discrepancy can be related to the tight definition of grain boundary half width derived from the EM data, as we measured it including any small variation of the U–O distance compared to the U–O distance in the bulk.
The oxygen diffusion profiles of all six grain boundaries (Figs.
5–7) show enhanced relative oxygen diffusion coefficients between 2000K and 3000K, which is in line with observations from previous computational studies [21,22].
The increase in oxygen diffusion is facilitated by the presence of grain boundaries, and it is not limited to just the grain boundary interface (mismatch), but also to the region adjacent to the interface.
The width of the grain boundary region with increased oxygen diffusion is different for each grain boundary.
Unlike the Σ3(111) and Σ11(311) boundaries, the remaining boundaries display anisotropy in the oxygen diffusion profiles.
This anisotropic behaviour of oxygen diffusion is particularly noticeable at low temperatures in the Σ19(331) boundary.
The large variation in oxygen diffusion behaviour between the different grain boundaries appears to be related to the variation in structures of the different systems.
For example, the Σ5(210) shows a phase change, which coincides with a marked change in oxygen diffusion behaviour above 2400K.
This structural rearrangement causes the conformation of the boundary (the pattern of the interface in Fig.
4) to be close to that of the Σ19(331) boundary; thus, the diffusion behaviour of the two grain boundaries becomes similar (Figs.
6 and 7).
However, as the relative oxygen diffusion coefficients are still very different, we infer that the transport of oxygen is mostly influenced by the orientation of the two grains rather than by the interface pattern.
However, this idea is not supported by the Σ3(111)and Σ11(311) boundaries which both have similar oxygen diffusion profiles and intensities.
The relative oxygen diffusion coefficient at the interface and in the area adjacent to the interface can vary significantly with changing temperatures.
However, at temperatures close to 3000K the relative oxygen diffusion coefficient in the grain boundary region decreases sharply, due to fast oxygen ion diffusion in the bulk region.
Furthermore, at higher temperatures the oxygen diffusion profiles became more isotropic compared to lower temperatures.
As one would expect, the features in the oxygen diffusion profiles should be related to the d-spacing, the U–U distance projected onto the normal to the grain boundary plane.
However, as for the half width, the correlation is not straightforward and cannot be generalized.
A further complexity exists when comparing values of relative oxygen diffusion coefficients (DO,rel) amongst the grain boundaries, as the enhancement of oxygen diffusion is also dependent on the distance from the boundary (i.e., it depends on the grain boundary width).
Thus, we calculated an average contribution to the enhancement of oxygen diffusion for each grain boundary, by dividing the sum of the relative oxygen diffusion coefficients (DO,x,rel, DO,y,rel and DO,z,rel) in the grain boundary region by the grain boundary width.
The resulting average relative oxygen diffusion coefficient (DO,ave,rel) as a function of the temperature is shown in Fig.
8.
Diffusion data is shown only up to the highest temperature where only oxygen diffusion was seen, excluding therefore those temperatures at which grain boundary diffusion was seen.
All grain boundaries predicted DO,ave,rel to increase with temperatures up to approximately 2500K.
Above this temperature, all systems showed a reduction relative to the bulk in the DO,ave,rel, with convergence predicted to occur at temperatures just over 3000K as a result of the fast oxygen ion diffusion taking place in the bulk region.
The Σ3(111) system has the greatest DO,ave,rel over the entire range of temperatures, which might be related to its high stability, as it has the lowest formation energy, and to its small grain boundary width.
The lowest DO,ave,rel is seen for the Σ5(210) boundary, which shows no increase in oxygen diffusion compared to the bulk up to 2500K, where a sudden sharp increase is due to the structural change (Fig.
4).
As the oxidation of UO2 systems is governed by oxygen diffusion, evaluating the effect of the grain boundaries on the oxygen diffusion in polycrystalline systems is of extreme importance.
The total relative oxygen diffusion coefficient (DO,tot,rel) as a function of temperature can be evaluated for three different theoretical models (Fig.
9).
We consider three different polycrystalline systems.
The first model assumes that the system comprises of spherical grains with equal number of all boundaries.
DO,tot,rel is therefore the average of the DO,ave,rel for each boundary at each temperature as all the grain boundaries are equally present in the polycrystalline system.
The second model assumes that the system consists of grains with shapes related to formation energy of the grain boundaries.
DO,tot,rel is therefore the weighted average of the DO,ave,rel for each boundary at each temperature as all the grain boundaries are present in the polycrystalline system depending on their formation energy.
The third model assumes the system consists of grains of octahedral shape expressing the most stable {111} surface.
DO,tot,rel is therefore the DO,ave,rel of the Σ3(111) grain boundary at each temperature, as the polycrystalline system is dominated entirely by the most stable Σ3(111) grain boundary.
The third model gives rise to the highest enhancement in the oxygen diffusion as the polycrystalline system is dominated by the Σ3(111) grain boundary which shows the highest increase relative oxygen diffusion coefficient.
The first model gives rise to the lowest enhancement in the oxygen diffusion, as grain boundaries with low DO,ave,rel, such as Σ5(210) and Σ5(310), contribute equally to the DO,tot,rel.
The second model results in an intermediate enhancement of oxygen diffusion, between the first and the third, as in this case the grain boundary formation energy dictates the extent that each grain boundary contributes to DO,tot,rel.
In this case, the presence of grain boundaries with low formation energy and low DO,ave,rel, e.g.
Σ5(310) (Ef=0.76eV), is balanced by the presence of grain boundaries with low formation energy and high DO,ave,rel, e.g.
Σ3(111) (Ef=0.30eV) and Σ9(221) (Ef=0.48eV).
We have generated a robust rigid ion potential model for UO2 intended for use on large systems.
Structural and elastic properties are well reproduced as well as the activation energy of oxygen migration and the anion Frenkel energy.
Six translated mirror tilt grain boundaries were studied.
Grain boundary diffusion was observed at high temperatures for some of the grain boundaries.
The Σ5(210) is predicted to undergo a reversible phase transition at high temperature.
The annealed Σ19(331) displays a different pattern from the minimum energy structure predicted using energy minimisation.
These two examples suggest that structures predicted by energy minimisation can be used to scan between possible initial configurations, but they might not be the most reliable structures for investigating dynamical properties.
The grain boundary interface (mismatch) and the regions adjacent showed significantly enhanced diffusion of oxygen with a directional dependence as a function of grain boundary structure and temperature.
Only Σ3(111) and Σ11(311) displayed isotropic oxygen diffusion while all other grain boundaries showed highly anisotropic oxygen diffusion.
At high temperature, when fast oxygen ion diffusion in the bulk region is activated, the grain boundary region shows low enhancement of oxygen diffusion, which suggests that the contribution of grain boundaries to oxygen diffusion in polycrystalline systems is less significant with increasing temperature.
Finally, as oxidation of UO2 systems is governed by oxygen diffusion, oxygen diffusion in polycrystalline systems can be highly influenced by the distribution of grain boundaries.
We suggest that polycrystalline systems comprised of spherical grains will exhibit a lower enhancement of oxygen diffusion compared to polycrystalline systems comprised of euhedral crystals which might decrease the overall rate of oxidation of the system.
We acknowledge AWE and EPSRC (EP/I03601X/1) for funding.
Computations were run on HECToR (EP/F067496) and ARCHER (EP/L000202) through the Materials Chemistry Consortium funded by EPSRC and on the HPC resources (Aquila) at the University of Bath.
© British Crown Copyright 2014/MOD.
Published with permission of the controller of Her Britannic Majesty’s Stationary Office.
Structure of micro-crack population and damage evolution in quasi-brittle media Mechanical behaviour of quasi-brittle materials, such as concrete and rock, is controlled by the generation and growth of micro-cracks.
A 3D lattice model is used in this work for generating micro-crack populations.
In the model, lattice sites signify solid-phase blocks and lattice bonds transmit forces and moments between adjacent sites.
Micro-cracks are generated at the interfaces between solid-phase blocks, where initial defects are allocated according to given size distribution.
This is represented by removal of bonds when a criterion based on local forces and defect size is met.
The growing population of micro-cracks results in a non-linear stress–strain response, which can be characterised by a standard damage parameter.
This population is analysed using a graph-theoretical approach, where graph nodes represent failed faces and graph edges connect neighbouring failed faces, i.e.
coalesced micro-cracks.
The evolving structure of the graph components is presented and linked to the emergent non-linear behaviour and damage.
The results provide new insights into the relation between the topological structure of the population of micro-cracks and the material macroscopic response.
The study is focused on concrete, for which defect sizes were available, but the proposed methodology is applicable to a range of quasi-brittle materials with similar dominant damage mechanisms.
The mechanical behaviour of quasi-brittle materials, such as concrete, graphite, ceramics, or rock, emerges from underlying microstructure changes.
The meaning of microstructure differs for different media and could be characterised by intrinsic length scales.
For concrete these could be aggregate sizes and inter-aggregate distances, while for rock systems these could be sizes of blocks formed between existing small fractures.
At the engineering length scale, orders of magnitude larger than the microstructure scales, the mechanical behaviour can be described with continuum constitutive laws of increasing complexity combining damage, plasticity and time-dependent effects [1–4].
In these phenomenological approaches damage represents reduction of the material elastic constants.
From microstructure length scale perspective damage in quasi-brittle media is introduced by nucleation and evolution of micro-cracks, where a micro-crack means a fracture of the order of the microstructure scale(s).
Potentially the effect of the micro-cracks formed under loading could be captured by continuum damage models calibrated against engineering scale experiments.
The phenomenology, however, cannot help to understand the effects of the generated micro-crack population on other important physical properties of the material.
In many applications the quasi-brittle materials have additional functions as barriers to fluid transport via convection, advection, and/or diffusion.
It is therefore important to take a mechanistic view on the development of damage by modelling the evolution of micro-crack population.
This can inform us about changes in the transport properties.
Such a mechanistic approach needs to account for the microstructure in a way corresponding to the micro-crack formation mechanism [5].
In concrete, micro-cracks typically emerge from pores in the interfacial transition zone between the cement paste and aggregates [6].
In rock systems, micro-crack generating features could be existing fractures as well as pores.
Concrete will be used in this work to demonstrate the proposed methodology, because data for the micro-crack initiating features in this material is readily available.
Discrete lattice representation of the material microstructure seems to offer the most appropriate modelling strategy for analysis of micro-crack populations.
Discrete lattices allow for studies of distributed damage without constitutive assumptions about crack paths and coalescence that would be needed in a continuum finite element modelling.
For lattice construction, the material is appropriately subdivided into cells and lattice sites are placed at the cell centres.
The deformation of the represented continuum arises from interactions between the lattice sites.
These involve forces resisting relative displacements and moments resisting relative rotations between sites.
Two conceptually similar approaches have been proposed to link local interactions to continuum response.
In the first one, the local forces are related to the stresses in the continuum cell, e.g.
[7,8].
In the second one, the interactions are represented by structural beam elements, the stiffness coefficients of which are determined by equating the strain energy in the discrete and the continuum cell, e.g.
[9,10].
In both cases explicit relations between local and continuum parameters can be established for regular lattices.
It has been previously shown that regular 3D lattices based on simple cubic, face-centred cubic and hexagonal closely-packed atomic arrangements can be used to represent materials exhibiting cubic elasticity.
However, the only isotropic materials such lattices could represent are materials with zero Poisson’s ratio [11].
A bi-regular lattice that can represent all materials of practical interest has been proposed recently [12].
It can be seen as a lattice based on body-centred cubic atomic arrangement with two types of links - between neighbours along body diagonals (nearer) and between neighbours along cell edges (further).
This lattice, currently formed by beams clamped at sites, is used in the current work together with microstructure data for concrete obtained with X-ray computed tomography.
Failure models based on microstructure data and the novel lattice have been previously used for modelling tensile and compressive behaviour of cement [13] and the compressive behaviour of concrete under various complex loading conditions [14].
This work makes a step into developing our understanding of the micro-crack population and its relation to macroscopic damage.
Further, the structure of the micro-crack population will provide the means to study the changes in transport properties with damage in future studies.
Most of the work relating micro-crack populations to elastic moduli follows the fundamental paper [15], where analytical statistical derivation of the relation was provided.
We follow the interpretation given in [16], in which the damage is measured as a relative change of the elastic modulus and related to micro-crack population via(1)D=1-EE(0)=βNT∑c≥1c3N(c),where c is some measure of micro-crack size, N(c) is the number of micro-cracks of size c, NT is the total number of sites capable of nucleating micro-cracks, and β is a scaling parameter reported as 0.47π for cracks in a 2D medium.
Eq.
(1) is our point of comparison for the simulations performed with the lattice model for various loading cases.
In the current work we are interested in testing the range of applicability of Eq.
(1) and understanding the reasons for deviation from this rule, should such occur, by explicitly analysing the micro-crack population growth.
It should be mentioned, that we do not presuppose a value for the scaling parameter β.
The simulations are performed with a 3D model and used to derive mechanistically relations between mechanical damage parameters and geometrical characteristics of formed micro-crack populations.
The scaling parameter β could be extracted from such relations only for special loading cases as shown in Section 4.
The lattice model used in this work is illustrated in Fig.
1.
The unit cell, shown in Fig.
1(a) is a truncated octahedron – a solid with six square and eight regular hexagonal boundaries.
The 3D space can be compactly tessellated using such cells, with each cell representing a material microstructure feature, e.g.
a block or a grain, in an average sense.
This representation is supported by physical and statistical arguments [12].
In brief, the truncated octahedron is topologically closest to the average cell in a random Voronoi subdivision of space.
A discrete lattice is formed by placing sites at the centres of the cells and connecting each site to its 14 nearest neighbours; example is shown in Fig.
1(b).
The lattice contains two types of bonds.
Bonds denoted by B1 are normal to the square boundaries and form an orthogonal set.
For convenience this set is made coincident with the global coordinate system and B1 are referred to as the principal bonds.
Bonds denoted by B2 are normal to the hexagonal boundaries.
The hexagons lie on the octahedral planes with respect to the selected coordinate system, hence B2 are referred to as the octahedral bonds.
If the spacing between sites in the principal directions is denoted by L, bonds B1 have length L1=L, and bonds B2 have length L2=√3 L/2.
Presently, the bonds are represented by structural beam elements of circular cross sections, with R1 and R2 denoting the radii of beams B1 and B2, respectively.
The beams are clamped at the lattice sites.
The two types of beams have identical Young’s modulus, Eb, and Poisson’s ratio, νb.
This reflects the understanding that locally, i.e.
at a cell length scale, the material is homogeneous.
It has been previously shown [12], that by calibrating four parameters: R1/L, R2/L, Eb, and νb, the lattice can produce any material with cubic elasticity and with special selections a large class of isotropic elastic materials with Poisson’s ratios of practical interest.
The reference material in this work is a concrete with E=46GPa and ν=0.27, for which the calibration, assuming isotropic elasticity, yields R1/L=0.2; R2/L=0.32; Eb=90GPa; and νb=0.4 [14].
The commercial software Abaqus [17] with Euler–Bernoulli beam formulation has been used for the calibration and the analyses reported in this work.
Microstructure data for the reference material was obtained using X-ray Computed Tomography as reported in [14].
The pore size distribution was obtained by segmentation of reconstructed 3D images.
The studied regions of interest had dimensions of 1700×1200×1200 voxels with a voxel size of ca.
15μm, allowing for a minimum detectable pore radius of ca.
15μm.
The number of pores measured experimentally was n≈41,500.
The measured pore radii, ci, were used to construct a cumulative probability distribution (CPD) with standard median ranking, where for pore radii ordered as c1⩽c2⩽⋯⩽cn, the cumulative probability for pores with radii less than ci is given by F(c<ci)=(i−0.3)/(n+0.4).
The CPD for the reference material is shown in Fig.
2(a), where the minimum and maximum pore radii are also depicted.
The CPD is used to assign pore sizes to the lattice bonds.
For each bond a uniformly distributed random number 0⩽p<1 is generated and the assigned pore radius is calculated from c=F−1(p).
This is performed by interpolation between experimental data points for given p. The process ensures that the distribution of pore sizes in the model comes from the same population as in the experiment, a standard statistical argument.
A fragment of the model with distributed pores is given in Fig.
2(b).
The cell size, L, is calculated such that the volume of all distributed pores divided by the volume of the cellular structure equals the material porosity.
For the reference material the porosity is ca.
5%, which leads to cell sizes in the range 1.8–2.0mm, depending on the random distribution.
The pore sizes shown in Fig.
2(b) are to scale with the sketched cellular structure.
With respect to the cellular structure pores reside at cell boundaries.
The lattice bonds are also depicted (with radii not to scale) to show that pores reside at bond centres.
Damage in the lattice model is introduced by removal of bonds.
Propensity for bond failure is measured by the parameter(2)Π=NNf+|S|Sf+|T|Tf+|M|Mf,where N and S are the normal and shear forces in the beam; T and M are the twisting and bending moments; Nf, Sf, Tf, and Mf are critical values.
N is positive for tension and negative for compression.
S and M are obtained from the values in the two directions normal to the beam axis using the square root of squares rule.
Eq.
(2) provides an interaction between the different forces that allows for failure when Π⩾1 under the combined action of normal and shear stresses [14,18].
Taking only the first and fourth term was previously used in criteria with no account for shear, e.g.
[19].
The second and third term allow for shear failure similarly to [10].
The failure parameters Nf, Sf, Tf, and Mf can be related [18].
For a beam of circular cross section of radius R, the tensile failure stress is σf=Nf/(πR2).
The maximum bending stress is σmax=4M/(πR3), which equals σf when Mf=Nf R/4.
Similarly, the shear failure stress is τf=Sf/(πR2).
The maximum torsion stress is τmax=2T/(πR3), which equals τf when Tf=Sf R/2.
Thus Π requires two material parameters: σf and τf.
Noting that for quasi-brittle materials typically 1⩽τf/σf⩽2 [18], in this work τf=2σf is used, representing more brittle materials.
The tensile failure strength of a bond, σf, is related to the size of the pore assigned to the bond.
The relation used here is simpler than in the previous work [14] and based on the assumption that σf is the beam remote stress for which the average stress in the beam ligament outside the pore attains a critical value σ0.
Thus(3)σf=σ01-cR2,where c and R are the pore and beam radii, respectively, and σ0 can be interpreted as the tensile strength of the material without a defect.
With this setup and the choice τf=2σf the failure model requires a calibration of a single parameter, σ0, against experimental stress–strain curve.
However, since the beams behaviour is linear elastic, the choice of σ0 would affect only the calculated macroscopic stresses but not the order in which damage (beam failures) would evolve in the system.
Because the interest here is investigating the evolution of damage, σ0=1MPa is used for the calculations, noting that macroscopic stress response can be simply scaled by another value of σ0.
Note, that the sizes of the distributed pores affect the failure stresses of individual bonds via Eq.
(3), hence each bond has a different propensity for failure calculated by Eq.
(1).
A model of size (20L, 20L, 20L) was used for numerical simulations.
The lattice contained 17,261 sites and 113,260 bonds: 49,260 B1 and 64,000 B2.
The global coordinate system (X1, X2, X3) was coincident with bonds B1, so that the boundary planes X1=0, X1=20L, X2=0, X2=20L, X3=0, X3=20L contained 21×21 sites (nodes).
Boundary conditions were applied via prescribed displacements of nodes on boundary planes.
Only displacements normal to a plane were applied to its nodes, while other displacements and rotations were unconstrained.
Therefore the only non-zero reaction forces at nodes were those normal to their plane.
The displacements and forces of nodes on a plane with normal Xi are denoted by Ui and Fi.
Table 1 shows the conditions on planes X1=20L, X2=20L, and X3=20L for the analysed cases.
Additionally, U1=0 on X1=0; U2=0 on X2=0; U3=0 on X3=0, apply to all.
For cases where nodal reaction forces were determined from analysis, the macroscopic stress in the respective direction was calculated as the ratio between the total reaction force and the boundary area, i.e.
σi=ΣFi/400L2.
For the cases where nodal displacements were determined from analysis, the macroscopic strain in the respective direction was calculated as the ratio between the average displacement and the model length, i.e.
εi=ΣUi/(212×20L).
The evolution of damage was simulated by failure of bonds, controlled by an in-house code, and repetitive solution for equilibrium performed by Abaqus with constant applied displacements.
The values of di were selected so that the strain energy density in the system prior to damage was one for the four cases for the purpose of comparison.
At each step the in-house code obtains the forces and moments in all bonds and calculates the propensity for failure, Π, for each bond.
The bond with maximum Π is then removed and the updated lattice is solved for equilibrium.
This leads to redistribution of forces for the continuous damage evolution.
The magnitudes of Π at which consecutive failures occurred can be used to cut-back the applied strain and resulting stress and obtain a macroscopic stress–strain response.
For a selected physically based tensile strength, σ0, and a given load, the propensities for failure of the bonds can be ordered by decreasing magnitude.
Since the problem is elastic the applied load can be arbitrary.
The bond with the highest propensity will determine the load level at which this bond would fail, while all other bonds would remain intact.
The macroscopic stress and strain at this load level provide a point on the stress–strain curve.
The full curve is then constructed from the points corresponding to the individual failure events.
Details of the process and resulting curves for uniaxial loadings can be found in [14,19].
It should be mentioned that this construction can also be used for complex loadings, but the response curves that can be constructed would relate stress tensor invariants to strain tensor invariants.
Since the focus of this work is on the relation between damage and crack population, the construction of stress–strain behaviour curves is not necessary and we use an arbitrary tensile strength, σ0.
For the study we define four damage parameters, measuring the relative changes of the hydrostatic stress and the three components of the stress deviator by:(4a)DH=1-σHσH(0)(4b)Di=1-SiSi(0),whereSi=σi-σH,i=1,2,3It is straightforward to see that these parameters must remain equal when the deformation and damage of the material are isotropic.
In such cases the definitions are also equivalent to the damage parameter defined via relative reduction of Young’s modulus or shear modulus, such as the one used in Eq.
(1).
A bond failure is thought of as a micro-crack nucleation, specifically as a separation between the adjacent cells in the cellular structure along their common face.
Initially, the micro-cracks may be dispersed in the model reflecting the random distribution of pore sizes and the low level of interaction due to force redistribution.
Interaction and coalescence may follow as the population of micro-cracks increases.
These situations are illustrated in Fig.
3.
The structure of the failed surface can be represented with a mathematical graph, where graph nodes represent failed faces and graph edges exist between failed faces with common triple line in the cellular structure, i.e.
where two micro-cracks formed a continuous larger crack.
With reference to Fig.
3, each failed face is a graph node and each pair of neighbouring failed faces is a graph edge.
Generally, the graph of a failed surface is a disconnected set of sub-graphs or components, some of which could be single nodes as at the start of damage evolution, Fig.
3(a), while others could be connected sets representing larger micro-cracks as the coalescence develops, e.g.
the connected micro-crack in Fig.
3(b).
For the analysis, nodes are equipped with weights equal to the failed face areas.
Edges are equipped with weights equal to the shortest path along connected faces between their centres.
Hence, the graph representing the entire failed surface contains a number (possibly zero) of isolated nodes (micro-cracks of one failed face); a number (possibly zero) of sub-graphs with two nodes and one edge (micro-cracks of two neighbouring failed faces); and so on until the largest sub-graph in terms of connected nodes (failed faces).
The components of a failed surface graph are sorted into sets according to their areas A1<A2 …<Ak, so that each set contains Ni disconnected components of area Ai.
The linear size of a component is approximated with the square root of its area so that the moment of the crack population is formed using (compare to Eq.
(1))(5)M=1AT∑i=1kAi3/2Ni,where AT is the total area of the faces in the cellular structure.
This can in principle be replaced with a linear measure to conform to Eq.
(1).
A realistic choice is to use the component diameter which is the maximal shortest path between component’s nodes calculated with the weighted edges.
The process, however, is computationally expensive and does not lead to noticeable changes in the results for the cases analysed here.
Eq.
(5) is used after each failure event to calculate the evolution of the moment with damage.
In addition, the maximal component is monitored.
This is the largest connected cracked surface.
Fig.
4 shows the results of the evolution of the damage parameters defined by Eq.
(4) as functions of the moment of crack population defined by Eq.
(5).
Recall that a damage parameter, based on the relative change of the Young’s modulus equals the damage parameters based on the individual stress components, deviatoric and hydrostatic, when the material remains macroscopically isotropic.
In this case the same damage parameter describes the relative change of the shear modulus.
The results for the cases of uniaxial extension, unconfined (a) and confined (d), show equality of the four damage parameters (approximate in case 4).
This suggests that microscopic isotropy is maintained during damage evolution and the results reproduce very closely the linear relation predicted by the theory and given by Eq.
(1).
Notably, an estimate for the slope of the linear function from the figures is about 1.5, which is very close to the value of β reported in relation to Eq.
(1) [16].
Hence, the relation between damage and micro-crack population descriptor in 3D under the special cases of uniaxial tension (a) and uniaxial extension (d) is practically equivalent to the analytical result for 2D.
In the cases of plane stress (b) and plain strain (c), however, the development of damage is radically different, illustrating the development of damage-induced anisotropy.
In this case the damage parameter Di represents the relative reduction of the longitudinal shear modulus in direction Xi.
Note, that this is not the shear modulus relating shear stress to shear strain.
Considering the definition given by Eq.
(4b), a damage parameter larger than one indicates negative resistance to shear in a particular direction, i.e.
the resistance shifted from the initial positive to a negative value as the micro-crack population evolved.
Inversely, a damage parameter smaller than zero indicates a resistance to shear larger than the initial one, i.e.
the micro-crack population evolution increased the shear resistance of the material in the corresponding direction.
In both plane cases, the evolution of D1 suggest that the system undergoes transition into negative longitudinal shear resistance, quite more pronounces in the plane strain case (c), while the shear resistance in direction X2 increases from its initial value.
Negative shear resistance is not unusual for anisotropic materials.
The bounds for Poisson’s ratios in such materials calculated in [20,21] allow for negative longitudinal shear moduli with the values recorded here.
The thermodynamic stability of the system when one of the longitudinal shear moduli is decreasing is ensured by the increase of the other longitudinal shear modulus.
The results merely show that extreme anisotropy has been developed in the material with the evolution of micro-crack population under the two plane cases.
The development of the hydrostatic damage is also affected in these cases, as it cannot be described as a linear function of the cracked area moment.
The mechanical damage in this case cannot be measured by a single parameter hence the plots are against a measurable characteristic of the micro-crack population.
To understand what causes the anisotropy in the plane cases the structure of the crack population needs to be studied in more detail.
This requires a single damage parameter; an appropriate choice is the relative reduction of the strain energy density in the system, D=1−W/W(0), which is found to be approximately equal to the damage parameter defined via the relative reduction of the hydrostatic stress in all cases, see Fig.
5(a).
The development of the maximal graph component, i.e.
the main crack, with damage is shown in Fig.
5(b) with the ratio between the area of the maximal component, Am, to the total cracked area, A.
It is clear that the main crack becomes dominant very early in the development of damage (at damage less than 1%) and its relative area grows nearly exponentially for all cases.
It seems therefore sufficient to examine the structure of the maximal component as the damage develops.
Fig.
6 shows the development of the maximal component area, split into the areas of surfaces normal to the three principal axes, A1, A2, A3, and the surfaces formed on octahedral planes, A4.
All areas are normalised with the total areas of the corresponding boundaries in the cellular structure.
In the cases of uniaxial extension, unconfined (a) and confined (d), the development of the main crack involves creation of surfaces normal to the applied load and on octahedral planes.
Although there is a difference between the two cases in the rates of creation of normal and octahedral surfaces, the overall balance results in isotropic damage, see Fig.
4(a and d).
In the plane stress (b) and plane strain (c) cases, the development of the main crack follows very different patterns.
The parallel increase of normal to the first loading axis and octahedral surfaces in plane stress, Fig.
6(b), seems to be responsible for the immediate development of damage-induced anisotropy, which after that appears to be moderated by the development of surfaces normal to the second loading axis.
The constraint in plane strain, Fig.
6(c), leads to a delayed but rapid increase of surfaces normal to the first loading axis together with a lower rate of creation of octahedral surfaces.
This appears to delay substantially the development of cracked surfaces normal to the second loading axis and results in significantly higher anisotropy.
It should be noted that the structure-damage relations reported here were found qualitatively independent of the random assignment of pores in the lattice model as well as of the shape parameter of the pore distribution.
This has been confirmed by a number of simulations with different shape of distribution and random assignments.
One parameter that may affect the outcomes is the shear to normal strength ratio; this is a subject of ongoing work.
It is further understood that the outcomes reported here are principally related to the selected lattice connectivity.
However, the detail to which the fracture surface topography can be studied with the proposed model is higher than the detail allowed by previous models based on cubic lattices.
One unknown in the analysis is whether the crack development in the lattice is energetically equivalent to the development of continuous cracks.
This question remains to be addressed in a future work.
The current observations suggest that a common, constraint independent, damage evolution law might not be feasible to achieve.
In such case it seems that a lattice-based analysis might be necessary as a sub-modelling approach to inform the behaviour of finite elements in a continuum model.
However, if a damage evolution law can be derived from simulations of this kind, it is clear than it must depend on the three invariants of the stress tensor.
This observation challenges the use of damage evolution laws based on a single damage parameter for loads different from uniaxial extensions.
The last question of interest in this work is related to the use of the weakest-link statistics for global failure predictions.
It was suggested in [16] that weakest-link should be applied to the population of micro-cracks in the system.
However, from the simulations performed here it is evident that a single crack, the maximal connected component of the cracked surface, becomes rapidly dominating the behaviour, Fig.
5(b), with few much smaller components disconnected from the main crack.
This does not allow for invoking the weakest-link, because the interactions between micro-cracks via load redistribution cannot be captured by extreme-value statistics.
Another approach would be to base the weakest-link statistics on the microstructure information, in this case the size distribution of crack-initiation features, the pores.
This is similar to the approach used in the modelling of cleavage fracture, where second-phase particles are considered to be cleavage initiators.
In order to show whether this is a realistic approach, a comparison is made between the probability density of pore sizes distributed in the lattice and pore sizes contained in the maximal component at failure.
The results are shown in Fig.
7 for the pores in the entire lattice (a) and three of the loading cases as depicted.
The results shown correspond to one and the same random assignment of pore sizes.
Evidently, the probability density of the pores belonging to the final fracture surface is different from the lattice distribution and depends on the loading mode.
While the initial damage may start at one and the same location in the system, the nature of loading develops the main crack in different ways and the final failure cannot be described as a weakest-link event using the statistics of the sizes of the failure initiation sites.
This makes it difficult to derive a load-independent, purely microstructure based relation between the macroscopic damage and the probability of failure.
The outcome challenges the use of the weakest-link statistics for assessing global failure.
It supports further the suggestion that macroscopic failure analysis needs to be performed with an underlying lattice-based analysis of local micro-crack propagation.
The proposed model uses structural beam elements to represent interactions between sites (material blocks).
While this is in line with many previous works, e.g.
[9,10,19], the formulation introduces a relation between forces and moments governed by the implemented beam theory.
This is a limitation of the model from the solid mechanics perspective.
In a more general case, the interactions between sites should be represented by bundles of six spring-like elements, resisting independently the relative translations and rotations between sites [11].
In such case, the lattice would represent a generalised continuum with independent translational and rotational degrees of freedom and would require the calibration of one additional material parameter (in the isotropic case), called coupled-stress constant [22].
Closed form relations between the translational spring constants and the classical macroscopic elastic properties have already been obtained, using the energy equivalence approach in [11], and reported in [23].
These have been successfully used for modelling damage evolution in nuclear graphite with no account for rotations [24].
The rotational spring constants, however, require the knowledge of the coupled-stress constant of the material which depends on the sizes of and the distances between the microstructure features at lattice sites.
Experimental and analytical work is ongoing to establish methodology for calculation of the coupled-stress constant for given microstructure.
The purpose and strength of the proposed model is to provide the link between the specific microstructure features and the emergent macroscopic mechanical response.
This is in addition to the natural way with which micro-crack generation, interaction, growth and coalescence are analysed, with no reliance on constitutive assumptions for these processes.
Generally, the microstructure features that can be accounted for in the model are of two types: (1) features determining lattice cells and residing at lattice sites, e.g.
concrete aggregates or rock blocks and (2) features capable of micro-crack initiation residing at lattice cell boundaries, e.g.
pores or existing fractures.
Such a methodology has been used in a work related to nuclear graphite [25].
In this work the site features have not been explicitly accounted for, which is presently a limitation to the physical realism of the microstructure representation.
Their length scale has not been used in the model construction to derive the model length scale L. Instead, the model length scale has been determined entirely from the material porosity and the assumption that the micro-crack initiating features are present on all boundaries of the cellular lattice.
The reason for this is the lack of experimental data for site features’ sizes and volume density, or alternatively statistics of distances between them, in the study material.
With such data, the model length scale will be more physically based, e.g.
on the average distance between site features or on the distribution of sizes and volume density of features.
The assignment of micro-crack initiating features to boundaries would then be performed either to a fraction of the cell boundaries or to all with multiple defects in some boundaries, depending on the material porosity.
Experimental and modelling work is ongoing for the determination of all microstructure parameters for more realistic model construction.
This will be reported in a future publication.
The loading cases analysed in the work have been selected for two reasons.
Firstly, to examine the relation between micro-crack size population and damage evolution previously derived [15,16].
Secondly, to study the behaviour of a volume element ahead of a macroscopic crack, where plane stress and plane strain conditions prevail towards the ends and the middle section of the crack front, respectively.
The latter is in view of a future development of microstructure-informed mechanistic methodology for the assessment of integrity of quasi-brittle materials containing macroscopic flaws (cracks, fractures).
Such a methodology can be based either on coupled models for material behaviour, involving damage evolution laws, e.g.
[26], or on uncoupled models based on the statistical size distribution of failure initiating features, e.g.
[27].
In the former case, the stability or growth of a macroscopic flow is determined by the changing material properties as damage ahead of the flow evolves.
The results in this work suggest that the damage evolution law for coupled models must depend on three invariants of the stress tensor in the general case.
This is of particular importance in geomechanics, where the stress fields are either biaxial on surface or triaxial in depth.
In the case of uncoupled models, the results of this work suggest that the widely used weakest-link statistics for global failure assessment [27] cannot be safely applied on the basis of measured size distribution of micro-crack initiating features.
It seems that the development of new coupled models for quasi-brittle media, specifically microstructure-informed damage evolution laws, would be the more rewarding way ahead.
The four loading cases considered in the work need to be complemented by analyses of a range of triaxiality conditions.
This is a subject of ongoing work for selected microstructures.
Potential limitation of the approach is that each particular microstructure would require separate analyses for sufficient number of stress triaxiality conditions to derive a macroscopic damage evolution law.
This could be overcome by incorporating the proposed methodology as a sub-model to continuum finite element models, with lattice covering one element or a region of elements as required by the element size and the microstructure length.
An additional strength of the proposed methodology is the explicit analysis of the connected components of the fractured surface.
These provide the easy pathways for transport through the porous/fractured medium and can be naturally represented by pore networks with appropriate local transport coefficients.
Thus the evolution of damage can be complemented by pore space models, e.g.
[28,29], to analyse the resulting evolution of the medium transport properties.
Coupled mechanical-transport models informed by the microstructure are currently being developed for applications in the safety analysis of engineered barriers in nuclear waste disposal facilities, and in the area of hydrocarbon recovery with hydraulic fracturing.
It was demonstrated that in cases of non-uniaxial extension, such as plane stress or plane strain found ahead of a main crack, the micro-crack population development was responsible for elastic anisotropy with extreme variations of longitudinal shear moduli.
It was shown that the damage-induced anisotropy was a complex function of the crack population structure.
A load-independent damage evolution law might not be achievable and explicit analysis of crack population development, e.g.
using a lattice model, might be necessary to complement continuum finite element analysis of failure.
It was shown that the maximal connected component of the crack population, i.e.
the largest crack, became dominant very early in the process of macroscopic damage and controlled the ultimate failure.
The analysis if this component suggested that the global failure could not be treated as a weakest-link event.
The graph-theoretical approach to the analysis of micro-crack populations showed significant potential to reveal the underlying topological structure of the cracked surface.
Further work is required to link the topological structure to a measure for global probability of failure, or develop microstructure-informed damage evolution laws.
The analysis of micro-crack populations allows for the natural construction of discrete transport models, with which to study the changes of macroscopic transport coefficients, e.g.
permeability or diffusivity, with the development of damage.
The author acknowledges the support from EPSRC via grant EP/J019763/1, “QUBE: Quasi-Brittle fracture: a 3D experimentally-validated approach”, and from BNFL for the Research Centre for Radwaste & Decommissioning.
Synthesis of carbon-13 labelled carbonaceous deposits and their evaluation for potential use as surrogates to better understand the behaviour of the carbon-14-containing deposit present in irradiated PGA graphite The present work has used microwave plasma chemical vapour deposition to generate suitable isotopically labelled carbonaceous deposits on the surface of Pile Grade A graphite for use as surrogates for studying the behaviour of the deposits observed on irradiated graphite extracted from UK Magnox reactors.
These deposits have been shown elsewhere to contain an enhanced concentration of 14C compared to the bulk graphite.
A combination of Raman spectroscopy, ion beam milling with scanning electron microscopy and secondary ion mass spectrometry were used to determine topography and internal morphology in the formed deposits.
Direct comparison was made against deposits found on irradiated graphite samples trepanned from a Magnox reactor core and showed a good similarity in appearance.
This work suggests that the microwave plasma chemical vapour deposition technique is of value in producing simulant carbon deposits, being of sufficiently representative morphology for use in non-radioactive surrogate studies of post-disposal behaviour of 14C-containing deposits on some irradiated Magnox reactor graphite.
The decommissioning of the UK's first generation of gas-cooled, graphite-moderated (Magnox) reactors will lead to approximately 45,000 m3 of irradiated reactor core graphite, with a packaged volume of 59,000 m3, for geological disposal [1].
An important radionuclide in safety assessments for the disposal of radioactive waste in a geological disposal facility (GDF) is the long lived isotope 14C (half-life 5730 years) [2].
With an approximate total 14C activity of more than 7000 TBq arising from Magnox graphite cores and the additional volume of graphite waste arising from advanced gas-cooled reactors (AGR) [2], investigation of the behaviour of 14C associated with such wastes after closure of a geological disposal facility is important.
Whilst reactor graphite has been extensively studied from a physio-mechanical standpoint, related to core integrity, relatively little research effort has been placed on understanding the behaviour of the graphite and constituent 14C in a geological disposal environment.
Recent research [3] providing post mortem analysis of irradiated graphite from two Magnox reactor cores highlighted the presence of a carbonaceous deposit on the exposed surfaces of the graphite bricks (channel and interstitial walls) from one of the reactors that has a pronounced and markedly different morphology to the bulk graphite.
The extent of this deposit is likely to be a worst case scenario and it is anticipated that not all Magnox reactors may contain such significant deposits.
However, these surface deposits have been determined to have a significant 14C content compared to the bulk graphite [4] that has been created via formation pathways discussed elsewhere [5].
It is not understood how these deposits will behave in a GDF setting in comparison to the graphite which it coats.
Specifically there is a gap in the understanding of the release rate and magnitude of the labile 14C fraction, of which 14C located in deposited material may contribute significantly, with this labile fraction expected to achieve relatively early release in the lifetime of a GDF [6].
The pronounced “cauliflower-like” morphology observed is not unique to nuclear reactors and similar morphologies have been commonly reported within the scientific literature for carbon from a range of deposition techniques unrelated to nuclear applications [7–11].
At present such deposits are of specific interest in geological disposal of graphite waste from the decommissioning of Magnox reactors, as the deposited material may be present and represent a significant fraction of the labile 14C.
The Magnox reactors represent the first generation of gas-cooled reactors in the UK that used carbon dioxide (CO2) as the primary coolant and a honeycomb network of graphite bricks to provide neutron moderation.
During reactor operation significant amounts of carbon monoxide (CO) was produced from the CO2 coolant.
This CO in turn can be radiolytically polymerised to form a carbonaceous deposit on free surfaces [12].
This non-graphitic carbon deposit is significantly more chemically reactive to air than the underlying graphite [12,13].
During the lifetime of some Magnox reactors, small quantities of methane gas were injected into the coolant gas to inhibit weight loss of the graphite core due to radiolytic oxidation [14].
Methane (CH4) is a precursor for carbonaceous deposits that form a sacrificial layer protecting the underlying graphite from excessive weight loss [15] and reduction in mechanical strength [16].
It is assumed nitrogen incorporation during deposit formation is the subsequent production route for the high 14C levels observed.
CH4 is also a commonly utilised feedstock gas for the production of diamond and other carbon coatings by the process of chemical vapour deposition (CVD) [17].
The growth of carbon materials by CVD involves the excitation of a carbon-containing precursor gas using a thermal or plasma energy source that creates activated radicals that will bond to a suitable exposed surface.
Therefore, even though differences exist in the formation of carbonaceous deposits from CO and CH4, both include the activation of carbon-containing gas creating activated carbon species that will bond to surfaces.
Recent work [3] showed that graphite from the Oldbury Magnox power station, which had methane introduced into the coolant gas, had a significant deposit on the fuel and interstitial channel walls of the graphite bricks.
This suggested that the deposit formed may be due to methane.
A comparison of the morphology and density of such deposits will help determine whether a 13C methane deposit can be used as a simulant for the surface deposit found on irradiated graphite in further work.
If 13C carbonaceous deposits can be used as a simulant for the deposits seen on irradiated graphite it will allow easier, non-radioactive investigations of the potential release of 14C from deposits on irradiated graphite in a geological disposal environment including the potential microbial interaction with such material.
If the deposits observed on the graphite behave differently to the underlying graphite it may lead to a significantly different release rate for 14C from the deposit than from the underlying graphite when contacted by groundwater some time after the closure of a geological disposal facility.
Microbial colonisation may also be more likely on the deposit than the underlying graphite due to the increased surface area due to the amorphous nature of the material.
The use of a 13C simulant allows wider access into the research of nuclear graphite, which contains many other radionuclides such as 60Co, as facilities to handle radioactive materials are not required.
Isotopic differences in the precursor material should not alter the chemical nature and/or effect the chemistry of the deposited carbon material.
To this end, 13C has previously been used as a common isotopic tracer in biological systems [18] and implanted in graphite [19] as a non-radioactive proxy for 14C.
In the current work we demonstrate the use of microwave plasma CVD to create a carbonaceous layer on graphite substrates that exhibit similar morphologies and densities to deposits observed to have formed in-service on Magnox graphite moderator blocks.
The non-radioactive isotope 13C was selected as a tracer during CVD deposition such that deposit–substrate interfaces could be clearly resolved using imaging mass spectrometry analysis to determine the degree of material mixing and substrate etching.
The present work is part of a larger programme (C14-BIG) directed at gaining a better understanding and predicting the release of graphite derived 14C from a GDF and the influence of microbial activity under alkaline conditions expected to predominate for a significant time in a cement-based near field of a geological disposal facility after closure.
Pile Grade A (PGA) graphite was provided by Magnox Limited as a surplus material from the commissioning of the Wylfa nuclear power reactors, Wales.
This graphite was trepanned into cores of 12 mm diameter using a stainless steel coring tool.
The cores were then cut into 2 mm thick discs using a South Bay Technology Inc. Model 650 low speed diamond cutting wheel with deionised water used as coolant.
This process gave a flat surface that was a suitable substrate for deposition.
Subsequently 12C and 13C carbonaceous deposits were formed on the graphite surfaces using microwave plasma chemical vapour deposition (MPCVD), Fig.
1.
Coating was carried out using a computer-controlled 2.45 GHz microwave generator (variable power output – maximum 1000 Watts), TE01 single mode cavity (Sairem downstream plasma source WR340), double plunge microwave tuner, mass-flow controllers (MFC) and a carrier (Argon) and precursor gas at a total flow rate of 50 cm3 min−1.
Sample coatings were made at methane concentrations of 2, 10 and 20% for 12CH4 and 2% for 13CH4.
For coating, each cylindrical PGA graphite disc was placed on a glass sinter situated inside a quartz tube which was aligned to position the disc within the centre of the waveguide.
The tube was then connected to the mass-flow controllers, a gas flow was established and then the system was placed under a low vacuum.
Once a 1000 Pa system pressure had been achieved the microwave generator was switched on and the microwave reflectance was reduced, as much as possible, using the double plunge microwave tuner.
Once the microwave reflectance was tuned the CVD coating process was left to proceed for a period of 30 min [20].
Additionally, deposition was performed at varying pressures (1000, 5000, 10,000 Pa), however a flow rate of 50 cm3 min−1 for the gas mixture did not achieve a system pressure of less than 700 Pa. A lower flow rate of 20 cm3 min−1 was applied at 10% 12CH4 so that a system pressure of 500 Pa could be achieved, additionally growth was performed at 10 Pa system pressure at this reduced flow rate.
1-2 mm particles were also produced alongside the disc samples due to crucible size restrictions for the Linkam catalyst stage for Raman spectroscopy.
Additional PGA graphite was provided by the National Nuclear Laboratory (NNL).
This graphite was sectioned into smaller rectangular sheets using a JCB toolbox saw and then cut into smaller monoliths using an Erbauer ERB180C tile cutter (with no coolant) thus making the graphite more manageable.
The graphite monoliths were then put into a metal container and placed into a 10-ton hydraulic press, where a pressure between 5 and 10 tonnes of pressure was used to break the graphite down into smaller pieces.
The pieces were then subsequently filtered using a 3 compartment Fisherbrand stainless steel sieve (aperture sizes: >2 mm, 1–2 mm and <1 mm) and the 1–2 mm particles were retained for subsequent microwave deposition.
Both the larger and smaller pieces were repeatedly pressed until all of the graphite was left as a mixture of either particles or powder, following sieving.
A selection of virgin PGA samples (i.e.
without deposit) and irradiated graphite specimens extracted by trepanning from a Magnox power station were also analysed for comparison, exact details previously described in Ref.
[4].
A Helios NanoLab 600i combined SEM/FIB system (FEI, Oregon USA) was used to obtain scanning electron micrographs.
The focused ion beam (FIB) was utilised to precision mill trenches to allow the thickness and morphology of the deposit to be determined with nanometre accuracy and to allow subsequent analysis using other techniques.
Electron micrographs were acquired using an accelerating voltage of 15 kV, an electron beam current of 0.17 nA and a dwell time of 100 μs.
Trenches were FIB milled with the use of a Ga+ ion source with an accelerating voltage of 30 kV.
A Selective Carbon Mill (SCM) gas was used throughout to enhance milling rates.
The SCM admits small amounts of water vapour directly over the milling area, promoting gasification of the milled material, enhancing the etch rate and reducing redeposition.
It also minimises beam damage and therefore reduces the need to deposit platinum on the surface as a protective measure.
Initially a 20 nA beam current was used to generate coarsely defined trenches, with subsequent incremental reductions in ion current to reach a final beam current of 0.9 nA for surface finishing.
The milled trenches had approximate dimensions of 50 μm × 56 μm × 20 μm (x, y and z respectively).
The trench faces were smooth and flat, allowing for direct and high spatial resolution observation of structures and features.
For isotopic analysis of the samples, an in-house built magnetic sector secondary ion mass spectrometer (MS-SIMS) was utilised.
Full details of the system are described elsewhere [21].
In summary the system comprised of a focused gallium ion gun (FEI electronically variable aperture type) fitted to a Vacuum Generators model 7035 double-focussing magnetic sector mass analyser with a channeltron detector.
The sample was held at a 4 kV potential during analysis.
The equipment was controlled using PISCES software, written in-house by Dayta Systems Ltd (Thornbury, UK).
The system was capable of providing selected ion mapping and depth profiling with sub-micron resolution.
MS-SIMS analyses were performed in negative ion mode for both spectral acquisition and secondary ion imaging.
Mass spectra and depth profiles were initially acquired from 4 different areas of the 2% 12C and 13C methane deposits, detecting mass/charge (m/z) signals at 12, 13, 24 and 26 Da.
These ion signals are generated due to the C− and C2− ions derived from sputtered 12C and 13C respectively.
Mass spectra were obtained by scanning through the mass range 0–100 Da in 0.05 Da steps, with duration of 100 ms per step and 200 s in total.
Data acquisition was performed at a low magnification to reduce beam damage (area analysed ∼0.25 mm2) and with a 3 nA beam current.
Identification and calibration of the exact m/z values for use in subsequent depth profiles and images were achieved with the use of these survey spectra.
Depth profiles record the ion yield intensity from selected sputtered analyte ions over time while rastering the ion beam over a selected area.
As the deposits are suitably thick it is not anticipated that the depth profile will sputter enough material to immediately expose the underlying graphite.
This allows the signal to be averaged over a set period of time and then the ratio between signals to be compared.
Depth profiles were acquired for 1800 s with a beam current of 3 nA and area analysed of approximately 2500 μm2.
Electronic gating was used throughout to eliminate signal created at the margins of the etched area.
Signal averages and ratios were calculated from 200 s to 1800 s, disregarding the first 200 s of data as this was the observed transient period for the experiment.
The species compared were the C2- ions at 24 and 26 Da, rather than 12 and 13 Da, due to the strong signals obtained from these species, and also to avoid some prominent mass interferences.
Interference peaks are difficult to eliminate, however the use of the C2- peak is appropriate as the present work is not trying to identify trace elements but aiming to investigate whether the surface deposits are formed of 13C, to what extent 13C is incorporated into the graphite and how thick the overall deposit is.
Secondary ion images were recorded from the FIB milled trenches using the C2- ions (24 and 26 Da).
The images were obtained by selecting the m/z ratio of the ion of interest, and then raster scanning the ion beam over a defined area of the sample.
The images presented in this paper were acquired over a total area of approximately 0.0225 mm2.
Each image was acquired over a 60 s period using a 0.3 nA beam current to give the best possible spatial resolution whilst still maintaining sufficient ion signal.
A CCR1000 catalyst stage reactor system connected to a T95 system controller and LinkPad interface (Linkam, Surrey UK) was used for the thermal oxidation of the PGA graphite 1–2 mm particles.
For in situ spectral acquisition, a LabRAM HR800 confocal Raman microscope (Horiba Jobin Yvon, Kyoto Japan) was used.
The sample was heated up in the crucible inside of the catalyst stage from room temperature up to 600 °C (at 10 °C min−1), with a 50 cm3 min−1 flow of air.
Spectra were acquired using a 532 nm laser, a 50X long-working distance objective, a 300 g mm−1 grating, and spectral acquisition times of 25 s every 50 °C.
The heating regime and the spectral acquisition parameters for automated analysis were controlled using a built-in Linkam module script in the Horiba Labspec 6 software package.
The Raman spectroscopy system was calibrated using the 520 cm−1 peak from a silicon crystal.
Spectral analysis, during thermal oxidation in air, of virgin PGA graphite and PGA graphite with 12C and 13C carbonaceous deposits was carried out to analyse the thermal profile of the surface material (i.e.
graphite substrate) and the “cauliflower-like” carbonaceous deposit.
This technique allows for analysis of the thermal oxidation properties/reactivity of the different carbon materials and also surface chemical changes due to thermal oxidation.
The deposit formed on irradiated graphite taken from Oldbury Magnox reactor has a distinct and pronounced morphology, Fig.
2a, compared to virgin PGA graphite, Fig.
2b [3].
For comparison, electron micrographs of the 2% 12CH4 and 2% 13CH4 deposits can be seen in Fig.
3, a and b respectively.
The distinction between deposit and underlying graphite should be noticeable due to the lack of characteristic features in the deposit that are routinely seen in all PGA graphite such as shrinkage cracks and ligaments between pores [22], Fig.
4.
The deposits found on irradiated graphite have a ‘cauliflower-like’ appearance due to an agglomeration of irregular spheres, Fig.
5.
After FIB milling the internal morphology of the 2% 13CH4 and 2%, 10% and 20% 12CH4 deposited samples can be seen in Fig.
6a, b, c, and d respectively.
The 2% 12C and 13C methane CVD deposits were observed to have a porous, ‘feathery’ texture that appears to be significantly less dense than the underlying graphite.
For the irradiated graphite however, there was very little distinction in density or fine structure between the deposit and the underlying graphite (the deposit appears to have a lower porosity compared to virgin PGA, Fig.
5).
It is possible that the underlying PGA graphite in the irradiated samples is protected from radiolytic oxidation by the carbon deposit, leading to the deposit and underlying graphite being difficult to distinguish [15].
Further investigation using greater methane concentrations showed increases in the apparent density of the deposit (which was only determined visually), Fig.
6 (b), (c) and (d), that are more closely comparable to the deposit found on irradiated graphite.
Deposits produced at system pressures of 5000 and 10,000 Pa were of different morphology, instead comprising an agglomeration of spherical deposits that were not as extensive or as thick as those grown at the lower pressure of 1000 Pa. Reducing the flow rate to 20 cm3 min −1 allowed a system pressure of 500 Pa to be achieved, however even though the surface topography of the deposit was similar to irradiated material and the other cauliflower-like deposits formed, the internal morphology exhibited extensive porosity and this did not appear suitable as a simulant, Fig.
7(a).
Conversely, growth at a system pressure of 1000 Pa at this reduced flow rate formed a deposit that was very similar to that grown at 50 cm3 min−1, Fig.
7(b).
The deposit formed at 1000 Pa pressure at 10% methane concentration showed the closest resemblance to those seen on Oldbury irradiated Magnox graphite and was deemed to be the most suitable for use as a simulant.
Survey spectra from the 2% methane 12C and 13C deposits are shown in Fig.
8 (a) and (b) respectively.
Signals recorded at mass/charge peaks of 13 Da (13C−) and 26 Da (13C2−) are significantly greater in the 13C deposit compared to the 12C deposit, although these signals are also present in the 12C sample due to 12CH− and 12CN− species respectively.
The mean ratio (n = 4) between the peak heights at 26 Da and 24 Da for the 12C deposit was found to be 0.14 ± 0.03.
The mean ratio (n = 4) for the 13C deposit was 115.3 ± 19.1.
This increase of several orders of magnitude is strong evidence that the deposit is predominately 13C as the interfering peak from 12CH at 13 Da is unlikely to be higher in the 13C sample.
The errors given here are likely to be due to the strong dependence of signal intensity on location and geometry of the sample in the SIMS system [23].
The areas analysed were selected randomly and the only criteria for examination was that they produced sufficient SIMS signal to allow analysis.
Due to the surface not having a uniform, flat surface there are likely to be topographic effects that will affect the signal recorded.
This has been studied by other authors [23–25] with suggestions that the changes may be due to the incident angle of the beam, the height of the features and variations in the electric field due to topographic features that may lead to trajectory changes of the secondary ions [24].
SIMS ion signal maps have been recorded for 26 Da and 24 Da for a 13C sample, Fig.
9 (a) and Fig.
9 (b) respectively.
For the 13C deposit the mass peak signal at 26 Da is present primarily on the deposit with a significant reduction in signal in the underlying graphite with the 24 Da signal being the reverse, with a more intense signal recorded in the underlying graphite than in the deposit.
This shows that the 13C is deposited on top of the underlying graphite.
The signal at the bottom of the trench has a relatively high intensity for both 24 and 26 Da, and this may be due to re-deposition of sputtered material originating from the 13C deposit during FIB milling of samples [26].
A three-vectored graph displaying Raman shift, intensity and temperature (x, y and z axis respectively) was used to illustrate the Raman spectra at each temperature during the thermal oxidation experiment.
The Raman spectra are displayed between 1100 and 1700 cm−1 to allow the critical peaks related to both 12C and 13C carbonaceous materials to be compared.
The 12C peaks are the 12D peak at ∼1350 cm−1 and the 12G peak at ∼1575 cm−1 and the 13C peaks are the 13D peak at ∼1300 cm−1 and the 13G peak at ∼1525 cm−1.
The thermal oxidation spectral profile for a virgin PGA graphite 1–2 mm particle is shown in Fig.
10.
This spectral profile shows that there was a negligible change in the intensity of the D and G peaks between 50 and 600 °C.
This indicates that between 50 and 600 °C the surface of the PGA graphite undergoes very minimal surface oxidation and that the PGA is mostly unreactive.
As the surface of the virgin PGA material remains relatively unchanged during thermal oxidation it will readily allow for any spectral changes, due to the thermal oxidation of 12C and 13C carbonaceous deposits, to be isolated.
The thermal oxidation spectral profiles for a 2% 12CH4 and 13 CH4 deposit on a PGA graphite particle are shown in Figs.
11 and 12 respectively.
Fig.
11 shows that there is a noticeable decrease in the 12D peak intensity between 400 and 600 °C.
This indicates that the 12C carbonaceous deposit begins to thermally oxidise at approximately 400 °C and appears to have been completely removed by 600 °C indicated by the intensity of the 12D peak at 600 °C, showing the spectral profile of the virgin PGA graphite material.
There is a noticeable decrease in the 13D & 13G peak intensities between 450 and 600 °C in Fig.
12, which are solely present due to the 13C carbonaceous deposit.
This indicates that the 13C carbonaceous deposit begins to thermally oxidise at approximately 450 °C and appears to have been completely removed by 600 °C indicated by the absence of the 13D & 13G peaks at 600 °C, showing the spectral profile of the virgin PGA graphite material.
The intensities of the 12D and 12G peaks (PGA graphite) do not decrease but in fact increase relative to the decrease in the intensities of the 13D and 13G peaks (13C carbonaceous deposit), which also illustrates that the surface of the virgin PGA material, as a base substrate, remains relatively unchanged during thermal oxidation.
As the Raman peaks associated with the deposits decrease between 400 and 600 °C it indicates that the carbonaceous material on the surface has a similar oxidation temperature to that of the carbonaceous deposits found on irradiated PGA graphite (M. P. Metcalfe, personal communication, 11th November 2013).
Fig.
13 illustrates the isothermal profiles of virgin PGA graphite, irradiated PGA graphite deposit & a 12C microwave simulant deposit at 450 °C, in air, over a 50 h period.
The oxidation of virgin PGA graphite is negligible whereas the irradiated PGA graphite deposit & the C-12 microwave simulant deposit show significantly greater rates of oxidation and are clearly more reactive.
Initially the rates of thermal oxidation remain fairly similar for the first 5 h for the irradiated PGA graphite deposit & the 12C microwave simulant deposit but for the next 45 h the irradiated PGA graphite deposit shows a greater rate of thermal oxidation.
This deviation in rates of reactivity may be due to irradiated damage caused to the underlying PGA graphite in the irradiated PGA graphite sample whereas the underlying PGA graphite in the microwave simulant underwent no irradiation and started off as pristine virgin PGA graphite.
However the microwave simulant carbonaceous deposit reactivity seen in the TGA isothermal data shows a similar reactivity to that of the carbonaceous deposit seen in irradiated PGA graphite.
Previous examination [3] of irradiated graphite from Magnox reactors has shown that during generation lifetime a carbonaceous deposit can be formed on the fuel and interstitial channel walls of the graphite moderator that has a markedly different morphology to the underlying PGA graphite.
This work aimed to form a similar carbonaceous deposit using 13C precursor gas to allow subsequent investigation of the behaviour of such deposits in leaching and microbial studies pertinent to examining graphite degradation and 14C release in a GDF [27].
Use of a simulant allows future experiments to be performed more easily than using irradiated graphite due to a removal of the need to work with radioactive materials.
However, the use of simulants necessitates care to ensure that they are representative of the properties being examined.
With the use of several experimental techniques (FIB, SEM, MS-SIMS, Raman) this work has examined the internal morphology as well as the surface topography of carbonaceous deposits formed using microwave plasma CVD and compared them to irradiated graphite trepanned from a Magnox power station graphite core.
Microwave plasma CVD has been used to form adherent carbonaceous deposits on the surfaces of virgin (unirradiated) PGA graphite discs.
Microwave plasma CVD is widely used to grow other carbon materials with differences in growth parameters (precursor gas, temperature, pressure, microwave power) leading to different allotropes being formed most notably Carbon-Nanotubes (CNT) [28,29] and diamond [30,31].
Initially, 12C precursor gas, using a system pressure of 1000 Pa with a flow rate of 50 cm3 min−1, was used, primarily due to the high cost of labelled isotopic gases, and with the use of scanning electron microscopy the surface topography was found to be very similar to the ‘cauliflower-like’ deposits found on irradiated graphite [3].
However, after sectioning with a focused ion beam it was found that the internal morphology was more porous than the deposit found on irradiated graphite.
This is believed to be due to the growth rate, approximately 50 μm h−1, of the deposit being too rapid to allow a dense deposit to be formed.
By comparison, growth rates of diamond using microwave plasma CVD are usually in the region of 1 μm h−1 [32] and these form ‘solid’ deposits.
By increasing the methane concentration in the precursor gas mix an increased density in the deposit was achieved, likely due to the increased availability of carbon radicals available for deposition.
It should be noted that the deposits formed on irradiated graphite are formed at conditions that are very difficult to replicate, pressures of 1–3 MPa, temperatures of approximately 400 °C and in the presence of a neutron flux [33], therefore the high density of the deposits found on irradiated graphite is likely due to the high pressure environment, whereas in microwave plasma CVD low pressures are used so that the plasma can be sustained.
Further experiments were carried out to investigate the parameters which can affect the growth rate of carbonaceous deposits and to determine whether a more representative carbonaceous deposit could be formed using microwave plasma CVD.
Experiments carried out at 200 W using 2, 10 & 20% CH4 failed to generate carbonaceous deposits.
However, deposition at 400 W induced a rapid growth of carbonaceous material.
Further tests were carried out at both 5000 & 10,000 Pa pressures using 10% CH4.
Deposits were produced for both pressures; however these deposits showed a thin agglomeration of carbonaceous spheres on the graphite substrate.
This difference in form and thickness shows that growth at higher pressures is not suitable in producing an analogous material for studying irradiated material.
At pressures of 500 Pa with the reduced flow rate the deposit was not analogous of those found in irradiated material, suggesting that the most representative deposit is formed at system pressure of 1000 Pa with a 50 cm3 min−1 flow of 10% CH4:90% Ar.
Growth using 13C precursor gas showed a similar topography/morphology to 12C deposits indicating that there is no appreciable difference in the growth mechanism between the different isotopes, thereby justifying the use of this simulant to study the behaviour of carbonaceous deposits found on irradiated graphite.
The clear separation of the deposit and underlying graphite shown by isotopic imaging using a MS-SIMS has shown that a deposit is formed, and cross-sectional images indicate that the topography and morphology are very similar to the ones found on irradiated graphite.
Catalyst stage Raman spectroscopy combined with TGA have shown these deposits to be of a similar reactivity to those found on irradiated graphite.
These deposits appear to be suitable for further studies involving microbial systems to examine the possible release of the deposit into the environment in a geological disposal facility.
Based on the thermal oxidation behaviour, the density difference in the surface deposit materials between irradiated and simulant samples does not appear to significantly influence observed reactivity.
With the surface layers exhibiting rapid degradation at much lower temperatures than the underlying graphite.
Carbonaceous 12C and 13C deposits were formed on Pile Grade A graphite using microwave plasma deposition and examined using Focused Ion Beam, Scanning Electron Microscopy and Magnetic Sector-Secondary Ion Mass Spectrometry.
Several conclusions can be drawn:1.The surface topography of both 12C and 13C deposits formed by MPCVD are very similar to the ‘cauliflower-like’ deposits found on graphite samples trepanned from a Magnox reactor.
Deposits formed at 1000 pa system pressure with a 50 cm3 min−1 flow of 10% CH4:90% Ar showed the closest resemblance to the deposits on the irradiated material.
The internal morphology of the deposit is slightly more porous than that found in irradiated graphite.
However, variations in methane concentrations and gas pressure can affect the density of deposited material.
To summarise, there is a potential use of the 13C containing deposits synthesised in this work to act as simulants in future studies aimed at better understanding and predicting the post-disposal behaviour of irradiated graphite waste in a geological disposal environment and the associated release profile of 14C arising from the labile deposit.
The authors would like to thank the EPSRC and Radioactive Waste Management for the funding of this work (The post-disposal Behaviour of C-14 and Irradiated Graphite [BIG], Grant No EP/1036354/1).
The support of our co-workers at the University of Huddersfield (www.hud.ac.uk/c-14-big) and advice from colleagues in Magnox Ltd and the National Nuclear Laboratory is also gratefully acknowledged.
All relevant data are available on Harvard Dataverse at DOI: http://dx.doi.org/10.7910/DVN/5GZ4JK.
Control of laser induced molecular fragmentation of n-propyl benzene using chirped femtosecond laser pulses We present the effect of chirping a femtosecond laser pulse on the fragmentation of n-propyl benzene.
An enhancement of an order of magnitude for the relative yields of C3H3+ and C5H5+ in the case of negatively chirped pulses and C6H5+ in the case of positively chirped pulses with respect to the transform-limited pulse indicates that in some fragmentation channel, coherence of the laser field plays an important role.
For the relative yield of all other heavier fragment ions, resulting from the interaction of the intense laser field with the molecule, there is no such enhancement effect with the sign of chirp, within experimental errors.
The importance of the laser phase is further reinforced through a direct comparison of the fragmentation results with the second harmonic of the chirped laser pulse with identical bandwidth.
The control over the fragmentation and chemical reactions using shaped ultrafast laser pulses was demonstrated by different schemes of experiments by various groups of scientists [1–7].
All of their experiments are based on the closed loop approach using learning algorithms to control the laser field with feedback from the experimental signal.
The resultant pulse shapes obtained by an adaptive approach were not always found to be a globally optimized solution [2,3,8,9].
Furthermore, the physical significance of such complex shaped pulses to the actual processes leading to discrimination among the fragmentation channels and their mechanisms are often too difficult to apprehend limiting the generalization of such feedback schemes.
A continued effort, therefore, exists on the control of such fragmentation process from simpler pulse modulation concepts.
One of the easiest pulse modulation schemes is frequency chirping.
Chirping essentially refers to the process of arranging the frequency components in a laser pulse with certain phase ordering.
Linear ordering can be easily achieved by dispersing the ultrafast pulses through a pair of gratings.
This ordering of frequency components results in the lengthening of an otherwise bandwidth limited ultrafast laser pulse.
For positively chirped pulse leading edge of the pulse is red shifted and the trailing edge is blue shifted with respect to the central frequency of the pulse.
Negative chirp corresponds to the opposite effect.
The first experimental demonstration of control using simple linear chirped laser pulses was by Warren and co-workers in the early 1990s [10,11].
Subsequently, several experimental and theoretical developments have made linear chirped pulse control a very attractive field of research [12–17].
Control of fragmentation with simple linear chirped pulses as a control scheme has also become an active field of research in recent years [4,9,18–22].
It has been known [9,14,18,22] that the fragmentation processes in polyatomic molecules induced by an intense ultrafast laser field can sometimes exhibit sensitive dependence on the instantaneous phase characteristics of the laser field.
Depending on the change in sign the chirped laser pulses, fragmentation could be either enhanced or suppressed [14,18,22].
Controlling the outcome of such laser induced molecular fragmentation with chirped femtosecond laser pulses has brought forth a number of experimental and theoretical effects in the recent years.
However, efforts are continuing for a specific fragment channel enhancement, which is difficult since it also is a function of the molecular system under study [20,22–24].
Here we report the observation of a coherently enhanced fragmentation pathway of n-propyl benzene, which seems to have such specific fragmentation channel available.
We found that for n-propyl benzene, the relative yield of C3H3+ is extremely sensitive to the phase of the laser pulse as compared to any of the other possible channels.
In fact, there is almost an order of magnitude enhancement in the yield of C3H3+ when negatively chirped pulses are used, while there is no effect with the positive chirp.
Moreover, the relative yield of all the other heavier fragment ions resulting from interaction of the strong field with the molecule is not sensitive to the sign of the chirp, within the noise level.
The Laser system used in this experiment is a Ti:Sapphire multipass amplifier (Odin, Quantronix Inc.), which operates at 800nm with 50fs FWHM pulses at 1kHz with energy of ∼1mJ.
It is seeded with a homebuilt Ti:Sapphire Oscillator (starting point: K&M Labs Inc. oscillator kit).
The oscillator is pumped by a Nd:YVO4 (Verdi 5, Coherent Inc.), resulting in femtosecond pulses with a centre wavelength of 800nm and a spectral bandwidth of 50nm FWHM at 94MHz repetition rate and an average power of 400mW.
The oscillator output is stretched and then fed into the Ti:Sapphire multipass amplifier which is pumped by the second harmonic of Nd:YAG laser operating at a repetition rate of 1kHz (Corona, Coherent Inc.).
The production of chirped ultrafast laser pulses is straightforward [18,25]: it is built-in our suitably modified compressor for the amplified laser system.
As we increase the spacing between the compressor gratings relative to the optimum position for minimum pulse duration of 50fs, we generate a negatively chirped pulse.
Conversely, we obtain the positively chirped pulse by decreasing the inter-grating distance.
Pulse durations were measured using a home-made intensity auto-correlation for the transform-limited pulse, as well as for the various negatively chirped and positively chirped pulses (Fig.
1, inset bottom right).
The pulses were further characterized by second harmonic frequency resolved optical gating (SHG-FROG) technique.
Fig.
1 (inset bottom centre) shows a typical SHG-FROG trace of our near transform-limited pulse that was collected using GRENOUILLE (Swamp Optics Inc.).
In Fig.
1 (inset bottom left), we show the SHG spectrum of the of the transform-limited pulses after frequency doubling with 50μm type-1 BBO crystals as well as the spectrum of the transform-limited pulse collected with a HR-2000 spectrometer (Ocean Optics Inc.).
The laser pulses are then focused with a lens (focal length=50cm) on a supersonically expanded molecular beam of n-propyl benzene at the centre of a time of flight chamber.
The polarization of the laser was horizontal as it enters the mass spectrometer and is perpendicular to ion collection optics.
The design of our molecular beam setup (Fig.
1) consists of three chambers with differential pumping by turbo pumps.
The three chambers were designed with differential pumping to attain a supersonic molecular beam region, a laser interaction region and a mass detection region, respectively.
The source chamber or the supersonic molecular beam region is pumped by 2000ls−1 turbo molecular pump (Varian Turbo-V2K-G).
The ionization chamber (or the buffer chamber), where the laser interaction occurs, is pumped by a 500ls−1 turbo pump (Varian Turbo-V551 Navigator).
Finally, the mass detection region is pumped by a 300ls−1 turbo pump (Varian Turbo-V301 Navigator).
An engineering drawing of the system was prepared using AUTOCAD software.
The vacuum chambers were fabricated based on our design and requirements for a complete oil-free system using SS-304 stainless steel and conflate connections.
The pressure in the ionization chamber during the experiment was kept at 10−7torr (base pressure of 5×10−9torr).
A pulsed valve (Series 9, General Valve) of 0.5mm diameter operating at 10Hz repetition rate and a skimmer of 1mm diameter were used to introduce the supersonic molecular beam into the laser interaction region, where it is exposed to the intense laser field (1kHz amplified Ti:Sapphire laser pulses as detailed earlier).
The pulse duration of the molecular beam is kept at 100μs.
The molecular beam and laser pulses are synchronized by a delay generator.
The n-propyl benzene sample at room temperature was used without further purification (98%; Sigma–Aldrich) and was seeded in Helium at 2atm backing pressure.
The mass spectra were recorded with a Wiley-McLaren type linear time of flight mass spectrometer.
The ions are detected using an 18mm dual micro-channel plate (MCP) detector coupled to a 1GHz digital oscilloscope (Lecroy 6100A).
The mass resolution of n-propyl benzene cation was calculated and was found to be t/2Δt∼1100.
Mass spectra were typically averaged over 500 laser shots.
The mass spectra from our particular beam chamber constructed with dry-scroll pumps and turbo-molecular pump as described above has the advantage that it does not contain any extraneous water and hydrocarbon peaks and thus has better sensitivity for organic samples as reported here.
Study of aromatic hydrocarbons [26] has indicated different fragmentation channels.
A systematic study of aromatic compounds with increasing chain-length of substituent alkyl groups has indicated that the fragmentation process is enhanced as the chain-length of the alkyl substituent on the benzene ring increases [26].
We have chosen to apply chirped pulse fragmentation control on certain members of these systematically studied aromatic compounds.
In general, as reported previously for benzene and toluene, p-nitro toluene [22], we also find that chirping favors the formation of smaller fragments as compared to the heavier ones.
However, n-propyl benzene has the unique property of enhancing a particular fragmentation channel under the effect of chirp.
The phase (φ(ω)) of the laser pulse which is centered at ω0, can be expanded around ω0 to second order in ω:(1)φ(ω)≃φ(ω0)+11!∂φ∂ωω=ω0(ω-ω0)+12!∂2φ∂ω2ω=ω0(ω-ω0)2.Here, the second order term is responsible for group velocity dispersion.
In fact, β=∂2φ∂ω2ω=ω0 is linear chirp coefficient (chirp parameter in the frequency domain) introduced by the compressor and is defined as second derivative of the spectral phase at the center frequency.
The linear chirp coefficient (β) can be calculated using the equation [27–29]: τ2=τ02+4βln2τ02, where τ is the pulse duration of the chirped laser pulse and τ0 is the chirp-free pulse duration of the transform-limited pulse in FWHM.
The experimental error in the chirp value calculated from the equation mentioned above is about ±9%.
We record the TOF mass spectra (Fig.
2) of n-propyl benzene using linearly chirped and unchirped ultrafast laser pulses with constant average energy of ∼200mW.
Next, we compared the corresponding peaks in mass spectra by calculating their respective integrals under the peaks and normalizing them with respect to the molecular ion.
These results also conform to the case when we just compare the simple heights of the individual peaks.
When the linear chirp of the laser pulse is negative, the relative yields of the smaller fragment ion, such as, C3H3+ (mass to charge ratio, i.e., m/z=39) is largely different from those obtained using transform-limited pulses or from the positively chirped pulses, as reflected in Fig.
3a.
The relative yield of C3H3+ reaches maximum when the linear chirp coefficient (β, calculated by using the equation as mention earlier) is −8064fs2 and pulse duration is of 450fs.
We would like to point out that the fragment ion C6H5+ (m/z=77) yield is more when the chirp is positive (β=+6246fs2), and this can be attributed to a different fragmentation pathway [18] (Fig.
3b).
However, the observation of enhancement for only one chirp sign implies that the observed enhancements are not due to the pulse width effects, they rather depend on the magnitude and sign of the chirp [2,4].
Hence coherence of the laser field plays an important role in this photofragmentaion process.
It is also seen that relative yields of the heavier fragments like C7H7+ (m/z=91) is not affected by the sign of the chirp.
The relative yield of C7H7+ decreased in both the directions of the chirp and is at its maximum when the pulse is transform-limited (Fig.
3c), indicating that the fragment yield only depends on the laser peak intensity as dictated by its pulse width.
We have also seen that the integrated SHG intensity at different chirp is symmetrically decaying around 0fs2 (Fig.
3d), which confirms that there is nothing systematic in the laser pulse causing the enhancements in the fragmentation process.
Many experimental efforts have been made in recent years to control the fragmentation using chirped laser pulses.
A general conclusion from several recent studies [9,14,18,22] is that the fragmentation process is preferred over the formation of molecular ion as the magnitude of chirp is increased.
Our results also obey the same conclusion.
Recently, Dantus and co-workers [22–24] also found out that change in the fragmentation with chirped laser pulses depends on the molecular structure.
This is clearly evident in our results.
Dantus et al.
have shown that fragmentation of p-nitro toluene is enhanced when chirped laser pulses were used.
The enhancement of the fragment ions of p-nitro toluene, such as, C3H3+, C5H5+ was independent of the sign of the chirp but some smaller fragment like NO2+ does depend on the sign of the chirp and it gets maximized when the chirp is negative.
As discussed by Lozovoy and Dantus [22], the mechanism of fragmentation of p-nitro toluene follows these steps: first, an ionization of the parent molecule occurs; next is the isomerization of p-nitro toluene ion, which is followed by the elimination of NO2+ ion leading to C7H7+ tropylium ion; and this finally dissociates to form C3H3+ ion.
However, in case of n-propyl benzene, C3H3+ ions are generated from C7H7+ ions that form from the ionization of the parent molecule followed by the elimination of C2H5+ ions.
There is a large energy barrier (∼9eV) that needs to be surmounted to achieve ionization of molecules like p-nitro toluene.
The laser intensity needed for ionization causes excitation in a large number of intermediate rovibronic states some of which include dissociation continua.
Therefore, the resulting coherent superposition undergoes very fast delocalization.
This was one of the reasons why fragmentation does not depend on the parameter, time delay between sub-pulses, in the case of p-nitro toluene.
In our case, for n-propyl benzene, the fragmentation mechanism (Fig.
4) is almost same, but it does not have such energy barrier, and also due to the longer alkyl chain-length, energy redistribution is relatively faster than the other alkyl derivative of benzene.
Thus, fragmentation is more, which easily leads to dissociation into C3H3+.
We think this can be the reason why fragmentation pattern of n-propyl benzene shows the dependence on the sign of the chirp.
We should also consider the fact that the chirped pulse gives a linear delay between high and low frequencies within the bandwidth of the laser pulses.
This temporal progression can be linked to pump–dump processes [13,30].
Varying the time delay between the high and low frequencies within the band width of the laser pulses, may coherently drive the vibrations that could be implicated in selective bond cleavage.
Fragmentation pattern depend upon the molecular potential energy surface in the optical field and also on the efficiency with which energy from the optical field is coupled into the molecule.
A red-blue sequence of photon interaction starts and ends at different points of PES than does a blue-red sequence, and thus there is ample reason to believe that the negative chirp will enhance the process of fragmentation as it results in a better overlap between the pump and dump pulses as compared to that of the transform-limited case.
We have demonstrated that the phase characteristics of the femtosecond laser pulse play a very important role in the laser induced fragmentation of polyatomic molecules like n-propyl benzene.
The use of chirped pulse leads to sufficient differences in the fragmentation pattern of n-propyl benzene, so that it is possible to control a particular fragmentation channel with chirped pulses.
Overall, as compared to the transform-limited pulse, negatively chirped pulses enhance the relative yield of C3H3+, C5H5+, while the relative yield of C6H5+ increases in case of positively chirped pulse.
In fact, for the C3H3+ fragment, the enhancement is almost six times for the negatively chirped pulse (β=−8064fs2) as compared to that of the transform-limited pulse.
T.G.
thanks the Graduate Fellowship program of the UGC.
D.G.
acknowledges the support from DST-Swarnajayanthi Fellowship, Govt.
of India and Wellcome Trust (UK) International Senior Research Fellowship.
Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.chemphys.2009.04.009.
Supplementary data
A study into stress relaxation in oxides formed on zirconium alloys Pressurised and boiling water reactors contain zirconium alloys, which are used as nuclear fuel cladding.
Oxidation of these alloys, and the associated hydrogen pick-up, is a limiting factor in the lifetime of the fuel.
To extend the burn-up of nuclear fuel requires control of the oxidation, and therefore development of a mechanistic understanding of the cladding corrosion process.
Synchrotron X-ray diffraction (S-XRD) has been used to analyse oxide layers formed during in-situ air oxidation of Zircaloy-4 and ZIRLO™.
Analysis shows that as the oxide thickness increases over time there is a relaxation of the stresses present in both the monoclinic and meta-stable tetragonal phases, and a reduction in the tetragonal phase fraction.
To better understand the mechanisms behind stress relaxation in the oxide layer, finite element analysis has been used to simulate mechanical aspects of the oxidation process.
This simulation was first developed based on stress relaxation in oxides formed in autoclave, and analysed ex-situ using S-XRD.
Relaxation mechanisms include creep and hydrogen-induced lattice strain in the metal substrate and creep in the oxide layer.
Subsequently the finite element analysis has been extended to stress relaxation observed by in-situ S-XRD oxidation experiments.
Finite element analysis indicates that the impact of creep in the oxide is negligible, and the impact of both creep and hydrogen-induced lattice strain in the metal substrate metal is small.
The implication is that stress relaxation must result from another source such as the development of roughness at the metal–oxide interface, or fracture in the oxide layer.
Zirconium alloys are used as fuel cladding in pressurised and boiling water nuclear reactors.
As such these materials are exposed to a large number of environmental factors that will promote degradation mechanisms such as oxidation.
At high burn-ups, i.e.
extended service life, oxidation and the associated hydrogen pick-up can be a limiting factor in terms of fuel efficiency and safety.
The oxidation kinetics for many zirconium alloys are cyclical, demonstrating a series of approximately cubic kinetic curves separated by transitions [1–5].
These transitions are typified by a breakdown in the protective character of the oxide and are potentially linked to a number of mechanical issues.
Understanding how these issues influence oxidation is a key to developing a full mechanistic understanding of the corrosion process.
Synchrotron X-ray diffraction (S-XRD) experiments have shown that oxides formed on zirconium alloys are strongly compressed and composed of both monoclinic and stabilised tetragonal phases.
Published results suggest residual stresses ranging from −3800MPa to −80MPa, and tetragonal phase fractions ranging from 50% to 5%, depending upon the position on the corrosion kinetics curve and alloy composition [4–8].
Although there is some variation close to transition in the corrosion kinetics, each of these papers show that over several microns of oxide growth there is a gradual reduction in both the average compressive stress and the tetragonal phase fraction.
It is implied from this that some combination of mechanisms are relaxing the compressive stress and destabilising the tetragonal phase.
Possible mechanical factors include oxidation-induced creep and strain, or crack formation in the oxide layer [9–11].
The tetragonal to monoclinic phase transformation is considered as a key component in the oxidation process as it could potentially cause the crack and pore formation observed using TEM [11,12].
This would provide fast ingress routes for oxygen containing species thereby accelerating the corrosion kinetics, and degradation of zirconium alloy components.
A number of attempts have been made to experimentally define oxidation-induced strain in the metal substrate as a result of creep and hydrogen pick-up.
Donaldson et al.
oxidised samples of cold work stress relieved Zircaloy-4 tube in air, with wall thicknesses between 0.58 and 0.08mm, at temperatures in the range of 350–450°C for times ranging from 152 to 753days.
Axial strains were measured at regular intervals.
Across the temperatures measured, total strains at the end of the tests ranged between 0.0016–0.0265 in the thinnest tube and 0.0003–0.0047 in the thickest.
In particular, the elevated strain with decreasing substrate thickness is evidence of oxidation-induced creep as reducing substrate thickness would increase the oxidation-induced stress in the metal substrate [13].
Both Blat et al.
and Barberis et al.
have attempted to measure strains resulting from both oxidation-induced creep of the metal substrate and hydrogen-induced lattice strain [9,10].
Blat et al.
oxidised recrystallised 0.45mm thick Zircaloy-4 sheet at 360°C, with simulated primary water chemistry.
Oxide thicknesses ranged from 2.1 to 7.3μm, i.e.
100–400days exposure.
Including both hydrogen pick-up and oxidation-induced creep, strains ranged from 0.00012 to 0.00066.
Barberis et al.
carried out a significant number of tests into oxidation-induced strain due to creep and hydrogen pick-up.
Oxidising recrystallised M5 tube in 360°C water for ∼220days resulted in an oxide ∼3.7μm thick and a diametral strain of ∼0.00015 [10].
Although measuring such small levels of strain is problematic, these papers show clear evidence of creep deformation in the metal substrate as a direct result of oxidation.
In-reactor zirconium alloys are exposed to a range of stresses from sources such as channel bowing, oxidation, and pellet clad interaction (PCI).
These factors can limit the lifetime and safety of the fuel, hence research into creep mechanisms in zirconium alloys is significant [14–16].
The most commonly researched creep mechanism is power law creep, which is based on dislocation climb and glide.
Although near surface S-XRD of the metal substrate has given stress values of 50–100MPa [5], this does not correlate with calculations for the bulk metal balancing stress based on the average in-plane stress in the oxide layer measured using S-XRD.
Instead, Barberis et al.
calculate bulk tensile stresses present in the metal substrate that are of the order of ∼15MPa [10].
Extrapolating the stress measured at the near surface to the bulk of the metal substrate is problematic due to potential for localised stress effects relating to roughness at the metal–oxide interface [17].
Taking the bulk stress to be ∼15MPa puts oxidation-induced creep in a low stress regime.
Numerous mechanisms have been presented in the literature to explain creep in this regime including Coble, Nabarro-Herring, Harper-Dorn, Ashby-Verrall and Grain Boundary Sliding (GBS) [16].
Assignment of a specific creep mechanism is based on stress, temperature, and grain size [15,16,18–22].
In the three main pieces of work that study oxidation-induced creep, all strains in the substrate are measured empirically; and no discussion is given to assigning an actual creep mechanism to the observed behaviour [9,10,13].
For many years manufactured samples of tetragonal and cubic zirconia (ZrO2) have been known to be superplastic and capable of significant levels of strain under the appropriate experimental conditions.
Strain rates of up to 1×10−3s−1 have been recorded at 1150°C for nanocrystalline, yttria stabilised tetragonal zirconia [23].
This has led to a significant amount of research into creep [23–28], and a number of tracer element diffusion tests designed to define the diffusion coefficients have been conducted [29–32].
However this temperature is much higher than would be observed in reactor under normal operating conditions, and the morphology of the oxides are very different.
As yet there appears to be no experimental work that directly confirms the presence of creep in the oxide layer during oxidation at 360°C.
The hydrolytic component of the zirconium corrosion process leads to the generation of hydrogen, a percentage of which is known to be absorbed into the metal substrate [1].
As an interstitial element it occupies the tetrahedral sites between lattice planes and causes growth of the metal substrate.
Blat et al.
used a gaseous hydrogen charging technique to measure the hydrogen-induced lattice strain.
Results taken between 300 and 2000wppm hydrogen predict strains in recrystallised Zircaloy-4 sheet of 1.15×10−6per wppm [9].
Vizceno et al.
aimed to quantify this effect by heating cathodically charged Zircaloy-4 tube to 300°C in a push-rod differential dilatometer.
Hydrogen concentrations in the range of 150–400wppm gave an axial elongation of 5.21×10−6per wppm [33].
The following work combines newly presented in-situ S-XRD data on the development of phase fraction and stress as an oxide grows, existing ex-situ S-XRD measurements on autoclave tested samples, and significant finite element analysis of the oxidation process.
The aim of the work is to establish if a combination of mechanisms including creep and hydrogen-induced lattice strain could explain oxide stress relaxation observed experimentally.
Materials included samples of recrystallised Zircaloy-4 and ZIRLO™ sheet, cut into coupons with respective dimensions of 30×20×0.6mm3 and 30×20×0.45mm3.
Sample preparation involved pickling in HF solution (5%HF, 45%HNO3 and 50%H2O) [34].
All materials were provided by Westinghouse, and the chemical compositions for these materials can be found in Table 1.
All synchrotron X-ray diffraction experiments were carried out at the EDDI beam-line at BESSY II (Berlin, Germany) [35].
EDDI is a polychromatic energy dispersive beam-line allowing rapid acquisition of the diffraction peaks in the energy range of 8–120keV.
The classical sin2ψ technique was used to characterise the biaxial in-plane compressive residual stress in the oxide layer by tilting the sample through a range of ψ angles and measuring diffraction patterns through each angle.
The specific peaks of interest for measuring residual stress in thermally grown zirconium oxides are the (1¯11)m monoclinic and (101)t tetragonal peaks.
In addition to these two reflections the (111)m monoclinic peak was incorporated into the Garvie–Nicholson formula for defining the tetragonal zirconia phase fraction [36]:(1)ftet=It(101)It(101)+Im(111)+Im(1¯11)where Ixxx are the averaged integrated intensities of each reflection along the range of ψ angles.
These techniques have been used for both the previously published ex-situ analysis of samples oxidised in an autoclave [5], and the in-situ air oxidation experiments presented here.
More detailed description of the sin2ψ and Garvie–Nicholson techniques used in similar experiments can be found in [5].
The key in-situ oxidation experiments used a heated sample stage, allowing diffraction patterns to be recorded every 4.5min as the oxide layer is formed.
Samples from each of the alloys were subjected to the thermal profiles discussed in the following section, and the evolution of each oxide phase was recorded.
By calculating the stress in the monoclinic and tetragonal phases and weighting them based on the corresponding phase fraction it is possible to define an average bulk oxide in-plane compressive stress.
The previously presented S-XRD experiments involved removing a number of samples from the autoclave at different oxidation times and analysing them ex-situ.
The results indicated a gradual relaxation in the in-plane biaxial compressive stress with increasing oxidation time and oxide thickness [5].
This has formed the basis for the development of a finite element analysis.
The in-situ tests reported in this publication show more detailed information on stress relaxation over time for single samples, thereby removing the issues of variation between samples.
A similar finite element analysis has then been extended to simulate these in-situ experiments.
Fig.
1a shows the oxidation kinetics for Zircaloy-4 oxidised in an autoclave at 360°C in simulated primary water [5,34].
S-XRD measurements reported by [5] are used here to inform simulations by finite element analysis.
Available samples were sent to Westinghouse for analysis of hydrogen concentration using a hot vacuum extraction procedure.
Fig.
1b shows the resulting concentrations, along with calculated values described in Section 3.3.
Fig.
2 shows a schematic of the sample setup for the in-situ S-XRD experiment, illustrating the two positions at which two temperature readings were taken during heating.
In-situ S-XRD oxidation was carried out in air.
Here T1 and T2 represent the temperature of the heating plate and of the sample surface, respectively.
For each of the applied thermal profiles there was a significant difference between the reading for the sample stage and the thermocouple.
Therefore, in order to determine the weight gain with time during oxidation for each in-situ S-XRD experiment, a number of air oxidation tests were carried out using a Vecstar Ltd. air furnace for both alloys.
T1 and T2 recorded during the S-XRD experiment were used to determine the upper and lower temperature limits.
Samples were removed from the furnace every 7.5min.
Eddy current measurements (ECT probe) of the original in-situ samples were carried out to determine the final oxide thickness after testing, which is very similar to the method used previously by Park et al.
[37].
Using T1 and T2 as upper and lower temperature limits, a sensitivity study was carried out to determine the temperature profile vs. oxidation time required to give the appropriate level of oxidation.
Fig.
3 shows the temperature profiles taken from the heating stage, the thermocouple at the sample surface and the estimated temperature profile based on the oxide thicknesses.
Based on the oxide thicknesses the samples appear to have been exposed to thermal profiles up to 710°C (A) and 650°C (B), with a heating rate of ∼11.2°C/min and a hold time at temperature of ∼40min.
Figs.
4 and 5 show the different results for the furnace oxidation of Zircaloy-4 and ZIRLO™ samples, with oxide thickness being based on weight gain measurements.
To simulate the mechanical aspects of the oxidation process the Abaqus finite element code was used to create an elastic generalised plane strain model representing a thin slice in the middle of a sheet sample; see Fig.
6 for a schematic illustration.
This model represents a section of the sample with a width of 2μm, a height of 0.33mm (Zircaloy-4) or 0.227mm (ZIRLO™), and is composed of 4 node bilinear elements.
The coordinate system has direction X parallel to the metal–oxide interface and direction Y perpendicular to the metal–oxide interface.
A symmetry boundary condition was applied to the bottom surface of the model, fixing movement in direction Y, to represent a sample oxidised on both sides.
A second symmetry boundary condition was applied to the left edge, fixing movement in direction X, and a linear constraint equation was applied to the right edge to ensure the entire face strained evenly in direction X.
This combination of boundary conditions and constraints allows the model to represent a small section in a relatively large sample.
The near surface region has been partitioned into 20 layers of equal thickness based on the maximum oxide thickness of each of the samples being modelled.
These layers simulate a perfectly flat metal–oxide interface.
Although the actual metal–oxide interface has been seen to be rough, with an amplitude in the order of 0.1μm [38], it is not the intention of this work to analyse the impact of these features.
Each of the partitioned layers has two steps associated with it.
Step 1 is a static linear elastic step in which a metal layer transforms into an oxide that includes a change in the material properties and a volumetric expansion based on the Pilling-Bedworth ratio.
The expansion is defined using an orthotropic strain tensor first published by Parise et al.
[17].
It gives an out-of-plane expansion of 0.54 perpendicular to the metal–oxide interface, and a lateral in-plane expansion of 0.005.
It is the lateral expansion, and consequent constraint provided by the metal substrate, that will generate the strong compressive stress in the oxide and the weak tensile stress in the relatively thick metal substrate.
As such this value of 0.005 represents the level of strain required to completely relax the stress in the first oxide that forms.
Step 2 is a time dependent visco-plastic step, and uses a time interval based on the thickness of the partition and the predicted oxidation kinetics shown in Figs.
1, 4 and 5.
It is during this visco-plastic step that any creep based relaxation may occur.
All calculations and models presented are in mm and MPa, with time in hours.
Table 2 shows the bulk elastic properties used.
This combination of partitioned layers and steps allows the model to represent the mechanical aspect of an advancing oxide-metal interface.
It should be noted that this type of simulation does not take into account localised effects that could potentially increase or decrease the stress in the oxide.
The metal–oxide interface is known to form undulations or roughness [38], and absorb oxygen into the near surface metal [39], both of which could potentially relax stress in the oxide be deformation and increase of the interfacial area.
Research presented in this paper indicates that at least some of the tetragonal phase present in the oxide transforms into monoclinic.
This phase transformation has an associated volume expansion that could introduce additional stress into the oxide [11,40].
These localised effects are not the focus of this work and as such have not been incorporated into the simulations.
Without the application of creep or hydrogen-induced lattice strain the above models are fully elastic, and for an oxide 0.001mm thick will give an in-plane compressive stress of -1749MPa in the oxide layer and an in-plane tensile stress of 5.3MPa in the metal substrate.
This model can be validated mathematically.
Let E′=E1-ν to account for the stress being in two dimensions.
The compatibility relationship can be defined by Eq.
(2).
The in-plane strain in the oxide must equal the in-plane strain the metal substrate.
(2)σ2E2′=Δ-σ1E1′where Δ is the strain due to the oxidation expansion, σ1 is the in-plane stress in the oxide (compressive), σ2 is the in-plane stress in the substrate (tensile), E1 is the Young’s modulus of the oxide, and E2 is the Young’s modulus of the substrate.
The equilibrium relationship can be defined by Eq.
(3).
The in-plane force in the oxide must be equal to the in-plane force in the metal substrate.
(3)2σ1t1=t2σ2where t1 is the oxide thickness and t2 is the substrate thickness, assuming oxidation occurs on both sides of the sample.
Combining Eqs.
(2) and (3) can give the in-plane stress in the oxide.
(4)2t1E2′t2+1E1′-1×Δ=σ1Or combining Eqs.
(2) and (3) can give the in-plane stress in the metal substrate(5)1E2′+t22E1′t1-1×Δ=σ2where E1=253,000MPa (oxide Young’s modulus), E2=96,000MPa (metal Young’s modulus), v1=0.282 (oxide Poisson’s ratio), v2=0.34 (metal Poisson’s ratio) [5], t1=0.001mm (oxide thickness), t2=0.66mm (metal thickness), Δ=0.005 (lateral expansion), resulting in an oxide stress, σ1=−1740MPa.
The elastic properties used in all simulations are bulk values.
Both the oxide layer and metal substrate are known to be highly textured, potentially leading to a range of different elastic properties.
A short sensitivity study using Eq.
(2) indicated that increasing the Elastic modulus and/or the Poisson’s ratio would increase the magnitude of the in-plane stresses in both the metal and the oxide.
Using the same approach as Parise et al.
[17], this would simply require a reduction in the in-plane expansion component in order to match the in-plane compressive stress in the oxide with the values determined experimentally.
After modification of the in-plane expansion component the impact on the stress balance, i.e.
the ratio of stresses in the oxide and the metal, is negligible and therefore these changes would not impact the simulated stress relaxation.
Corrosion of zirconium alloys in water, at 360°C, proceeds via the following reaction [1]:(6)Zr+2H2O→ZrO2+2H2Assuming the pick-up fraction of released hydrogen being absorbed into the metal substrate is around ∼0.2 [41], and for simplicity a constant pick-up fraction throughout the process the total amount of hydrogen picked up at any point along the corrosion curve can be estimated using the following equation:(7)nH=2fLOC0.5LMwhere nH is the hydrogen concentration in moles H/mm3, f is the hydrogen pick-up fraction (0.2), LO is the oxide thickness (in mm), LM is the metal substrate thickness, and C is the concentration of oxygen in zirconium dioxide (9.5×10−5molesO/mm3).
Fig.
1b shows how the calculated hydrogen concentrations (wtppm) compare with the values described in Section 2.3.
The only significant deviation is at 200days oxidation time.
It has been previously observed that different areas of a sample surface go through transition at different points in time [5,34].
Given that only 10–15% of the sample is analysed for hydrogen concentration, the deviation can be explained by natural variation in the oxide thickness across the sample surface.
For an oxide 3.3μm thick on a metal substrate 0.66mm thick this gives a hydrogen concentration of ∼58wppm.
This is in fairly good agreement with the values shown by Videm et al.
[42].
A simple way to relate hydrogen concentration (moles H per mm3) to volumetric expansion is to use the partial molar volume of H in α-Zr (V‾H=1670mm3/mol [43]).
By making the simplified assumptions that the expansion due to hydrogen pick-up is averaged throughout the sample, and that the volumetric expansion is isotropic, the below equation can be used to estimate the shape change in the X, Y or Z directions.
(8)εijk=1+nHV‾H3-1For hydrogen concentrations in the range of 150–400wppm Vizceno et al.
gave an axial strain of 5.21×10−6perwppm [33].
For hydrogen concentrations in the range of 300–2000wppm Blat et al.
gave strains of 1.15×10−6 per wppm [9].
For a theoretical test concentration of 300wppm; using the partial molar volume gives a strain in each direction of 0.00108, the Vizceno data gives 0.00156 strain, and the Blat data gives 0.00035 strain.
Although the variation between the three values is reasonably large, it gives some support towards basing the expansion in the metal substrate on the partial molar volume of hydrogen in zirconium.
This is incorporated into the simulation by applying a strain tensor to the metal substrate.
The good correlation with literature, and the impact of the hydrogen induce lattice strain on oxide stress shown later in the paper, indicates that the use of isotopic hydrogen induced expansion is appropriate for this work.
The value of the strain is based on the hydrogen concentration for that stage of oxidation.
The interpretation therefore is that this will relax the stress in any existing oxide and effectively reduce the in-plane expansion of future oxide growth.
In this analysis creep has been incorporated into both the oxide layer and the metal substrate.
It is possible to determine the most likely mechanisms present by consideration of the stress level, grain size, temperature and material.
Calculations indicate a stress level in the order of ∼6MPa putting the mechanism below the transition to power law creep [16].
The temperatures used during these experiments range from 360 to 710°C, and previous characterisation of the Zircaloy-4 material discussed in Section 2 gave a mean grain size in the order of ∼15μm.
This would appear to discount Harper-Dorn and Nabarro-Herring based mechanisms [16,19].
Although some form of Ashby-Verall or grain boundary sliding mechanism may be plausible, Coble creep is the best established mechanism and has been incorporated into the metal substrate based on Eq.
(9).
(9)ε̇co=αcoδDgbkTΩπg3σwhere T is the temperature, k is the Boltzmann constant, g is the average grain size, Ω is the atomic volume of Zr, σ is the stress, αco is a coefficient, δ is the grain boundary width, and Dgb is the grain boundary diffusion coefficient.
The grain boundary diffusion coefficient is based on the experimentally derived value for the activation for grain boundary diffusion Q, as shown in Eq.
(10), which has an Arrhenius type relationship.
In this equation Do is a pre-exponential coefficient and R is the gas constant.
(10)δDgb=Doexp-Q(Jmol-1)RTDefining the activation energy for grain boundary diffusion (Qgb) is complex, and as shown in Table 3, values presented in literature demonstrate a significant range.
Two sets of values have been used in these simulations.
FEA-1 is defined in order to fit the experimentally observed ex-situ S-XRD data for Zircaloy-4 oxidised in primary water at 360°C.
It includes a modified Coble creep coefficient of αco=6, a grain boundary diffusion activation energy presented by Fiala et al.
of Qgb≈124kJ/mol, and a pre-exponential coefficient of Do=8.2×10−5mm3s−1 [19].
FEA- 2 is defined in order to give oxidation-induced strain rates in the range of 1.6×10−8–7x10−8h−1 as defined previously in literature for zirconium alloys oxidised at ∼360°C [9,10,13].
This includes the Coble creep coefficient of αco=14 as described by Franklin et al.
[15], a grain boundary diffusion activation energy defined by Ravi and presented by Charit et al.
of Qgb≈144kJ/mol [44], and a pre-exponential coefficient of Do=1.0×10−4mm3s−1.
As discussed earlier, manufactured Zirconia is known to be superplastic under appropriate experimental conditions.
Although temperatures in the range of 360–710°C would be very low for creep to occur in the oxide, typical grain sizes are very small on the order of ∼30nm, and stresses are very high in the range of 1–2GPa.
Although research into creep in manufactured zirconia is significant, there is variation between studies in key factors such as the phase fraction, stabilising elements, grain size, temperature, stress, and calculated activation energy.
These tend to range from 0–8 wt% yttria or calcium, 75nm-15μm grain size, 950–1600°C test temperature, with the creep test stresses reaching 200MPa.
Typically, the grain size power ranges between 2 and 3, and the stress exponents range from 1 to 2, leading most authors to the conclusion that either Coble or Grain Boundary Sliding creep mechanisms are in operation [23–28].
Identification of transitions in the creep mechanisms are very rare and difficult to validate [24].
A fundamental issue with all of the creep tests, particularly with very small grain sizes, is the tendency for dynamic grain growth to occur.
This has been known to generate additional strain in the materials and although attempts are frequently made to correct for this, it adds significant uncertainty to the analysis [23–28].
Another issue is the presence of the doping elements which are known to segregate to grain boundaries and have been linked with reduction in diffusion due to a solute dragging mechanism [26].
For the purposes of this work a Coble creep mechanism has again been incorporated making use of Eqs.
(9) and (10).
The only available data for zirconia is in the form of manufactured ceramics.
The value selected for the activation energy of grain boundary diffusion are defined for 3% yttria stabilised tetragonal zirconia with Qgb=506kJ/mol and a pre-exponential coefficient of Do=2.9×104mm3s−1.
Creep was modelled in Abaqus using the time hardening equation shown below(11)ε̇cr=Aabqσntmwhere ε̇cr is the creep strain rate, t is the time, and Aabq is a constant.
As discussed, Coble creep has been identified as a likely creep mechanism to be dominant during the experimental work discussed in Section 2.
As Coble creep is a diffusion based mechanism there should be no hardening, hence n=1 (stress exponent) and m=−0.0001 (hardening factor), effectively making tm≈1.
Combining Eqs.
(9) and (11) effectively gives the following term that can be implemented into the finite element analysis.
(12)Aabq=αcoδDgbkTΩπg3 Fig.
7 shows the tetragonal phase fraction and stress state in the oxide of Zircaloy-4 after oxidation in primary water at 360°C taken from [5].
The original work presents the in-plane biaxial compressive stress experienced by the tetragonal and monoclinic phases separately.
Putting this data into Eq.
(13) allows definition of an average oxide stress.
(13)σavg=(ftσt)+(fmσm)where σavg is the average oxide stress, σt is the stress in the tetragonal phase, σm is the stress in the monoclinic phase, ft is the tetragonal phase fraction and fm is the monoclinic phase fraction.
From Fig.
7 it can be seen that there is a gradual reduction in both the tetragonal phase fraction and the average residual stress present in the oxide.
Fig.
8 displays typical diffraction patterns for a sample of Zircaloy-4 oxidised in-situ up-to 710°C, at an incident angle of 3° and ψ angles between 26.5° and 63.4°.
The patterns show two monoclinic peaks, one detectable tetragonal peak, and two relatively strong zirconium peaks.
Comparisons of the patterns show considerable variation in signal intensity depending upon the ψ angle, which is attributed to the strong crystallographic texture in the oxide and metal substrate.
Fig.
9 shows the integrated intensities for the monoclinic (1¯11)m peak and tetragonal (101)t peaks for the Zircaloy-4 and ZIRLO™ samples oxidised under thermal profiles A and B.
The elevated integrated intensities observed for oxides formed on ZIRLO™ compared with Zircaloy-4 at 710°C supports the idea that ZIRLO™ should exhibit a thicker oxide layer at this temperature.
The initial oxide formed during the oxidation experiment did not provide enough diffraction signal and it was only after about 15min that fittable peak were observed, although the initial peak fits resulted in relatively high uncertainties.
It should be noted that the calculation of an average oxide stress, as required for the simulations, is dependent on obtaining monoclinic and tetragonal stresses and the correlating phase fraction.
As such each average oxide data point requires the inclusion of the (1¯11)m, (111)m and (101)t peaks, with a high intensity and a good fit to maximise accuracy, with the result that only data after ∼42min have been included.
Figs.
10–13 show the tetragonal phase fraction and the monoclinic, tetragonal and average oxide stresses for Zircaloy-4 and ZIRLO™ at temperatures up-to 710°C (Test A) and 650°C (Test B).
Typically, the range of stresses and phase fractions are similar across samples and test conditions.
The general trend is towards a gradual reduction in the tetragonal phase fraction, and stress levels demonstrated by both oxide phases.
For both materials increasing the temperature, and therefore increasing the oxidation kinetics, leads to a greater reduction in stresses present in both phases and averages.
It has been observed that for the ZIRLO™ sample oxidised up to 710°C (Test A), the stress values become positive after ∼73min.
Interestingly the diffraction patterns showed a very weak signal for the beta phase in the metal substrate at this temperature (Fig.
14).
This allotropic phase transformation of Zr would result in an expansion in the metal substrate and reduced creep resistance [45], which could drive the oxide stress to become positive.
Considering an entirely elastic model of the discussed Zircaloy-4 oxidation in primary water at 360°C, from an oxide thickness of 0.16μm-3.3μm, the average in-plane stress in the oxide is reduced from 1.76GPa to 1.73GPa, whilst the average in-plane stress in the metal increases from 0.56MPa to 11.8MPa.
The volume expansion created by the oxidation of the metal generates a force in both the metal and oxide which is translated into a stress based on the elastic properties and the relative thicknesses of the oxide and the metal.
Making the oxide thicker or reducing the thickness of the metal substrate will change the stress balance and increase the tensile stress present in the metal.
With the addition of visco-plastic properties the maximum stress in the metal substrate will be limited by creep deformation.
By taking an average of the in-plane stress through the thickness of the model oxide at different time intervals it is possible to compare with the S-XRD data as shown in Figs.
7 and 10–13.
Fig.
7 shows that the creep inputs defined for FEA-1 (Section 3.4.1) give a good fit to the stress relaxation defined using S-XRD, with the average oxide stress dropping from −1.76GPa to −1.01GPa over 220days and 3.3μm of oxide growth.
However, the resultant strain rate due to creep was ∼2.3×10−10s−1.
Blat et al.
oxidised recrystallised Zircaloy-4 sheet at 360°C in primary water and recorded strain rates in the order of ∼2×10−11s−1 [9].
Barberis et al.
oxidised recrystallised M5 sheet in 360°C water and recorded strain rates in the order of ∼5.6×10−12s−1 [10].
Donaldson et al.
oxidised tubes of stress relieved cold worked Zircaloy-4 in 350°C air and recorded axial strain rates of ∼4.4×10−12s−1 [13].
These strain rates are an order of magnitude lower than the strain rates shown using the metal creep inputs defined for FEA-1.
The fit provided using the inputs for FEA-2 is relatively poor, with the average oxide stress dropping from −1.76 GPa to −1.6GPa over 220days and 3.3μm of oxide growth.
However, the resulting creep strain rate is down to ∼2.44×10−10s−1, this is typical what should be expected based on the results defined in literature [9,10,13].
Figs.
10–13 also show the predicted oxide stress for Zircaloy-4 and ZIRLO™ oxidised in air at 710°C (Test A) and 650°C (Test B).
Air oxidation at high temperatures means that the corrosion kinetics are comparatively fast and the creep strain rates should be accelerated.
In spite of the fact the hydrogen-induced lattice strain should not be present during an air oxidation experiment the experimentally observed stress relaxation in the oxide is greater compared to tests carried out at lower temperatures.
Although there is some offset in terms of stress, the simulation results given for the model equations defined by FEA-1 give reasonable agreement with the observed trends in stress relaxation.
This is the case for both materials and temperature profiles.
However, these are based on creep data that gave excessive levels of strain at lower temperatures.
As expected, the results based on the model inputs for FEA-2 give much lower levels of stress relaxation.
Unfortunately, comparison of oxidation-induced strain rates at these temperatures is not possible, but extrapolation from the lower temperatures indicates that this combination of mechanisms cannot explain the observed stress relaxation.
Significant deviation is seen to occur for the ZIRLO™ material oxidised at 710°C after ∼65min.
The explanation given for this is the formation of β-Zr in the metal substrate detected during the synchrotron experiment (Fig.
14) [46].
This has resulted in the average oxide stress becoming tensile, an effect which cannot be due to oxidation-induced strain as it is the oxide stress that drives the creep strain in the metal substrate.
The stress gradient is steeper for the ZIRLO™ than the Zircaloy-4 meaning more creep has occurred.
This is because the thinner metal substrate and thicker oxide layer associated with the ZIRLO™ material creates a greater tensile stress in the metal substrate.
It is important to note that the same creep equations and coefficients have been applied to both alloys.
The key difference between the materials is the presence of niobium.
The maximum solubility of niobium (∼0.6wt%) is lower than the concentration of niobium present in the ZIRLO™ (0.92wt%) [44].
The remaining niobium forms second phase particles, and as such it is difficult to see how the addition of niobium could impact the diffusion based Coble creep mechanism.
The average in-plane stress discussed in the previous section is taken from a series of stress distributions at specific time intervals, defined by the thickness of the partitions shown in Fig.
6 and the specific oxidation kinetics.
If the stress relaxation was a result of creep in the metal then it would be expected that there should be a stress gradient through the oxide layer.
Fig.
15 shows the distributions in the pre-transition region for the simulation of Zircaloy-4 oxidised in primary water at 360°C based on the FEA-1 inputs fitted to the observed stress relaxation.
In the work by Polatidis et al.
efforts were made to assess the through oxide stress distributions based on changes in the diffraction angle [5].
In this case the stress distribution is defined by increasing the depth over which the in-plane oxide stress is averaged.
This same averaging approach has been applied to the through oxide stress distribution of Zircaloy-4 oxidised in primary water at 360°C, based on the FEA-1 inputs at 220days oxidation, and is compared with SXRD depth distribution data shown in Fig.
16.
It can be seen that the fit is poor.
This can either be explained as an issue relating to the formation of a network of lateral cracks after ∼2μm oxidation, or simply that the observed stress relaxation is not a product of creep in the metal substrate.
For air oxidation the two primary relaxation methods included in the models are creep of the oxide itself and creep of the underlying metal substrate.
In the case of autoclave oxidation in primary water, estimates of the hydrogen-induced lattice strain have also been included.
Fig.
17 shows total strain, metal creep strain and hydrogen-induced lattice strain for the finite element analysis of Zircaloy-4 oxidised in primary water at 360°C.
This is based on the inputs for FEA-1 for which the metal creep properties were fitted to explain the observed stress relaxation, and FEA-2 for which the properties are defined to so that the oxidation-induced strain is close to that observed in literature.
In all of the models analysed creep in the oxide layer was negligible.
Assessing the data in Fig.
17, it can be seen that for FEA-1 the metal creep is very much dominant.
However, these strain rates are an order of magnitude greater than anything observed in literature for analysis of oxidation-induced strain [9,10,13].
The results for FEA-2 show that the creep strain rates in the metal substrate are roughly double the hydrogen-induced lattice strain.
This is consistent with the final figure shown by Barberis et al.
in [10] comparing creep, hydrogen strain and free growth for Zircaloy-4 grid during irradiation at 325°C in a PWR.
This further demonstrates the more appropriate applicability of FEA-2.
As discussed, oxides formed on zirconium alloys contain a percentage meta-stable tetragonal phase which is known to be stabilised by the application of an in-plane compressive stress [47].
Hence, relaxation of this stress should reduce the stabilising effect on the tetragonal phase and may partly explain the pre-transition reduction in the tetragonal phase fraction [47,48].
The expansion caused by the phase transformation is known to cause fracture in manufactured stabilised tetragonal zirconia [11], which would allow fast ingress routes into the oxide layer for oxygen containing species.
Comparing with experimental results in literature, finite element analysis has shown that bulk relaxation mechanisms including creep of the oxide layer, creep of the metal substrate, and hydrogen-induced lattice strain are not great enough to explain the observed stress relaxation.
As such another mechanism must be responsible for most of the stress relaxation in the oxide observed using S-XRD.
Although there is also the possibility of oxygen pick-up during oxidation, this will already be included in the oxidation-induced strains published in literature, which are much lower than that required.
More significant may be the development of roughness or undulations at the metal–oxide interface [38,49].
FEA and TEM have both shown that the formation of interface roughness is accompanied by significant localised plastic deformation in the metal substrate [17,50].
Increasing the area of the interface by plastic deformation could potentially reduce the stress in the oxide layer by reducing the constraint.
The interfacial roughness has also been associated with strong out-of-plane tensile stress [17], and the formation of small pre-transition lateral cracks and networks of lateral cracks associated with the transition in the corrosion kinetics [5].
EELS analysis has shown that after oxidation the near surface metal contains sub-oxides and regions of oxygen saturated Zr [39].
This oxygen saturation may well impact localised deformation by hardening the substrate and causing localised expansion.
However, given the substrate is still constrained by the metal substrate this is expected to have a negligible impact on the macro-scale oxide stress relaxation.
Although these features have been observed frequently using both SEM and TEM their impact on the stress state and mechanical behaviour of the oxide has not yet been fully explored and understood [2,3,5,8,51].
S-XRD has been used in-situ to analyse the oxides formed on ZIrcaloy-4 and ZIRLO™ during air oxidation at temperatures up to 650°C and 710°C.
This includes quantifying the stresses in the monoclinic and meta-stable tetragonal phases, and tetragonal phase fraction in the oxide layer.
Results indicate a gradual reduction in the residual stress and tetragonal phase fraction.
These trends are shown to be similar to samples oxidised in autoclave, with a simulated primary water environment at 360°C, analysed ex-situ using S-XRD.
Results include definition of appropriate thermal profiles and correlating oxidation kinetics.
Based on ex-situ S-XRD data from autoclave oxidised Zircaloy-4, and the current S-XRD analysis of in-situ oxidised Zircaloy-4 and ZIRLO™, finite element analysis has been used to capture some of the mechanical components of the oxidation process.
This allows testing of mechanisms that could potentially explain the observed stress relaxation in the oxide layer including; hydrogen-induced lattice strain, oxide creep, and metal creep.
In all of the analyses oxide creep was found to be negligible.
When using a set of material inputs that might explain the observed stress relaxation, creep strain rates in the metal substrate are seen to be an order of magnitude greater than any oxidation-induced strain measured in literature.
Using creep data and hydrogen-induced lattice strain comparable with literature gave a very low level of stress relaxation.
The conclusion from this is that bulk relaxation mechanisms including oxide creep, metal creep and hydrogen-induced lattice strain, are not sufficient to explain the observed stress relaxation.
This emphasises the need to understand other mechanical degradation mechanisms, such as the development of roughness at the metal–oxide interface, and lateral cracks in the oxide layer.
The authors would like to thank the EPSRC for funding of the Nuclear EngD studentship (Platt), the PhD studentship (Polatidis) and an EPSRC Leadership Fellowship (Preuss).
The authors would like acknowledge their collaborators from AMEC, with thanks to Westinghouse for supplying material.
Finally grateful acknowledgement is given for the provision of beam time at BESSY II.
Electrical-thermal analysis of III–V triple-junction solar cells under variable spectra and ambient temperatures The influence of the incident spectral irradiance on the electrical and thermal behaviour of triple-junction solar cells has been investigated.
A spectral dependent electrical model has been developed to calculate the electric characteristics and quantify the heat power of a multijunction solar cell.
A three-dimensional finite element analysis is also used to predict the solar cell’s operating temperature and cooling requirements for a range of ambient temperatures.
The combination of these models improves the prediction accuracy of the electrical and thermal behaviour of triple-junction solar cells.
The convective heat transfer coefficient between the back-plate and ambient air was found to be the significant parameter in achieving high electrical efficiency.
These data are important for the electrical and thermal optimisation of concentrating photovoltaic systems under real conditions.
The objective of this work is to quantify the temperature and cooling requirements of multijunction solar cells under variable solar spectra and ambient temperatures.
It is shown that single cell configurations with a solar cell area of 1cm2 can be cooled passively for concentration ratios of up to 500× with a heat sink thermal resistance below 1.63K/W, however for high ambient temperatures (greater than 40°C), a thermal resistance less than 1.4K/W is needed to keep the solar cell operating within safe operating conditions.
area (m2) air mass (–) speed of light in vacuum (m/s) heat capacity (J/(kgK)) concentration ratio (–) direct normal irradiance (W/m2) external quantum efficiency (–) energy band-gap (eV) spectral DNI (W/m2/nm) Planck’s constant (Js) conv.
heat transfer coeff.
(W/(m2K)) current (A) short-circuit current (A) dark saturation current (A) current density (A/m2) short-circuit current density (A/m2) dark saturation current density (A/m2) thermal conductivity (W/(mK)) Boltzmann constant (eV/K) Rs intensity coefficient (–) diode ideality factor (–) incident power (W) maximum power output (W) elementary charge (C) heat power (W) heat flux rate (W/m2) heat generation (W/m3) series resistance (Ω) shunt resistance (Ω) Rs at low intensity (Ω) Rs at high flux (mΩ) thermal resistance (K/W) number of cycle iterations (–) ambient temperature (°C) solar cell’s temperature (°C) voltage (V) voltage at maximum power (V) open-circuit voltage (V) ratio of top to middle Jsc (–) zenith angle (°) material dependent constant (eV/K) material dependent constant (K) constant (–) temperature difference (°C) emissivity (–) electrical efficiency (–) optical efficiency (–) constant (A/(cm2K4)) wavelength (nm) density (kg/m3) Stefan-Boltzmann constant (W/(m2K4)) Aluminium Oxide or Alumina Concentrator Cell Assembly Concentrating Photovoltaic Concentrator Standard Test Conditions Direct Bonded Copper Electrical Model Finite Element Analysis Finite Element Thermal Model Gallium Indium Arsenide Gallium Indium Phosphide Germanium Generalised Minimal RESidual method High Concentrating Photovoltaic Infrared Multijunction Maximum Power Point Root Mean Square Error Simple Model of the Atmospheric Radiative Transfer of Sunshine, version 2 Ultraviolet Triple-junction Multijunction (MJ) solar cells are made of III–V compound semiconductors and are used in space and terrestrial applications.
Currently the state-of-art solar cell on the market is the lattice matched triple-junction (3J) solar cell made of GaInP/GaInAs/Ge subcells (Guter et al., 2009; Helmers et al., 2013).
These subcells, are monolithically connected in series in a specific way to absorb a larger proportion of the solar spectrum and thus, to achieve higher conversion efficiencies.
To date, the highest recorded efficiency for a 3J solar cell is 44.4% and 46% for 4J (NREL, 2015).
Such solar cells can be economically viable if sunlight is concentrated by a factor greater than 300× (Cotal and Frost, 2010; Kinsey et al., 2008; Verlinden et al., 2006).
High concentrations result in high heat flux on the solar cell’s surface and a rapid increase in the cell’s temperature.
High temperatures reduce the electrical conversion efficiency because of the temperature dependence of the open-circuit voltage (Voc) and the maximum power voltage (Vmp) (Cotal and Sherif, 2006).
It has been shown that under 500× concentration and without any cooling arrangements, the solar cell can exceed 1000°C (Araki et al., 2002; Cotal and Frost, 2010; Kuo et al., 2009; Min et al., 2009; Nishioka et al., 2006; Ye et al., 2009).
This emphasises the need for appropriate cooling technology to decrease the temperature to within safe operation limits and to avoid suboptimal performance and risk of system failure.
The recommended operating temperature varies for different manufacturers; Spectrolab Inc. suggests a maximum operating cell temperature of 100°C (Spectrolab, 2009b), Azurspace GmbH 110°C for their latest product 3C42A (Azurspace, 2014) and 150°C for the old product 3C40A (Azurspace, 2010) while Sharp data are given for up to 120°C (Segev et al., 2012).
Reliability analysis on 3J solar cells have shown that, at operating conditions of 820× and 80°C, the warranty time was found to be 113years; at 100°C however, the warranty time was reduced to 7years (Espinet-González et al., 2014).
It is also worth noting that, in high temperatures (over 120°C, 1×), the voltage output of the low energy band-gap germanium subcell decreases to almost zero (Helmers et al., 2013; Nishioka et al., 2005).
Therefore, to avoid long term degradation problems and also the risk of destroying the connections (melting), the concentrator cell assembly (CCA) should not operate in excess of 100°C.
MJ solar cells are usually characterised in laboratory facilities under Concentrator Standard Test Conditions (CSTC).
These conditions correspond to cell temperature Tc=25°C, air mass 1.5 direct (AM1.5D) and Direct Normal Irradiance DNI=1kW/m2, although in the field, the atmospheric conditions can vary significantly (Kinsey, 2010).
Due to the fact that the subcells of the 3J solar cell are monolithically connected and also because of their sensitivity to the spectral variations and intensity of sunlight, the prediction of the electrical and thermal behaviour is still challenging (Steiner et al., 2012).
There also exists a limitation relating to the in-series connection of such solar cells; a mismatch in the current produced by each subcell will limit the overall output to the lower value; this, in turn will result in greater heat production within the cell.
Therefore, by applying a simple DNI value as an input in thermal models may give inaccurate results.
It is important therefore, to develop smart algorithms, models or methods to realistically determine the electrical performance of the cell to accurately determine the thermal characteristics, temperature and cooling requirements of the system.
Concentrating photovoltaic (CPV) thermal numerical models and experimental designs have been thoroughly discussed in literature using passive (Araki et al., 2002; Chou et al., 2012; Kuo et al., 2009; Natarajan et al., 2011; Wang et al., 2013) and active (Al-Amri and Mallick, 2013; Kribus et al., 2006; Royne and Dey, 2007; Zhu et al., 2011) cooling techniques.
According to Royne et al.
(2005), who presented an extensive review on different cooling techniques, passive cooling can be sufficient for single cell geometries and solar flux up to 1000 suns where a “large area” is available below the cell for a heat sink.
For densely packed cells and concentration ratios (CR) higher than 150 suns, active cooling is necessary (Royne et al., 2005).
It was also concluded that the thermal resistance (Rth) of the cooling system must be less than 10−4m2K/W for concentration levels above 150×.
However, the spectral effects on electrical efficiency and hence, the temperature are not included in the aforementioned thermal models.
In addition, the prediction of solar cell’s temperature is very important for the electrical characterisation of CPV modules.
Rodrigo et al.
(2014) reviewed various methods for the calculation of the cell temperature in High Concentrator PV (HCPV) modules.
The methods were categorised based on: (1) heat sink temperature, (2) electrical parameters and (3) atmospheric parameters.
The first two categories are based on direct measurements of CPV modules in indoor or outdoor experimental setups and presented the highest degree of accuracy (Root Mean Square Error (RMSE) 1.7–2.5K).
Most of the methods reviewed by Rodrigo et al.
(2014) calculate the cell temperature at open-circuit conditions.
Methods that predict the cell temperature at maximum power point (MPP) operation offer a more realistic approach since they include the electrical energy generation of the solar cells (i.e.
real operating conditions); Yandt et al.
(2012) described a method predicting the cell temperature at MPP based on electrical parameters and Fernández et al.
(2014b) based on heat sink temperature with absolute RMSE 0.55–1.44K.
Fernández et al.
(2014a) also proposed an artificial neural network model to estimate the cell temperature based on atmospheric parameters and an open-circuit voltage model based on electrical parameters (Fernandez et al., 2013a) offering good accuracy (RMSE 3.2K and 2.5K respectively (Rodrigo et al., 2014)).
The main disadvantage of the aforementioned methods is that an experimental setup is required to obtain the parameters used for the cell temperature calculation.
Despite the fact that several electrical models and experimental procedures for MJ solar cells have been described thoroughly in literature (Ben Or and Appelbaum, 2013; Dominguez et al., 2010; Fernández et al., 2013; Fernandez et al., 2013b; Kinsey and Edmondson, 2009; Kinsey et al., 2008; Rodrigo et al., 2013; Segev et al., 2012; Siefer and Bett, 2014) which included the spectrum and irradiance dependence, the challenge to develop an integrated thermal-electrical model which predicts the cell temperature and includes the cooling needs is still unsolved.
This study builds on a methodology (Theristis and O’Donovan, 2014) which considered a constant spectral response at 25°C and AM1.5D.
The current methodology combines three models; the solar spectral irradiance is generated by the NREL Simple Model of the Atmospheric Radiative Transfer of Sunshine, version 2 (SMARTS2) (Gueymard, 1995, 2001; Gueymard et al., 2002), an electrical model (EM) uses a single diode model to simulate the electrical characteristics and heat power of a 3J solar cell at MPP (i.e.
connected to an inverter) and a finite element analysis thermal model (FETM) that uses the heat power as an input from the electrical model in order to predict the temperature and the cooling requirements as a function of ambient temperature.
The single diode model was used to model the electrical characteristics of a 3J solar cell.
According to Segev et al.
(2012), the one-diode equivalent circuit model is adequate to describe a 3J solar cell in practical applications.
Each junction of the solar cell can be represented by an equivalent circuit model and therefore, by connecting them in series, the one diode equivalent circuit model for a 3J solar cell can be obtained (Fig.
1).
This model differs from the two diodes in the number of diodes that describe the saturation current.
In the single diode model, the diode represents recombination in both the depletion and quasi-neutral regions (Segev et al., 2012).
If the shunt resistances (Rsh,i) are sufficiently large to be neglected, the current density–voltage (J–V) relationship is given by(1)Ji(V)=J0,i(Tc)·eq(Vi-Ji·A·Rs)ni·kB·Tc-1-JSC,i(Tc),where i is an index for each subcell (1 for top, 2 for middle and 3 for bottom), J0 the dark saturation current density, q the elementary charge, V the voltage, J is the current density, A the area, Rs the series resistance, n the diode ideality factor, kB the Boltzmann constant and JSC is the short-circuit current density.
The dark saturation current density is strongly affected by temperature and is described as(2)J0,i(Tc)=ki·Tc(3+γi/2)e(-Eg,i(Tc)/ni·kB·Tc),where k and γ are constants.
The energy band-gap Eg, decreases with increasing temperature and is given by the Varshni relation (Varshni, 1967):(3)Eg,i(Tc)=Eg,i(0)-αiTc2Tc+βi,where Eg,i(0) is the energy band-gap of i subcell at 0K and α, β are material dependent constants.
The short-circuit current density distribution for each subcell as a function of temperature is calculated using Eq.
(4):(4)Jsc,i(Tc)=∫λi,minλi,maxq·λ·EQEi(λ,Tc)·ηopt(λ)·CR·G(λ)h·c·dλ,where λi,min and λi,max correspond to the wavelength range of each subcell, λ is the wavelength of the incident photons, EQEi is the External Quantum Efficiency, ηopt the optical efficiency, CR is the concentration ratio, G(λ) is the spectral DNI, h is Planck’s constant and c the speed of light in a vacuum.
EQE is defined as the ratio of the number of carriers collected by the cell to the number of incident photons.
The total current density output due to the series connection is given by the minimum current density of the three subcells;(5)J=min(J1,J2,J3).
Solving Eq.
(1) for Ji=0, the open-circuit voltage for each subcell is obtained:(6)Voc,i=ni·kB·TcqlnJsc,i(Tc)J0,i(Tc)+1.
The voltage in each junction can be also calculated by rearranging Eq.
(1):(7)Vi=ni·kB·TcqlnJSC,i(Tc)-JiJ0,i(Tc)+1-Ji·A·Rs,i.
The total voltage output is the sum of the voltage in each junction, therefore:(8)V=∑i=13Vi,V=kB·Tcq∑i=13nilnJSC,i(Tc)-JJ0,i(Tc)+1-J·A·Rs.
The solar cell’s efficiency is defined as the proportion of the maximum power output of the cell to the DNI which is incident on the cell:(9)ηcell=PoutPin=Pm∫2804000CR·A·λ·G(λ)·ηopt(λ)·dλ.
Therefore, the heat power produced on the cell is(10)qheat=Pin·(1-ηcell).
An analytical FETM has been developed to predict the thermal behaviour of 3J solar cells.
The multijunction solar cell is attached to a Direct Bonded Copper (DBC) substrate for heat dissipation and electrical insulation.
The heat is transferred by conduction between the solid layers of the receiver.
Some heat is lost to the environment, due to natural convection and surface to ambient radiation from all free surfaces.
In the case of a passively or actively cooled receiver, the heat is transferred by conduction between the solid layers of the receiver and the steady state equation is given by the Fourier’s law of heat conduction:(11)qcond″=-k∇T,where q″ is the heat flux rate (W/m2), k the heat conductivity and ∇ is the three-dimensional operand.
The solar flux that is transformed to heat must be dissipated from the bottom substrate or cooling system to the environment or harnessed for use in another application.
The heat which is dissipated either by natural or forced convection is described by(12)qn/f,conv″=hn/f·ΔT,where h is the heat transfer coefficient (natural or forced) and ΔT the temperature difference between the cell and the ambient air or ultimate heat sink.
The heat, which is lost to the environment, due to natural convection occurs on every surface that faces the ambient.
COMSOL Multiphysics contains the correlations for each surface orientation (vertical, horizontal or inclined); these can be found in Incropera and DeWitt (1996).
The heat loss due to radiation is given by:(13)qrad″=ε·σ·(T4-Tamb4),where ε is the material’s emissivity and σ the Stefan–Boltzmann constant.
The heat transfer at solid interfaces is defined by the following heat equation to simulate the thermal behaviour:(14)qheat‴=ρ·Cp∂T∂t-∇·(k∇T),where the first term disappears in steady state problems and qheat‴ is the heat source (W/m3) which is calculated from the electrical model divided by the cell volume.
The models described above are simulated based on the flowchart in Fig.
2.
The solar spectrum is generated using the SMARTS2.
Clear sky days are assumed and the zenith angle (z), and hence the air mass (AM) is considered to be the only variant that affects the direct spectral irradiance.
The simulations are conducted in steady state.
The EM runs for a given CR, an initial Tc of 25°C, AM1D⩽AM⩽AM15D and the heat power is then introduced in the 3D FETM in COMSOL.
Solar spectra ranging from AM1D to AM15D have been chosen as a rigorous test for this integrated model.
They are not location specific; they are used to demonstrate the applicability of the model to a wide range of solar geometries.
For 25°C⩽Tamb⩽45°C and 1200W/(m2K)⩽hconv⩽1600W/(m2K) at the back surface of the CCA, the cell’s temperature is predicted from the thermal model and is then imported to the electrical model.
The procedure is repeated until a steady state is reached; i.e.
|Tc(s)−Tc(s+1)|⩽0.002K, where s is the number of cycle iterations.
Table 1 summarises the variable input parameters used for the simulation program, the range of each parameter, the model from which they are generated and the model that uses them as an input.
The maximum convective heat transfer coefficient considered in this study is 1600W/(m2K), as this has been shown by Mudawar (2001) to be the maximum achievable under passive cooling conditions.
Lower convective heat transfer coefficients are not reported as they were found to be insufficient to maintain the cell temperature below 100°C.
This section describes an application of the aforementioned methodology using the C1MJ CCA from Spectrolab.
Literature based data from Kinsey and Edmondson (2009) and Segev et al.
(2012) are used in the EM.
The CR discussed in this section is for 500× unless otherwise stated.
The generated direct spectral irradiance from SMARTS2 is shown in Fig.
3.
For the sake of clarity some air mass values are not illustrated.
The integration of the spectral irradiance at a specific air mass gives the irradiance intensity; the values are shown in Fig.
5 as a function of z.
Other parameters were kept constant at the reference conditions of the standard ASTM G173-03 (Gueymard and Myers, 2010) (precipitable water 1.42cm, rural aerosol model, turbidity value 0.084 specified as aerosol optical depth at 500nm).
Fig.
4 shows the percentage of ultraviolet (UV, 280–400nm), visible (400–780nm) and infrared (IR, >780nm) light as a function of air mass.
It can be seen that for AM⩾AM3D the IR wavelengths have the highest proportion while the UV component is zero for AM>AM7D.
The EQE of the Spectrolab C1MJ multijunction solar cell, as characterised by Kinsey and Edmondson (2009) for a temperature range between 25°C and 75°C, was used for this application.
The bottom subcell was measured using a C1MJ subcell isotype.
The input parameters used for the electrical model are listed in Table 2.
The cell area was taken as A=1cm2 and the optical efficiency ηopt=0.8.
The series resistance as a function of the incident power was calculated according to Spectrolab (2009a):(15)Rs=RS0CRKc+Rs∞,where RS0=11mΩ is the series resistance at low intensity, Rs∞=40Ω is the series resistance at high flux and Kc=1.75 is a series resistance intensity coefficient.
Table 3 shows the fitting parameters for the C1MJ single diode model which were adopted directly from Segev et al.
(2012).
From Eq.
(4), the Jsc distribution for each subcell can be calculated.
As mentioned above (see Section 2.1.
), higher Tc decreases each subcell’s band-gap causing the EQE to shift towards the longer wavelengths and therefore the Jsc follows the same behaviour (Jsc,3 is plotted separate for clarity, see Figs.
6 and 7).
Figs.
8 and 9 show the effect of AM; higher air mass values show a significant drop in the short wavelength region (see also Figs.
3 and 4) and therefore the effect on higher band-gap subcells is higher than the low band-gap (germanium) subcell.
This will be the case especially for Jsc,1 which decreases rapidly for AM>AM2D acting as the current limiting subcell.
Also considering that in the winter period, the AM will always be greater or equal to 2 at middle to high latitudes, the subcells will never be current matched (Faine et al., 1991).
This has an impact on the electrical performance of the cell since the excess current will be transformed directly to heat.
Moreover, by comparing Figs.
3 and 4 with Figs.
6–9, it is apparent that the germanium subcell will never limit the current output.
The Voc dependence on temperature under variable AM is plotted in Fig.
10.
Increasing temperatures result to an increase in the J0 which, in turn, decreases the Voc (see also Eqs.
2, 3 and 6).
The relative temperature coefficient range is between −0.16%/K for AM1D to −0.18%/K for AM15D.
This shows that there is only a weak dependency of AM change on the Voc temperature coefficient.
By increasing the AM, the Voc decreases by 0.48%/AM at 25°C, 0.56%/AM at 45°C, 0.61%/AM at 65°C and 0.63%/AM at 75°C.
This reduction is due to the Jsc decrease.
Since the bottom subcell will never limit the current (as explained in Section 4.2.1.)
the ratio of the top to the middle subcell’s short-circuit current density (X=Jsc,1/Jsc,2) is used for comparison.
Fig.
11 shows that the maximum efficiency is achieved when the top and middle subcells are current matched under any temperature.
Also the middle subcell is the current limiting cell only for air mass values lower than AM1.5D while for all other air mass values the current limiting subcell is the top subcell.
X is shown only for 25°C for clarity purposes because it is very close to the short-circuit current ratio at higher temperatures (X at 75°C is 0.58% higher for AM1D and 3% for AM15D).
The analysis of the triple-junction solar cell’s electrical output is important to quantify the heat power which is produced and needs to be dissipated by the cooling mechanism.
In order to calculate the heat power over a range of air mass values and temperatures, Eq.
(10) is used.
The maximum heat power is found to be 25.5W at AM1D and 75°C (Fig.
12).
Inset graph in Fig.
12 shows the air mass values of interest for the thermal model; thermal issues are not significant for AM>AM2D, since any cooling mechanism which is designed to dissipate the heat at AM⩽AM2D, will be adequate for any range of higher air mass values.
The maximum heat power produced on the cell due to current mismatch is quantified using Eq.
(16) (Rabady, 2014) and is shown graphically in Fig.
13.
The minimum heat power due to current mismatch is found when the top and middle subcells generate the same current (i.e.
under AM1.5D), however the increasing operating temperature shows a reduction of 13% which is due to the reduction of the Eg which in turn reduces the Voc.
For AM>AM2D the heat increases sharply because a subcell limits the current until AM>AM7D where the heat power is reduced mainly due to the decrease in the spectral irradiance intensity.
(16)qheat,CM⩽∑13|Isc,i-Itotal|·VOC,i For validation purposes, the electrical model was simulated for CR=555×, ηopt=1 and was compared with measured data from Kinsey and Edmondson (2009).
The C1MJ short-circuit current density values were adopted directly from Kinsey and Edmondson (2009) for the four measured temperatures.
The RMS error for the efficiency was calculated according to:(17)RMSE=∑i=1mηcell,meas(Tc)-ηcell,calc(Tc)2m,where ηcell,meas is the measured electrical efficiency at Tc from Kinsey and Edmondson (2009) and ηcell,calc is the calculated electrical efficiency from the model.
These are shown graphically in Fig.
14 for RMS error 0.25%.
The calculated heat power from the electrical model was used as an input to the thermal model.
The geometry and thermal boundary conditions of the C1MJ model are shown in Fig.
15 and Table 4.
The 3J solar cell is modelled as one entity (Germanium cell) because the top and middle subcells are much thinner than the bottom and therefore they would not affect the thermal model.
This statement is confirmed by Chou et al.
(2012).
The solar cell is attached on a DBC substrate which is made of copper/Al2O3 ceramic/copper.
The electrical connections are made of silver.
The cell is connected to a 12A Schottky diode which, for simplicity is not modelled.
The CCA (solar cell, DBC, connections) is modelled for this application in order to be more generally applicable and not specific to one particular module where all the bespoke design of packaging materials would need to be introduced.
The thermophysical properties and dimensions are listed in Tables 5 and 6 respectively.
The simulation ran using the Generalised Minimal RESidual method (GMRES) which is an iterative solver.
The CCA configuration was meshed using the physics controlled mesh sequence as part of COMSOL.
A mesh independency analysis was conducted by progressively increasing the number of elements until the temperature change was minimised; this was found to be at approximately 400,000 elements.
Due to significantly lower computational time and relatively small error of 0.03% in maximum temperature, a fine mesh setting with 237,288 elements over a 435mm3 mesh volume was used.
The 3J C1MJ solar cell is modelled as a heat source.
All the free areas at the top release heat to the environment through external natural convection and surface radiation.
The back-plate’s surface (copper) releases heat to the environment through surface to ambient radiation and also convection where the convective heat transfer coefficient is varied and discussed in greater detail in this section.
The air temperature is also varied.
For reliability purposes, all the cases up to a cell temperature of 100°C are examined, as the cell can degrade if operated at higher temperatures for a prolonged time (Espinet-González et al., 2014).
As described in Section 3 and the flowchart in Fig.
2, the integrated model runs iteratively for an initial temperature of Tc(s)=25°C; the electrical model calculates the heat power at 25°C and the thermal model runs for steady state.
The calculated Tc(s+1) from the thermal model is then imported back to the electrical model to calculate the heat power at Tc(s+1).
The iterations are continued until a difference lower or equal to 0.002K is achieved.
Fig.
16 shows the integrated volumetric solar cell’s temperature after 6 iterations for convective heat transfer coefficients ranging from 1200W/(m2K) to 1600W/(m2K) and a constant Tamb=35°C.
The solution is shown to converge in all cases after the 3rd iteration.
Fig.
17 shows the temperature distribution across the C1MJ solar cell for AM1D, hconv=1600W/(m2K) (i.e.
Rth=1/(hconvA)=1.22K/W, area of 5.13×10−4m2) and Tamb=45°C.
A maximum temperature of 90.33°C is observed in the centre of the cell while the temperature of the top layer of the DBC board, which is not illuminated, is from 70°C at the edges to 80°C near the cell.
The integrated volumetric temperature of the solar cell is 86.82°C.
In Fig.
18, the solar cell’s temperature is estimated for 1200W/(m2K)⩽hconv⩽1600W/(m2K) and 25°C⩽Tamb ⩽45°C.
Each point in the graph represents 5 simulations/iterations as shown in Fig.
16.
Ambient air temperature has a strong influence on the cell’s temperature, with approximately degree directly proportion increase in temperature with air temperature.
At AM1D, where the integrated direct spectral intensity is 988.8W/m2 and an ambient temperature of 45°C, the C1MJ CCA can be cooled sufficiently by a convective heat transfer coefficient, hconv >1200W/(m2K) if a maximum operation temperature at 100°C is assumed.
However, if the maximum temperature is set at 90°C, then hconv should be higher than 1400W/(m2K).
For the same spectral conditions and ambient temperature of 35°C, a heat transfer coefficient, hconv >1200W/(m2K) can adequately cool the solar cell’s temperature well below 90°C.
At AM1.5D conditions and ambient air temperature of 35°C, the maximum solar cell temperature is 81.93°C for a surface convective heat transfer coefficient of 1200W/(m2K) and as low as 72.12°C for hconv =1600W/(m2K).
However, under extreme conditions (Tamb =45°C), the maximum temperature is 92.59°C for hconv =1200W/(m2K) and 82.64°C for hconv =1600W/(m2K).
At higher values of air mass, a higher thermal resistance is adequate and therefore, only up to AM2D are presented.
It is also shown that Rth ⩽1.4K/W (hconv >1400W/(m2K)) can be sufficient to maintain the cell below a safe operating limit without risking any long term degradation of the system.
For locations with ambient temperatures lower than 40°C, a higher heat sink thermal resistance may be acceptable.
An integrated solar spectrum dependent electrical-thermal model is described for 3J solar cells under concentration followed by an application for the C1MJ CCA.
While other traditional models predict the cooling requirements and thermal behaviour using constant parameters (AM1.5D or Global, Tamb =25°C, constant electrical efficiency) or empirical data from regression analysis, these models are not applicable to other assemblies since the geometry varies for different manufacturers.
Also, since the solar spectrum is transient during the day, the AM1.5D does not offer representative results of the realistic operation of the solar cell in the field.
Instead, designing the cooling or heat sinking requirements at AM <AM1.5D is much preferable because the 3J solar cell is not current matched and also because the heat is higher, due to higher solar radiation intensity.
This model examines the thermal behaviour of 3J solar cells under variable air mass, ambient temperature thus electrical characteristics and therefore, it can accurately quantify the thermal power which needs to be dissipated, including the excess thermal output due to current mismatch.
It is found that CPV single cell configurations of 1 cm2 area, can be adequately cooled passively with a heat sink thermal resistance below 1.63K/W while for locations with extreme ambient conditions, a thermal resistance less than 1.4K/W is needed to keep the CCA operate under 90°C.
Solar cells with lower area can withstand higher concentrations for the same thermal resistance values or higher thermal resistance at CR of 500×.
This study investigates the thermal behaviour of a solar cell assembly; however the concentrator optics are not modelled in terms of their spectral transmittance or reflectivity as a function of temperature.
Increasing temperatures on refractive optics will result in a change in the refractive index of the lens due to thermal expansion; this will lead to an increase in the focal length and therefore change the overall system power generation (Hornung et al., 2012).
Also, the non-uniformity of the irradiance on the surface of the solar cell has not been considered in this work; Jaus et al.
(2008) considered the inhomogeneous intensity of the sun by dividing the solar cell area into different regions.
Jaus et al.
(2008) did not consider the spectral dependent irradiance, which is considered here.
If the spectral optical efficiency as a function of temperature and the inhomogeneity of spectral irradiance are incorporated in the model, the accuracy of the model is likely to be increased further.
The support of the Engineering and Physical Sciences Research Council (EPSRC), UK and the Department of Science and Technology (DST), India is acknowledged for funding the BioCPV project (EP/J000345/1).
The authors would like to thank Dr. Christian A. Gueymard for his comments on the SMARTS2 simulations.
A classic case of Jahn–Teller effect theory revisited: Ab initio simulation of hyperfine coupling and pseudorotational tunneling in the 12E′ state of Na3 The predictive capabilities of current ab initio approaches are tested in a benchmark study on the well known case of the Na3 ground state.
This molecule is small enough to be treated with computationally demanding methods, but also shows an interesting interplay between Jahn–Teller-, spin-orbit-, rovibrational- and hyperfine-interactions.
The necessary parameters for the effective Hamiltonian are derived from the potential energy surface of the 12E′ ground state and from spin density evaluations at selected geometries, without any fitting adjustments to experimental data.
We compare our results to highly resolved microwave spectra, with the aim to improve previous assignment attempts, where some parameters had to be estimated from fits to measured spectra.
The instability of molecular configurations in electronically degenerate states with respect to vibrational modes of appropriate symmetry is known as the Jahn–Teller (JT) effect.
As a prominent example of a local breakdown of the Born–Oppenheimer approximation, it is a well-studied phenomenon in molecular as well as solid state physics.
The most common case, denoted as E×e, describes the lifting of the degeneracy of a doubly degenerate electronic state (E) in first order by a doubly degenerate vibrational mode (e) [1,2].
Among the simplest and probably most thoroughly studied representatives of this type are the homonuclear alkali metal trimers.
As one of them, the Na3 molecule stands out due to the availability of highly resolved microwave spectra [3], which allowed for the analysis of the rotational structure of its ground state.
Each rotational state is split into 80 sub levels due to fine structure from spin-rotation interactions, hyperfine coupling of the three nuclei I=32, and pseudorotational tunneling.
When analyzed properly, information can be obtained about the barrier height between the three equivalent minima of the ground state potential surface that correspond to the distorted obtuse triangular shape of the molecule.
Hyperfine splittings provide insights into the s-type character of the unpaired electron and its probability density at the respective nuclei.
The saturation behavior of rotational transitions with increased microwave intensity provides an estimate for the magnitude of the electronic dipole moment.
The experimental resolution was limited by an instrumental linewidth of 20kHz, somewhat typical for microwave spectroscopy in a molecular beam.
The spectra, measured in the lab of the senior author of this current article, could be interpreted as being in qualitative agreement with an electronic shell model for small metal clusters.
The theoretical part of Ref.
[3] was devoted to the derivation of an effective Hamiltonian for the description of pseudorotational tunneling, spin-rotation and magnetic hyperfine effects.
Despite the thorough development of a suitable formalism, the underlying parameters for the simulation of spectra had to be optimized in a trial-and-error way, aiming for the best match between calculation and measured spectra.
Given the extremely rich pattern of substructure over a range of 30MHz for each rotational transition, it became obvious that all named line splitting effects had to be rather small.
A straightforward extraction of parameters as well as tunneling splittings was hampered by the lack of line by line assignments.
Only upper and lower limits could be derived for the Fermi contact interaction, the electric dipole moment and the pseudorotational barrier.
Ab initio calculations of adequate precision, which were not available back then, should provide a better understanding of the rovibrational structure, giving detailed information about the overall charge distribution and the local spin density at the nuclei.
Ideally, a connection should be found to the previous treatment in terms of Jahn–Teller effect theory and the approach of an effective Hamiltonian.
In this article, which can be considered as follow-up of Ref.
[3], we revisit this system from the theoretical side, aiming for a more stringent derivation of parameters.
We further investigate the performance of several quantum chemistry methods in the case of a conical intersection between potential energy surfaces of two electronic states.
Electronic degeneracy is a particularly difficult case, which has been treated predominantly by multi-configuration self-consisted field methods.
This classic approach is also the method of our choice, but will be compared to single-reference Coupled Cluster (CC) theory and a fully relativistic Fock-Space Coupled Cluster (FSCC) approach.
We discuss the importance of relativistic effects, the effects of spin–orbit coupling, and the impact of simplifications such as frozen core orbitals or the usage of pseudopotentials on the JT parameters for the 12E′ ground state.
Their dependence on features of the PES and the spectroscopic consequences of deviations between the different methods will be critically analyzed.The parameters are then used to set up an effective Hamiltonian for the prediction of microwave spectra analogous to Ref.
[3].
The complex interplay between a large amplitude vibrational motion, coupled to the rotational angular momentum of the molecule, and the interaction between the unpaired electron spin with the three Na nuclei will be treated within the formalism of the internal axis method (IAM) [4,5].
This article is structured as follows.
We give a short review of previous experimental efforts and highlight the problem of unresolved discrepancies between theoretical predictions and measured microwave spectra.
This is followed by an outline of our computational approach and the analytical description of the 12E′ potential energy surface.
We derive a set of parameters for the construction of an effective Hamiltonian, and diagonalize it to obtain information on line positions and intensities.
The predicted spectra are then compared to measurements reported in Ref.
[3].
The sodium trimer has a long history of theoretical and experimental studies.
A pioneering theoretical paper of Martin and Davidson published in 1978 showed that the obtuse isosceles geometry is lower in energy than the linear conformation [6].
Several extended PES scans of Na3 and other alkali trimers followed this initial study, employing DFT [7], complete active space SCF [8], or a configuration interaction approach based on valence bond wave functions [9].
Recently, the applicability of density functional theory (DFT) to JT-distorted systems has also been tested for Na3 [10], and the B-X transition has been revisited as well, applying state-averaged multi-reference configuration interaction with a large active space in order to derive more accurate non-adiabatic coupling terms for an improved interpretation of photoabsorption spectra [11–13].
Experimentally, first rotational constants of the Na3 ground state became available from optical–optical double resonance spectroscopy of the A(2A2)-X(2B2) optical transition (C2v symmetry notation) [14,15].
The electronically excited B(2A1′) state was rotationally analyzed in an OODR scheme involving both the A-X and the B-X transitions [16,17].
As Na3 served as model X3 molecule concerning the so-called molecular Aharanov–Bohm effect [18], criteria for the manifestation of a Berry phase in the nuclear wave function were searched.
Such criteria are typically based on high resolution spectroscopy with unambiguous assignments.
The first such unambiguous analysis addressed the Na3 B state and is based on the assignment of vibronic levels in terms of integer or half-odd integer pseudorotational quantum numbers [19].
Such an assignment was possible by measuring the Coriolis splitting of individual rotational levels in the B state [16] and led to the conclusion that it represents a case of pseudo-Jahn–Teller coupling with a geometric phase of zero.
A second criterion for geometric phase effects was demonstrated by Kendrick [20], who showed how the energetic ordering of vibronic states of A1, A2 and E symmetry contained in the Na3 ground state PES changes whether a geometric phase of π is introduced to the vibrational wave function or not.
Busch et al.
[21] applied this criterion to their spectroscopic analysis of the A-X optical transition and confirmed the existence of Berry’s phase in this case.
In the course of the A-X and B-X spectroscopy, different Hamiltonian operator approaches were used.
The B (22E′) excited state shows an almost free pseudorotation, while the X (12E′) ground state and the A (2E′) excited state of the sodium trimer (all in D3h notation) were treated as rigid asymmetric rotors of C2v symmetry [17].
A rotation-pseudorotation-formalism for X3 molecules was developed [22] and applied to the B state [23], indicating that a coupling of electron spin and pseudorotation angular momentum was more important than traditional spin-rotation terms for an overall agreement with experimental data.
Busch et al.
combined their analysis of rovibrational data for the X and A state with ab initio calculations [21].
In 2002, highly resolved microwave absorption measurements of the rovibrational spectra in the ground state introduced the possibility to study hyperfine interactions [3].
In an extension of the Na3 B-X spectroscopy, OODR spectra were measured far into the potential energy regime above the point of degeneracy of the three-surface potential (2E′ and 2A1′ in D3h symmetry).
A preliminary analysis concluded with the result that the 2E′ and 2A1′ states interact via a linear and quadratic pseudo-JT coupling, combined with a ‘proper’ term accounting for anharmonicity [24].
The ground state of Na3 was optimized in C2v symmetry using the def2-QZVP basis set of Weigend and Ahlrichs [25] in combination with several different computational methods.
We apply a single-reference spin restricted open-shell variant of the Coupled Cluster method with single and double excitations plus perturbative triples (denoted as CC) [26,27], a complete active space SCF calculation (CASSCF) followed by multi-reference configuration interaction approach with singles and doubles excitations (MRCI), and a fully relativistic calculation employing Fock Space Coupled Cluster theory (FSCC) [28–31] with a Dyall basis set [32].
The first two methods are used as implemented in the Molpro suite of programs [33], the latter is computed with the Dirac program package [34].
Spin density evaluations at the Coupled Cluster level of theory are done with the CFOUR program package [35].
The MRCI and CC calculations were performed in an all electron approach as well as in combination with an effective core potential (MRCI+ECP, CC+ECP) [36] replacing the neon core of each sodium atom.
We further tested the addition of a core polarization potential (MRCI+CPP, CC+CPP) to account for correlation effects between valence and the replaced core electrons.
In the MRCI calculations, the three valence electrons were included in the active space, which comprised 24 orbitals.
For the all-electron MRCI calculation, the lower-lying occupied orbitals were kept doubly occupied, but their coefficients were fully optimized at the CASSCF step.
The computationally less demanding approach of completely frozen core orbitals was investigated for the CC method, denoted as CC+LC.
The importance of relativistic effects was tested also at the CC level of theory by including the Douglas–Kroll correction of 8th order (CC+DK).
An accurate description of the Jahn–Teller-distorted potential energy surface of the 2E′ ground state is a prerequisite for the construction of our model Hamiltonian.
We start with an optimization of the geometry and introduce the common analytical model potential.
The results of our geometry optimizations are summarized in Table 1.
It contains relative energies and geometries of the A1, B2 and E′ state.
A very striking result is that the omission of inner-shell electron correlation leads to significant deviations in bond lengths.
For methods which consider only valence electrons (MRCI, MRCI+ECP, CC+LC) the obtained bond lengths are about 0.15Å larger than for calculations including inner-shell electron correlation (MRCI+CPP, CC, CC+DK).
Interestingly, the two strategies of including more electron correlation seem to have opposite effects on the bond angle, which is particularly evident for the obtuse geometry of the 12B2 state.
While the angle decreases for the 1 2A1 state and increases for the 12B2 in the MRCI+CPP calculation, the CC calculations show the opposite behavior.
Benchmark calculations including the Douglas–Kroll correction (CC+DK) indicate a negligible influence (less than 0.01 Åand 0.1 °) on the geometry.
Rotational constants for the global minimum geometry (2B2) and the first order saddle point (2E′) are given in Table 2.
Negative asymmetry parameters κ are obtained, indicating the geometric proximity to a prolate symmetric top in both isosceles geometries.
The potential energy surface (PES) is written as a function of the two symmetry-adapted coordinates Qx and Qy, which correspond to the degenerate pair of symmetric and asymmetric vibration, respectively [1,2].
The third vibrational degree of freedom, the breathing mode of the molecule, can be treated separately for this class of molecules.
Its corresponding symmetry-adapted internal coordinate is denoted as Qs.
These coordinates are related to the 6 in-plane, space-fixed Cartesian coordinates by the matrix equation [37,9,38](1)Q→=13-10123212-320132-12-32-12-1012-321232x→,with Q→=(Qx,Qy,Qs) and x→=(x1,y2,x2,y2,x3,y3)T, where xi,yi denotes the Cartesian displacement coordinates of the ith nucleus.
The Jahn–Teller theorem states that the doubly degenerate E′ (D3h) ground state of the sodium trimer splits into two states of lower symmetry (C2v), denoted as A1 and B2.
Since only Qx and Qy are involved we switch to polar coordinates defined as(2)r2=Qx2+Qy2;tan(ϕ)=Qy/Qxand write the two-dimensional PES as(3)∊±(r,ϕ)=V2ar2+V3acos(3ϕ)r3±rV1e2+(2V1eV2ecos(3ϕ))r+(2V1eV3e+V2e2)r2+(2V2eV3ecos(3ϕ))r3+V3e2r412,with V2a as elastic force constant, V3a as cubic force constant and Vie,{i=1,2,3}, as linear, quadratic, and cubic coupling parameters.
Expanding this equation to the third order in r we obtain(4)∊±(r,ϕ)=V2ar2+V3acos(3ϕ)r3±rV1e+V2ecos(3ϕ)r2,if cubic vibronic coupling is neglected and the assumption of V2e≪V1e is made.
For zero anharmonicity (V3a=0) and linear JT coupling only, the PES shows radial symmetry, resembling the well-known Mexican hat shape.
In this case, minimum energy geometries are those that fulfill r=|V1e|/|2V2a|.
The so-called pseudo-rotational path gets distorted by an additional warping if quadratic JT coupling is present, leading to three global minima separated by three saddle points.
The analytical description derived above allows for a compact representation of the potential energy surface via a series expansion at Qx=Qy=0.
These coefficients can be easily obtained by fitting the analytical expression of Eq.
(4) to a set of ab initio data points.
This strategy is very useful for the discussion of vibrational features of rovibronic transitions, e.g.
in the experimentally studied A-X, B-X, and B′-X band systems of Na3, where big parts of the PES need to be taken into consideration.
However, an economic, global description of the PES with a limited set of parameters comes at the cost of reduced local accuracy.
This can lead to problems in cases where the nuclear wave function is highly localized at points far off the center of the expansion, which is the case in the lowest vibrational mode of the 2E′ electronic ground state of Na3.
Hence, for the discussion of rovibrational features, we focus in our fits on the lower branch of the PES near the global minimum and employ a local description introduced by Hougen [4,5].
Two approaches are considered here for the extraction of global parameters from a series of single-point energy calculations on the 12E′ ground state.
In the first method (see Table 3) we cut through both branches ∊+ and ∊- at ϕ=0, which corresponds to a scan over the Qx coordinate with Qy=0.
Assuming the functional dependence given in Eq.
(3), we apply a nonlinear least-square fit to 26 Qx data points in the range from −1.2 to 1.0Å.
The results are summarized in Table 3.
To improve the estimate of V2e we fall back on an alternative method (see Table 4) as suggested in Ref.
[39], which is based on cuts through both PES branches at constant r but varying angle, with ϕ running from 0 to 2π/3 in steps of 0.23rad, taking advantage of the threefold symmetry of the PES.
Eq.
(4) can be reordered to(5)∊±(r,ϕ)=c±+A±cos(3ϕ),where we have adsorbed terms independent of ϕ into coefficients c±, and all oscillating terms into A±cos(3ϕ), with the amplitudes A± given as(6)A±=V3ar3∓V2er2.The oscillating behavior of both branches is determined by the relative magnitude of V2e and V3a.
We extract the unknown parameters by fitting a cos(ϕ) function to the ab initio data points.
Note, however, that this method can only be combined with the MRCI and FSCC approach since both electronic states are of the same symmetry as soon as Qy≠0, and the upper surface has to be treated as an electronically excited state.
Results are given in Table 4.
With this method, the confidence intervals for V2e and V3a can be narrowed down to 2% and 0.5%, respectively.
Following the approach of Ref.
[40], an effective spin–orbit (SO) coupling term Δ=αζ/2 is introduced to Eq.
(3), with α as the spin–orbit coupling constant and ζ as the projection of the electronic orbital angular momentum onto the C3 axis.
With the same assumptions of negligible V3e and V2e≪V1e one obtains a corrected formula which reads [41](7)∊±(r,ϕ)=V2ar2+V3acos(3ϕ)r3±rΔ2/r2+V1e2+2V1eV2ecos(3ϕ)r+V2e2r212.We apply the state-interacting method as implemented in the MOLPRO program, and obtain the spin–orbit eigenstates from the diagonalization of an effective SO-Hamiltonian in the basis of the two electronic states.
The SO-coupling removes the electronic degeneracy and turns the conical intersection into an avoided crossing.
A value of Δ=1.24cm−1 can be extracted from SO-corrected single point energy evaluations at equilateral geometry (r=0), where the correction is maximal [42].
In the fully relativistic FSCC calculation, the degeneracy at Qx=Qy=0 is automatically removed, and a splitting of Δ=1.76cm−1 is obtained.
The inclusion of either SO-coupling value (or its complete neglect) in the fitting procedure yields practically the same JT parameters for the PES.
By defining a new length in multiples of (ℏ/(mω))1/2, with ω as the vibrational frequency of the e mode and m as its reduced mass, and a new energy variable in multiples of ℏω, we can rewrite Eq.
(7) as(8)∊±(r,ϕ)=12r2+v3acos(3ϕ)r3±rΔ′2r2+v1e2+2v1ev2ecos(3ϕ)r+v2e2r212.In this dimensionless unit system the force constant v2a equals 1/2.
The JT-coupling terms v1e and v2e are sometimes denoted as k and g/2 [2].
Our results are summarized in Table 5 and compared to values found in the literature.
Note that the parameters given in Tables 3 and 4 create a surface with minima at ϕ=π/3,π and 5π/3, corresponding to the common choice of positive Qx coordinates for acute geometries.
However, for the discussion of rovibrational spectra within the effective Hamiltonian approach of Ref.
[3] it is more convenient to use an inverted definition of Qx, which leads to a PES with minima positions at ϕ=0,2π/3, and 4π/3.
Using the same formulas for the PES as given above, a rotated surface can be obtained by flipping the signs of V2e and V3a.
We stick to this convention for the remainder of the article.
In this and the next two subsections we link our computational chemistry approach to a model Hamiltonian for Na3 as presented in Ref.
[3].
The large values for anharmonicity v3a and quadratic JT coupling v2e cause high barriers for the pseudorotation, which in turn leads to a high localization of the nuclear wave function in one of the three basins of the PES.
To each of three different geometries or ‘frameworks’ there corresponds a vibrational function [3](9)ϕi(r,φ)=12gi(r,φ)-gi(r,φ+2π),where gi(r,φ) denotes an auxiliary function defined as(10)gi(r,φ)=exp(-k1(r-r0)2-k2[1-cos(φ/2-(i-1)π/3)]),a Gaussian-like peak centered around the minimum of each basin, located at (r0,(i-1)2π/3), for i=1,2,3.
Linear combinations as written in Eq.
(9) ensure that each wave function ϕi is producing the correct Berry phase factor π for a closed loop around the conical intersection of the two electronic states at (0,0) [43].
Within the extended permutation-inversion group G122, which is a suitable description of the molecular symmetry for the rotational analysis,[22] we obtain the following relations for a repeated application of the C3 rotation operation, denoted here as (123):(11)ϕ2(r,φ)=(123)ϕ1(r,φ),(12)ϕ3(r,φ)=(123)2ϕ1(r,φ),(13)-ϕ1(r,φ)=(123)3ϕ1(r,φ).
With the help of these functions the vibrational part of the Hamiltonian (describing the nuclear motion on the lower branch of the PES) can be put into the following matrix representation [3],(14)Hvib=H0+h2-h2+h2H0+h2-h2+h2H0,with the shorthand notations H0=〈ϕi|H|ϕi〉=E0 and h2=〈ϕi|H|ϕj〉.
E0 corresponds to the ground vibrational energy, while h2 represents the overlap integral between two neighboring vibrational functions ϕi and ϕj.
A diagonalization of H yields one non-degenerate and two degenerate eigenvectors,(15)ϕA1d′=(ϕ1-ϕ2+ϕ3)/3,EA1d′=E0-2h2ϕEda′=(2ϕ1+ϕ2-ϕ3)/6,EEda′=E0+h2ϕEdb′=(ϕ2+ϕ3)/2,EEdb′=E0+h2which can be distinguished by symmetry labels of the G122 group.
While E0 can be easily derived from the ab initio calculations of the previous sections, the evaluation of h2 is not straightforward.
Due to the complexity of the analytical form of the PES given by Eqs.
(4) or (7) as a function of the parameters v2a,v3a,v1e and v2e, an analytical link between h2 and the JT surface parameters is no longer feasible.
Instead, we employ the variational principle and optimize the nuclear wave functions defined by Eq.
(9) and (10) for the lower PES of the 12E′ by minimizing the expectation value for the energy with respect to the test function parameters r0,k1 and k2, (16)f(k1,k2)=〈ϕi(k1,k2)|H^vib|ϕi(k1,k2)〉〈ϕi(k1,k2)|ϕi(k1,k2)〉.The expressions in brackets are calculated by numerical integration of the 2D Schrödinger equation on a cartesian grid with a spacing of 0.01Å.
Our results for k1,k2, and most importantly, for the overlap integral h2, are summarized in Table 6 for the various ab initio data sets.
Note that the value of h2 is negative, which shows that (at least for the lowest rotational level with quantum number N=0, see next section) the E-type sublevel is below the A-type sublevel.
A last parameter θ2, needed in the next section for the evaluation of tunneling matrix elements of the rovibrational Hamiltonian, is related to the molecular geometry via [3](17)θ2=2π3(Qs,02+2r02)(Qs,02+r02),where the subscript 0 indicates values of Qs and r (the radial distance in the Qx–Qy plane) at the position of the three global minima of the PES in Fig.
1.
It is derived from a geometrically simplified assumption for the pseudorotational path between two frameworks [4,3].
The unsaturated electron spin of the Na3 molecule can couple to the rotational angular momentum.
This interaction energy can be written as the expectation value of a spin-rotation Hamiltonian for a given electronic wave function,(18)〈ϕel|Hsr|ϕel〉=-geμ0μBZe8π∫dr∑k=13|ϕel(r)|2|r-rk|3[(r-rk)×vk]·s,where the space representation of the singly occupied valence electron orbital of Na3 is written as ϕel(r).
Z denotes the nuclear charge of sodium, μ0 the vacuum permeability, μB the Bohr magneton, ge the electron g-factor, s the electron spin, rk the position of each atom, and vk its velocity in the valence electron coordinate system.
Eq.
(18) is obtained by translating the classical Hamiltonian of Refs.
[44,45] into its quantum mechanical equivalent and replacing the electron point charge by an integration over the continuous probability density |ϕel(r)|2.
The velocity vk can be expressed as ω×rk, with ω as the angular velocity corresponding to the molecular rotation.
The latter can be written as(19)ω=(ωx,ωy,ωz)=Nx̂Ix,NŷIy,NẑIz,where we have introduced the moments of inertia for rotations around the three molecule-fixed axes, and operators N^μ for the three components of the rotational angular momentum.
The molecule is placed into the xz-plane, with its twofold rotational axis coinciding with x. Eq.
(18) can now be written in its usual form as(20)H^sr=∑μν=x,y,z∊μνN^μS^ν.The corresponding Cartesian spin rotation parameters ∊μν are obtained via integration over the ab initio electron density using the formulas(21)∊μμ=-geμ0μBZe8πIμ∫dr∑k=13|ϕel(r)|2|r-rk|3[(rν-rνk)rξk+(rξ-rξk)rνk]and(22)∊μν=geμ0μBZe8πIμ∫dr∑k=13|ϕel(r)|2|r-rk|3(rμ-rμk)rνk.In these equations, the subscripts μ,ν and ξ are placeholders indicating the three distinct cartesian components x,y and z.
For a planar molecule, only five spin-rotation parameters can be non-zero.
We use the CFOUR program package [35] to calculate spin densities at the Coupled Cluster level of theory and integrate over a grid of 81×81×81 points with a spacing of 0.25 Bohr.
The very small values obtained for the three different molecular geometries are summarized in Table 7.
Fermi contact interactions can be directly derived from the ab initio calculations by evaluating the spin density at the positions of the three nuclei.
The hyperfine splitting constant for atom k is related to the electron spin density via the equation(23)ahfsk=23μ0geμBgnμn|ϕ(rk)|2with ge,gn,μ0,μB,μn and |ϕel(rk)|2 as the electron spin g-factor, the nuclear spin g-factor, the vacuum permeability, the Bohr magneton, the nuclear magneton and the electron spin density at the kth nucleus, respectively.
This contribution to the hyperfine structure is historically known as the Fermi contact term.
For a single sodium atom a value of 885.816MHz can be found in the literature [46].
Due to neglected relativistic effects [47], numerical inaccuracies, and the incompleteness of our basis set we can only retrieve 70% of the atomic value at the CC level of theory (618MHz).
The splitting parameters for each atom in the Na3 molecule, listed in Table 8, have therefore been corrected for this deviation by multiplying a factor of 1.4334.
Interestingly, only the apex atom of the acute geometry shows a significant hyperfine splitting.
The spin dipole part, which arises from the magnetic dipole interaction of nucleus k with the magnetic moment of the valence electron, can be calculated as an expectation value over the spin density via [48](24)Cμνk=μ0geμBgnμn∫dr|ϕel(r)|2|r-rk|53(rμ-rμk)(rν-rνk)-δμν|rk|2,with μ and ν as placeholders for the Cartesian components and the same constants as defined for Eq.
(23).
Our values for each atom, obtained from the CC densities at 2B2 geometry by numerical integration as described above, are summarized in Table 9.
The Coriolis coupling of the angular momentum generated by the large amplitude motion with the rotational angular momentum leads to a rotational dependence of the tunneling splitting.
This is taken into account via the IAM approach of Hougen and Coudert [4,5], assuming a single tunneling path, corresponding to a 120° rotation in the Qx–Qy plane, which connects the three non-superimposable minimum energy configurations or frameworks.
Choosing the G12 permutation-inversion group, a basis set of symmetry-adapted wave functions will be derived.
We then construct an effective Hamiltonian and diagonalize its matrix representation in the given basis set.
For details and derivations of formulas for the matrix elements of rotational-pseudorotational tunneling, electron-spin rotation and hyperfine interaction within the IAM approach we refer to an instructive series of articles on various molecules with large amplitude motions [4,5,49,3].
A recent review of the IAM method can be found in Ref.
[50].
Spin–orbit coupling, with a maximum impact of about 1cm−1 on the PES at equilateral geometry, can be fully neglected for a spectroscopic analysis near the three global minima.
Wang-type functions, which are linear combinations of symmetric top functions, are introduced as a basis of symmetry-adapted rotational wave functions,(25)|NKα〉=2-12|N,K〉+α|N,-K〉with K⩾0,α=+1 or −1, and |N,K〉 as the usual symmetric top functions.
The total basis set is then constructed as a tensor product of the previously introduced vibrational functions ϕn(r,φ) (in space representation, from now on written as |n(r,φ)〉 in Dirac’s notation) and rotational functions,(26)|NKαn〉=|NKα〉×|n(r,φ)〉, However, for a more convenient representation of the total Hamiltonian it makes sense to also define a symmetry-adapted basis set according to Eq.
(15),(27)|NKαΓ〉=|NKαΓrot〉×|Γvib(r,φ)〉,in which the operator H^ becomes a block-diagonal matrix H with elements(28)HN′K′α′nΓ′;N″K″α″mΓ″=〈N′K′α′n|H^|N″K″α″m〉δΓ′Γ″.
We obtain six blocks of different symmetry Γ, corresponding to the four non-degenerate symmetry labels A1d′,A2d′,A1d″,A2d″ (implying that both rotational symmetry species involved are the same, namely either both A1′ , A2′,A1″ or A2″, respectively), the doubly degenerate symmetry species Ed′ (where rotational symmetries can be any of A1′ or A2′) and the species Ed″ (where rotational symmetries can be any of A1″ or A2″).
These relations can be derived from the character table of the extended permutation-inversion group G122 after assigning symmetry labels to the |N,K,α〉 rotational functions according to Table 10 [3].
For the given basis set, the matrix representation of H^, a total effective Hamiltonian defined as(29)H^=H^vib+H^rot+H^sr+H^hfs,will be described in the next section.
It contains energy contributions from the vibrational ground state (H^vib), the rotational motion including pseudorotational tunneling (H^rot), the spin-rotation (H^sr) and the hyperfine (H^hfs) interaction.
Using the result of Eq.
(14), we can absorb the vibrational Hamiltonian for the vibrational ground state into the expression for the rotational energies and write the matrix elements of Hvib+rot as(30)HN′K′α′;N″K″α″=HN′K′α′1;N″K″α″1-2HN′K′α′1;N″K″α″2for the four blocks corresponding to non degenerate symmetry species (A1d′,A2d′,A1d″,A2d″), and(31)HN′K′α′;N″K″α″=HN′K′α′1;N″K″α″1+HN′K′α′1;N″K″α″2or(32)HN′K′α′;N″K″α″=±3HN′K′α′1;N″K″α″2for the two degenerate symmetry species (Ed′ and Ed″), if both rotational wave functions are of the same symmetry, or if their product is of A2′ symmetry, respectively.
The upper (lower) sign in Eq.
(32) refers to a rotational wavefunction |NKα〉 of A1′ or A1″(A2′ or A2″) symmetry.
In these equations, nondiagonal elements in the vibrational subspace, HN′K′α′1;N″K″α″2, are given by(33)HN′K′α′1;N″K″α″2=h2d(θ2)K″,K′N+α′d(θ2)K″,-K′NδN′N″or(34)HN′K′α′1;N″K″α″2=0,if the rotational wave functions are such that α′α″(-1)K′+K″=+1 or −1, respectively.
The right hand side of Eq.
(33) has to be divided by 2 whenever K′ or K″ is zero, resulting in a division by 2 in cases where both are zero.
Note the appearance of h2, the numerically evaluated overlap integral of the previous section, in Eq.
(33).
The symbol d(θ2)N denotes the small Wigner matrix element for a given value of θ2 and the quantum numbers N, K′, and K″.
This expression of nondiagonal elements in the vibrational part is the same for any tunneling event between geometry frameworks 1 to 3.
Its derivation is described in Ref.
[3].
The only remaining Eulerian-type angle θ2 can be related to the ab initio geometry via Eq.
(17) of the previous section.
Diagonal elements in the vibrational subspace, i.e.
rotational interactions within the same potential basin, are described by(35)〈N′K′α′1|H^|N″K″α″1〉=〈N′K′α′|E0+H^asym|N″K″α″〉,with E0 as the energy of the vibrational ground state taken from Table 5, and H^asym as the rotational Hamiltonian of an asymmetric rotor, which can be written as(36)H^asym=ANx2+BNz2+CNy2in the IIl representation for a molecule fixed Cartesian axis system with the rotational constants taken from Table 2.
Ni denotes the ith component of the molecular rotational angular momentum, and A,B,C are the corresponding rotational constants taken from Table 1, using the standard axes nomenclature for a near-prolate asymmetric rotor.
Following Ref.
[51], the matrix elements of Hasym can be evaluated with the help of Eq.
(25) and a matrix representation in the basis of symmetric rotor eigenfunctions |NK〉 given by(37)〈N′K′|Hasym|NK〉=δN′N∑k=02∑q(-1)N-K′Tqk(A,B,C)×NkN-K′qK〈N||Tk(N,N)||N〉,with the reduced matrix elements(38)〈N||Tk(N,N)||N〉=(-1)k(2k+1)12N(N+1)(2N+1)×NN1k1N.The set of three tensors Tk(A,B,C) with k=0,1,2 has only three non-vanishing components in the molecule fixed axis system, which are related to the asymmetric rotor constants given in Table 2 via the equations(39)T00(A,B,C)=-3-12(A+B+C),(40)T02(A,B,C)=6-12(2B-A-C),(41)T22(A,B,C)=T-22(A,B,C)=(C-A)/2.
The inclusion of spin-rotational interactions necessitates the extension of our basis set (defined in Eq.
(26)) by the two quantum numbers S and J, which refer to the total electron spin and the total angular momentum, respectively.
A matrix representation of the spin-rotational Hamiltonian in this new basis set, consisting of functions(42)ψNKαSJn=|NKα〉×|SJ〉×|n(r,φ)〉,can be obtained by combining Eq.
(25) for Wang-type rotational functions with the evaluation of matrix elements in the usual rotational basis set of Hund’s case (b) functions [51],(43)〈N′K′S′J′n′|Hsr|NKSJn|〉=δn′nδJ′J∑k=02(2k+1)12×[S(S+1)(2S+1)]12[(2N+1)(2N′+1)]12(-1)J+S+N′NSJSN′1×12(-1)k[N(N+1)(2N+1)]1211kN′NN+[N′(N′+1)(2N′+1)]1211kNN′N′×∑q=-kk(-1)N′-K′N′kN-K′qKTqk(∊),where Tqk(∊) denotes the irreducible tensor form of the nine molecule-fixed, Cartesian spin rotation parameters ∊ij derived earlier in Section 5.3.
With the molecule placed into the xz-plane we obtain the relationships given in Table 11.
All previous parts of the Hamiltonian are diagonal in the |SJ〉 subspace.
Hence, all previously defined matrix elements only need to be modified by additional Kronecker symbols δS′S″ and δJ′J″.
For the sake of readability we assume the non-hyperfine part of the Hamiltonian,(44)H^′=H^vib+H^rot+H^sr,as diagonalized in the basis given in Eq.
(42).
The obtained eigenvectors can be abbreviated as(45)|NKaKcΓSJF1,2〉,characterized by the good quantum numbers N for the rotational angular momentum, Ka and Kc as its projection quantum numbers in the limit of a prolate or oblate symmetric rotor, Γ as the overall symmetry of the rovibrational wave function, S as the total spin quantum number, J as the quantum number of the total angular momentum, and F1,2 as the usual labels for the level splitting caused by H^sr.
We write the total Hamiltonian now as(46)H^=H^′+H^hfs,with a hyperfine part [52,3](47)H^hfs=∑i=03aciŜ·Îi+∑i=03∑μ,νCμνiSμIiν,where Ŝ denotes the spin vector operator for the unpaired electron, Îi the nuclear spin vector operator of sodium atom i, and aci as the effective coupling constant describing the Fermi contact interaction.
The second term in Eq.
(47) stems from the spin-nuclear spin dipole interaction, described by a matrix Ci of coupling parameters for each nucleus.
Again we expand the basis set, this time by adding nuclear spin functions |IF〉 to Eq.
(45),(48)|NKaKcΓSJF1,2IF〉=|NKaKcΓSJF1,2〉×|IF〉,assuming the following vector coupling scheme [3](49)F=I+J;I=I1+I23;I23=I2+I3.With the total wave function of the molecular system obeying the Pauli principle, being either of A2′ or A2″ symmetry in the G12 group, and the electronic wave function being of A2d′ symmetry in the doublet ground state, we can derive the necessary symmetries of nuclear spin wave functions that give non-vanishing elements in a matrix representation of H^hfs.
To do this, we characterize the nuclear spin functions by their symmetry species γ in S3, the permutation group of three particles, and by the value of the total spin angular momentum I via(50)|γ,I〉=∑I23G(γ,I23,I)|I1,I23,I〉with the expansion coefficients G(γ,I23,I) given in Table 8 of Ref.
[3].
Denoting the symmetry species of S3 (which is isomorphic to C3v) as A1,A2,Ea and Eb, we can derive non-vanishing matrix elements between functions of the form(51)|NKaKcΓSJF1,2γIF〉for the non-degenerate rovibrational symmetry species.
Here, A2d′ and A2d″ (A1d′ and A1d″) species have to be paired with nuclear spin functions of A2 (A1) symmetry.
For degenerate rovibrational symmetry species the total wave function is a linear combination of the form(52)2-12|NKaKcΓSJF1,2EbIF〉-|NKaKcΓSJF1,2EaIF〉,with Γ being either Ed′ or Ed″.
A further advantage of the symmetry-adapted basis set of nuclear spin functions is the possibility to express all hyperfine matrix elements in terms of sodium atom 1 only.
Following Refs.
[3,53], we define nuclear reduction coefficients C(γ′,γ,I′,I) of the form(53)C(γ′,γ,I′,I)=(-1)I+I′[(2I+1)(2I′+1)]12×∑I23(-1)I23G(γ′,I23,I′)I1I′I23II11G(γ,I23,I).Introducing shorthand notations n and n′ for the here ineffective rovibrational quantum numbers, matrix elements between wave functions of Eq.
(51) are then given by(54)〈n′γ′I′F′|Hhfs|nγIF〉=δF′Fδγ′γ(-1)J+I1+FFI′J′1IJ×Cs〈I1||I1||I1〉C(γ′,γ,I′,I),with(55)〈I1||I1||I1〉=[I1I1+12I1+1]12and a reduced rovibrational matrix element Cs defined as(56)Cs=δΓ′Γ(ahfs1+ahfs2+ahfs3)×〈N′Ka′Kc′Γ′S′J′F1,2′||T1(S)||NKaKcΓSJF1,2〉+δΓ′Γ106×〈N′Ka′Kc′Γ′S′J′F1,2′||T1(S,Chfs1+Chfs2+Chfs3)||NKaKcΓSJF1,2〉.Note the symmetrized appearance of the Fermi contact coefficients for the three nuclei.
The two terms of the reduced matrix element are calculated in two steps.
First, we evaluate matrix elements in the |NKSJ〉 basis via(57)〈N′K′S′J′||T1(S)||NKSJ〉=δN′NδK′K-1S′+N+J′+1SS+12S+12J+12J′+112S′J′NJS1and(58)〈N′K′S′J′||T1(S,C)||NKSJ〉=SS+12S+12J+12J′+12N+12N′+112N′N2SS1J′J1∑q-1N′-K′N′2N-K′qKTq2C,where we use the relations(59)T02C=6-122Czz-Cxx-Cyy,(60)T±12C=-iCyz∓Cxz,(61)T±22C=Cxx-Cyy/2±iCxybetween the tensor representation of Ck and its matrix elements in the Cartesian, molecule-fixed basis.
In the second step, we transform into the |NKaKcΓSJF1,2〉 basis set representation using the eigenvectors obtained in the diagonalization of H^′.
The same procedure is applied to matrix elements between wave functions of Eq.
(52), which are given by(62)〈n′γ′I′F′|Hhfs|nγIF〉=δF′F(-1)J+I1+FFI′J′1IJ×〈I1||I1||I1〉×12Cs[C(Ea,Ea,I′,I)+C(Eb,Eb,I′,I)]+Ca[C(Ea,Ea,I′,I)-C(Eb,Eb,I′,I)].They contain an additional reduced rovibrational matrix element(63)Ca=(ahfs1-ahfs2)×〈N′Ka′Kc′Γ′S′J′F1,2′||T1(S)||NKaKcΓSJF1,2〉+106×〈N′Ka′Kc′Γ′S′J′F1,2′||T1(S,Chfs1-Chfs2)||NKaKcΓSJF1,2〉.which accounts for the asymmetric part of the hyperfine interaction.
Finally, the mixed matrix elements between nuclear functions of Eqs.
(51) and (52) need to be evaluated via(64)〈n′γ′I′F′|Hhfs|nγIF〉=±Ca(-1)J+I1+FFI′J′1JI×〈I1||I1||I1〉C(γ′,γ,I′,I),with the positive sign for Γ′=A2d′ or A2d″ and negative sign for A1d′ or A1d″.
We calculate the matrix elements of the effective Hamiltonian defined above for a set of parameters derived from the ab initio calculations described in Section 4, and diagonalize its matrix representation H in the given basis set.
While the eigenvalues correspond to line positions in the experimental spectra, the information on line intensities can be obtained from the eigenvectors.
In general, the probability for a transition from an arbitrary state |a〉 to a state |b〉 is proportional to the squared dipole transition matrix element,(65)I∝|〈a|T1(μ̂)|b〉|2,with T1(μ̂) as the irreducible tensor representation of the molecular dipole moment.
Since we know the representation of any eigenvector |a〉 in the basis given above, it is sufficient to calculate matrix elements〈N′Ka′Kc′Γ′S′J′F1,2′γ′I′F′|T1(μ̂)|NKaKcΓSJF1,2γIF〉of the dipole operator in the eigenbasis of H^.
Combining the Wigner–Eckart theorem with standard formulas for the evaluation of tensor product matrix elements [54,51] we can compute these matrix elements via(66)〈N′Ka′Kc′Γ′S′J′F1,2′γ′I′F′|T1(μ̂)|NKaKcΓSJF1,2γIF〉=(-1)J′+I+F+1[(2F+1)(2F′+1)]12J′F′IFJ1δII′δγγ′〈N′Ka′Kc′Γ′S′J′F1,2′|T1(μ̂)|NKaKcΓSJF1,2〉,where we have decoupled the nuclear spin I from the total angular momentum J.
Using the eigenfunctions of H^′, the remaining matrix element can then be evaluated in the |NKSJ〉 basis, where it takes the form(67)〈N′K′S′J′|T1(μ̂)|NKSJ〉=δSS′(-1)N′+S+J+1[(2J+1)(2J′+1)]12N′J′SJN1×∑q(-1)N′-K′[(2N′+1)(2N+1)]12N′1NK′qKTq1(μ̂).Since the molecular dipole moment operator transforms as r, its components are given by -212(μx+iμy),μz and -212(μx-iμy) for T11,T01 and T-11, respectively.
Assuming a constant dipole moment, and using the value at the minimum geometry, we identify T01 as the only non-vanishing component along the C2 axis of the isosceles triangle.
Denoting the matrix representation of μ̂ as μ and the eigenvector |a〉 as a→ in the |NKaKcΓSJF1,2γIF〉 basis set, the intensity for a transition a→b is described by(68)Iab∝(a→T·μ·b→)2.In Ref.
[3], frequencies and relative intensities of hyperfine components for three rotational transitions 414←303,404←313 and 313←202 were measured by microwave absorption in a double resonance scheme with an instrumental line width of 20kHz.
First, we plot all three of them together in row (a) of Fig.
2 to demonstrate the rotational spacing, and compare the experimental data to theoretical predictions at the MRCI+ECP+CPP and CC level of theory (row b and c, respectively).
These two methods have been selected for the following reason.
At this spectral resolution, the former method is also representative for the CC+CPP and the FSCC methods applied in this work, since these approaches produced very similar geometries i.e.
rotational constants.
The CC method, however, shows significant deviations, since it yielded rotational constants differing by up to 300MHz (see Table 2), which does not even allow for a correct assignment of the center of each rotational transition.
Besides the high density of lines in the substructure of each rotational transition, the experiment revealed another peculiarity, which lies in the fact that the strongest transitions were not saturated even at a microwave intensity of 1W/m2.
This could only be explained by a very small dipole moment in the order of 10−2 Debye, a feature which is confirmed by all methods applied in this work (between 0.05 and 0.09 Debye, see Table 1).
In Fig.
3 we look at the hyperfine structure of each rotational transition in more detail.
Row (a) contains the experimental spectra.
The hyperfine pattern of each rotational transition spreads over a range of about 30MHz.
No individual microwave line could be resolved, but the good signal to noise ratio allowed the localization of 32, 19 and 7 spectral peaks for the 414←303,404←313 and 313←202 transition, respectively.
We compare these patterns now to our theoretical predictions at the MRCI+CPP level of theory, plotted in row (b).
While the width of the central substructure is in good agreement with the width of the experimental pattern in all three cases, the clear assignment of lines to spectral features is problematic.
The ab initio approaches produced small parameters for pseudorotational tunneling, spin rotation and hyperfine interaction, with none of them clearly dominating over the other.
This makes, by itself, the possibility of a line by line assignment highly questionable, since no quantum number, affiliated with a certain term in the effective Hamiltionan, remains a good quantum number in this case.
However, there is some correlation between the maximum peak positions in theory and experiment.
With regards to parameter dependence, we can distinguish between two groups of parameters.
The parameters A,B,C and θ2 are essentially defined by the molecular geometry, and do have a big impact on the outcome of spectral predictions.
Among them, the parameter θ2, being related to the shape of the path for pseudorotational tunneling, varies the least, which is little surprising, since it corresponds to an intrinsic feature of the PES in the given Jahn–Teller case.
The parameter h2 on the other hand, describing the overlap of the nuclear wave functions in adjoint minima, varies significantly between the CC method and all other methods.
In this case, we consider the very large value suggested by the Coupled Cluster approach as a clear outlier caused by the systematic error of applying a single-reference approach to a situation where static correlation is essential.
We note that this flaw is only weakly reflected in the geometries obtained at the minima on the PES (where the performance of a single reference method might improve), but more in the overall shape of the PES, and, subsequently, in the fitted JT parameters and the value of h2 in Hougen’s ansatz.
The second group of parameters comprises those which are derived from the electronic structure of the molecule at selected points on the PES.
It includes the hyperfine splitting constants ahfsk and Ck for each nucleus and the spin rotation parameters ∊μν, which depend on the spacial distribution of the spin density.
In the given case of a doublet state, this distribution is essentially coinciding with the probability density of the unpaired electron in the valence orbital of the molecule.
This orbital shows atomic p-type character and is delocalized over the whole molecule, a well known feature of the alkali trimers [55], which even led to a successful interpretation of excited states in an electronic shell model [9,56,57].
With this in mind, the spin density in the electronic 12B2 state can be thought as resembling a p-orbital with its axis parallel to the longer side of the isosceles triangle, but with its axis parallel to the C2 axis in the case of the electronic 12A1 state.
With the former corresponding to the global minimum geometry, and the latter to the electronic configuration in the moment of pseudorotational tunneling, it is obvious that both density distributions play a role in the experimentally observed hyperfine patterns.
Interestingly, our spin density analysis revealed a larger splitting term only for the apex atom in the acute geometry of the 12A1 state.
While the overall shape of the singly occupied orbital is of p-type, it is dominated by the contribution of the s electron at the tip.
However, evaluations of spin densities at the nucleus are only of moderate accuracy due to missing or incomplete consideration of relativistic effects and shortcomings of the basis sets used.
Furthermore, the possibility of generating spin density distributions and writing them to disk is not yet a well supported feature of quantum chemistry packages used, especially for highly-correlated methods.
We suggest that future work should focus on an improved ab initio description of the spin density of the molecule along the tunneling path before attempting to extend the effective Hamiltonian by yet another term.
In this article we demonstrated the applicability of standard methods of quantum chemistry to the molecular benchmark problem of the Na3 ground state, which features a coupling of pseudorotational tunneling and hyperfine interactions on the lower branch of a typical JT distorted E×e PES.
All ab initio methods confirm the experimental suggestion of a very small electric dipole moment (≈0.05 Debye) of the molecule in its 12B2 electronic ground state.
Deviations between the geometries for the extrema on the PES, obtained with the various methods, are small, but lead to significant differences in the rotational constants.
The largest deviation is obtained for the all-electron single-reference Coupled Cluster approach, which produces shorter binding lengths and a set of rotational constants which does not fit the rotational spacing of the experiment.
All other methods are in agreement with the rotational assignments given by the experiment.
We further made the attempt to simulate the hyperfine structure of each rotational transition without any fitting to experimentally measured spectra.
The necessary parameters were either derived from scans over the PES in the two JT-active vibrational modes or from single-point evaluations of the spin density at selected geometries (the obtuse global minimum geometry and the acute transition state).
Applying the internal axis method, we demonstrated how the ab initio results for energy and spin density distribution translate into parameters for the effective Hamiltonian.
The interplay of hyperfine interactions with the concerted large amplitude motion or pseudorotation of the molecule, which gives rise to extra tunneling splittings of comparable energy spacing, and with the coupling of the unpaired spin to the rotation of the molecule, leads to a complicated energy level structure without a clear dominance of a particular contribution.
The width of the central part in each predicted hyperfine structure is in agreement with the width of the experimentally observed pattern of the corresponding rotational transition, which indicates correct magnitudes for each contribution in the effective Hamiltonian.
However, a line by line assignment was not possible.
A more accurate description of the spin density along the pseudorotational pathway may lead to a better understanding of energy level structures, especially in cases where the rovibrational energy lies at or even above the barrier.
There is no conflict of interest.
This research has been supported by the Austrian Science Fund (FWF) under Grant FWF-E-530 P19759 & FWF-E-P22962 and the ERDF Program of the European Union and the Region of Styria.
The authors gratefully acknowledge support from NAWI Graz.
Degradation science: Mesoscopic evolution and temporal analytics of photovoltaic energy materials Based on recent advances in nanoscience, data science and the availability of massive real-world datastreams, the mesoscopic evolution of mesoscopic energy materials can now be more fully studied.
The temporal evolution is vastly complex in time and length scales and is fundamentally challenging to scientific understanding of degradation mechanisms and pathways responsible for energy materials evolution over lifetime.
We propose a paradigm shift towards mesoscopic evolution modeling, based on physical and statistical models, that would integrate laboratory studies and real-world massive datastreams into a stress/mechanism/response framework with predictive capabilities.
These epidemiological studies encompass the variability in properties that affect performance of material ensembles.
Mesoscopic evolution modeling is shown to encompass the heterogeneity of these materials and systems, and enables the discrimination of the fast dynamics of their functional use and the slow and/or rare events of their degradation.
We delineate paths forward for degradation science.
Energy materials are essential in our modern world and are expected to have useful lifetimes that extend from 25 to greater than 50years.
The need for long lifetimes and large investments are barriers that new energy producing technologies must be surmounted if they are to provide a substantial proportion of global energy.
These challenges were made evident by the Li-ion batteries in the Boeing 787 that were predicted to short circuit only once per 10 million flight hours [1–3].
Two adverse events grounded the whole fleet within four months of introduction, which was three orders of magnitude greater events than estimated.
We know much about the synthesis, properties, and function of energy materials, but we do not yet know how to address the fundamental degradation science of energy materials under real-world conditions and time spans.
Since the first large testing of crystalline silicon photovoltaics (PV) module’s reliability 40years ago, we have seen widespread global adoption.
The first 5MW PV power plant, developed in the 1970s as part of the DOE Block Grant Program, was predicted to have a 20year lifetime.
The site power decreased 10× faster than the predicted rate and the failed plant was decommissioned after only 5years [4].
Degradation-induced failures have been an ongoing characteristic of new and promising PV cells [5,6] and other energy materials even as producers continue to offer 25-year warranties.
The science of degradation of energy materials over time frames longer than >1Gs (31.7years) is a fundamental challenge of mesoscale science [7,8] and a transformational opportunity for energy materials, for the following reasons.
Realistic degradation studies face the same data and modeling challenges as in medicine, sociology or climate science, where “models and observational data together form an inseparable basis for scientific understanding and prediction” [9,10].
Degradation of energy materials distinguishes itself in that it evolves over long time-frames due to a multitude of distinct, complex, and interacting mechanisms that can lead to a variety of slow and/or rare events that eventually cause failure.
There are severe knowledge gaps in identifying, modeling, and reliably predicting the mesoscopic evolution that produce degradation, and in establishing an effective monitoring system of the evolving process of degradation over the relevant timescales to prevent failures (especially catastrophic failures).
It is essential to connect the mechanistic degradation pathways and their temporal evolution at the mesoscale so as to enable the identification of improved and longer-lived energy materials in real life.
Hence, the new degradation science examines degradation of a material or system, guided by real-world or realistic outcomes, whose fundamentals include modeling, monitoring, and prediction of a degradation process, as well as intervention, feature selection, and optimization aimed at improvement of materials and reduction of system failures.
A new interdisciplinary approach to degradation science calls for the involvement of materials science, physics, chemistry, statistics, computer science, engineering, and energy industries in the investigation of real-world degradation of energy material over their lifetimes.
After reviewing current PV energy materials research approaches (Section 2) we shall illustrate how this should be done (Section 3), what has been done recently (Section 4), and what challenges and new directions lie ahead (Section 5).
There have been transformative advances which provide the foundations for a new degradation science.
First is the multitude of advances in nanoscience from the scientific community and, from the broader world, advances in computers, communication, and computation.
Since the National Nanotechnology Initiative in 2000 [11], there has been tremendous progress including detailed understanding of science at the nanometer scale and from the femto- to attosecond scale [12,13].
Fundamental advances in glasses [14], nanoscale materials [15–18], interactions [19], fabrication and assembly [20–24], and systems [25–28] have ensued.
A detailed understanding of the basic nanoscience that underpins the beneficial function of energy materials provides the first building block of degradation science.
Within nanoscience, multi-scale modeling of materials [29] which connects microscopic/atomistic mechanisms with higher level, coarse grained, mesoscopic and even macroscopic models [30–34] helps us understand the fundamental origins of the physical properties of materials.
This multi-scale modeling research has focused on spanning length scales underpinning the effective parameter passing approach, linking atomic scale behavior to experimentally determined macroscopic properties.
The Materials Genome Initiative [35,36] is an example of accelerating materials discovery modeling at the micro- and mesoscopic levels to guide the experimental synthesis of new materials [37].
Still, multi-scale modeling has not elucidated the range from femto- to gigaseconds needed to provide fundamental guidance on the mesoscopic evolution of materials and their long term degradation in function and properties.
For this challenge, we start with newly available data from large and diverse experiments.
These varied datasets provide important information to fit appropriate physical and statistical models and identify the fundamental mechanisms of energy material degradation.
These are then incorporated into a network model of their mesoscale evolution over lifetime.
As nanoscience and multi-scale modeling advance, they will continue to provide mutual benefit by illuminating fundamentals and identifying critical contributors and effects.
The tremendous advances in computation and communications [38] and open access, code and data manipulation [39–45] over the past ten years are a second building block for the opportunities in degradation science.
Distributed computing [46–48] improved Internet connectivity and mobility.
The ubiquity of sensors make for unprecedented big data streams which can be utilized for experimental studies of energy materials in the laboratory and in real-world conditions [49].
Data science has grown beyond the purely computational advances which have been the focus of science (e.g., high performance computing).
With increased data volumes and variety and the associated advances in informatics for petabyte scale analysis, it is now possible to study large populations of real (as opposed to idealized or simplified) energy materials under real-world conditions and over very long time frames.
These epidemiological or population-based studies can complement our traditional small sample size, laboratory-based experiments, providing additional statistically sound information to bridge the 24 orders of magnitude in time required for femto- to gigasecond science.
The Materials Genome Initiative and advances in nanoscience have allowed nascent energy materials to be developed; however, a predictive framework for those materials properties over time in real-world applications is lacking.
For example, there is much research on new batteries and improved storage capacity for applications in electronics, transportation, and grid [50], yet degradation science must be applied to understand the contributing factors that limit the number of charge cycles and the basic mechanisms and pathways that lead to end-of-life failures [17].
Functional energy materials are complex materials with homo- and heterogeneous interfaces and substantial variances among samples in a population.
By virtue of their energy function, they are non-equilibrium systems with cyclic operating conditions and stressors and thus have high spatio-temporal complexity.
For energy applications spanning the time domain from femto- to gigaseconds, a new approach is needed that can distinguish the large dynamics of function from the slow/rare events of degradation and their differing temporal regimes.
For example, damage initiation, accumulation, and growth will eventually lead to a transition such as a sudden precipitation or a possible bifurcation into a new regime.
Similarly, environmental conditions, as encountered in permafrost or desert or given by daily or annual cycles of the seasons, can produce results quantitatively different from a well controlled laboratory-based study.
To understand the degradation significance of each of the heterogeneous aspects of materials and devices across populations and characteristic time scales, all respective data needs to be accessible to scientific inquiry.
The focus of this research is the development of mesoscopic-evolution network models which integrate physical and statistical models phenomena.
These network models, exemplified in Fig.
1 for poly(methyl methacrylate) (PMMA) acrylic, link micro- and mesoscopic degradation in order to understand the stressor/mechanism-mode/responses of the PVs in real-world use over their lifetime.
Three distinct communities (scientist, engineers, owners/operators) with distinct goals have worked in research and development of energy materials.
Scientists typically pursue laboratory-based research topics related to materials performance.
Engineers seek standards to guide product development and safety of holistic systems.
Owners and operators are most concerned with real-world performance and return on investment.
These communities overlap and interact, but their varied approaches have hindered the development and permeation of a cohesive scientific foundation and culture across the energy materials community as a whole.
Laboratory-based studies of degradation in PV energy materials have typically focused on a specific observed failure mode and sought to accelerate the presentation of that mode.
This methodology has been used in potential-induced degradation [51], however, no physical modeling of causes is provided.
Novoa et al.
[52] examined the bonding strength of PV module backsheets to comparable materials in a simulated PV module in laboratory accelerated aging conditions.
Peike et al.
[53] studied the effect of the system interactions on the power performance of a solar cell when laminated in various structures.
Koehl et al.
[54] tied the real-world climatic data of moisture content and hydrolysis reaction rates in data-driven modeling of accelerated aging conditions.
Physical models of moisture ingress and diffusion into real materials and systems have been employed to estimate water concentration [55].
These studies tended to focus on distinct modes observed in the accelerated aging of PV energy material expected from industry testing standards; however, there is little correlation with real-world observations and few insights that are extrapolated to actual use-conditions.
Internationally-recognized standards for certifying the performance and safety of commercially available PV products are in existence for both technologies [56] that include measurements [57] and accelerated exposure conditions [58].
However, several high-profile fires and other non-conformances have been well-documented [59].
Standardized laboratory-based testing is based on short duration, usually 3–4months, with binary (pass/fail) outcomes.
These tests are designed to assure a uniformity of quality at manufacture, although the tests are widely believed to provide a rapid means of detection for known failures or degradation modes of a product in its intended environment.
This statement is highly relevant to this discussion because the failure and degradation modes must be known fully (i.e., there is no predictive quality to these tests and yet there is an implicit linkage to the end-use environment).
An example of these test procedures for crystalline silicon PV is IEC 61215 [56].
Due to the binary nature of the results, problems identified by tests cannot include corrective action insights and are subsequently learning limited.
An epidemiological approach to data collection and analytics has only recently been applied in real-world studies of PV systems [60–63,5].
The published real-world studies were usually used to monitor and model performance at coarse time scales or to determine average degradation rates over long periods.
In these longitudinal studies, as little as one data point per month was collected over time and a simple linear degradation trend was fitted and compiled for sites around the globe.
Jordan and Kurtz [64] reported real-world performance of over 2000 power plants to obtain a mean linear model of PV module degradation rate equal to 0.5%/year and compared these results to other studies.
However, a set of modules exposed to hot/dry conditions underwent mean degradation rates of 1.5%/year [65].
These approaches are quite different from the approaches used in the climate science community, where statistical analysis is applied to all variables of the system [66] and statistically significant relationships are identified [67].
The basic science of degradation will span the time domains from mechanisms to lifetime and will provide an integrated mesoscopic evolution modeling methodology.
This methodology encompasses the active mechanisms and determinants of the performance, safety, and function of energy materials over diverse real-world conditions.
The challenge of establishing degradation science as the foundation of energy materials research is both cultural and scientific.
The integration of laboratory-based and real-world studies is both a prerequisite for developing a full mesoscopic evolution science and a necessary step for deeper understanding of the effects of degradation.
Bringing the approaches of different cultures and communities (scientists, engineers, and owners/operators of essential energy materials and infrastructure) together will be critical to speeding up the invention, implementation, and penetration of new energy materials in the real world beginning with data acquisition to the final realization of reliable prediction and monitoring of degradation.
The data acquisition strategies must balance the relevant scales and volumes of the datasets to be used in the physical and statistical modeling.
Approaches for extraction of the necessary information must be able to disregard spurious information, so as to develop a working network of models for each active mechanism related to each degradation pathway on the mesoscopic physical level and the data-driven statistical model level.
To capture the temporal evolution of the energy material over long time frames, appropriate informatics methods are needed to balance data volume (e.g., simple univariate time-series data streams with high-dimensional volumetric imaging datasets) while considering their respective information contents [68,69].
The raw data and extracted information must be accessible for query and modeling.
Similarly, the modeling approaches used to understand and parameterize active mechanisms and phenomena over lifetime fall into the broad categories of micro-, meso- and macroscopic approaches.
Laboratory and real-world experimentation, informatics, analytics, and the development of network models for mesoscopic evolution of energy materials over lifetime together constitute the field of degradation science.
Laboratory-based experiments seek the deterministic modeling of mesoscale evolution by studying the effects of multiple stressors, experimentally controlled mechanisms, expected active mechanisms, and multiple responses critical to lifetime performance.
The stressors of the real world cannot be experimentally controlled, but must be actively monitored and their cumulative effect over time explicitly integrated, as is done in our global SunFarm Network (Fig.
2).
In these observational real-world studies, active degradation may include unexpected yet significant mechanisms.
The systems under study typically have strong heterogeneity with large variances that increase over time among diverse evolutionary paths and modeled often using stochastic assumptions [70].
When planning laboratory experiments, the number and types of repetition can be determined or estimated using results from a pilot study or similar historical studies to reduce over- or under-sampling and ensure sufficient statistical power [71,72].
While a prospective real-world population-based observational study involves uncontrolled environmental stressors, and special attention is needed to faithfully record potential relevant variables for follow up studies.
This is especially important in order to infer causal relationships between stressors, mechanisms, and responses [73,74].
It is beneficial to formulate the degradation science mesoscopic evolution models using a stress/mechanism-mode/response framework [75] to target our full library of degradation mechanisms and pathways with sufficient sampling and data acquisition for both real-world and laboratory-based experiments.
Thorough planning of the sample populations, evaluations, and datastreams is necessary to maximize the information yield [71,74].
Efficient statistical planning is important to allow the study to minimize bias and ensure reproducibility.
To explore or test microscopic mechanisms of response to a specific applied stressor or stressors, which were either observed in a real-world study or hypothesized based on physics theory, carefully designed laboratory-based experiments are essential.
Prospective randomized comparative studies repeated over time under various types of stressor conditions (that mimic the real-world environment) will be more effective than retrospective studies in evaluating causal effects of different stressor conditions on different material types.
To elucidate the library of possible degradation mechanisms, not only are materials science and physics needed, but also statistics in modeling degradation and identifying important features/factors that impact degradation.
The interaction of these sciences from pilot and intermediate studies will advance the degradation science.
Ultimately the material degradation library comprises a network of submodels, both physical and statistical, generating a system of multivariate equations for response prediction given applied multifactor, sequential, or cyclic stressors.
We envision an informatics and data analytics environment to manage and support the entire data lifecycle including data assembly, data reuse, query, exploratory data analysis, and data sharing.
A pilot environment called E-CRADLE™ (Energy Common Research Analytics and Data Lifecycle Environment) is under development for such a purpose (Fig.
3).
The key architecture component of E-CRADLE™ is a domain-specific ontology governing data acquisition and ingestion, data annotation and curation, data assembly, and user-interfaces for accessing data.
E-CRADLE™ is designed to help ease the data management burden for the entire community by sharing the expertise and data resources, as well as performing exploratory data analysis for development of physical and statistical models.
A community-wide resource such as E-CRADLE™ would enable pilot studies from a wide range of investigators and would support continued large-scale studies as new data becomes available.
Our approach is consistent with and leads in the direction the federal “open science” mandates [39,41,42].
Physical models usually assume upwards progression in the space–time scales or in the nature of implied coarse graining.
The latter is unavoidable since a complete atomistic/quantum description on a microscopic level is usually not realistic and mesoscopic or even macroscopic physical models are unavoidable.
The range of applicability of these models is limited by the extent of coarse graining that is implied by the experimentally determinable parameters and relations, as shown below: (a) Microscopic physical models specifically refer to atomic/molecular aggregate mechanisms describable by quantum-electrodynamic Hamiltonians as functions of quantum variables solved at a finite temperature.
They have in principle no implied coarse graining and no free parameters.
Examples include quantum theory of the photovoltaic effect starting from a microstructural lattice-electron description of a semiconductor device or density functional theory of optical spectra of materials [76].
(b) Mesoscopic physical models imply first level of coarse graining described with Hamiltonians that do not depend anymore on quantum variables but on coarse grained order parameters with assumed symmetries, coupled with phenomenological constants that only in principle depend on microstructural parameters [77].
Examples include continuum description of various ordering phenomena (liquid–solid phase transition) with Landau-type order parameter theories [78] or the continuum description of structural defects (dislocations and disclinations in crystals) in mesoscopic ordering [79].
These models can seldom be reduced to some features of the microscopic description.
(c) Macroscopic physical models depend on macroscopic variables and their connection with the micro-world of quantum variables or even the meso-world of order parameters is very tenuous.
They imply even more coarse graining, losing completely the connection with micro- or mesostructural variables and are more based on experiments and statistical data then on micro- or mesoscopic considerations.
Examples include Navier–Stokes hydrodynamics [80], reaction–diffusion systems [81], flocking phenomena in birds [82], or epidemiological models in population dynamics [83].
These models are not even in principle reducible to meso- or microscopic underlying descriptions.
Defects and disorder are concepts that arise naturally with growing coarse graining of the meso- and/or macrostructural model description [84].
While thermal disorder leading to thermal fluctuations (or noise) and entropy variation is ever present, structural defects and disorder are usually imposed by external constraints or non-equilibrium driving mechanisms.
Specifically, in the context of aging and degradation mechanisms of energy materials, externally controlled or uncontrolled stressors introduce structural defects that can either couple to the thermal fluctuations (annealed disorder) or remain independent of the thermal fluctuations (quenched disorder) [85].
Both, however, introduce degradation mechanisms that lead to a progressive disordering of the material eventually completely destroying its functionality.
Examples of the quenched disorder model include photo-oxidation yellowing of the PV energy materials, where defects are chemical and essentially immobile in nature, progressively accumulating under stressor conditions leading to degradation irrespective of the thermal fluctuations.
Hazing of polymeric materials would be an example of annealed disorder, where stressor generated microscopic defects interact with the matrix taking advantage of the underlying thermal fluctuations to migrate and nucleate into macroscopic disorder degrading the material’s functionality.
Statistics is the science of data.
Evidence from the real world and implications from experimental studies lie in data.
Any statistical analysis of data is based in the model (implicitly or explicitly) and analysis technique (or analytics) used.
The model and analytics must be suitable and responsive to the degradation science objectives.
The statistical models and analytics needed for modern degradation science include, but are not limited to, the following: (a) Statistical models for the performance of microinverters and PV modules in real-world and suitable experimental settings, when no physical models exist for guidance.
These models may be parametric, semi-parametric, nonparametric, longitudinal, or include change points, depending on data and objectives.
(b) Statistical feature selection techniques to find important factors that affect power performance.
(c) Statistical measures that handle large and high-dimensional data.
(d) Statistical design strategies that structure studies for significance/information extraction, such as those that will lead to and validate accelerated experiments in labs for long-term reliability studies (in cooperation with materials science).
(e) Statistical analysis of observational studies at different depths and (sub) population levels from different locations under different stressors, leading to individualized predictive models that can be updated automatically as more data becomes available, much like how typical weather forecasting is updated from a month ago to a day ago.
(f) Statistical process surveillance that monitors and alerts a potential failure or problem of a degradation system.
(g) Statistical study of pilot data or interim analyses for updated exploration and discoveries.
(h) Statistical methodology for (d), (e), and (f).
In addition to building statistically informed submodels following above steps, a network model (or a system of structural equation models) which links both physical and statistical submodels needs to be developed to reveal larger frameworks of degradation systems (Section 3.5).
Linking physical and statistical models into networks of models on different spatial/temporal scales, using torrential real-time data streams, is among the major challenges facing predictive capabilities of the degradation science.
In this respect there exists definite similarities with modeling of climate processes [10] and modeling of social evolution processes [86].
However, there is an extra layer of challenge to materials degradation science; specifically, it would take at least 25years to have real-world reliability data evidence if there was not an effective interaction of physical and statistical models to link the real-word and laboratory studies.
Even multi-scale modeling, including recent coarse-grained simulations of biophysical and chemical systems [32,87,88], cannot be seen as a guiding paradigm for material degradation because it is driven exclusively by physical models with well defined and controlled stressors, leading to the effective parameter passing approach to link the microscale with macroscale properties.
Thus, it is imperative to have an evolution of updated data (and studies of these data) to mimic real-world settings as closely as possible (based on physics and statistics) in labs where an accelerated exposure can be made.
In this way, it is possible to derive more reliable predictive models of degradation (than existing models) in a much shorter time frame (than 25years), using a suite of real-world and laboratory data (such as data from our partner SunFarms).
Mesoscopic generalized structural equation models, semi-supervised by domain knowledge (semi-gSEM) [89], based on SEM theory of statistics, epidemiology and mathematical sociology, is a model sample that connects the physical and statistical mechanistic submodels into a network semi-gSEM model of degradation pathways that encompasses the lifetime temporal evolution.
In this way we take simultaneous advantage of the SEMs exploratory and confirmatory statistical models, elucidating statistically significant relationships in complex systems with both measured and latent variables, as well as of the intermittent micro-, meso- and macroscopic models with internal mechanistic variables, whenever they are applicable or indeed existent, capturing relationships/couplings among all the variables available from laboratory-based and real-world experiments.
Another important feature of torrential real-time data streams (over extremely long time-scales coupled with flexible networks of embedded submodels) is that they make it possible to detect/analyze the impact of highly improbable events [90], that would be missed in any laboratory-based time framework, and ways the system can self-organize and cope/fail under externally imposed disorder stressors.
Recent advances show that there exist materials that show an inherent antifragility [91] in their response to degradation disorder, including solid-electrolyte interfaces [92] and synthetic polymers [93].
To understand the degradation science, progress in informatics and statistics developments are made, together with clusters of on-going laboratory and real-world studies guided by (updated) domain knowledge and updated evidence, especially in the methodology relating stressors with a response in a material or system.
In order to enable the studies of mesoscopic evolution using temporal analytics, researchers in different fields need an infrastructure encompassing the laboratory-based controlled stressors and the real-world uncontrolled stressors for data sharing and analysis.
CRADLE™ serves as the platform of data acquisition, data storage, data processing and data presentation, enabling physical and statistical model development and application with a focus on mesoscopic evolution and temporal analytics.
E-CRADLE™, based on the multi-modality, multi-resource, information integration environment [94], is the first example of CRADLE™ infrastructure focused on handling the PV system and environmental monitoring data time-series data-stream from the SunFarm network and laboratory-based PV data.
All the SunFarms are equipped with minute-by-minute PV power and environmental data monitoring instruments, with live data-streams flowing to the SDLE center [95].
Laboratory-based data are typically spectra and images.
About 200GB of data is generated each year.
The nature of the data indicates that it is subject to typical big data problems: high volume, velocity, and variety.
Compared with the traditional way of using MySQL database to store and retrieve data, E-CRADLE™ leverages Hadoop [96], Hadoop distributed file system (HDFS) and HBase [97] and shows significantly better scalability on data processing (up to 20 times faster), data storage and data retrieval [98].
E-CRADLE™ (Fig.
3) is based on Cloudera Express [99] as a combination of CDH and Cloudera Manager.
By leveraging Cloudera Express, E-CRADLE™ provides batch processing, interactive query, interactive search, and interfaces with multiple programming language, such as Hive or Pig to process data.
In-place analytics on Hadoop is being implemented using RHadoop [100] and other R packages to enable researchers to manage, analyze and model degradation science problems in place without requiring data downloading, thereby enabling a data-centric approach of moving the analytics to the data.
We developed a domain-semi-supervised generalized structural equation modeling (semi-gSEM) methodology that can be used to relate physical mechanistic submodels and data-driven statistical submodels as networks of mechanisms/modes with statistically significant pathway relationships [89].
A component of SEM is to have network relationships and couplings amongst variables (stressor/mechanism-mode/response) which can be rank-ordered as contributors for degradation.
Temporal evolution, damage accumulation and transitions (i.e.
change points) among mechanisms and modes and will be accounted for in these semi-gSEM models which can be combined into a mesoscopic evolution network model.
Our semi-gSEM is similar to the SEM [101,102] from sociology [103], psychology [104], marketing [105], epidemiology [83], or chemical or biological reaction network modeling [106,34] in that there is a system of equations depicting multiple paths of different factors (in our case, the stressors) impacting on intermediate factors (or outcomes) and then the final outcomes (in our case, the power performance).
Our semi-gSEM was generalized to allow different paths to have different functional forms and include nonlinear relationships; and was semi-supervised based on the current knowledge of physics and chemistry relating to degradation mechanisms/modes (e.g., using plausible functional forms such as linear, quadratic, exponential, or logarithmic, along with inclusion of change points).
Generalizing the SEM models allows for the different stages of degradation to be effectively modeled.
For small scale networks with simple relations, the resulting semi-gSEM model contains coupling coefficients (βij) amongst variable pairs (i,j) [89].
The coupling βijdenotes the coefficient vector of the regression model that predicts variable i and j, where the functional form is chosen from a domain guided pool of candidates and the final form is determined by the empirical evidence.
βij can also depend on other intermediate variables such as Irrad(S1),Temp(S2), and RH (S3) that denote temperature, irradiance level and relative humidity among the most influential factors in degradation.
The βij may also subject to an additive error or uncertainty that is due to random error and any unobservable factors such as latent variables that characterize critically important physical/chemical mechanisms but are not directly observed.
A large-scale network model can be conceptually divided into a network among submodels.
For example, a study of degradation of acrylic [107] (Fig.
1) includes two submodels, Tinuvin bleaching and chain scission, each forms a degradation mechanism.
The concentration of the Tinuvin, denoted by Cti, reduces over time (Tinuvin bleaching).
Cti can has significant impact to the system performance, but is a latent variable (i.e., not directly observed).
In order to measure the change of Cti and assess its effects on the rest of the system, the physical Tinuvin bleaching (M4) submodel suggests that three observable variables, UV absorbance, mechanical degradation, and the rate of Tinuvin bleaching capture the necessary information.
The mathematical constraints upon these three variables are also given by the Tinuvin bleaching submodel.
Tinuvin bleaching submodel provides a mathematical constraint on above observable variables.
Similarly, the chain scission (M5) submodel also provides suitable observable intermediate variables and constraints.
The strengths of relations among submodels are denoted by β's, coupling strengths, which denote vectors of coupling coefficients among key variables of stressors, mechanisms/modes, and system response.
In this type of network model, we use β→, called coupling strength, to denote the vector of coupling coefficients between observable variables of different submodels, stressors and system response.
The current stage semi-gSEM methodology follows two principles.
Principle 1 determines the univariate relationships between stressor/mechanism-mode/response variables under the assumed Markovian property that if given the value of the current variable, then future and past variables are independent to each other, or the current variable is sufficient to relate the next level variable.
Principle 2 simultaneously considers the stressor variables and the mechanism/mode variables acting on the response variable collectively by an additive model.
The final collective additive model is determined by a generalized stepwise variable selection [89].
Acrylic polymer, or poly(methyl methacrylate) (PMMA), has been aged at the SDLE Center according to the degradation science methodology: real-world and accelerated aging studies have yielded data that are cross correlated and a predictive model was built in the semi-gSEM framework.
Acrylic is used in PV energy materials that reflect and concentrate light (such as back-surface mirrors) and Fresnel lenses (such as concentrating PV systems [108]).
The material must withstand harsh outdoor conditions including UV radiation and other stressors [75], yet unstabilized acrylic is susceptible to photo-, thermal and chemical degradation that manifest as changes in optical and mechanical properties [109].
Stabilizer concentration and degradation in relation to polymer degradation was the subject of a laboratory-based study to elucidate the multivariate system degradation library.
A network of submodels, shown in Fig.
1, was developed based upon semi-gSEM methodology to predict lifetime and optimal stabilizer concentrations [107].
Photodegradation studies of both unstabilized acrylic materials and those compounded with Tinuvin UV absorber have shown reciprocity in response to identical doses across a range of irradiance levels (1×, 5×, 50×), confirming that the active degradation mechanisms remain constant independent of irradiance intensity.
However, spectral effects are observed when contrasting full spectrum and UV-only irradiance sources – similar active degradation mechanisms are observed in the UV region response (changes in the fundamental absorption edge due to chain scission and UV absorber bleaching), but yellowing progresses much faster in response to the full spectral exposure due to additional photodarkening of the yellowing chromophores.
Data-driven semi-gSEM modeling of these study results give insights into the interrelated mechanisms at play such as changes in the fundamental absorption edge due to chain scission are strongly associated with the yellowing response.
In Fig.
4, a mesoscopic evolution model including UV absorber (Tinuvin) bleaching (TB), chain scission (CS), and mechanical degradation (Mech Deg) is shown, which encompasses the three stages in this materials temporal evolution to failure, over a 25-year period.
Stage one encompasses tinuvin bleaching until concentration of tinuvin (Cti) reaches zero.
A change point is seen in this grade at 12years (Cti)when mechanical degradation accelerates (stage 2) and is the factor that defines the usable lifetime of this grade of acrylic.
This photodarkening effect initially decreases tenfold with the addition of the Tinuvin stabilizer present in multipurpose (MP) grade, with the fundamental mechanism of photobleaching of the Tinuvin UV absorber determining the Tinuvin consumption rate.
Once the Tinuvin is consumed, backbone chain scission dominates the degradation (Stage 2), and this is followed by dramatic loss of mechanical properties (modulus) in the third stage of degradation [110].
Similarly to Section 4.2, bulk polymer of polyethylene-terephthalate (PET) samples were exposed to real-world and accelerated aging to model real material degradation under hydrolytic and photolytic mechanisms leading to a transition and loss of mechanical properties.
PET is a critical component in photovoltaic backsheets due to its high dielectric strength, but the material is highly susceptible to environmental stresses.
PET degradation mainly occurs via photolytic, hydrolytic, and thermal cleavage of an ester bond and results in discoloration and/or hazing, decreased molecular weight, and increased crystallinity.
Photodegradation and/or photo-oxidation mainly proceeds via Norrish type I or Norrish type II reactions that determine further degradation pathways [111–114].
Hydrolysis mechanisms are more complex when chemical reactions like autocatalysis due to active carboxylic acid end groups are taken into account and various kinetic models have been discussed in the literature [115–121].
Failures such as cracking and delamination in backsheet films caused by aging [122,52] may result in dielectric withstand breakdown of PV systems.
To study photodegradation of unstabilized PET, a laboratory-based experiment utilizes a completely randomized longitudinal design (i.e., unstabilized PET samples are randomly assigned four exposure types and followed over time with repeated measurements).
The four different laboratory-based accelerated conditions are continuous and cyclical UVA light, heat and humidity in accordance with ASTM G154 [58], and also damp heat and humidity freeze conditions based on IEC 61215 [56] standards.
Three outcomes are measured to characterize photodegradation: (a) yellowing under UV irradiance, (b) hydrolysis (i.e., moisture induced hazing) and (c) embrittlement.
In addition, one sample is retained from further exposure at each time epoch for future investigation.
Fig.
5 summarizes results as yellowing is predominant with exposures consisting of UVA light irradiance while hazing is caused by high level of moisture content and is more predominant when moisture is coupled with UVA light irradiance.
Nanoindentation shows that as haze development increases, PET embrittlement results in loss of mechanical properties and physical integrity of the material, which reduces dielectric strength.
The methodology of degradation science extends beyond bulk material as was applied to accelerated aging of polycrystalline silicon (pc-Si) solar cells, comprising a wealth of functional interfaces of disparate materials with a quantifiable electrical performance rating.
The cells, which are a constituent of commercial PV modules, were exposed to weathering conditions in accordance with ASTM G154 [58].
The observed response was maximum power reduction as evidenced by the I–V characteristics.
These data were mapped onto a macroscopic physical model of the solar cell device using an equivalent circuit depiction containing a single diode to represent the photoactive pn junction and parallel and series resistors to represent losses [123].
This simplistic model mathematically predicts the I–V characteristics well, and allows for parametrization of the curve for enhanced information extraction and mesoscopic insights to performance loss [124].
In this way the results indicated an increase in the solar cell series resistance that suggests degradation is caused by increased losses at the semiconductor–metal junction.
Fig.
6 depicts a series of representative I–V curves for a solar cell throughout its exposure.
The inset clearly shows that for the sample ensemble the extracted series resistance is increasing in time, which is suggestive that contact resistance of the screen-printed silver conductor lines on the cell are the primary cause of performance loss.
Optical analysis of the screen-printed silver conductor lines that constitute the front contact of the cell indicated corrosion-related oxide nanoparticle formation as evidenced by a decay in the optical second harmonic generation signal [125].
In this way the underlying physical phenomenon of series resistance increase can be probed and understood as part of a mesoscopic model linking the macroscopic diode model.
This modeling methodology is useful for confirmatory science and can act as a validation of or constraint upon a semi-gSEM network modeling approach.
An example of the semi-gSEM methodology applied to a combination of disparate PV energy materials that comprise a commercial PV module were modeled to find degradation pathways under laboratory-based accelerated aging [89].
The multivariate interactions of mechanistic degradation is complex and best suited for analysis consisting of a network of submodels that give predictive quality.
Initial results from full-sized pc-Si PV modules indicated that UV stressors applied according to IEC 61215 UV preconditioning were not sufficient to induce a significant response in the measurable characteristics of the modules within 3000h.
Modules exposed to damp heat from IEC 61215 demonstrated significant degradation within 1890h, with two dominant mechanistic degradation pathways evident involving several component PV materials.
Semi-gSEM analytics of the experimental data, as represented schematically in Fig.
7, indicated that moisture and thermal stressors activate PET hydrolysis (M5) in the pc-Si PV backsheet, and ethylene–vinyl acetate (EVA) encapsulant hydrolysis (M4) in the interior of the module [89].
The rapid equilibration of external environmental moisture levels at these conditions to the interior of the module causes these two degradation pathways to occur in parallel, and appear to be correlated.
PET hydrolysis (M5) results in a loss of mechanical, moisture barrier, and dielectric properties in the backsheet as described in Section 4.3.
EVA hydrolysis (M4) results in the generation of acetic acid CHAc [126] within the interior of the pc-Si PV module and is strongly correlated to loss of electrical properties (R8 and R9).
Integrating the above result on non-encapsulated solar cells the screen-printed silver conductor lines are the suspected root cause of power loss, resulting from the additional acetic acid stressor.
(See Fig.
8).
In order to cross-correlate model prediction to the real world, the performance of a large plurality of PV modules is being acquired for statistical modeling.
In an effort to assess and quantify effects of uncontrolled real-world stressors on module performance, potential covariates together with performance measures are collected in a minute-by-minute data stream on 60 pc-Si modules from 20 distinct manufacturers.
The stressors and covariates include irradiance levels, ambient temperature, physical module locations and weather conditions such as fog level, cloudiness, snow or rain.
The Sunfarm is also continuously monitored to capture unforeseen operation conditions of the PV modules to help capture outliers and to ensure data homogeneity for follow-up studies.
In the modeling phase, hierarchical and k-means clustering methods were used to discover naturally arising self similarity groups and performance relationships among the data over time.
The data suggested a statistical multiple regression model for predicting the real-world module performance in terms of logit transformed power outputs with significant stressors.
The findings are again mapped onto physical models in a similar way to the macroscopic diode model described above in Section 4.4.
Changes in variances and divergence in performance across a homogeneous study population in time can be used to highlight the nature of damage accumulation, degradation and failure.
The population members who fail first can be observed to diverge from the mean population behavior earlier in the study, for a truly non-stochastic degradation pathway.
Ultimately a purely data-driven statistical model can be used to inform the laboratory based studies.
A microinverter is typically connected to one PV module to convert the DC output of the PV module to utility AC (Fig.
9(a)).
In a similar way to the PV modules, reported data on the microinverter performance naturally allows for extended power electronics informatics and analytics.
Fig.
9(c) shows the pairs plot and the correlation coefficient between different environmental, application stressors, and the system response.
Furthermore, the direct impact of each stressor on thermal performance are also evaluated.
The analysis showed that the critical factors is a mixture of application stressors: PV module temperature, AC power output of the microinverter, and environmental stressors: irradiance and ambient temperature [127].
A multiple linear regression model was developed to predict the noon time microinverter temperature at real-world operation.
Fig.
9(b) shows comparison between actual and predicted microinverter temperature during noon time in a typical cloudy day.
Even in this complex device, a switch-mode power converter, the microstructural evolution of the power components results in performance degradation at the mesoscale, but the stressors and responses are largely unknown.
Application stressors, such as high speed grid disturbances, high frequency switching power cycling, combined with slower events such as module power cycling and heat/humidity give a rich set of variables that impact inverter mesostructural evolution.
By acquiring time-series datasets, typically on the minute-by-minute level, one can identify factors through which some population members diverge from the mean behavior and become a subset of extremal or even failing devices.
One seeks out the relationships between these variables and again, mapping the data onto physical models allows close examination of what is essentially a closed device.
For example in MOSFETs utilized in microinverters physical models of efficiency loss indicate that switching losses have two contributors: output capacitance loss, Poss=12CossV2fs and conduction losses Pcl=ID2Ron, regression fitting determines the physical parameters of the device and allow for monitoring in time to infer degradation mechanisms, such as an Ron increase signifying thermal runaway of the FET die.
Mechanisms can then be confirmed by laboratory-based analytic investigation.
In many ways, this is easier and more productive way to determine the critical factors on mesoscopic evolution over lifetime.
Difference can arise from climatic, application stressors, design, construction, etc.
Degradation science as outlined in this paper poses several new challenges, complementing traditional research approaches with new epidemiological, analytical, modeling and visualization algorithms, including harnessing diverse data provenience and formats as well as de novo data types for research, with experiments conducted under broad and uncontrolled conditions.
The real-world massive data input and processing naturally enforces sharing of research data [128] and processing codes [44,45], as well as promotes open data, open science, and open government policies [41,42].
Standard methods developed for single scale, separate, or moderate-sized datasets do not scale to modern big data from multiple sources.
Six common challenges are recognized by the National Research Councils Committee on the Analysis of Massive Data [38].
(a) Tracking data provenance.
The context in which the original data has been collected needs to be carefully formulated, managed, and shared with original data.
Data about data, or metadata, is a crucial aspect of data annotation and curation that will affect the aggregation and sharing of data from multiple heterogeneous sources.
(b) Coping with sampling biases and heterogeneity.
Energy material data may come from (hidden) heterogeneous groups, and contain outliers/corruptions, measurement errors, missing values or be subject to selection biases.
It is important to develop methods to extract true signals from noisy and incomplete data.
(c) Working with different data formats and structures.
Because of the variety of energy material data originating from different vendors an important aspect of informatics work would be to map and align such data to ensure that semantics is preserved during data transmission, transformation and integration.
(d) Scalability of computational algorithms for processing and analyzing energy materials data.
Advances in cloud-computing have provided a promising computational paradigm for scale up computational speed.
However, specific MapReduce algorithms for specific application tasks are yet to be developed.
(e) Ensuring data integrity, data security, and enabling data integration and data sharing.
This has been incorporated in the design of CRADLE™ and will be fully developed and deployed.
(f) Data visualization.
Methods for visualizing massive energy materials data need to be further developed for on-the-fly analytics and degradation pathway network decision-making.
To address these challenges the energy materials community needs to develop a specific ontology that captures the scope and depth of the energy materials domain.
An energy materials ontology (EMO) can serve as a foundation for addressing the challenges of data provenance, data variety, and facilitate data integration and sharing.
Such an approach has been proven imperative for the biological research community as represented by the Gene Ontology [129] for disease specific consortia such as epilepsy research community [130] or the ontological resources provided by the National Library of Medicine encapsulated in the Unified Medical Language System [131].
The important and direct roles a domain ontology can play for managing big data has been demonstrated in [132], where a common ontology directly supports data capture, data integration, data sharing, and user interfaces for data retrieval.
Big data is commonly characterized by variety, volume, velocity and veracity [133].
Progress in distinct domains may require different strategies.
It is important to resist the temptation of a monolithic approach [134].
Two recent national center projects from the domain of biomedicine offer a blueprint for degradation science in its approach to creating a community-level data and informatics infrastructure.
The first is the National Sleep Research Resource (NSRR [135]), which offers free web access to large retrospective collections of de-identified physiological signals and clinical data elements collected in well-characterized research cohorts and clinical trials.
The underlying architecture is motivated from the PhysioMIMI project, in its ontology-driven architecture governing data curation, data integration, data access and data analytics.
The second is Center for SUDEP Research (CSR [136]), which adopts MEDCIS as its data and informatics platform, where the purpose is to prospectively follow a larger group of epilepsy patients at an elevated risk for SUDEP overtime, from multiple hospital Epilepsy Monitoring Units.
Again, a dedicated Epilepsy and Seizure Ontology serves as the core knowledge source driving the entire data lifecycle.
The general ontology-guided approach to developing information systems is discussed in a larger context in [137].
In both of the exemplar projects, big data challenges such as processing and annotating large collection of physiological signals and managing patient records with thousands of uneven attribute columns unfolded themselves from the agile development process and computation need, motivated by the projects grander vision.
In addition to an ontology-driven system architecture, context-aware, role-based access management, user-centered, interface-driven development and agile process in partnership with domain experts are key elements for progress.
A true collaborative partnership between computer scientists and domain experts, in the spirit of team science, sets a solid organizational foundation for projects such as NSRR and CSR.
In the context of degradation studies, we refer to “Big Data as a frame of mind, or a bigger vision, in perceiving the science and engineering landscape from a grander data scale, emboldened by the scalability of cloud computing, such as MapReduce for massive parallel processing” [138].
Such an approach can dramatically accelerate the speed of analysis in cases of complex tasks that are previously less computationally feasible [139].
We believe that such a scalable approach is beneficial for degradation research in general, even for computationally feasible problems, because it allows us to ask bigger questions and to answer them faster, putting the computational barrier in the back of our minds so we can focus more on the scientific content.
A challenge to development of a holistic network of submodels fully characterizing the time-evolution of material is the integration of models and datasets from disparate experiments and the merging of hypothesis-driven research and epidemiological research.
Examples of laboratory based studies in a stress/mechanism-mode/response framework are often initially guided by hypothesis, as in a semi-supervised fashion, and then a data-driven mode mesoscopic model is constructed and informed by existing physical models.
Real-world studies by contrast are more clearly epidemiological in structure and lend themselves to predictive modeling based purely in statistics.
However, as we’ve shown, the projection of these statistical models onto physical models can provide fundamental insights into mechanisms of degradation and serve to validate and verify the predictions given by laboratory-based studies or act as regression constraints.
Relevant statistical analytics need not only overcome the heterogeneous nature of different types/evolution of data, but also the high/huge dimensions resultants from multiple factors and measurements obtained on the same module/material over time, as well as individual patterns that can only be learned more accurately as data are updated under each particular environment.
The heterogeneous real-life environment can lead to measurement errors, irregular sampling points and censoring (as we have seen in our SunFarm study [140]).
Thus analytics that lead to effective measures that are fast to compute, effective for prediction and well-conditioned under this heterogeneity will be important as they are to classification of huge volumes of astronomical light curves [141].
These measures are also analogous to descriptors used in image analyses.
Individualized longitudinal predictive models that can be updated over time [142] will be extremely important for critical system monitoring.
The software packages for effective change point analyses that are applicable to degradation science and monitor system transitions are needed [143,144].
The semi-gSEM results to date contain no explicit time dependency in the mesoscopic evolution, although this time dependency is an eventual necessity for a complete picture of material degradation.
A time dependency in the modeling must discriminate the dynamics of typical function, and be sensitive to the material mesoscopic evolution related to degradation.
The temporal analytics must have the sensitivity to distinguish and discover the potential slow and rare events associated with degradation and failure.
These concepts require high density datastreams as inputs to the model so that information entropy is not limiting to the discovery science.
The segmentation of time-scales into cycles (daily variation), steps (functional degradation response from stressors), and stages (phase transformations) enables temporal discrimination.
Temporal discrimination among dynamics, function and the slow or rare degradation mechanisms and modes is achieved through data filtering, dimensional reduction or frequency banding using spectral time-series methods [145], with the caveat that data are always retained so that the filtering can be applied repetitively as more data become available.
This temporal segmentation is similar conceptually to the analysis of Brownian motion by Einstein, where the physical picture of Brownian motion is described as a function of collective, stochastic phenomena governing the ensemble of particles to the motion of a single particle on a larger order of time, given by the square root of elapsed time [146].
The inclusion of time is a specific challenge due to the bridging of multiple temporal orders of magnitude from the initiation and accumulation of damage through the phase transformation leading to cascading macroscopic performance loss on a decade time-scale.
These timescales of transformation are mechanistically dependent, as depicted in Fig.
1, and the network of submodels must be inclusive of these various time stages.
Phase transformations are not modeled by a priori physical models to an exact degree, and it is probable that the supervised or statistical validation provided by physical models based in hypothesis-driven science are valid within these time stages, and they must be linked by parametric statistical models such as regression splines.
The regression splines can effectively model these temporal change points and ensure a continuity of the predictive model.
Integrating laboratory-based and real-world data-driven studies by necessity implies not only bridging micro-, meso-, and macro-scopic spatio-temporal scales, but also connecting the deterministic and stochastic approaches to modeling [147].
Stochastic reaction–diffusion systems [148] are an example of this type of connection in systems that, while being described by deterministic equations, also allow for intrinsic fluctuations that turn out to be an important factor leading to drastic changes in the dynamics [149].
Degradation mesocale evolution, allowing for externally induced structural disorder of either annealed or quenched type, would introduce an additional distinct level of coupling between thermal fluctuations and external stressor generated disorder that has never been explored before.
The interplay of stochastic and deterministic evolution, informed by semi-gSEM merging of physical and statistical models, should help us in identifying the deterministic part of the mesoscale evolution dynamics on one hand, but also the effects of structural disorder stemming from the externally imposed stressor fluctuations on the other.
This is of course a challenge that any modeling of the degradation phenomena will have to face in one way or another.
To us data-driven statistical modeling coupled to multi-level multi-scale physical models, bridging across gaps of approximations, is the way to address this challenge.
Based on recent advances in nanoscience, data science and the availability of massive real-world datastreams, the mesoscopic evolution of energy materials can now be more fully studied.
The temporal evolution is vastly complex in time and length scales and is fundamentally challenging to scientific understanding of degradation mechanisms and pathways responsible for energy materials evolution over lifetime.
We propose a paradigm shift towards mesoscopic evolution modeling, based on physical and statistical models, that would integrate laboratory studies and real-world massive datastreams into a stress/mechanism/response framework with predictive capabilities.
These epidemiological studies encompass the variability in properties that affect performance of material ensembles.
Mesoscopic evolution modeling is shown to encompass the heterogeneity of these materials and systems, and enables the discrimination of the fast dynamics of their functional use and the slow and/or rare events of their degradation.
We delineate paths forward for degradation science.
The authors would like to acknowledge the funding for this work.
The SDLE center was established through funding through the Ohio Third Frontier, Wright Project Program Award Tech 12-004.
The acrylic research was supported by Ohio Third Frontier Award Tech 11-060 in collaboration with Replex Plastics.
The PV module damp heat study was supported by Underwriters Laboratories.
The real-world PV module studies are supported the Bay Area Photovoltaic Consortium Prime Award No.
DE-EE0004946, Subaward Agreement No.
60220829-51077-T.
Sample tracking was provided by REDCap [150] funded by the Clinical and Translational Science Collaborative (CTSC) under Grant No.
UL1TR000439.
Additional funding and support from DuPont, Saint Gobain, 3M, SunPower, Solexel, Kent Displays, MCCO Inc., Q-Lab, Underwriters Laboratories and IIT-GN is acknowledged.
Additionally, the authors acknowledge the thoughtful and helpful comments of the reviewers.
Numerical time-step restrictions as a result of capillary waves The propagation of capillary waves on material interfaces between two fluids imposes a strict constraint on the numerical time-step applied to solve the equations governing this problem and is directly associated with the stability of interfacial flow simulations.
The explicit implementation of surface tension is the generally accepted reason for the restrictions on the temporal resolution caused by capillary waves.
In this article, a fully-coupled numerical framework with an implicit treatment of surface tension is proposed and applied, demonstrating that the capillary time-step constraint is in fact a constraint imposed by the temporal sampling of capillary waves, irrespective of the type of implementation.
The presented results show that the capillary time-step constraint can be exceeded by several orders of magnitude, with the explicit as well as the implicit treatment of surface tension, if capillary waves are absent.
Furthermore, a revised capillary time-step constraint is derived by studying the temporal resolution of capillary waves based on numerical stability and signal processing theory, including the Doppler shift caused by an underlying fluid motion.
The revised capillary time-step constraint assures a robust, aliasing-free result, as demonstrated by representative numerical experiments, and is in the static case less restrictive than previously proposed time-step limits associated with capillary waves.
The numerical simulation of interfaces with surface tension is only stable if the propagation of capillary waves, which are interface waves governed by surface tension, is temporally resolved by the applied numerical time-step.
The phase velocity of capillary waves is given as [1](1)cσ=ωσk=σkρa+ρb, which follows from the dispersion relation(2)ωσ2=σk3ρa+ρb, where ωσ is the angular frequency of capillary waves, k is the wavenumber, σ is the surface tension coefficient and ρa and ρb are the densities of the two fluids.
Thus, capillary waves of short wavelength travel faster than capillary waves of long wavelength.
For numerical simulations this anomalously dispersive behaviour of capillary waves leads to very rigid time-step restrictions.
In a numerical framework, the shortest wavelength unambiguously resolved by the computational mesh and, therefore, the lower limit for the wavelength, is [2–4](3)λmin=2Δx, with Δx representing the mesh spacing.
This definition of the minimum resolved wavelength is best understood as a sampling criterion.
Considering a series of standing waves, the amplitude at any given point on a wave is repeated at distances of integer-multiples of half the wavelength.
Hence, in order to unambiguously represent the amplitude of the wave as well as its dispersion, the phase shift between two neighbouring sampling points, i.e.
computational nodes with respect to numerical simulations, has to be smaller than π [3], which corresponds to λ/2.
Because an adequate spatial resolution of waves requires at least 6–10 cells per wavelength [4], the dynamic behaviour of the shortest spatially resolved capillary waves cannot be represented in a physically accurate manner.
Hence, capillary waves with a wavelength between 2Δx and, approximately, 6Δx are not part of the physical solution and can, therefore, be considered spurious capillary waves.
The origin of spurious capillary waves are perturbations of the interface induced by external forces, for instance due to the collision of the interface with an obstacle, as well as the finite accuracy of numerical algorithms and discretisation errors, in particular parasitic currents.
Parasitic currents, even if they are small in magnitude, cause a local displacement of the interface that initiates capillary waves of short wavelength.
Other frequently encountered sources of spurious capillary waves are sudden changes in pressure or the impact of eddies on the interface.
Brackbill et al.
[5] recognised the necessity to temporally resolve the propagation of the shortest numerically represented capillary waves and derived the capillary time-step constraint(4)ΔtσBKZ≤Δx2cσ=(ρa+ρb)Δx34πσ, where superscript BKZ stands for the surnames of the original authors, based on the phase velocity of inviscid capillary waves cσ and the shortest spatially resolved wavelength λmin.
The common consensus in the research community is that the capillary time-step constraint is required as a result of the explicit treatment of surface tension [5–18].
Brackbill et al.
[5] and many consecutive researchers have suggested that this capillary time-step constraint can be mitigated if the surface force is treated implicitly.
This assumption has motivated recent efforts to devise an implicit or semi-implicit implementation of the surface force, in order to lift the time-step restrictions in interfacial flow simulations [9,14].
Hysing [9] proposed a semi-implicit formulation of the surface force for finite element methods based on the CSF model of Brackbill et al.
[5], introducing an additional implicit term which represents additional shear stresses tangential to the interface.
Hysing [9] reported stable results for time-steps up to two orders of magnitude higher than the capillary time-step constraint.
The semi-implicit formulation of Hysing [9] was later modified for finite volume methods by Raessi et al.
[14], who reported that the additional shear stresses at the interface enables them to exceed the capillary time-step constraint by at least factor five without destabilising the solution.
A study recently presented by Denner and van Wachem [19] suggests that the success of this additional interfacial shear stress term is based on the dissipation of the surface energy of capillary waves with short wavelength, independent of whether this additional shear stress term is implemented explicitly or implicitly.
A different picture is presented by considering the resolution of capillary waves from a signal processing viewpoint.
Sampling a spatiotemporally evolving wave, temporal aliasing occurs if the sampling frequency is below the Nyquist sampling rate, which stipulates that the sampling rate has to be at least twice the maximum frequency of the sampled signal in order to avoid temporal aliasing [20].
This suggests that a maximum time-step with respect to a given mesh spacing exists, similar to the capillary time-step constraint given by Eq.
(4), which assures an aliasing-free result.
Exceeding this time-step eventually results in aliasing of the capillary waves, leading to an ambiguous representation of the frequency and amplitude of capillary waves [3], an unphysical evolution of the interface and potential divergence of the numerical solution algorithm.
Since the temporal sampling rate is independent of the applied numerical technique, the time-step constraint imposed by the Nyquist sampling rate does not depend on the numerical treatment of surface tension.
In recent years, several studies reported coupled numerical frameworks, e.g.
[21–24], where the primitive variables are solved in a single system of linearised equations.
A coupled implicit approach provides a strong, implicit pressure-velocity coupling, which is particularly desirable for the simulation of two-phase flows, given the discontinuous pressure jump resulting from surface tension and the potentially large density and viscosity ratios between the interacting fluids.
Denner and van Wachem [25] have recently presented a coupled numerical framework for two-phase flows on arbitrary meshes, including a segregated implementation of the interface advection and an explicit treatment of surface tension.
The results reported by Denner and van Wachem [25] demonstrate the robustness and accuracy of the coupled framework, even for very high surface tension and large density ratios [25,26].
However, a coupled numerical framework in which the governing equations as well as the interface advection are solved in a single equation system and in which surface tension is implemented implicitly has not been reported yet.
This article investigates the time-step requirements associated with resolving the dynamics of the equations governing capillary waves, focusing on two main objectives: (a) to determine whether explicit and implicit treatments of surface tension have different time-step requirements with respect to the dispersion of capillary waves, and (b) the formulation of an accurate time-step criterion for the propagation of capillary waves based on established numerical principles and signal processing theory.
A fully-coupled numerical framework with implicit coupling of the governing equations and the interface advection and an implicit treatment of surface tension is proposed, based on the coupled framework of Denner and van Wachem [25].
This fully-coupled implicit framework is used to study the temporal resolution of capillary waves with explicit and implicit treatment of surface tension.
Furthermore, a revised capillary time-step constraint is proposed, based on a detailed analysis of the numerical domain of dependence and spatiotemporal sampling requirements of capillary waves, including the superposition of capillary waves on an underlying fluid motion.
This article is structured as follows.
Section 2 outlines the governing equations and the general numerical framework.
In Section 3, the explicit and implicit treatments of surface tension are presented and a coupled flow-interface advection methodology is proposed.
Subsequently, Section 4 investigates the dispersion of spurious capillary waves in real fluids.
In Section 5, the temporal resolution requirements for the simulation of capillary waves are devised and a revised capillary time-step is proposed.
In Section 6, the surface tension treatments are compared with respect to the capillary time-step constraint and the revised capillary time-step constraint is validated.
The article is concluded in Section 7.
The considered incompressible Newtonian fluids are governed by the continuity equation and the momentum equations, defined as(5)∂ui∂xi=0,(6)ρ(∂ui∂t+uj∂ui∂xj)=−∂p∂xi+∂∂xi[μ(∂ui∂xj+∂uj∂xi)]+ρgi+fs,i, where u is the velocity, t represents time, p stands for the pressure, ρ is the density, μ is the viscosity of the fluid, g is the gravitational acceleration and fs is the volumetric surface force.
In Eq.
(6) the transient term is discretised using the Second-Order Backward Euler scheme and convection is discretised using a central differencing scheme.
If non-isothermal flows are considered, the fluids are additionally governed by the energy equation(7)cpρ(∂T∂t+ui∂T∂xi)=∂∂xi(Λ∂T∂xi), where T is the temperature, cp is the specific heat capacity and Λ is the thermal conductivity.
The Volume of Fluid (VOF) method [27] is adopted to capture the interface between two incompressible and immiscible fluids.
In the VOF method, the local volume fraction in each cell is represented by the colour function γ, defined as(8)γ(x,t)={0fluid a,1fluid b.
Thus, the interface is located in every mesh cell holding a colour function value of 0<γ<1.
The density ρ and the viscosity μ are defined based on the colour function γ as(9)ρ=ρa(1−γ)+ρbγ,(10)μ=μa(1−γ)+μbγ, where subscripts a and b denote the two fluids.
If heat transfer is considered, the specific heat capacity cp and the thermal conductivity Λ are treated in the same manner as density and viscosity.
The colour function γ is advected by the hyperbolic advection equation(11)∂γ∂t+ui∂γ∂xi=0 based on the underlying flow with velocity u.
In the present study a compressive VOF methodology is applied as detailed in [28], using algebraic discretisation schemes to discretise Eq.
(11) and transport the colour function in a time-marching fashion.
The CICSAM scheme by Ubbink and Issa [29] is used to discretise the spatial advection term in Eq.
(11) and the transient term of Eq.
(11) is discretised using a Crank–Nicolson scheme.
The error in mass conservation of this compressive VOF method is of the order of the applied solver tolerance, as reported by Denner and van Wachem [28] for various representative test cases on structured and unstructured meshes.
Assuming surface tension is a volume force acting in the interface region, the surface force per unit volume is described by the CSF model [5] as(12)fs=σκmδΣ, where σ is the surface tension coefficient, κ represents the interface curvature, m is the unit normal vector of the interface and δΣ is the, so-called, interface density.
Following Brackbill et al.
[5], the interface density is defined as(13)δΣ=|∇γ|.
Thus, with the interface normal vector m determined as(14)m=∇γ|∇γ|, the volumetric surface force becomes(15)fs=σκ∇γ.
If heat transfer is considered, the temperature-dependency of the surface tension coefficient as well as the resulting thermocapillarity is included in the definition of the surface force and Eq.
(15) becomes [30](16)fs=σκ∇γ+∇sσ|∇γ|, where |∇γ| is the interface density as defined in Eq.
(13) and ∇sσ is the gradient of the surface tension coefficient tangential to the interface, given as(17)∇sσ=σT[∇T−(∇T⋅m)m].
A linear relationship between temperature gradient and surface tension coefficient is assumed, with the surface tension coefficient being(18)σ=σ0+σT(T−T0), where σ0 is the surface tension coefficient at temperature T0 and σT is the temperature coefficient of surface tension.
In order to ensure a balanced-force implementation of the surface force, Eq.
(12) is discretised on the same computational stencil as the pressure gradient [25].
The pressure gradient term of the momentum equations as well as the gradient of the colour function are evaluated using the Gauss theorem, defined for the colour function gradient of mesh cell P as(19)∇γ|P=1VP∑fγfnfAf, where VP is the volume of cell P, subscript f denotes the mesh faces bounding cell P, nf is the outward-pointing unit normal vector of face f and Af stands for the area of face f. The surface force is taken as is, and no convolution is applied to smooth the surface force [31].
The solution procedure follows a coupled, implicit implementation of the primitive variables, following the work of Denner and van Wachem [25], where the governing equations of the flow are solved in a single linear system of equations, given as(20)(Aux00Apx0Avy0Apy00AwzApzAucAvcAwcApc)︸A⋅(ϕuϕvϕwϕp)︸ϕ=b, where A is the coefficient matrix, ϕ is the solution vector and b is the right-hand side vector.
Inside matrix A, Aji represents the coefficient submatrix for primitive variable j of the i-th equation, with superscripts x, y and z denoting the three momentum equations and c denoting the continuity equation.
The solution vector ϕ is constituted by the solution subvectors of the three velocity components u, v and w, and pressure p. The discretised energy equation solved for temperature T is added to the linear system of equations, Eq.
(20), in non-isothermal cases.
In order to provide an additional relationship between pressure and velocity to close the linear system of equations, the continuity equation is formulated using a specifically constructed advecting velocity un and is defined as(21)∂ui∂xi|P≈1VP∑f(uf⋅nf)Af≈1VP∑fufnAf=0, where f denotes all bounding faces of a given mesh cell P, nf is the outward-pointing unit normal vector of face f and Af is the area of face f. For isothermal two-phase flows on arbitrary meshes, Denner and van Wachem [25] devised the advecting velocity ufn based on a momentum interpolation method as(22)ufn=ufnf−αfdˆf[pQ−pP|sf|−ρf2(∇p|PρP+∇p|QρQ)sf|sf|]+αfdˆfσ[κfγQ−γP|sf|−ρf2(κP∇γ|PρP+κQ∇γ|QρQ)sf|sf|]+αfdˆf[ρf(g⋅sf|sf|)−ρf2(Sg,PρP+Sg,QρQ)sf|sf|]+cfdˆf[ufn,t−Δt−uft−Δtnf], where cf, df and αf are coefficients based on the flow field and the mesh orientation, see [25] for details, and sf is the vector connecting the cell centres P and Q adjacent to face f, as depicted in Fig.
1.
The gravity source term Sg is discretised as [25](23)Sg,P=1VP∑f(g⋅af)ρfnfAf, where af=xf−xP is the co-variant cell-face vector.
The advecting velocity resulting from Eq.
(22) couples pressure and velocity and assures a discrete balance between pressure gradient, gravity and surface force.
The pressure terms in Eq.
(22) represent a filter that is nonzero if the pressure profile is locally cubic or higher, thus, suppressing cubic and higher-order pressure fluctuations, which are responsible for pressure-velocity decoupling on collocated meshes.
In the continuity equation, Eq.
(21), the velocity term ufnf as well as the pressure gradient across the cell face αfdˆf(pQ−pP)/|sf| of the advecting velocity un, Eq.
(22), are implemented implicitly.
The advecting velocity is also applied to define the volume flux Ff=ufnAf used for the convection term of the momentum equations, Eq.
(6), and the spatial advection term of the interface advection equation, Eq.
(11), thus, providing a consistently defined spatial advection of momentum and the VOF colour function [28].
Each time-step consists of a finite number of non-linear iterations to account for the non-linearity of the governing equations (inexact Newton method).
At the end of each non-linear iteration the deferred terms of the equation system, such as the face flux Ff, are updated.
This iterative procedure continues until the non-linear problem has converged to a sufficiently small tolerance.
The main convergence criterion considered in the presented simulations is the divergence of the velocity field, representing the conservation of continuity.
The chosen tolerance, ranging between 10−6 and 10−8 in the simulations presented in this article, should reflect the expected length and time scales of the flow field.
For two-phase flows, the required number of non-linear iterations to convergence, typically 3–30, is predominantly governed by the prevailing pressure gradient at the interface.
Two ways of implementing the interface advection and the surface force are considered in this article and explained in the following sections: (a) an explicit implementation of the surface force with a segregated advection of the interface and (b) an implicit implementation of the surface force with a coupled interface advection.
Comparing these two surface force implementations will reveal the effect of an explicit and an implicit treatment of surface tension on the capillary time-step constraint.
Applying an explicit treatment of surface tension, the interface is advected prior to computing the flow field at a given time instant in a segregated fashion, as schematically depicted on the left in Fig.
2.
A linear equation system is constructed, containing the discretised VOF advection equation, Eq.
(11), for every mesh cell, using the face fluxes Ff resulting from the previous time-step.
After the colour function field has been advected, the colour function gradient as defined in Eq.
(19), the fluid properties, see Eqs.
(9) and (10), and the interface curvature κ are updated.
In this case, the CSF model is implemented as an explicit source term in the momentum equations.
Therefore, the colour function gradient and the interface curvature updated after the interface advection are applied to calculate the surface force as defined in Eq.
(15).
Hence, the surface force remains constant during the iterative solution of the equation system given in Eq.
(20) for any given time-step.
The velocity field and the interface advection as well as the surface force and the pressure gradient are coupled explicitly.
For the implicit treatment of surface tension, the interface advection equation becomes part of the flow equation system presented in Eq.
(20).
Thus, for isothermal flows, the equation system contains five linearised equations and five unknown variables for each mesh cell and follows as(24)(Aux00ApxAγx0Avy0ApyAγy00AwzApzAγzAucAvcAwcApcAγc0000Aγγ)︸A⋅(ϕuϕvϕwϕpϕγ)︸ϕ=b, where Aγi represents the submatrix with the coefficients of the VOF colour function γ for equation i, with superscript γ denoting the VOF advection equation.
The solution algorithm is schematically illustrated on the right of Fig.
2.
In this implementation, the discretisation of the colour function gradient, Eq.
(19), applied to determine the surface force fs as well as the colour function gradient across the mesh faces of the continuity equation, αfdˆfσ(γQ−γP)/|sf| as given in Eq.
(22), are implemented implicitly.
The interface curvature as well as the fluid properties are deferred, i.e.
taken based on the result of the previous non-linear iteration.
The surface force is, therefore, not a constant throughout a given time-step unlike in the explicit implementation described in the previous section, but is directly coupled and fed-back to the flow field in each iteration.
The dispersion of capillary waves as given by Eq.
(2) is, strictly speaking, only valid in ideal, inviscid fluids without gravity.
In reality, however, the dispersion of capillary waves is influenced by gravity, molecular viscosity in the bulk phases as well as surface-active substances.
Following Eqs.
(1) and (2), the phase velocity cσ and angular frequency ωσ of capillary waves in ideal, inviscid fluids rises to infinity for increasing wavenumber.
In real fluids, however, viscous stresses attenuate interface waves and become increasingly significant with increasing wavenumber, as the angular frequency becomes [32](25)ω=ω(k)+iΓ(k), where Γ represents the damping rate.
The phase velocity of capillary waves reduces accordingly.
The presence of surface-active substances has a similar effect on the dispersion of capillary waves, increasing the dissipation of surface energy and reducing the angular frequency.
At high wavenumber, which is the focus of the work presented in this article, interface waves are dominated by surface tension, whereas gravity plays a negligible role.
For decreasing wavenumber, however, the effect of gravity becomes increasingly important and the impact of surface tension gradually diminishes.
For wavenumbers [33,34](26)kg<ρgσ the dispersion relation of interface waves, so-called capillary-gravity waves, is given as(27)ωg2=σk3ρa+ρb+(g⋅nΣ)k, where g is the gravitational acceleration and nΣ is the normal vector of the interface.
For interface waves of very long wavelength the effect of surface tension becomes negligible.
Spurious capillary waves as well as the flow field in the vicinity of these waves are not accurately represented by the spatial discretisation and the numerical methodology.
Hence, the impact of viscous stresses on spurious capillary waves is not accurately modelled in numerical simulations.
The viscous length scale with respect to the frequency of capillary waves is [35,36](28)lν=μρω, which Prosperetti [35] associated with the penetration depth of vorticity generated by interface waves.
The vorticity generated by interface waves decays exponentially in the viscous bulk phases with increasing distance to the interface [1,32] and this vorticity becomes insignificant at a distance dν=2πlν from the interface [1].
As a result, the vorticity generated by the shortest spatially resolved waves is not discretely represented by the computational mesh and, thus, viscous stresses cannot act at the corresponding length scales.
For instance, for a capillary wave propagating on a water–air interface with wavelength λ=10−3 m, dν=6.83×10−5 m in the water phase and dν=2.65×10−4 m in the air phase.
If this wave represents the shortest spatially resolved wave, i.e.
λ=2Δx, the mesh spacing is Δx=5×10−4 m and, consequently, the vorticity generated by this capillary wave is not discretely resolved.
The shortest spatially resolved capillary waves are, therefore, not attenuated by viscous stresses, at least for a computationally significant time span after inception, and the phase velocity relevant for the temporal resolution of these waves is(29)c≈cσ=σkρa+ρb.
In order to test the approximation made in Eq.
(29), the damped oscillation of a small-amplitude capillary wave in viscous fluids is simulated with various mesh resolutions.
The wavelength of the capillary wave is λ=1 m, and, as in previous studies [8,15,37], the initial amplitude of the capillary wave is h0=0.01λ, with a Laplace number of La=σρλ/μ2=3000.
The non-dimensional viscosity of the fluids is ϵ=4μπ2/ρωσλ2=6.472×10−2.
Both fluids are initially at rest, gravity is neglected and the motion of the interface is induced by surface tension only.
The domain is λ in width (x-direction) and 3λ in height (y-direction), identical to the domain used by Popinet [15].
All domain boundaries are treated as free-slip walls.
The periodic motion of the capillary wave is discretised with 1000 time-steps Δt per period, resulting in a time-step well within the capillary time-step constraint as given in Eq.
(4).
The damping coefficient of the damped oscillatory motion of the standing wave is calculated as(30)Γ=ln⁡(|a0||a1|)t1−t0, where a is the wave amplitude, t stands for time and superscripts 0 and 1 are two extrema with respect to the temporal evolution of the wave amplitude.
This damping coefficient is compared with the damping coefficient Γexact obtained from the analytical solution for a standing capillary wave in the limit of equal fluid properties, small amplitude and a domain of infinite extend, devised by Prosperetti [35,38].
Fig.
3 shows the relative damping coefficient Γ/Γexact obtained by varying the resolution in the x-direction and the y-direction.
The damping coefficient reduces rapidly for decreasing mesh resolution with respect to both coordinate axes, as observed in Fig.
3.
Although the specific case of λ=2Δx=2Δy cannot be considered for a standing wave, the results strongly support the approximation made in Eq.
(29) for the phase velocity of spurious capillary waves, in particular for waves with wavelength λ→λmin=2Δx.
The simulation of interfacial flows with surface tension is only stable if spatially resolved capillary waves are appropriately resolved by the applied time-step.
The applicability of the capillary time-step constraint as presented in Eq.
(4) has been demonstrated by a large number of studies since its introduction [5,8,14,15,17,39,40].
Nevertheless, the analysis presented in Sections 5.1–5.4 suggests that the capillary time-step constraint derived by Brackbill and co-workers [5] is incomplete and founded on incorrect assumptions, such as the influence of waves travelling in opposite directions (see Section 5.3).
A revised capillary time-step constraint is proposed in Section 5.5 based on the presented analysis.
The principle of domain of dependence, initially introduced by Courant et al.
[41], stipulates that in each time-step the numerical solution must not advance further in space than the physical solution over the same time increment.
Hence, the value at a given computational node at the new time level is a function of its neighbour values at the previous time level, defined as(31)Δtud≤1, with u being the maximum propagation speed of information, Δt is the time-step and d is the distance between computational nodes.
The condition given by Eq.
(31) is widely known as CFL condition and essentially couples the numerical solution to the physical solution.
With respect to capillary waves, the maximum speed of propagation is the phase velocity cσ, as defined in Eq.
(1), of the shortest spatially resolved capillary wave, with a wavelength of λmin=2Δx.
The shortest spatially resolved wave is equivalent to the maximum wavenumber unambiguously resolved by the computational mesh(32)kmax=πΔx, which forms the basis of the derivation of Eq.
(4).
The CFL condition for capillary waves can, therefore, be expressed as(33)Δtσ≤Δxcσ.
A similar stability condition, or in fact the same stability condition applying central differencing, can be deduced from a von Neumann stability analysis (see e.g.
[42]) with respect to the numerically stable solution of capillary waves.
It is important to understand that numerical stability is not a sufficient condition for physically viable results.
If the CFL condition is violated, even a numerically unconditionally stable differencing scheme may lead to unphysical results and to unsustainable numerical errors, which can eventually cause the solving algorithm to diverge.
Signal processing offers an alternative viewpoint with respect to the numerical resolution of capillary waves.
According to the Nyquist–Shannon sampling theorem, the sampling frequency has to be at least twice the maximum frequency of the sampled bandlimited signal, given as(34)fs≥2fmax, also known as Nyquist sampling rate, in order to avoid temporal aliasing [20].
Temporal aliasing leads to abrupt spatiotemporal variations and an ambiguous representation of frequency and phase speed of a given signal [3], with large local gradients.
Even small temporal aliasing errors of a spatially propagating signal have been found to lead to substantial errors upon integration or differentiation [4,43].
At the maximum spatially resolved wavenumber, defined in Eq.
(32), the sampling frequency is(35)fs≥2fσπkΔx=ωσkΔx.
Given the angular frequency of capillary waves, ωσ=cσk, the sampling frequency becomes(36)fs≥cσΔx.
Hence, the capillary time-step based on the Nyquist–Shannon sampling theorem follows as(37)Δtσ=1fs≤Δxcσ, which is exactly the same time-step condition as given by the CFL condition, see Eq.
(33).
Crucially with respect to the numerical representation of interface waves, the time-step requirement associated with the Nyquist–Shannon sampling theorem, Eq.
(37), is independent of the numerical implementation, since the Nyquist sampling rate is a result of the discrete spatiotemporal representation of a continuous signal and not the result of the numerical discretisation of the governing equations.
Hence, the temporal resolution requirement imposed by the Nyquist sampling rate applies independent of the particular choice of numerical discretisation schemes and numerical implementation methodology.
Brackbill et al.
[5] derived the capillary time-step constraint given in Eq.
(4) based on the phase velocity of capillary waves cσ and the assumption(38)cσΔtσBKZΔx≤12, arguing that the value of 0.5 on the right-hand side is necessary to account for the case of two capillary waves travelling in opposite directions.
Taking this assumption into account, the capillary time-step constraint introduced by Brackbill et al.
[5], Eq.
(4), corresponds to the time-step condition derived from the CFL condition and the Nyquist sampling rate, as given in Eqs.
(33) and (37).
However, the assumption that two oppositely travelling waves divide the time-step requirement in half is incorrect.
The time-step limit depends only on the local mesh spacing and the local phase velocity.
The numerical differencing scheme treats each wave individually and, thus, two oppositely travelling waves do not affect the numerical stability.
Hence, from a numerical viewpoint, the time-step is not directly affected by waves propagating in opposite directions.
However, two oppositely travelling waves are superimposed and may decrease the time interval between successive local extrema, thus, locally increasing the effective wavenumber sampled by the numerical algorithm, which becomes(39)kˆ=kI+kII, where subscripts I and II denote the two oppositely travelling waves.
The special case of two oppositely travelling waves with λ=2Δx entering the same mesh cell at the same time-step results in a standing wave and has, therefore, no impact on the phase velocity.
In fact, a standing wave forms whenever two waves of the same wavelength and frequency interfere [34].
Waves with a wavelength λ<2Δx are not unambiguously represented by the spatial discretisation and can be ignored.
The highest local wavenumber is, hence, generated by two oppositely travelling waves, where one wave has the minimum wavelength λI=2Δx, while the other wave has a wavelength λII→2Δx and satisfies the condition λII>2Δx.
Thus, the wavenumbers of these waves are kI=kmax and kII→kmax, respectively, and, based on Eq.
(39), the interference of these waves results in an effective wavenumber of(40)kˆ→2kmax=2πΔx.
The increase in effective wavenumber kˆ caused by oppositely travelling capillary waves leads to a numerical misrepresentation of the dispersion of these waves.
Given the dispersion relation of capillary waves, see Eq.
(2), this increase in effectively sampled wavenumber corresponds to a maximum increase in phase velocity by factor 2 and the effective phase velocity becomes(41)cˆσ=σkˆρa+ρb=2cσ.
Considering the motion of a two-phase flow, waves on the interface may be considered in a reference frame moving with the flow velocity tangential to the interface.
In this case a stationary computational mesh has to be regarded as a stationary observer, resulting in a Doppler shift of the capillary wave frequency.
The phase velocity of capillary waves relative to the stationary mesh is, hence,(42)c=cσ+k|k|⋅uΣ=cσ+uΣk, where k is the vector of the wavenumber, uΣ is the velocity of the flow at the interface and uΣk is the velocity of the flow at the interface parallel to the wavenumber vector.
Thus, the underlying motion of the flow tangential to the interface increases the maximum phase velocity of capillary waves relative to the stationary computational mesh and, consequently, reduces the maximum time-step that assures a robust numerical solution.
Based on the analysis presented in Sections 5.1–5.4, a revised capillary time-step constraint is proposed.
Assuming a static case (uΣ⋅k=0) and given the time-step requirement for capillary waves derived from the CFL condition, Eq.
(33), and the Nyquist–Shannon sampling theorem, Eq.
(37), as well as the increase in effective phase velocity caused by waves travelling in opposite directions, Eq.
(41), the revised capillary time-step constraint is defined as(43)Δtσstat≤Δxcˆσ=(ρa+ρb)Δx32πσ, where superscript stat denotes the static case.
Comparing this revised capillary time-step constraint with the capillary time-step constraint introduced by Brackbill et al.
[5], given in Eq.
(4), it is obvious that the revised capillary time-step constraint defined in Eq.
(43) is less restrictive.
The only difference between these two time-step constraints are the assumptions about oppositely travelling waves.
The static case, however, only applies to a very limited number of scenarios, since most flows feature a finite velocity along the interface.
Considering fluid motion tangential to the interface by including the Doppler shift as defined in Eq.
(42), the revised capillary time-step constraint becomes(44)Δtσdyn≤Δxcˆσ+uΣk, where superscript dyn denotes the dynamic case.
Strictly speaking, the definition of the time-step constraint as given in Eq.
(44) is only correct on equidistant meshes.
On arbitrary meshes it is more descriptive to define the capillary time-step constraint based on the vector sΣ connecting neighbouring computational nodes in the interface region.
The general formulation of the revised capillary time-step constraint can, thus, be expressed as(45)Δtσ≤min⁡(|sΣ|)cˆσ+uΣk.
Any time-step that fulfills this requirement should result in a robust numerical solution without aliasing.
Two representative test cases are considered in this study to scrutinise the numerical stability of the explicit and implicit treatments of surface tension and to validate the proposed capillary time-step constraint.
First, the numerical stability of the proposed solution methods is tested without perturbation of capillary waves, focusing only on the implementation of surface tension.
Second, the accuracy and applicability of the revised capillary time-step constraint is validated, with explicit and implicit treatment of surface tension.
The capillary time-step constraint is commonly attributed to numerical stability considerations associated with the explicit treatment of surface tension [5–18], a claim which has never been substantiated by theoretical analysis or numerical experiments.
If the capillary time-step constraint is a requirement for numerical stability due to the explicit treatment of surface tension, the capillary time-step constraint should only depend on the magnitude of the force due to surface tension and, therefore, must be valid even if no capillary waves are propagating on the interface.
A well-known example of such a stability requirement is the explicit treatment of diffusion, which is only stable for time-steps Δt≤ρΔx2/2Dϕ, where Dϕ is the relevant diffusion coefficient.
As elaborately explained by Patankar [44], among others, exceeding this time-step limit changes the coefficients of the primitive variable and, as a result, violates the physical cause-effect relationship.
However, with the explicit as well as the implicit treatment of surface tension, the volumetric source term representing the surface tension, Eq.
(15), is a function of the surface tension coefficient, the interface curvature and a passive scalar, i.e.
the interface indicator function.
The source term representing the surface tension is not a function of the primitive variables and the type of implementation does not change the matrix coefficients of the primitive variables.
The implementation (explicit or implicit) of the surface force as defined by Brackbill et al.
[5], given in Eq.
(15), should, therefore, not impact the numerical stability.
Consequently, if interface perturbations are absent and assuming all other time-step requirements that are not a result of capillary waves are fulfilled, the stability of the solution algorithms presented in Section 3 should be independent of the applied time-step.
In order to scrutinise the general numerical stability of the explicit and implicit treatments of surface tension, the thermocapillary migration of a fluid particle with Marangoni number Ma=ρcpUrr0/Λ=0.01 and Reynolds number Re=ρUrr0/μ=0.01 is simulated, where the reference velocity is defined as [45–47] Ur=σT∇Tr0/μ.
The considered fluid particle has a radius of r0=0.5 m and the domain size is 4 m×4 m×4 m. Both phases have a viscosity of μ=0.193649 Pas, a specific heat capacity of cp=0.1 Jkg−1K−1 and a thermal conductivity Λ=1.93649×10−2 Wm−2.
The density of the continuous phase is ρc=0.1 kgm−3, whereas the density of the dispersed phase ρd is varied to accommodate different density ratios.
Gravity is neglected and the fluid properties of the bulk phases are assumed to be constant in space and time.
The temperature difference between the top and the bottom wall is fixed at ΔT=T1−T0=50 K. The surface tension coefficient σ is varied to accommodate different capillary numbers Ca=Urμ/σ and the temperature dependence of the surface tension is σT=−0.0012 Nm−1K−1.
Because Re≪1, the interface can be regarded as nondeformable and the exact values for interface curvature and interface normal vector are prescribed at the interface.
All domain boundaries are taken to be free-slip walls and the mesh resolution is 10 cells per fluid particle diameter.
Because the applied numerical framework maintains an exact discrete balance between pressure gradient and surface force, and because the geometrically exact interface normal vector and curvature are prescribed, parasitic currents are absent and the interface is not perturbed by any other source [25].
Young et al.
[48] reported an analytical solution for the migration velocity of a fluid particle at zero Marangoni number in the creeping flow regime in a domain of infinite extend, given as(46)UYGB∞=2σT∇Tr0(4+2ΛdΛc)μc+(6+3ΛdΛc)μd, where superscript ∞ denotes the domain of infinite extend and subscripts c and d denote the continuous and the dispersed phase, respectively.
Klein and Feuerbach [49] found Eq.
(46) to hold for a Marangoni number of Ma≤2 with respect to the continuous phase and Ma≤10−2 of the dispersed phase.
In order to account for the finite size of the domain, the theoretically devised migration velocity is corrected using the semi-empirical correlation of Harmathy [50] and is given as(47)UYGB=UYGB∞[1−(2r0L)2], with L being the domain size perpendicular to the direction of migration.
Fig.
4 shows the temporal evolution of the migration velocity of the fluid particle for three different time-steps and a capillary number Ca=0.01, with the explicit as well as the implicit surface tension treatment.
The predicted migration velocities are in excellent agreement with each other as well as with the expected value as given by Eq.
(47), even if the static capillary time-step constraint, Eq.
(43), is breached by three orders of magnitude and irrespective of the surface tension treatment.
The acceleration phase of the fluid particle is clearly not accurately captured with Δt=103Δtstat, yet the terminal migration velocity is still predicted accurately.
Crucially, the numerical stability is unaffected by the particular implementation of surface tension, even for Δt=103Δtstat≈1.41×103ΔtBKZ.
Furthermore, the numerical stability and predictive quality of the migration velocity are independent of the capillary number, see Fig.
5, and the density ratio, see Fig.
6.
This demonstrates that the capillary time-step constraint is completely independent of the surface tension treatment, especially considering that the treatment of surface tension is irrelevant for the numerical stability even for the case with Ca=10−2, ρc/ρd=1024 and Δt=103Δtstat as well as the case with Ca=10−4 and Δt=104Δtstat.
It is worth mentioning that the Courant number for all simulated time-steps is Co=|u|Δt/Δx<0.81.
The discrete representation of capillary waves with a wavelength close to the minimum spatially resolved wavelength, i.e.
λ→2Δx+, is inherently inadequate and the interface curvature of such waves is numerically ill-defined, which means the dynamics of the interface are no longer accurately represented and which can lead to parasitic currents that impact the result.
Consequently, it is very difficult to numerically generate such spurious capillary waves in a way that allows to compare different time-steps and numerical methodologies and, at the same time, observe the evolution of the spurious capillary waves.
Film flows are ideally suited to study the propagation of capillary waves and, in particular, the time-step associated with capillary waves, since the interface as well as the flow field are clearly oriented.
Moreover, waves with a frequency higher than the neutral stability frequency fc are naturally attenuated by the acting surface tension, whereas waves with a frequency lower than fc evolve into long-wave instabilities [51].
Since spurious capillary waves typically have a frequency higher than the neutral stability frequency, assuming an adequate mesh resolution, any observed short wave amplification is a numerical artifact and not part of the physical solution.
A vertically falling water film in contact with air is simulated to determine the impact of the treatment of surface tension on the capillary time-step constraint and to verify the revised capillary time-step constraint proposed in Section 5.5.
The applied three-dimensional computational domain has the dimensions 100hN×3hN×0.1hN, where hN is the height of the undisturbed falling film (Nusselt height).
The domain is represented by an equidistant Cartesian mesh with a resolution of 10 cells per film height hN.
The liquid phase has a density of ρl=998 kgm−3 and a viscosity of μl=10−3 Pas, the gas phase is assumed to have a density of ρg=1.205 kgm−3 and a viscosity of μg=10−5 Pas.
The surface tension coefficient at the interface separating both phases is σ=7.2205×10−2 Nm−1.
The liquid film flow is simulated with two different Reynolds numbers, as previously considered by Nosoko and Miyara [52]: Re=51.5 with hN=2.51×10−4 m and Re=75 with hN=2.85×10−4 m. At the bottom (liquid-side) wall a no-slip condition is enforced and the top (gas-side) boundary is a free-slip wall.
A semi-parabolic velocity profile is imposed for the liquid phase at the inlet, given as(48)u(x=0,0≤y≤hN)=32(2yhN−y2hN2)uN, where uN is the mean velocity, based on the Nusselt flat film solution [53],(49)uN=ρlghN23μl.
The velocity of the gas phase at the inlet is set to u(x=0,hN<y≤3hN)=1.5uN.
The film is initially flat, with h(x=0)=hN prescribed at the inlet, and the velocity field is fully developed.
The frequency of the shortest spatially resolved capillary waves, fmax≥59.9×103 s−1 for Re=51.5 and fmax≥49.5×103 s−1 for Re=75, are two orders of magnitude larger than the upper bound of naturally amplified frequencies, which is fc=169.3 s−1 and fc=223 s−1 [52], respectively.
Hence, any observed amplification of spurious capillary waves is purely numerical.
In the considered cases, interface waves are perturbed by an explicitly imposed interface curvature of κin=(AΔx)−1 at the inlet, where A is a constant depending on the simulation setup, providing a time-invariant perturbation and natural excitation of instabilities.
In particular the sudden perturbation of the flat film at the beginning of the simulation triggers interface waves with a wide range of wavelengths.
The magnitude of the perturbation, controlled by constant A, has to be chosen carefully, because if the perturbation is too small, capillary waves vanish immediately.
On the other hand, if the magnitude of the perturbation is too large, the largest interface waves triggered by the perturbation break and small-scale capillary waves are absorbed.
For the presented simulations, constant A is ranging from 9.5 to 22.15.
Fig.
7 shows the result of this film flow at Reynolds numbers Re=51.5 and Re=75, simulated with Δt=ΔtσBKZ and treating surface tension as an explicit source term.
In both cases, the interface near the inlet adapts to the explicitly imposed curvature at the inlet and triggers a long-wave instability, clearly visible in Fig.
7 for both Reynolds numbers.
This is expected since a vertically falling film is unstable to long-wave perturbations for all finite Reynolds numbers [53].
No unphysically amplified spurious capillary waves are visible in either case.
In Figs.
8 and 9, results for the simulated Reynolds numbers are shown, this time simulated with Δt=2ΔtσBKZ and treating surface tension as an explicit and an implicit source term.
For both Reynolds numbers the interface waves generated by the perturbation at the inlet are artificially amplified, regardless of the surface tension treatment.
The unbounded amplification of these waves leads, eventually, to the divergence of the solving algorithm.
The type of surface tension treatment, i.e.
explicit or implicit, has evidently no effect on the capillary time-step constraint with respect to numerical stability.
The amplification of spurious capillary waves observed in Figs.
8 and 9 is smaller for the implicit treatment of surface tension for both Reynolds numbers, which can be attributed to the different feedback structure of the explicit and the implicit treatments of surface tension, as illustrated in Fig.
2.
Applying a time-step of Δt=0.99Δtσstat, i.e.
the revised capillary time-step constraint excluding the Doppler shift, to simulate the falling water film using the explicit treatment of surface tension, perturbations at the inlet are unphysically amplified for Re=51.5 as well as Re=75, as shown on the left of Figs.
10 and 11.
However, adapting the time-step dynamically to the revised capillary time-step constraint by taking into account the Doppler shift resulting from the underlying flow motion, Δt=0.99Δtσdyn, the interface waves resulting from the perturbation at the inlet are propagated without amplification for both Reynolds numbers, as shown on the right of Figs.
10 and 11.
The same behaviour can be observed if the magnitude of the perturbation is increased, see Fig.
12, the surface tension is treated implicit, as shown in Fig.
13, or if the mesh resolution is increased, see Fig.
14.
Although small-scale waves are visible in Figs.
11 and 12 applying Δt=0.99Δtσdyn, these waves are not amplified and are advected without aliasing, hence, these waves do not cause numerical problems.
Interestingly, the spurious capillary waves evolve more quickly using the implicit surface tension treatment than with the explicit surface tension treatment for Re=75 and Δt=0.99Δtσstat, as observed in Figs.
11 and 13, contrary to the behaviour observed with Δt=2ΔtσBKZ shown in Fig.
9.
This suggests that an a priori statement about the evolution of spurious capillary waves cannot be made with respect to the surface tension treatment.
It is worth pointing out that the Doppler shift relative to the effective phase velocity of the fastest capillary waves in the presented cases is only uΣk/cˆσ≈0.073 for Re=51.5 and, dependent on the mesh spacing, uΣk/cˆσ≈0.081–0.099 for Re=75.
This difference is small compared to its profound effect on the numerical simulation of capillary waves and the accurate prediction of this sharp transition provides a strong argument for the revised capillary time-step constraint.
The temporal resolution of spatially resolved capillary waves typically imposes a significant restriction on the applied time-step associated with solving the governing equations and is a major contributor to the required computational resources for two-phase flow simulations.
The hypothesised reason for this time-step restriction is the explicit treatment of surface tension and it is widely assumed that an implicit treatment of surface tension would remove or at least mitigate the constraint on the temporal resolution [5,9,14].
To elucidate the actual mechanism for the restriction of the time-step associated with solving the equations governing a two-phase flow dominated by surface tension, this article has revisited the hypotheses for the time-step restriction and has compared the explicit treatment and the implicit treatment of the force resulting from surface tension.
A fully-coupled numerical framework for two-phase flows with an implicit implementation of surface tension has been introduced in this article.
This fully-coupled framework has then been used to compare the influence of the surface tension treatment on the time-step restrictions resulting from capillary waves.
The conducted study demonstrates that restrictions on the numerical time-step resulting from capillary waves are valid and unchanged regardless of the numerical treatment of surface tension.
Since surface tension is not a function of pressure or velocity, the change in implementation does not affect the matrix coefficients of the primitive variables and, thus, numerical stability is independent of the treatment of surface tension.
Further analysis shows that the capillary time-step constraint is a requirement imposed by the spatiotemporal sampling of capillary waves, which is independent of the applied numerical methodology.
The presented analysis of temporal resolution requirements associated with capillary waves reveals shortcomings of the original capillary time-step constraint, as introduced by Brackbill et al.
[5].
A revised capillary time-step constraint has been proposed in Eq.
(45), based on sound numerical and signal processing principles.
The revised capillary time-step constraint also includes the Doppler shift resulting from flow relative to a stationary computational mesh, which has been neglected in previous studies.
The presented results successfully validate the accuracy of the revised capillary time-step constraint and highlight the impact of the Doppler shift.
The authors are grateful to the Engineering and Physical Sciences Research Council (EPSRC) for their financial support (grant EP/K008595/1).
LES of turbulent liquid jet primary breakup in turbulent coaxial air flow A robust two-phase flow Large Eddy Simulation (LES) algorithm has been developed and applied to predict the primary breakup of an axisymmetric water jet injected into a surrounding coaxial air flow.
The high liquid/gas density and viscosity ratios are known to represent a significant challenge in numerical modelling of the primary breakup process.
In the current LES methodology, an extrapolated liquid velocity field was used to minimise discretisation errors, whilst maintaining sharp treatment of fluid properties across the interface.
The proposed numerical approach showed excellent robustness and high accuracy in predicting coaxial liquid jet primary breakup.
Since strong turbulence structures will develop inside the injector at high Reynolds numbers and affect the subsequent primary breakup, the Rescaling and Recycling Method (R2M) was implemented to facilitate generation of appropriate unsteady LES inlet conditions for both phases.
The influence of inflowing liquid and gas turbulent structures on the initial interface instability was investigated.
It is shown that liquid turbulent eddies play the dominant role in the initial development of liquid jet surface disturbance and distortion for the flow conditions considered.
When turbulent inflows were specified by the R2M technique, the predicted core breakup lengths at different air/water velocities agreed closely with experimental data.
Atomisation of liquid jets in coaxial air flow (air-blast or air-assisted atomisation) has been widely used in combustion systems of gas turbines and rocket engines.
Rapid liquid fuel atomisation exerts an important influence on fuel/air mixing, and thus affects combustion performance significantly.
Study of this atomisation process is fundamentally important, but also very challenging.
In order to describe the atomisation of a single round liquid jet injected into a coaxial annular gas flow, the following characteristic non-dimensional parameters have traditionally been used: the gaseous Weber number WeG, liquid and gas Reynolds numbers ReL and ReG, momentum flux ratio M, and momentum ratio MR, defined as:(1)WeG=ρG(UG-UL)2DLσ(2)ReL=ρLULDLμLReG=ρGUGDGμG(3)M=ρGUG2ρLUL2MR=ρGUG2AGρLUL2ALHere, DL is the liquid jet round nozzle diameter; DG is the hydraulic diameter of the annular gas nozzle; AL and AG are cross-sectional areas of round liquid and annular gas nozzles; ρL and ρG are the densities of liquid and gas; μL and μG are liquid and gas dynamic viscosities; UL is the liquid injection speed; UG is the velocity of the coaxial gas flow; finally σ is the liquid surface tension coefficient.
Experimental studies of air-assisted atomisation using a coaxial jet configuration have been carried out by many researchers; a recent review by Dumouchel (2008) has provided a useful summary.
Faragó and Chigier (1992) classified the air-assisted atomisation into five regimes (axisymmetric Rayleigh breakup, non-axisymmetric Rayleigh breakup, membrane breakup, fibre breakup, and superpulsating breakup) via a map of gaseous Weber number vs. liquid Reynolds number.
Lasheras and coworkers (Lasheras et al., 1998; Lasheras and Hopfinger, 2000) carried out their experiments using a different atomiser in term of geometrical dimensions (liquid jet diameter, gas/liquid diameter ratio), and suggested that the momentum flux ratio M is an important and additional parameter to ReL and WeG for a universal classification of air-assisted atomisation.
The primary breakup of a liquid jet in a coaxial flow can be divided into two stages: initial jet surface perturbation is triggered near the nozzle exit; this perturbation is then amplified under the influence of aerodynamic forces, resulting in jet breakup.
When the liquid flow is turbulent inside the nozzle (as is inevitable if the central and/or annular jet Reynolds numbers are large enough), Eroglu and Chigier (1991) and Mayer and Branam (2004) argued that the initial perturbation arose from eddies originating in the liquid jet.
For the case that jets are injected from a nozzle under laminar conditions, Marmottant and Villermaux (2004) suggested that the initial destabilisation is caused by a Kelvin–Helmholtz instability; the most unstable wavelength is then proportional to the thickness of the gaseous boundary layer formed in the annular nozzle.
In the second stage, the initial surface perturbations grow due to aerodynamic interactions, liquid structures protruding from the liquid surface are accelerated by form drag due to the gas flow, making them subject to the Rayleigh–Taylor instability, and finally ligaments and droplets disintegrate from the liquid jet surface.
The liquid core length (or liquid jet breakup length) LC is the axial downstream location where the continuity of the liquid jet discharged from the nozzle exit is interrupted over the entire jet cross-section, and is considered a fundamental and important parameter for evaluation of atomisation performance.
Measurement of LC has been carried out by many authors (see Eroglu et al., 1991; Engelbert et al., 1995; Lasheras et al., 1998; Porcheron et al., 2002; Leroux et al., 2007), and several correlations have been proposed as indicated in Table 1.
Although each correlation shows appropriate agreement with the experimental data from which it was deduced, no single correlation is able to predict correct liquid core length for other experiments.
Since the characteristics of the flow developed inside the nozzles can considerably influence the primary breakup, the liquid core length will inevitably be strongly dependent on the details of the injector geometry.
This is a primary cause of the scatter or discrepancy in currently available empirical correlations.
Another cause of inaccuracy is measurement error in the shadowgraph technique, which has commonly been used in experimental studies.
Droplets stripped off the periphery of the central liquid jet during the early stages of breakup can obscure observation of the liquid core due to the line-of-sight nature of the technique.
A novel optical technique, based on internal illumination of the continuous liquid jet core by Laser Induced Fluorescence (LIF) has been proposed by Charalampous et al.
(2009a,b) for conducting measurements of liquid core length.
Their data demonstrated that LIF can provide more accurate detection of the liquid jet geometry than the shadowgraph technique.
Numerical modelling of liquid jet atomisation has made significant progress since the 1970s, with the promise of eventually achieving as much success as for single phase flow (Fuster et al., 2009; Gorokhovski and Herrmann, 2008).
In the last decade or so, most proposals have adopted the more expensive but more advanced Direct Numerical Simulation (DNS) or Large Eddy Simulation (LES) approach to turbulence modelling in order to capture unsteady effects on the interface dynamics.
Unsteady numerical simulations can show many more details than are possible to capture in experiments, providing further insight into atomisation mechanisms.
However, such numerical modelling of primary breakup of a liquid jet under the influence of strong aerodynamic and turbulence effects is still very challenging, especially for high liquid/gas density ratio O(1000).
Numerical error arising from the high density ratio can be large (sufficient even to cause the simulation to fail), and many published numerical simulations to date have been limited to a liquid/gas density ratio no more than 100 (see Herrmann, 2010, Level Set (LS) interface tracking method; Herrmann et al., 2011, LS; Desjardins et al., 2008, LS; Pai et al., 2008, LS; Kim et al.
(2007), LS; Fuster et al., 2009, Volume of Fluid (VOF); Tomar et al., 2010, VOF; Ménard et al., 2007, Coupled LS and VOF (CLSVOF); Lebas et al., 2009, CLSVOF; Shinjo and Umemura, 2010, CLSVOF).
Since the majority of liquid jet atomisation experiments are carried out at atmospheric pressure with high density liquids, quantitative comparison between numerical modelling and experiment is quite rare.
A robust method capable of dealing with the high density ratio of for example air–water systems is therefore of great interest (Fuster et al., 2009).
In order to deal with this problem, Rudman (1998) proposed to advect the momentum using a density estimated from the interface geometry in cells intersected by the surface, aiming to improve the consistency between interface and momentum transport.
This technique as well as a two-velocity Ghost Fluid Method were investigated by Desjardins and Moureau (2010), although they have not yet been demonstrated to work well in simulating liquid jet atomisation.
Sussman et al.
(2007) proposed an approach using a liquid velocity field extrapolated across the interface into the gas phase region.
This approach was applied by Li et al.
(2010) to simulate a liquid jet in air cross-flow at a high (650) density ratio.
Adaptive Mesh Refinement and the removal of under-resolved small liquid structures were both necessary since the experimental data selected for comparison were far downstream.
In spite of the advanced modelling, agreement with measurements was relatively poor (Li et al., 2010).
A robust two-phase LES algorithm also making use of a modified extrapolated liquid velocity field has recently been proposed by Xiao (2012), and validated against experiments by simulating droplet and liquid jet primary breakup; it was demonstrated that the proposed method showed high robustness and good accuracy compared with experiments for air–water systems.
The objectives of the current paper are therefore: (i) to simulate a round water jet injected into a coaxial air flow at atmospheric pressure (a high density ratio of 830) and compare the predicted results directly with the experimental data and (ii) to investigate the mechanisms behind the initial jet surface disturbance and the liquid jet primary breakup.
Note: the aspects of liquid jet primary breakup which are given particular focus in the present paper are the initial destabilisation of the liquid/air interface and the location of first complete rupture of the jet core.
Whilst the subsequent ligament and droplet formation is captured in the simulations shown, the measured data used do not allow quantitative assessment of these aspects, and, in the far downstream region of the solution domain, the mesh density currently used is inadequate for this purpose, so this has been left for a separate study.
The experimental tests of Charalampous et al.
(2009a,b) are simulated, as the LIF technique used there can provide more accurate measurements of liquid jet core length.
Due to the high Reynolds number of both liquid and gas streams, turbulent boundary layers undoubtedly develop on the internal nozzle walls of the injector system, and may influence significantly the primary breakup process.
As in any LES prediction, the generation of unsteady 3D correlated inlet conditions is a challenging task.
The Rescaling and Recycling Method (R2M) developed by Xiao (2012) and Xiao et al.
(2010) has shown good performance in single phase flows, and will therefore be implemented here.
Since the inflow conditions can be specified as whatever one wishes in numerical modelling, the effect of turbulence on primary breakup is investigated by switching between laminar and turbulent inflow conditions.
Since the breakup length is a parameter of great importance in performance assessment of air-assisted atomisation, the dependence relationship of the breakup length on the characteristic parameters (M, WeG, ReL) will be carefully examined.
In this paper, we first give a description of the two-phase LES formulation and associated numerical methods; the R2M approach for LES inlet condition is then described.
Finally, a detailed analysis of the simulation results and both a qualitative and quantitative comparison between numerical predictions and experiments will be given.
In the current study of two-phase flow modelling, both liquid and gas are assumed to be incompressible and immiscible.
The derivation of the two-phase flow LES formulation is described in detail in Xiao (2012).
The philosophy behind this formulation is as follows: the liquid/gas interface is resolved directly without modelling Sub-Grid Scale (SGS) features; the usual spatially filtered LES formulation is employed in the single-phase flow regions; an appropriate treatment is adopted when discretising the governing equations (when interpolating cell face fluxes) for cells which are intersected by the interface for both phases.
Although a sub-grid interface dynamics model has been proposed by Herrmann (2013) for sub-filter surface tension induced interface motion, it requires an interface tracking or capturing method that can provide significant sub-filter resolution, making it difficult to implement in the current code.
Similarly, a modelling approach to sub-grid surface tension was recently proposed by Aniszewski et al.
(2012) and has produced interesting results by comparing LES results with a-priori DNS results, but this still needs further development and validation before application to the high Re flow which is of prime interest in the current paper.
Therefore, these sub-grid interface models have not been implemented in the current LES formulation.
Effectively, the present LES formulation is similar to that referred to as quasi-DNS/LES in Gorokhovski and Herrmann (2008), i.e.
an under-resolved DNS of interface tracking combined with an LES of the single-phase regions of the flow.
A coupled Level Set (LS) and VOF (Volume of Fluid) method (CLSVOF) is used here to capture and evolve the interface.
The Level Set ϕ is a signed distance function from the interface satisfying ∇ϕ=0.
The interface is defined by ϕ=0, with ϕ>0 representing liquid and ϕ⩽0 representing air.
ϕ is evolved by the simple advection equation using the resolved velocity field (note: in what follows a spatially filtered (resolved) quantity is indicated by an overbar) and ignoring the contribution of any Sub-Grid-Scale (SGS) velocity effects:(4)∂ϕ¯∂t+Ui‾∂ϕ¯∂xi=0 To maintain the signed-distance property, the re-initialisation equation is also solved:(5)∂φ∂τ=S(φ0)1-∂φ∂xk∂φ∂xkS(φ0)=φ0φ02+d2where τ represents pseudo time, S(φ0) is a modified sign function, φ0=φ(xi,τ=0)=ϕ¯(xi,t), d=max(Δx,Δy,Δz).
After solving this equation to the steady state in the interface vicinity, ϕ¯ is replaced by φ.
The VOF function F is defined as the volume fraction occupied by the liquid in each computational cell.
The resolved evolution of the VOF function is governed by:(6)∂F‾∂t+Ui‾∂F‾∂xi=0 With residual or SGS stress tensor τijr modelled by a simple Smagorinsky eddy viscosity approach, the governing equations for the resolved velocity field are:(7)∂Ui‾∂xi=0(8)∂(Ui‾)∂t+∂(Ui‾Uj‾)∂xj=-1ρ∂P‾∂xi+1ρ∂(τij‾+τijr)∂xj+gi+1ρFiST‾(9)τ‾ij=2μSij‾τijr=2μrSij‾Sij‾=12∂Ui‾∂xj+∂Uj‾∂xi(10)μr=ρ(CSΔ)2S‾S‾=2Sij‾Sij‾(11)ρ=ρG+(ρL-ρG)H(ϕ¯)μ=μG+(μL-μG)H(ϕ¯)H(ϕ¯)=1ifϕ¯>00ifϕ¯⩽0Here, gi is gravitational acceleration, Δ represents the filter width, taken as the cube root of the local cell volume and the value of the Smagorinsky constant CS is set in all the calculations reported below to 0.1.
The surface tension term FiST is computed via:(12)FiST‾=σκ¯∂H∂xiκ¯=∂ni‾∂xini‾=-1∂ϕ¯∂xk∂ϕ¯∂xk∂ϕ¯∂xiHere, σ is the surface tension coefficient, κ is the interface curvature, and ni is interface normal vector pointing from the liquid phase to the gas phase.
The SGS term arising from filtering the surface tension term is neglected.
To maintain the implication of a sharp interface, fluid density and viscosity are in the present approach not considered as spatially filtered quantities, but are set to be the properties of liquid or gas depending on the local value of the resolved Level Set variable ϕ¯.
A CLSVOF methodology (Sussman and Puckett, 2000) for interface advection is adopted in the present approach since it offers an optimum combination of the good mass conservation property of the VOF approach and the convenient and accurate capability of LS for evaluation of interface geometrical properties.
Coupling of LS and VOF variables is enforced in the interface reconstruction step where both LS and VOF information can be used to good effect:•the interface normal vector is computed from LS (a smooth function) rather than VOF (a discontinuous function), since LS gives more accurate information on interface location and shape, the interface position in the cell is constrained by the VOF function, since this gives more accurate information on liquid volume conservation.
Based on such a reconstructed interface, the VOF function is evolved to the next time step, and the LS function is corrected.
A flow chart of the CLSVOF method is shown in Fig.
1.
The coupling of the LS and VOF methods occurs during the interface reconstruction and LS-redistance processes.
The detailed algorithm of the present CLSVOF method is as follows:•Initialise the LS and VOF functions at time step n=0: ϕn‾ and Fn‾.
Reconstruct the interface in cells where 0<Fn‾<1.
The interface normal vector ni is calculated from the LS function, and the position of the interface within the cell is constrained by the VOF function.
Advect the VOF function from Fn‾ to Fn+1‾ based on the reconstructed interface.
Advect the LS function from ϕn‾ to ϕn+1,∗‾.
The operator split method is used for solving both equations (see Xiao, 2012 and Sussman and Puckett, 2000 for details).
Reconstruct and constrain the interface in cut cells using the new LS function ϕn+1,∗‾ and the VOF function Fn+1‾.
Perform a re-initialisation step on ϕn+1,∗‾ to obtain the final level set function ϕn+1‾ with a recovered signed distance property.
In the current CLSVOF method, the normal vector is calculated directly by discretising the LS gradient using a finite difference scheme.
By appropriately choosing one of three finite difference schemes (central, forward, or backward differencing), it has been demonstrated that thin liquid ligaments can be well resolved see Xiao (2012).
Although a high order discretisation scheme (e.g.
5th order WENO) has been found necessary for LS evolution in pure LS methods to reduce mass error, low order LS discretisation schemes (2nd order is used here) can produce accurate results when the LS equation is solved and constrained as indicated above in a CLSVOF method (see Xiao, 2012), since the VOF method maintains 2nd order accuracy.
This is a further reason to adopt the CLSVOF method, which has been used for all the following simulations of liquid jet primary breakup.
Since the convection and diffusion terms are discontinuous across the interface, a cautious first order forward-Euler projection method was used for temporal discretisation of the two-phase flow governing equations (for more details see Xiao (2012)).
First, an intermediate velocity is computed from convection, diffusion and gravitational terms (spatial discretisation is described below) (NB surface tension is treated via the pressure term using the ghost-fluid approach (Fedkiw et al., 1999; Kang et al., 2000; Liu et al., 2000)):(13)Ui∗‾-Uin‾δt=-∂(Uin‾Ujn‾)∂xj+1ρn∂(τijn‾+τijrn)∂xj+gi Second, the intermediate velocity field is updated using a pressure gradient term to obtain the velocity at time step n+1:(14)Uin+1‾-Ui∗‾δt=-1ρn∂Pn+1‾∂xi Since the velocity field at time step n+1 must satisfy the continuity equation, a pressure Poisson equation may be derived by taking the divergence of the above equations to allow Pn+1‾ to be calculated:(15)∂∂xi1ρn∂Pn+1‾∂xi=1δt∂Ui∗‾∂xi The variables are arranged in a staggered manner in the current two-phase flow LES formulation: the pressure, LS and VOF are located at the cell centre; velocity components are located at corresponding faces.
In general, 2nd order central methods are used to discretise spatial derivatives.
Since the gas phase has a much smaller density and viscosity than the liquid phase, the velocity gradient in the gas phase is typically much larger than liquid phase.
However, for discretisation of the momentum equation for cells in the vicinity of the interface, it was found important to specify correctly which velocity should be used when calculating cell face fluxes associated with convection and diffusion terms.
If the velocity in the adjoining gas phase cell was used to discretise the momentum equation for an interface cut liquid phase cell, large momentum errors were induced in the vicinity of the interface.
In order to reduce this momentum error (arising from the density jump across the interface) an extrapolated liquid velocity field UiL‾ was introduced for spatial discretisation of convection and diffusion terms for interface cut cells.
The technique of an extended velocity field was used by Nguyen et al.
(2001) to simulate incompressible flames.
A projection method was used by Tanguy et al.
(2007) to impose a divergence-free condition for the extrapolated velocity field across the interface to tackle the problem of parasitic currents in their simulations of two-phase vaporising flows.
Though an extrapolation projection approach was used by Sussman (2003) to impose divergence-free condition in simulations of bubble growth and collapse, it was abandoned by Sussman et al.
(2007) in their later numerical formulation of incompressible two-phase flows.
In the current formulation, a constant extrapolation technique (Fedkiw et al., 1999; Aslam, 2003) was used for liquid velocity extrapolation, and an original method was developed to imposed the divergence free condition for the extrapolated liquid velocity.
For a detailed implementation of this technique, readers are referred to Xiao (2012).
It was also demonstrated in Xiao (2012) that: (i) a divergence free step for the extrapolated liquid velocity field was necessary to reduce the momentum error when the interface moved rapidly across a fixed grid and (ii) a more accurate capture of interface dynamics was obtained when the extrapolated liquid velocity field was also used in the VOF and LS transport equations.
The methodology developed in Xiao (2012) was used for all simulations reported here, including the use of a Box multigrid (Dendy, 1982) preconditioned conjugate gradient method for solution of the Poisson equation.
Fig.
2 summaries the final procedure of the developed two-phase flow LES.
The detailed algorithm for the current two-phase flow LES is as follows:•Based on the interface represented by the LS function ϕn‾, discretise the two-phase flow governing equations to solve for the velocity field at the next time step Uin+1‾.
Construct the extrapolated liquid velocity field UiL,n+1‾ using the extrapolation technique described in Xiao (2012).
Ensure continuity for the extrapolated liquid velocity ∂UiL,n+1‾∂xi=0 in the gas phase by a divergence free step.
Based on UiL,n+1‾ advect the LS and VOF functions to the next time step to obtain ϕn+1‾ and Fn+1‾ using the CLSVOF algorithm in Section 2.2.1 and Fig.
1.
Set the velocity in CVs which change from gas to liquid (i.e.
ϕn‾⩽0 but ϕn+1‾⩾0) to liquid velocity via Uin+1‾=UiL,n+1‾.
Repeat for further time steps.
The numerical implementation of the methodology described above was carried out in an existing multi-block structured mesh code for Large Eddy Simulation (LES) of single phase constant density turbulent flow.
A full description of the numerical techniques adopted in this code (LULES) may be found in Tang et al.
(2004).
The code solves for contravariant velocity components on a 3D orthogonal mesh, to give the code some complex geometry capability.
It has been validated against a range of incompressible turbulent flow problems, for example the unsteady aerodynamics of high swirl fuel injectors (Cheng et al.
(2012).
For the test cases reported here only Cartesian or cylindrical polar meshes are needed.
The Rescaling and Recycling Method (R2M) proposed in Xiao et al.
(2010) and Xiao (2012) has demonstrated a validated capability for generation of unsteady 3D correlated velocity fields for specification of LES inflow conditions.
The R2M technique was therefore used here to generate nozzle exit conditions matched as far as possible to the experimental configuration selected for LES of liquid jet primary breakup.
In order to generate realistic unsteady inflows for LES using the R2M technique, target values for statistical quantities (mean velocity and Reynolds normal stress (or rms intensity) fields) at the “main simulation” (MS) solution domain inlet plane need to be prescribed (i.e.
for axisymmetric flow inside both liquid and gas nozzles: Utarget (r), Vtarget (r), Wtarget (r), utarget′(r), vtarget′(r), wtarget′(r), where r is the radial co-ordinate and U, u′ (etc) represent mean and rms velocities respectively.
How these values are specified and the R2M technique applied is described below.
First, extra “inlet condition” (IC) solution domains (single block or multi-blocks as necessary) are created upstream of the inlet plane of the MS domain (here the nozzle exit plane).
The inflow conditions for the extra IC domain are generated by recycling the velocity field from a selected plane in the downstream region of the IC domain.
By rescaling the velocities, and solving the LES equations in the IC domain until a statistically stationary state is achieved, the resulting instantaneous flow field within the IC domain can achieve the target statistical characteristics whilst also possessing self-consistent spatial and temporal correlations.
This unsteady velocity field is then fed into the MS domain as inlet conditions.
In the simulation of round liquid jet primary breakup in coaxial gas flow, two cylindrical IC domains for liquid and gas respectively need to be created; in these domains cylindrical polar co-ordinates (x, r, θ) and velocity decomposition were used for convenience.
The generation of unsteady conditions for the liquid flow in the central round nozzle as well as the gas flow in the surrounding annular nozzle using the R2M approach was carried out as follows:1.Cylindrical IC domains within the central nozzle (and annular gas nozzle) were specified upstream of the MS domain inlet plane (nozzle exit).
The size of the IC domains in the radial and azimuthal (r, θ) directions were set by the injector nozzle geometry.
The size in the streamwise (x) direction was chosen so that the two point axial spatial correlations would fall to zero well within the IC domains, as required by the recycling technique and also to prevent the generated axial turbulent integral scale being constrained by the solution domain size.
Use recycling to provide inflow conditions for the IC domains.
The velocity field at a plane a short distance upstream of the IC domain outlets was recycled.
In addition, it is important that the mesh in the IC domains should be uniform in the streamwise and azimuthal directions to avoid any varying spatial filtering effects.
Initialise the velocity field in the IC domains; the instantaneous velocity field was generated by superimposing white noise with an intensity of utarget′(r), vtarget′(r), wtarget′(r) onto the mean velocity Utarget (r), Vtarget (r), Wtarget (r).
Run the simulation in both IC and MS domains simultaneously.
Rescale the flow field everywhere within the IC domains every k time steps (k=5 used in the current simulations) in the following way (described here for the axial component only for simplicity):•Calculate the mean velocity by spatial averaging in the x and θ directions (homogeneous directions) and temporal averaging with a weight that decreases exponentially backward in time (see Lund et al.
(1998) for more about temporal averaging):(16)Umeann+1(r)=kΔtT〈Un+1‾(x,r,θ,t)〉x-θ+1-kΔtTUmeann(r)(17)=kΔtT1PQ∑i=1P∑j=1QUn+1‾(xi,r,θj,t)+1-kΔtTUmeann(r)Δt is the computational time step, T is a characteristic time scale for the temporal averaging (in the current study chosen to be 10 times the time scale of the largest eddies), 〈 〉x–θ represents spatial averaging in the x–θ plane (containing a maximum of P and Q cells in the respective directions); Un+1‾(x,r,θ,t) is the current instantaneous solution.
Calculate the rms velocity in a similar way:(18)urmsn+1(r)=kΔtTUn+1‾(x,r,θ,t)-Umeann+1(r)2x-θ+1-kΔtTurmsn(r)2 Rescale the instantaneous velocity to create a new instantaneous velocity field:Un+1‾(xi,r,θj,t)=utarget′(r)urmsn+1(r)Un+1‾(xi,r,θj,t)-Umeann+1(r)+Utarget(r) (19)i=1,P;j=1,Q Rescale the other two components V and W using the same procedure.
In the section, the developed two-phase flow CLSVOF LES formulation is applied to predict the primary breakup of a single cylindrical water jet in a coaxial annular air flow.
As stated above, the test cases from Charalampous et al.
(2009a,b) are simulated.
The coaxial air-blast atomiser used is shown in Fig.
3.
The diameter of the liquid nozzle is DL=2.3mm; the inner and outer diameters of the annular gas nozzle are 2.95mm and 14.95mm respectively.
Table 2 lists the flow conditions simulated here (UG and UL are the area-averaged velocities at the exit of the nozzle and M is the gas to liquid momentum flux ratio).
The experiment was carried out with water and air at atmospheric pressure, so fluid properties used in the simulations are: ρG=1.205kg/m3, μG=1.836×10−5Pas, ρL=1000.0kg/m3, μL=0.848×10−3Pas, and surface tension coefficient σ=0.072N/m.
Note that both ReL and ReG are high enough that turbulent flow exiting the nozzle may be expected.
An initial simulation with uniform and laminar inflows for both water and air was first carried out, with the results shown in Section 4.2.
The R2M technique was then applied to generate realistic turbulent inflows and this simulation is presented in Section 4.3.
The mechanism of initial interface disturbance and subsequent liquid jet primary breakup is investigated in Section 4.4.
Liquid jet structures for a range of flow conditions were then simulated and are compared with experimental images in Section 4.5; a comparison of the LES-predicted liquid core length extracted from these simulations against the experimental data is given in Section 4.6.
Finally, the property of liquid volume conservation in the current LES predictions of liquid jet primary breakup is examined in Section 4.7.
The simulation domain co-ordinates were [0, 54], [−6, 6], [−6, 6] mm in the x, y and z directions respectively.
The axis of the nozzle lies in the x direction, with the Cartesian co-ordinate origin at the central nozzle centre.
Fig.
4 shows the Cartesian mesh used in the simulation.
In order to provide good resolution of the initial stages of primary breakup, a uniform fine mesh was used in the region [0, 54], [−2.43, 2.43], [−2.43, 2.43] mm with a cell size of 0.09mm (i.e.
∼DL/25); in the outer region of the domain, an expanding mesh was used to reduce computational cost.
The number of cells was 600×80×80 in x, y, z directions (4million in total).
The mesh in a part of the inner region is shown in Fig.
4; the red region corresponds to the initial condition for the liquid phase, i.e.
the cells where VOF was initialised to 1 and LS was set to be a positive distance to the interface.
Uniform and laminar inflows were first specified for both water and air streams at the nozzle exit.
Fig.
5 shows an instantaneous liquid jet structure predicted by LES for UG=47m/s and UL=4m/s; this shows several significant differences in comparison with the experimental shadowgraph image.
The predicted liquid jet is continuous without any breakup while ligaments and droplets are observed in the shadowgraph, and there is evidence that core continuity is disrupted towards the end of the experimental image.
Some small disturbances are also observed on the interface near the nozzle exit in the experiment.
However, in the simulation the predicted interface is very smooth in the first three DL downstream of nozzle exit; only further downstream do surface disturbances appear and grow, but slowly.
Since the internal flows inside the nozzles will be turbulent due to the high Reynolds numbers, turbulent eddies emerging from nozzle exit may destabilise the interface.
This simulation underlines the importance of accurate representation of nozzle exit conditions for correct simulation of turbulent liquid jet primary breakup.
As detailed above, in order to generate realistic LES turbulent inflow conditions using the R2M technique, mean velocity and rms profiles that are representative of experimental conditions at the nozzle exit are required as input.
2D axisymmetric RANS predictions using a low Re Reynolds Stress Transport turbulence model were therefore run using the Fluent CFD code for the internal nozzle flows (central (water) nozzle and outer (air) nozzle separately).
Fig.
6 shows the simulation domain for the water flow inside the central nozzle of the atomiser.
Note the presence of the optical fibre used for the laser illumination was included (white region in bottom left), since this represents a blockage to the internal water flow.
Predicted contours of turbulent kinetic energy (TKE), are shown to provide an indication of the rapid boundary layer growth on the nozzle wall at this Reynolds number.
A quad mesh with 180,000 elements was used, with the mesh generated to achieve values of y+∼0.1mm near the wall.
Fig.
7 shows the predicted radial profiles of mean streamwise velocity U, and the three normal stress intensities u′, v′, and w′ at the nozzle exit.
These formed the required R2M data input for the liquid flow.
Fig.
8 shows the TKE contours obtained for the gas flow inside the annular nozzle.
Turbulent boundary layers were also observed to develop on the walls after the contraction section.
A quad mesh with 170,000 elements was used.
Fig.
9 shows the predicted profiles of mean streamwise velocity and rms levels for air flow at the nozzle exit.
Again these formed the required target data for the R2M technique.
Figs.
10 and 11 show the domain for two-phase flow LES when realistic turbulent inflows were generated using R2M at nozzle exit.
The main simulation (MS) domain for resolving the liquid jet primary breakup was the same as that used in Section 4.2, and used the same Cartesian mesh.
The extra cylindrical IC domains created for both water and air flows emerging from the atomiser nozzle are shown upstream of the MS domain.
The cross-stream dimensions of the two IC domains were set to match those of injector nozzles at the nozzle exit plane.
Thus, the co-ordinates of the water flow IC domain were: [−8.4, 0.0], [0, 1.15], [0, 2π] in x, r, θ directions respectively; the co-ordinates of the air flow IC domain were [−8.4, 0.0], [1.475, 7.475], [0, 2π].
A cylindrical mesh was used to resolve the turbulent boundary layers inside the nozzles.
A uniform mesh was used in streamwise and circumferential directions whilst in the radial direction a finer mesh was used in the expected boundary layer regions.
Both IC domains had 58×52×68 cells.
In the LES calculations of the flow in the IC domains, the instantaneous velocities from the plane x=−0.15mm were recycled for use as inflow velocities to the IC domain inlet as explained above; a convective outflow condition was used at the outlet.
Figs.
12 and 13 show that the mean and rms values of the velocity field predicted in the IC domains agree well with the input target profiles.
The profiles at locations x=−1, −4, −7mm collapse together, indicating that the predicted turbulent flow is homogeneous in the streamwise direction.
Simulations in all domains were run simultaneously with the same time step.
At every time step, the instantaneous velocities from a selected plane (x=−0.0048m was used) were mapped from the cylindrical polar IC domain mesh onto the Cartesian MS domain mesh at the inlet plane of the MS domain.
Figs.
14 and 15 show contours of instantaneous water and air streamwise velocity at plane x=−0.0048m and also at the MS domain inlet.
It may be observed that the velocity fields at the two planes show the same large eddy structures, indicating a correct mapping process.
There is no doubt that the smallest resolved eddy structures are somewhat smoothed in this mapping due to a finer cylindrical mesh being used in the IC domain compared to the Cartesian mesh in the MS domain, but it is not believed this introduced significant error, since these eddies were of course much less energetic than the large scale structures.
Fig.
16 presents contours of the instantaneous streamwise velocity in the z=0 plane for both phases.
It is observed (especially evident for the liquid phase) that the turbulent eddies developing downstream of the MS domain inlet have similar structures as those in the IC domain, indicating that the turbulent eddies developed by the R2M technique in the nozzles were convected downstream as the liquid and gas were injected from their respective nozzle exits.
Fig.
17 compares the LES predicted interface topologies near the nozzle exit with the two different inflow treatments.
In contrast to the predicted smooth interface region observed when applying uniform and laminar inlet conditions, the interface is disturbed by turbulent eddies right after the jets exit the nozzles when using turbulent inflow conditions.
Fig.
18 shows the overall liquid jet structure predicted by LES when turbulent inflows are specified for both phases using R2M.
The growth of the surface disturbance in the liquid jet under aerodynamic forces is well reproduced by LES, and the breakup point of the liquid core now agrees well with the experiment.
Ligaments and droplets are ejected from the resulting liquid clusters, which is consistent with shadowgraph observations.
It is an interesting question as to whether it is liquid or gas flow turbulence (or both) which exerts the larger influence upon the interface, and this will be investigated in the following section.
In order to investigate in more detail the mechanism behind the initial interface instability and disturbance, two further simulations were performed: one with turbulent air inflow but uniform and laminar water inflow, and the other with these conditions reversed.
Views of the predicted liquid jet structures are shown in Fig.
19 together with results from the simulations described above using either both laminar or both turbulent inflows.
It is evident that the two-phase interface is disturbed immediately after the nozzle exit in the two simulations with turbulent water inflow, while a marked region of smooth interface exists downstream of the nozzle exit in the two simulations with laminar water inflow.
The liquid eddies rather than the gas eddies are clearly responsible for the initial interface disturbance, which is indeed rational since the liquid has a much larger inertia than the gas.
When uniform laminar inflow was specified for the liquid phase, the initial interface disturbance is mainly due to the shear force exerted by the gas flow.
It is thus perhaps surprising that the interface predicted with turbulent gas inflow had a longer smooth region than with laminar gas inflow.
The explanation is provided by Fig.
20.
With a turbulent boundary layer, the simulated gas flow had a larger low velocity recirculation (backflow) region behind the nozzle lip separating liquid and gas flows, and the gas velocity in the cell adjacent to the interface reached 10m/s only by x=4.4mm.
In contrast, when laminar inflow was used for the gas phase, the gas flow in the cell adjacent to the interface increased to 10m/s only 2mm after nozzle exit, and consequently destabilised the interface a shorter distance downstream of nozzle exit.
After the initial interface disturbance, the liquid jet surface perturbations grow under the aerodynamic interactions between gas and liquid flows, resulting in primary breakup.
Fig.
21 shows the velocity vector field and pressure contours in a slice through the jet centreline.
It is observed that gaseous vortices are predicted by the current LES in the wave troughs and are well resolved by the mesh.
For the protruding interface waves, the pressure on the upstream side is higher than on the downstream side, and thus the form drag arising from this pressure imbalance accelerates the protruding liquid.
Since the heavier liquid is accelerated by the lighter gas flow, Rayleigh–Taylor instability can develop on the protruding liquid waves, and thus finally ligaments and droplets are created and separate from the liquid jet surface as shown in Fig.
18(b); the liquid jet core then disintegrates into liquid clusters (see Figs.
18 and 19) which undergo further evolution under the action of aerodynamic forces leading to eventual complete primary breakup.
Fig.
22 compares the liquid jet structures predicted by the current CLSVOF LES method with shadowgraphs taken by Charalampous et al.
(2009a) for liquid jet primary breakup with different coaxial air velocities.
The breakup morphologies of the continuous liquid jet are well reproduced by the current method for all conditions.
As the air velocity increases, the predicted location where drops and ligaments are first seen moves towards the nozzle, in good agreement with experimental observations.
Due to the increasing aerodynamic forces, the dimensions of the predicted liquid ligaments and droplets resulting from the primary breakup becomes smaller.
If these structures enter a region where mesh resolution is inadequate, then the disintegration of these liquid structures into smaller droplets (secondary breakup) in the furthest downstream region is probably not well resolved for the high speed air velocity cases (UG=119m/s and 166m/s).
Fig.
23 shows simulated primary breakup at three different liquid injection velocities with a fixed coaxial air flow of 70m/s, together with shadowgraph images at the corresponding flow conditions.
As the liquid velocity increases, the liquid jet interface before the breakup point becomes noticeably rougher, due to the disturbing liquid eddies become more energetic as the Reynolds number of the liquid flow inside the nozzle increases from 5440 to 21,770.
This physical phenomenon is correctly captured by the current LES but obviously only when the R2M approach is applied to generate the turbulent inflow boundary conditions.
As the liquid injection velocity grows from 2m/s to 8m/s, the liquid jet breakup position moves downstream, in good agreement with the experimental images.
In the simulations, the liquid core length was determined by examining the cross-sectional area ratio of the liquid jet which was illuminated by the laser beam, in a manner very similar to the experimental LIF technique.
Fig.
24 demonstrates the liquid area (A) which is illuminated directly by the laser beam originating at the nozzle exit at plane x=19mm assuming total internal reflection of the light from the interface surface as done in the experiments.
A 2D sketch is provided but the illuminated area in 3D is calculated in the same way.
The ratio of area illuminated by laser beam to liquid nozzle exit area is calculated by α=A/A0.
The predicted variation of α in the x direction at t=24ms is shown in Fig.
25.
A small but non-zero value of α is chosen as the criterion to determine the liquid core length.
Fig.
26 shows the instantaneous and mean liquid core length calculated basing on the criterion α=0.05 for the case UG=47m/s, UL=4m/s when turbulent inflows were used for both phases.
A difference of 5% in the simulated liquid core length is observed when changing α from 0.05 to 0.1.
Fig.
27 shows the liquid core length predicted by the current two-phase flow LES for flows 1a–d in Table 2; UL is 4m/s in all cases, with the air velocity varied to produce different Weber numbers.
When laminar inflows were used for both phases, the predicted liquid core length was (as expected from the discussion above) much larger than the experimental measurements of Charalampous et al.
(2009b) for lower Weber numbers.
When turbulent inflows were specified for both phases, the simulated core length agreed very well with the experimental value for all Weber numbers, confirming that the initial interface perturbations caused by liquid eddies plays an important role in the resulting surface instability development and primary breakup process.
For the cases with higher gaseous co-flow (UG=119m/s and 166m/s), strong aerodynamic forces dominate the primary breakup process, and the liquid jet core is destroyed in a short distance and the difference of predicted core length between simulations with different inflow conditions is much smaller.
Overall, liquid core lengths are well reproduced by the developed two-phase flow LES method for all four flows when realistic turbulent inflows are provided for both phases using the R2M technique.
Fig.
28 shows predicted liquid core lengths for different liquid velocities with the same air co-flow (UG=70m/s).
For the two lower liquid injection velocities, the core length predicted by the current LES agrees well with the experimental measurements, but the predicted value is considerably larger than the LIF measurement for the case with highest liquid velocity UL=8m/s.
This marked difference is in all probability due to measurement error.
Charalampous et al.
(2009a) observed a significant decrease of the LIF signal intensity along the jet length, due to scattering of laser light by refraction at the water–air interface which was much more wavy at the highest liquid velocity.
This was observed to lead to undervalued measurements of liquid jet core length.
It is noted in Fig.
23(c) that few droplets are stripped off the liquid jet in the test section for this case (UL=8m/s).
In this instance, the shadowgraph technique is able to provide an accurate measurement, and indicated a core length of ∼13DL, in good agreement with the current LES.
Fig.
29 shows that the predicted liquid core length was a power law function of the momentum flux ratio when turbulent inflows were specified.
As the gas velocity increases while the liquid velocity was kept at 4m/s, the predicted liquid core length decreased with a power law exponent of −0.39.
As the liquid velocity increases while the gas velocity was kept at 70m/s, the predicted core length decreased with a power law exponent of −0.35.
The simulations carried out for uniform and laminar inflows produced a predicted power law exponent of −0.5.
These predictions are consistent with the power law exponents (from −0.5 to −0.3) reported based on a range of experimental measurements (see Lasheras et al., 1998; Engelbert et al., 1995; Leroux et al., 2007), and confirms the possibility that the difference of exponent values reported by different authors results from different atomisers used.
Finally, Fig.
30 indicates that the predicted liquid core length (with turbulent inflows) decreases as a power law function of Weber number when the liquid velocity is kept at 4m/s, with a power law exponent of −0.39, which is in good agreement with the value of −0.4 reported by Eroglu et al.
(1991).
It is very important to conserve fuel mass in simulations of fuel atomisation in an engine, and hence the liquid volume error in any method that claims to be a good candidate for primary breakup simulation is of prime importance.
This was one of the main reasons for adopting the CLSVOF methodology.
This particular aspect was therefore examined in the present simulations for a test case with UL=4m/s and UG=166m/s.
When it was judged that the simulated liquid jet primary breakup had reached a statistically stable state (e.g.
as shown in Fig.
26), the following three quantities were calculated from the LES results (the solution domain considered was the region enclosed by the black line in Fig.
31, an axial distance of 9.4DL, corresponding to 230% of the liquid core length for this case): the liquid volume injected into the domain at inlet during a time period t: Vin(t), the liquid volume flowing out of the outlet Vout(t), and the liquid volume change in the domain over this time period: ΔV(t)=V(t)−V(0) (V(t) is the liquid volume in the domain at time (t)).
The first two were estimated from surface integrals on the domain boundaries and the third by a volume integral of the VOF function within the domain.
If liquid mass is conserved, the three quantities should satisfy the relation: Vin(t)=Vout(t)+ΔV(t).
Fig.
32 presents the liquid volume budget for the simulation domain shown in Fig.
31.
Since the liquid mass influx is constant, Vin(t) grows linearly with time.
However, the rate of liquid volume exiting the simulation domain varies due to turbulent fluctuations and flapping of the liquid jet, and this also results in a temporal variation of ΔV(t).
It is observed in Fig.
32 that the calculated value of Vout(t)+ΔV(t) collapsed onto with an error of only 0.01%, demonstrating that the liquid volume is conserved well in the simulation of liquid jet primary breakup.
In order to examine whether the divergence free step applied to the extrapolated liquid velocity field was influential in the conservation performance of the scheme, a simulation without the divergence free step was run.
The result shown in Fig.
33 indicated that a significant increase in liquid volume error was observed in the primary breakup process (to 3.6%), confirming the importance of this part of the numerical algorithm.
The mesh used in the above simulations is referred to now as Mesh I.
A finer mesh, named Mesh II, was generated with a cell size of 0.06mm in the central uniform-mesh region of the MS domain (a 50% reduction in grid spacing); the mesh in the IC domain was kept the same as that in Mesh I.
Mesh II was used to investigate the influence of mesh size, and the results obtained are described below.
Due to the high liquid/gas density ratio of the present flow problem (∼800) and the high Re of the flow, the initial interface perturbation of a turbulent liquid jet in a turbulent coaxial gas flow and its early stage development is caused not by any fundamental Plateau–Rayleigh or Rayleigh–Taylor instability, and also not by gaseous turbulent eddies in the gas-flow boundary layer, but by the liquid turbulent eddies convected out of the nozzle, and this is clearly shown in Fig.
19.
The large liquid eddies have large turbulent energy and cause large-scale interface disturbance and the restoring surface tension corresponding to large-scale interface disturbances is weak due to small curvature.
Small-scale liquid eddies (including those not resolved in the LES) have small turbulent energy and cause small-scale interface disturbance, the restoring surface tension corresponding to small-scale interface disturbances is strong due to large curvature.
Therefore, the small-scale disturbances due to small eddies will be smoothed quickly by the strong surface tension near the nozzle exit.
The development of the large-scale disturbance due to large liquid eddies (well resolved by the current LES mesh and turbulence inlet condition treatment) thus dominates the interface disturbance which is principally responsible for the subsequent bulk liquid core breakup location.
Fig.
34 shows the liquid column surface disturbance due to liquid eddies on Mesh I and II for the case UL=4m/s where the gas velocity has also been reduced to 4m/s to minimise the effects of aerodynamic forces.
It is seen that the resulting large-scale interface disturbances are well resolved on both Mesh I and Mesh II.
The small-scale disturbances which are under-resolved on Mesh I but appear on Mesh II have only a marginal effect on the development of the surface disturbance.
After the initial interface distortion develops due to the liquid eddies, high and low pressure regions form as the gas flows around the disturbed but still attached interface structures.
The form drag due to upstream high pressure on the protruding structures and low pressure on their lee side accelerates the protruding liquid, amplifying the structure growth, which eventually results in breakup of the liquid jet column as well as formation of large liquid clusters (note that the shear drag from the gas boundary layer contributes little at high Re O(103–104)).
Therefore the important processes leading to breakup of the bulk liquid column are well resolved on the current mesh, assuring the validity of this numerical scheme for prediction of liquid core length.
It is also observed that the pressure on the upstream side of the protruding interface wave is higher than that on the wave crest, causing further protrusion of the liquid structure.
For the case with gas velocity of 47m/s the formation of protruding liquid structures and ligaments is adequately resolved as far as the column breakup process is concerned on the current mesh as the comparison with experiment shown in Figs.
18 and 22 demonstrates.
For cases with higher velocities, the breakup of the created liquid ligaments may be under-resolved, but this should not impact negatively on the liquid column breakup process.
Whilst under-resolved or inadequate SGS modelling might influence small scale droplet characteristics, it should not affect the liquid jet core breakup length LC.
This was confirmed by a grid resolution study carried out for the case with the highest gas velocity (166m/s) and the lowest liquid velocity (4m/s) where the resolution problem is most demanding.
Fig.
35 shows the predicted liquid jet breakup for this case on Mesh II, which may be directly compared with the Mesh I results seen in Fig.
22c.
While more small drops and ligaments are resolved on Mesh II than on Mesh I in the far downstream region, the liquid core breakup process and the resulting large liquid clusters are the same on both meshes.
Although the pinch-off of liquid ligaments and drops from the liquid jet core and the surface tension are probably under-resolved, this does not affect significantly the breakup process of the liquid core due to the following.
First, the resulting ligaments and droplets move and disperse in the radial direction and thus do not influence the liquid jet core behaviour.
Second, the breakup of the liquid jet core into clusters is still dominated by the aerodynamics rather than the surface tension due to the high Weber number based on the dimensions of the liquid clusters.
The liquid core length predicted on Mesh II only shows marginal difference (5% longer) from that on Mesh I.
Therefore, the breakup processes of the liquid column as a whole is well resolved on the current mesh, assuring the validity of this numerical scheme for prediction of liquid core length.
The primary breakup of a round water jet in coaxial air flow was simulated using a developed two-phase flow CLSVOF LES methodology.
The Rescaling/Recycling Method (R2M) developed in Xiao et al.
(2010) and Xiao (2012) was successfully implemented to generate unsteady turbulent inflows with appropriate statistical properties for both liquid and gas phases in simulations of air-blast atomisation.
When these realistic turbulent inflows generated by R2M were used, the simulated jet structures agreed qualitatively with experimental shadowgraph images and the predicted liquid jet breakup length agreed quantitatively well with experimental measurements.
The simulation results indicated that the liquid jet breakup length was a power law function of the gas/liquid momentum flux ratio with a power law exponent from −0.39 to −0.35.
By specifying different inlet conditions for liquid or gas flows, the effect of turbulent eddies developed inside the injector nozzles could be examined.
It was found that liquid turbulent eddies, which will exist under the high Re conditions found in practice, are responsible for the initial interface perturbations.
If laminar liquid inflow were to occur, the mean shear stress from the gas flow would then be the primary cause of the initial interface instability.
The developed two-phase flow LES methodology showed excellent numerical robustness and accuracy in modelling liquid jet primary breakup at high liquid/gas density ratio.
The work reported here was carried out in the Loughborough UTC in Combustion System Aerothermal Processes.
Financial support from EPSRC (SAMULET project and Dorothy Hodgkin Award for the first author) is gratefully acknowledged.
The influence of glass composition on crystalline phase stability in glass-ceramic wasteforms Zirconolite glass-ceramic wasteforms were prepared using a suite of Na2O–Al2O3–B2O3–SiO2 glass matrices with variable Al:B ratios.
Zirconolite was the dominant crystalline phase only for the most alumina rich glass compositions.
As the Al:B ratio decreased zirconolite was replaced by sphene, zircon and rutile.
Thermodynamic data were used to calculate a silica activity in the glass melt below which zirconolite is the favoured crystalline phase.
The concept of the crystalline reference state of glass melts is then utilised to provide a physical basis for why silica activity varies with the Al:B ratio.
Glass-ceramic wasteforms are being developed for the immobilisation of a diverse range of plutonium containing residues on the Sellafield site [1,2].
The target crystalline phase is zirconolite (CaZrTi2O7), which acts as the host for plutonium, whilst the glass matrix is intended to digest the remainder of the residue.
The wasteforms are to be produced by size reduction of the residue streams, blending with an appropriate precursor and consolidation by hot isostatic pressing (HIP).
It is emphasised that the term glass-ceramic as used here differs from the established Materials Science concept of a glass-ceramic in that the zirconolite phase forms at the maximum HIP temperature, rather than by separate nucleation and growth heat treatments of a glass.
The current glass-ceramic precursor composition for immobilisation of the residues was derived empirically and is given in Table 1, and a typical glass-ceramic wasteform microstructure is shown in Fig.
1.
The formulation in Table 1 arose because early inventories for the residues that would require immobilisation included a large quantity of calcium fluoride slags resulting from production of plutonium metal.
To achieve the desired high waste loadings of this slag would lead to a low durability glass matrix with conventional glass formulations, so high alumina contents were used to improve the glass matrix leach resistance.
Subsequently, the slags were removed from the residues inventory, greatly lowering the fluoride content of the glass matrix, but the high alumina level was retained.
Consequently, calcium fluoride was required in the precursor to facilitate digestion of the residues and enhance growth of the zirconolite grains.
During process development work, a 100kg batch of this precursor was prepared.
An error in the batching was made such that the quantities of alumina and titania were interchanged.
When this composition of precursor was HIPped, characterisation studies revealed that zirconolite had been destabilised, despite the fact that none of the zirconolite forming oxides had been reduced; this indicated that the glass composition plays an important role in determining the crystalline phase assemblage.
Our initial hypothesis was that the lower alumina content of the misformulated precursor allowed the calcium oxide that is required to form zirconolite to remain dissolved in the glass.
The formulation in Table 1 was derived by an empirical approach and led to a non-classical glass matrix.
Carter et al.
[3] and Zhang et al.
[4] took a more systematic approach to such glass-ceramic wasteforms.
These wasteforms were targeted at Hanford K-basin sludges and the immobilisation of the primary waste stream from production of molybdenum-99 at the Australian Nuclear Science and Technology Organisation site in Sydney respectively.
In the work of Carter et al.
and Zhang et al.
the intended crystalline phase was the closely related titanate pyrochlore, CaUTi2O7.
The glass matrix was formulated such that the trivalent species in the glass network, boron and aluminium, were charge compensated on a molar basis by sodium.
The stoichiometric composition of the glass in this wasteform was Na2AlBSi6O16.
This glass provides a method by which the glass composition can be varied systematically.
Given that the initial observations inferred an important role played by alumina, it was decided to prepare a suite of zirconolite glass-ceramics in which the glass matrix was defined by Na2Al1+xB1–xSi6O16 to investigate the role played by glass composition in controlling crystalline phase stability.
The x=1 end member gives the mineral albite, NaAlSi3O8.
The melting point of albite is 1120°C [5] and the composition cools to a glass at the cooling rates that occur during a HIP cycle.
From the available phase diagrams, [6] no boron analogue for albite was shown, and the liquidus estimated from the relevant phase diagram is 1100–1200°C.
No phase diagrams for the quaternary system Na2O–Al2O3–B2O3–SiO2 could be found.
A suite of six samples was prepared based on the glass composition Na2Al1+xB1–xSi6O16 described above, with x=0–1 in increments of 0.2.
The standard batch size comprised nominally 50g of glass together with 0.25 moles of the zirconolite forming oxides.
This blend gives an approximately equivolume mixture of glass and crystalline material if zirconolite forms as the crystalline phase.
The glass forming components were supplied by silica, alumina, sodium metasilicate and the glass frit used for high level waste vitrification on the Sellafield site.
The latter component was chosen as a stable source of boron oxide.
The glass frit is a mixed alkali borosilicate glass containing both sodium and lithium and for the purposes of glass composition calculation lithium was treated as being a molar equivalent for sodium.
In all formulations, sodium accounted for at least 85mol% of the total alkali.
The crystalline phase forming oxides were added as perovskite (CaTiO3), titania and zirconia.
The compositions of the six glass-ceramic samples are summarised by mass in Table 2, and the glass compositions are presented by mole in Table 3; the absolute molar quantities are then normalised to indicate the composition according to the overarching formula – Na2Al1+xB1−xSi6O16.
Note that because the glass components were calculated to give a constant nominal mass per batch, the molar amount of glass decreases slightly as alumina is substituted in for boron oxide in the glass network.
All samples were prepared from standard laboratory reagents.
Powder batches were milled in a Retsch PM 100 planetary mill for 20min at 300rpm using a 250ml hardened steel pot and 10mm diameter balls.
2-Propanol was used as a carrier fluid and the slurry dried at 80°C.
The dried powders were packed into straight-walled stainless steel HIP cans using a uniaxial pressure of 50MPa.
After welding on the lids, which included evacuation tubes, the HIP cans were baked out and evacuated at 600°C for a minimum of 4h and sealed.
The samples were then HIPped at 1250°C and 100MPa with a 2h dwell.
Heating rates were 10°Cmin-1–900°C, 8°Cmin-1–1000°C, 6°Cmin-1–1100°C, 4°Cmin-1–1200°C and 2°Cmin-1–1250°C; cooling was at 10°Cmin-1 until reduced by insufficient thermal transfer away from the HIP hot zone.
After HIPping the samples were removed from the HIP cans and analysed by X-ray diffraction (XRD) [Bruker D2 Phaser operating with Ni filtered Cu Kα radiation and a position sensitive detector] and scanning electron microscopy (SEM) operating in the backscattered electron mode and energy dispersive spectroscopy (EDS) [Jeol JSM 5600 and PGT IMIX].
XRD patterns from the x=0, 0.2, 0.4 and 1.0 samples are shown in Fig.
2 – the most intense reflections for each phase are annotated.
Although the intended zirconolite phase is apparent in all four traces it is clear that other crystalline phases have also formed, and that their abundance, inferred by the relative intensity of peaks, decreases as the Al:B ratio increases.
In conjunction with SEM/EDS the secondary phases were identified as zircon, (ZrSiO4) sphene (CaTiSiO5) and rutile (TiO2).
Rutile is somewhat difficult to identify unambiguously by XRD because of peak overlaps, and the strongest reflection that is unique to rutile is that at 2θ=54°.
In the x=1.0 composition, that is the aluminous end member of the suite of samples, the most intense reflection from the secondary phases, the {200} of zircon at 2θ=27°, shows that zircon is present only as a trace; and there is no evidence from XRD for sphene and rutile.
The XRD data are presented with comprehensive indexing as Fig.
A1 in the Appendix.
The microstructure of the x=0 sample is shown in Fig.
3.
On first inspection the atomic number contrast suggested two crystalline phases exist, which appear with light grey and mid grey contrast against the dark glass matrix.
Closer examination revealed slight brightness differences in both contrast levels.
EDS analysis of the crystalline phases indicated that for the light grey phase, the brighter of the two contrast levels corresponded to zircon whereas the darker was zirconolite.
Of the two mid grey phases, the brighter was rutile and darker was sphene.
It is clear that the experimental evidence did not support the initial hypothesis relating to increased solubility of calcium oxide in alumina poor glasses, because calcium oxide readily formed sphene rather than zirconolite in these compositions.
As the alumina content of the glass matrix increased with x, the amount of sphene, zircon and rutile decreased until at x=0.8 and x=1.0 zircon was present only as a trace, and sphene and rutile were absent based on XRD traces.
Hence, based on the observed phases it is suggested that there is an equilibrium:(1)CaZrTi2O7+2[SiO2]⇌CaTiSiO5+ZrSiO4+TiO2where [SiO2] represents silica as a component of a glass melt.
The competing reactions can be compared thermodynamically by:CaO+ZrO2+2TiO2+2SiO2→CaZrTi2O7+2SiO2andCaO+ZrO2+2TiO2+2SiO2→CaTiSiO5+ZrSiO4+TiO2The relevant standard Gibbs free energies of formation are: CaO=−603.1kJmol−1 [7] ZrO2=−1042.9kJmol−1 [7] TiO2=−888.8kJmol−1 [7] SiO2=−856.3kJmol−1 [7] CaZrTi2O7=−3515.6kJmol−1 [7] ZrSiO4=−1919.7kJmol−1 [7] CaTiSiO5=−2456.2kJmol−1 [8] It is recognised that these data are for a temperature of 298.15K, but in the absence of high temperature data it is necessary to make the assumption that they will remain essentially invariant with temperature.
Inspection of a wider range of data for the formation of mixed oxides from their parent oxides indicates this approximation is reasonable [9].
From these data it can be calculated that the standard Gibbs free energy for the formation of zirconolite and silica from the relevant oxides is −92.0kJ mol-1; and for the formation of sphene, zircon and rutile it is −128.5kJmol-1.
The thermodynamic data therefore indicate that the equilibrium for the principal reaction (Eq.
(1)), with all phases in the standard state, lies to the right; that is, zirconolite is not the thermodynamically favoured phase.
The overall free energy benefit in favour of sphene and zircon is -36.5kJmol-1.
To verify this analysis, a fully crystalline sample was prepared by sintering the above mixture of oxides at 1300°C for 50h.
XRD of this sample confirmed formation of a phase assemblage of sphene, zircon and rutile as shown by the diffraction pattern in Fig.
4.
The XRD data are presented with comprehensive indexing as Fig.
A2 in the Appendix.
Building on this perspective, qualitatively it can be observed that the equilibrium for the principal reaction (Eq.
(1)) will move towards zirconolite at low silica activity.
To quantify this the critical silica activity can be estimated as follows.
For any equilibrium reaction: ΔG=ΔG⊖+RT ln Qr where Qr is the reaction quotient defined here by: Qr=[CaTiSiO5][ZrSiO4][TiO2]/[CaZrTi2O7][SiO2]2where [CaTiSiO5] represents the chemical activity of sphene etc; and ΔG⊖ is the standard free energy change associated with the forward reaction, calculated above as −36.5kJmol-1.
At equilibrium ΔG=0 and the activity of all crystalline phases can be taken as unity.
Hence the reaction quotient reduces to:Qr=[SiO2]-2From this analysis the activity of silica in the glass matrix when the crystalline phase reaction is at equilibrium at 1250°C can be calculated as 0.24.
Current work is trying to put this thermodynamic perspective onto a firmer footing.
The findings here can be compared with other work on glass-ceramic nuclear wasteforms by Loiseau et al.
[10–12].
The materials in these studies were prepared by the established glass-ceramic method of quenching followed by nucleation and growth heat treatments.
Loiseau et al.
showed that zirconolite was the crystal phase that formed in the bulk of a sample, presumably by homogeneous nucleation, whereas anorthite (CaAl2Si2O8) and sphene, formed by heterogeneous nucleation, were observed in surface regions.
Long term heat treatments of these samples indicated that zirconolite was gradually replaced by sphene, anorthite and free zirconia.
This work again demonstrates the relative thermodynamic instability of zirconolite, although the heat treatments required for this phase to be destabilised could not credibly occur during long term wasteform storage and disposal.
This quantitative thermodynamic perspective does not necessarily lead to a physical understanding of why silica activity varies with glass composition, and with it crystalline phase formation; hence, our understanding of why zirconolite becomes the favoured phase in more alumina rich glass systems remains embryonic.
Initially, a simple mass balance was conducted in which it was assumed that the calcium oxide, zirconia and titania were partitioned exclusively into the four crystalline ceramic phases.
The remainder, containing the alkali oxide, boron oxide, alumina and silica not incorporated in zircon or sphene, was available to form a glass.
As the relative amount of the crystalline phases was varied, the quantity of silica in the remainder changed.
For the higher alumina content samples, the remainder did not have a viable glass forming composition when too much of the silica was consumed by sphene and zircon.
As silica was consumed, the composition of the remainder tended towards that of nepheline/carnegieite (NaAlSiO4) which has a melting point of 1520°C.
Formation of zirconolite liberated silica and made the remainder more amenable to glass formation.
It is possible that the thermodynamic benefit of forming a high entropy glass offsets the detriment of forming the less stable crystalline phase.
When the remainder contained increased levels of boron oxide, as silica was removed from the target matrix composition the liquidus temperature decreased slightly and glass formation remained viable despite silica being partitioned into zircon and sphene.
That said, the liquidus temperature of the albite–nepheline tie shows a eutectic, [5] hence silica poor deviations from the albite composition will still be amenable to glass formation and this argument may not explain the near complete elimination of silicate phases from the most aluminous samples, as indicated by XRD.
Indeed, the mass balance approach showed that a remainder corresponding to the eutectic composition formed when 30% of the calcium oxide, titania and zirconia existed as zircon, sphene and rutile.
An alternative approach is to consider the concept of the crystalline reference state (CRS) for glass melts, as discussed by Conradt [13].
The CRS for any multi-component glass is the set of equilibrium crystalline phases that will form if the melt is cooled sufficiently slowly for crystallisation to occur.
Typically, even a simple three component melt will have a CRS defined by three phases, because the glass composition will rarely correspond to an exact crystalline phase.
In the Na2Al1+xB1−xSi6O16 system under study, for the aluminous glass end member the CRS is clearly albite.
Notwithstanding the evidence from phase diagrams, a boron analogue of albite is known, the mineral reedmergnerite, but the syntheses described by Fleet [14] suggest it does not crystallise readily from the melt.
The phase is not recorded in the most recent Na2O–B2O3–SiO2 phase diagram that we can find [6].
Note that no quaternary Na2O–Al2O3–B2O3–SiO2 phases are documented, so the CRS of four component glasses is defined by the Na2O–B2O3–SiO2 and Na2O–Al2O3–SiO2 systems.
To quote from Conradt,“Indeed, there is ample experimental evidence that the heterogeneous nature of the CRS is reflected by the properties of a glass and its melt.
…….
Likewise, structural investigations reveal that the number and kind of structural (Short Range Order) entities in oxide glasses follow quite stringent rules of coexistence.
In the cited cases, the species can be unambiguously related one by one to the constitutional compounds of the CRS – not necessarily with respect to their actual structure, but to their number and stoichiometry.” Consequently, if the tendency for reedmergnerite to form is weak, it may be absent from the de facto CRS for the boron end member, and the CRS now includes silica along with the boron analogue of nepheline (NaBSiO4) or even NaBO2 – the available phase diagram is equivocal over the stability of the former compound.
If silica is a component of the de facto CRS for the boron end member, this may provide a physical basis for the thermodynamic argument that the activity of silica is higher in more boron rich glasses.
The formation of crystalline phases in glass-ceramic wasteforms has been shown to be dependent on the composition of the glass matrix.
Based on available thermodynamic data for the free energy of formation of the relevant crystalline phases, it is argued that zirconolite becomes the preferred crystalline phase at low silica activities – below [SiO2]=0.24 at 1250°C.
This criterion is apparently satisfied in the more aluminous glass compositions studied.
The concept of the crystalline reference state of a glass melt has been briefly discussed to provide a physical understanding of why this occurs.
The support of Sellafield Limited is acknowledged for fabrication of the initial samples.
This research was part supported by EPSRC under grant EP/L014041/1, and by provision of a studentship to ST with support from the Nuclear Decommissioning Authority.
NCH is grateful to the Royal Academy of Engineering and Nuclear Decommissioning Authority for funding.
The appended Figs.
A1 and A2 provide a detailed indexation of the XRD traces in Figs.
2 and 4 respectively.
In Fig.
A1 it can be seen that through the progression of glass formulation from samples 274 to 276 that the amounts of zircon, sphene and rutile decrease.
And by sample 279, only a trace of zircon remains from these three phases.
This trend is most clearly apparent when following the zircon peak at 2θ=27° and the sphene peak at 2θ=30°.
In Fig.
A2, for the crystalline phase forming oxides sintered in the absence of a glass matrix, the most intense reflection from the zirconolite phase, at 2θ=30.5°, is conspicuous by its absence.
Graphene-based materials: Synthesis and gas sorption, storage and separation Graphene-based materials have generated tremendous interest in a wide range of research activities.
A wide variety of graphene related materials have been synthesised for potential applications in electronics, energy storage, catalysis, and gas sorption, storage, separation and sensing.
Recently, gas sorption, storage and separation in porous nanocarbons and metal–organic frameworks have received increasing attention.
In particular, the tuneable porosity, surface area and functionality of the lightweight and stable graphene-based materials open up great scope for those applications.
Such structural features can be achieved by the design and control of the synthesis routes.
Here, we highlight recent progresses and challenges in the syntheses of graphene-based materials with hierarchical pore structures, tuneable high surface area, chemical doping and surface functionalization for gas (H2, CH4, CO2, N2, NH3, NO2, H2S, SO2, etc.)
sorption, storage and separation.
Graphene is a two-dimensional (2D) sp2 bonded carbon sheet, arranged in a hexagonal honeycomb lattice [1–4].
From a fundamental point of view, graphene is nothing but a single layer of graphite, which is an infinite three-dimensional (3D) material made up of stacked layers of graphene.
The layers in graphite interact weakly through van der Waals (vdW) forces.
In terms of properties graphene is unique; it is a soft membrane and at the same time possesses a high Young’s modulus, and good thermal and electrical conductivities [4–6].
In addition, a single-layer graphene is a zero band gap material and highly transparent, exhibits optical transmittance of 97.7%.
With its high theoretical specific surface area of ∼2600m2/g graphene provides a rich platform for surface chemistry [7–12].
The combined extraordinary physical and chemical properties of graphene, in turn, has ignited extensive research in nanoelectronics, supercapacitors, fuel-cells, batteries, photovoltaics, catalysis, gas sorption, separation and storage, and sensing [13–28].
A roadmap of graphene materials is described in a recent review [4].
It is important to note that most of the graphene properties are sensitive to structural defects and the number of layers [4,29–31].
Thus in order to exploit most of the proposed applications, the synthesis routes and conditions is important in tuning the structure and properties of graphene.
There are a number of reviews on graphene related materials for possible applications in relation to optical, electronic, photocatalytic and electrochemical properties [1–50].
However, there is a lack of consideration on their important molecular interactions, e.g.
for molecular adsorption and storage.
This article attempts to address such issues.
Several large scale processing methods have been involved for different graphene based materials through either graphitic top-down or molecular carbon precursor bottom-up approaches, as summarised in Fig.
1.
The flexibility in modification and functionalization of the graphene surface has opened up many possibilities for the development of tailored functional materials.
For example, surface modification has been applied to tune the band-gap of single-layer graphene for microelectronic devices.
Similarly, the intrinsic non-porous 2D graphene is tuned to highly porous 3D architectures for electrochemical devises (batteries, fuel-cells and supercapacitors) and gas sorption, storage, separation and sensing.
On a relatively large scale, graphene is mostly obtained from graphite precursors through oxidation–exfoliation–reduction, i.e.
in the form of graphene oxide (GO) as schematically shown in Fig.
1a [4,33,49–52].
The GO structure contains abundant oxygen-rich functional groups; hydroxide and epoxide groups on the basal plane and carbonyl and carboxyl groups on edges of the graphene sheets.
Thus the GO is hydrophilic in nature and soluble in water and several solvents.
GO with its lamellar water, a largely expanded and tuneable layer structure provides a rich platform for engineering a wide range of functionalities and reaction sites for chemical modifications.
Generally, the oxidation and reduction creates many defective sites on the graphene, which offers clear advantage in gas sorption, storage and separation and further functionalization.
Continued increase in energy demand and the urgency in reducing CO2 emission require rapid development of alternative and clean energy technologies [53–56].
Efficient ways of storing H2 [56–63] and capturing CO2 [54,55,64–72] are key challenges in the development of hydrogen fuel-cell vehicles and carbon capture systems, respectively.
Among the different methods, gas sorption and storage by physical adsorption in porous media is considered as a promising approach.
Here the gas sorption and storage capacity is mainly governed by a high accessible surface area and pore structure.
Many efforts have been made to synthesise a wide variety of tailor-made porous materials, such as zeolites, carbons, polymers, metal–organic frameworks (MOFs), and covalent organic frameworks (COFs) as adsorbents for H2 and CH4 storage, and carbon capture and separation from the flue gas [73–82].
The US Department of Energy (DOE) has set certain targets for H2 and CH4 storage materials for their practical applications [83–86].
Currently, the on-board H2 storage targets are: 5.5wt% (gravimetric) and 40g/L (volumetric) near ambient temperature.
Similarly, a new CH4 storage program has set the following targets: 0.5g(CH4)perg(sorbent) for gravimetric capacity and 11.741mmol/cm3 (ρ=0.188g/cm3) for volumetric capacity, which corresponds to the density of compressed natural gas (CNG) at 250bar and 298K.
The new volumetric target is equal to 263cm3 (STP: 273.15K, 1bar) per cm3, which is significantly higher than the previous target of 180cm3(STP)percm3 at 35bar.
Carbon based materials have been considered for promising gas sorption, storage, and separation because of the abundance, robust pore structure, tuneable porosity and surface area, light-weight, high thermal and chemical stability, and easy synthesis in industrial scale.
There is a considerable amount of interest in graphene related materials for gas sorption, storage and separation but still a lack of comprehensive review on such a topic [7,34,36,43,87–89].
Here, we present an up-to-date overview of the theoretical predictions and experimental results on the graphene-based materials towards gas sorption, storage, and separation with particular emphasis on H2, CO2, and CH4.
In addition, a detailed account is also provided on the adsorptive removal of toxic gas pollutants, such as NH3, NO2, SO2 and H2S.
Fig.
2 represents the overall overview of graphene-based materials and their possible applications in molecular adsorption and storage.
Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions.
The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution.
The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment.
Li et al.
summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90].
For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity.
Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment.
The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91].
Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O.
Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption.
For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.
Experimentally, at a given temperature, the quantity of adsorbed gas can be determined by an adsorption isotherm, generally carried out by one of the two methods: volumetric or gravimetric.
The adsorption isotherm (namely equilibrium isotherm) characterises the adsorption equilibrium.
The adsorptive isotherms of individual pure gases are also considered as a promising way to evaluate the adsorptive separation.
In case of membrane separation, the kinetic diameter of test gas plays an important role, where gas molecules with a kinetic diameter smaller or larger than the membrane pore diameter are separated by molecular sieving.
Molecular separation can also be facilitated by large differences in diffusion kinetics across a relatively thick porous membrane.
The planar sheet of graphene with its inherent specific surface area of ∼2630m2/g [92] in the ideal case has further motivated theoretical calculations for gas adsorption from the well-established carbon nanotubes and graphitic structures.
The adsorption of a monolayer of H2 on a single side of graphene sheet can lead to about 3wt% of H2 (H/C=0.18).
However, it is thermodynamically impossible for H2 molecules to penetrate between the graphene layers of graphite.
Deng et al.
design a Li-doped pillared graphene sheets (Li-PGS) and single-wall carbon nanotubes (Li-P-SWNT) [93].
Grand canonical Monte Carlo (GCMC) simulations predict a H2 storage capacity of 6.5wt% at 20bar and room temperature in Li-PGS with a doping ratio of Li:C=1:3 and an interlayer distance of 1.0nm (Fig.
3).
The Li dopants act as positive (acidic) cores to attract H2 molecules with a binding energy of ∼0.3eV.
An interlayer spacing of around 0.6nm with Li:C=1:6 leads to over 3wt% of H2 adsorption.
This interlayer spacing [94] or pore width [95] allows H2 molecules to interact both sides of graphene.
Thus the spatial distribution of molecular hydrogen adsorption on the graphene plane is very delocalised (Fig.
4).
The H2 interaction with graphene in bulk graphite is a localised phenomenon therefore adsorption generally restricted to the outermost graphene plane [96].
A maximum of 3.3wt% of H2 is achieved with one monolayer adsorption between two graphene layers separated by 0.6nm (Fig.
4).
At most two H2 monolayers could fit between the graphene layers when the layers are separated by 0.9nm, yielding a maximum capacity of 6.6wt% of H2.
Up to 16kJ/mol binding energy is obtained for an interlayer distance of ∼0.6nm [94,97,98].
For larger pore widths or interlayer spacing, the binding due to the second graphene layer/surface weakens [95].
For smaller separations, the exchange repulsion reduce the free energy and becomes positive for interlayer separations of <0.5nm.
Thus energetically the H2–graphite system at shorter separations is purely repulsive.
The gravimetric and volumetric H2 storage capacities in the C60 intercalated graphite (CIG) [99] approach the values obtained for Li-doped graphenes [93] at low temperatures.
In another case, a capacity of 3wt% of H2 is estimated at ambient in ∼0.7nm spaced carbon nanoscrolls followed by alkali doping [100].
First-principles calculations predict 3.4wt% of H2 and binding energy of (10–22)kJ/mol in the Li and organic molecules (benzene and tetrahydrofuran) co-intercalated graphite with an interlayer graphene distance of ∼0.77nm [101].
Furthermore, the charged graphenes show enhanced H2 binding energy up to ∼9kJ/mol compared to ∼5.8kJ/mol in a pristine graphene.
Similar binding energies are also achieved in Li-doped single-walled carbon nanotube (CNT) pillared graphene structures with intertube and interlayer distance of 1.5nm and 1.2nm, respectively [102].
The molecular dynamics simulations estimate H2 adsorption of up to 6wt% at 77K and 3wt% at 300K in the CNT pillared graphenes [103].
Furthermore, a detailed analysis and the theoretical limits of H2 adsorption and storage capacity of slit pores is also presented [104–106].
For example, practicable H2 adsorption capacity at room temperature is possible for graphene carbons with a slit-pore size between (0.8–1.1)nm, adsorption energy of ⩾15kJ/mol and a specific surface area of ∼2600m2/g.
The porous materials with micro-pores smaller than 0.7nm are not useful for storage application because of low delivery efficiency and large pores (>1.2nm) have difficulty in fulfilling the volumetric goals at room temperature.
The optimal volumetric storage is expected when the pore volume accommodate 1 or 2 layers of H2.
The loosely stacked layers in GO similar to graphite but with a much wider interlayer spacing (0.6–0.9)nm, would be more convenient to store H2.
However, it is very important to consider how accessible is the surface area between graphene layers when the GO layers are stacked together with the presence of surface functional groups and lamellar water molecules [107].
Under normal conditions, GO usually contains different oxy-functional groups, thus not all of the available sp2 carbons and interlayer spaces are accessible for gas sorption or storage.
In recent investigations, some open GO structures are modelled with pillared GO layers, in which further increase in the interlayer distance to about 1.1nm is shown as optimal for practicable H2 storage [107,108].
Burress et al.
design a series of idealised pillared-GO model systems with a phenyldiboronic acid linker, called GO frameworks (GOFs) (Fig.
5) [107].
The variation in linker concentration produces different interlayer distances, pore sizes, pore volumes and surface areas.
The GOF-32 structure is characterised by one linker per 32 graphene carbons predicted to hold ca.
6.1wt% of H2 at 77K and 1bar.
In another case, Li-doped CNT pillared GO (Li-PGO) model structures with an interlayer distance of 1.1nm and pore size of 2.3nm show a gravimetric and volumetric H2 capacity >10wt% and 55g/L, respectively at 77K and 100bar [109].
The interaction binding energy of H2 in the system is ∼15.5kJ/mol.
First-principles computations on the Ti-grafted GO show gravimetric and volumetric H2 storage of 4.9wt% and 64g/L, respectively [110].
Similar to the Li in Li-PGO, the Ti atoms bind strongly to the oxygen sites on the GO thus preventing them from clustering.
It is also estimated that each Ti binds multiple H2 molecules with suitable binding energy of (14–41)kJ/mol.
The interlayer distance between 1.2 and 1.4nm shows an optimal volumetric density of (64–74)g/L.
First-principles calculations on the covalently bonded graphenes (CBGs) show enhanced H2 binding energy of 20–150% or more compared to the isolated graphene [111].
The stable porous CBGs are formed through sp3 carbons at edges of the graphenes.
CBGs with adsorbed Ti atoms show ⩾4wt% of H2 adsorption depending on the pore size and length of the metal chains.
Simple graphene-based nanostructures show weak binding energy thus low H2 adsorption capacity at ambient conditions.
Thus chemical doping or structural modifications in the nanostructures has been carried out.
A considerable amount of theoretical work has been reported on the modified graphene structures with doping and/or surface metal (alkali: Li, Na, K; alkaline earth: Mg, Ca; simple: B, Al; transition: Sc, Ti, V, Ni, etc.
; and noble: Pd, Pt) dispersion.
This strategy increases the binding energy and adsorption of H2 due to the polarisation of H2 molecules by an electric field and/or hybridization of H2 σ or σ* orbitals with transition metal d orbitals (the so-called the Kubas interaction [112]) [112–114].
Lochan and Head-Gordon reported interesting theoretical results, which describes that the ionic metal atoms, such as Li+, Na+, Mg2+ and Al3+, hold up to six H2 molecules with moderate to very strong binding energies between (12–340)kJ/mol, which further depends on the cluster size and complexes [115].
Furthermore, to avoid the issue of structural instability and poor reversibility in metal atomic dispersion (because of the large cohesive energy of bulk transition metals, the aggregation/clustering seems to occur instead of being atomistically dispersed) the structural or chemical defects/doping are introduced in the substrate [111,116,117].
It has been showed that the alkali- and alkaline earth-metal adsorption on graphene surface leads to a metallic state due to charge transfer (Fig.
6) [118,119].
For instance, as shown in Fig.
6, up to 16wt% of H2 is predicted in the Li and B adsorbed graphene with a binding energy of ∼24kJ/mol [119–123].
H2 adsorption of up to 14.4wt% per molecule with an average binding energy of ∼40kJ/mol in ethylene–Ti complex is estimated [124,125].
A tensile strain of 10% on graphene increases the adsorption energy of Li (Ti) atom by around 75% (71%) and the gravimetric H2 storage up to 15.4wt% (9.5wt%) with an optimal binding energy of about 19kJ/mol [126].
By controlling the corrugation of individual layers of graphene, the density functional theory (DFT) calculations estimate up to 8wt% of H2 [127].
The corrugation of the graphene sheet and the controlled inversion of its curvature show a fast storage (on convex sites) and release (on graphene concave sites) of H2 at ambient conditions.
The B-doping is found to form an electron-deficient structure that enhances the H2 interaction energy [128–131].
The energy surface seems to extend over several graphene carbon sites, making the surface more heterogeneous.
B-doping in graphene also enhances the Li and Ca bonding strength on the graphene [130–133].
The DFT calculations estimate approximately (13, 9.9 and 7.9)wt% of H2 adsorption for the adsorbed Li, Al, and Ti atoms, respectively on the B-doped graphenes [133,134].
Lu et al.
show a H2 adsorption of (6.4 and 6.8)wt% (at 298K and 100bar) in a Li- and Ca-decorated B doped (pore rim of polyphenylene) porous graphene (Fig.
7) [135].
Up to 15wt% of H2 uptake is predicted in Be adsorbed on B-doped graphene with an average adsorption energy of ∼29kJ/mol [136].
The H2 adsorption capacity of 2.4wt% is also proposed in Si-doped graphene [137].
H2 adsorption of (6–9)wt% is estimated in the Ca-adsorbed/decorated graphene surfaces [132,138–141].
The Ca adsorption on zigzag edge, porous or covalent-bonded graphenes (CBGs) is also reported [142–144].
CBGs doped with B and decorated with Ca clusters show up to 6wt% of H2 adsorption [144].
The Ca atoms prefer to bind more strongly on the sp3 vertex sites of the pores than to planar graphene sites.
This leads to formation of Ca chain structures and exhibit mixed characteristics of multipole Coulomb and Kubas interactions with H2 molecules depending on the occupation of Ca s or d orbital-derived states.
A similar Mg-doped GO shows 5.6wt% of H2 adsorption at 200K [145].
The hydroxyl groups are reduced and Mg atom is bound to the surface of GO by the ring-opening reaction of epoxy in the form of –(C–O)x–Mg (x=1, 2).
On the surface the positively charged Mg sites and negatively charged O sites produce local electric fields separately to adsorb H2 molecule with a binding energy of (14.5 and 24)kJ/mol at O and Mg site, respectively.
The calculations on Al-doped graphenes also revealed interesting results for H2 adsorption [146–148].
The H2 uptake by light transition-metal (LTM=Sc, Ti, V) atom decorated graphenes has also been studied.
Each of the LTM atom seems to adsorb on both sides of graphene and hold up to four H2 molecules to give >6wt% of H2 adsorption with the average binding energies between (29–48)kJ/mol [149].
The Sc decorated on either armchair or zigzag edges of graphene nanoribbons show >9wt% of H2 adsorption [150].
Ni atomic dispersion on graphene yields about 3wt% of H2 but is expected to reduce experimentally due to the clusterization and nanoparticle formation [151].
Graphane is nothing but the graphene with alternately covalent bonded hydrogen atoms on each carbon atom on both sides of graphene sheet, as shown in Fig.
8 [152].
Structurally graphane is crumpled, rather than planar, because each hydrogen atom bonded to carbon pulls it a small distance out of the plane.
First-principles studies show that the chemisorption of hydrogen on both sides of graphene, i.e., with full coverage of hydrogen (CnHn, i.e., 8wt%) is the lowest energy structure due to the strain associated with the sp2 to sp3 conversion [153].
Due to its stable structure and small size, graphane has been also considered as a potential candidate for H2 storage if doped with alkali- and alkaline-earth metal (M) [154].
The H2 adsorption mechanism on these doped graphanes is explained by considering the fact that positively charged M+ ions polarise the H2 molecules, which are then held to the M+ ions by the vdW attractive forces.
Ab initio calculations by Antipina et al.
show that the graphane with chemically bonded alkali metals adsorbs up to 12wt% of H2, with a binding energy of ∼19kJ/mol [154].
The calculations by Hussain and Ahuja’s group also predict practicable H2 adsorption in the Li- and Ca-doped graphane with and without strain [152,155,156].
The optimised structure for the case of maximum coverage (5 H2 molecules, each on both sides of Ca atoms) is shown in Fig.
8.
The H2 adsorption by hydrogen spillover is a hydrogen dissociative atomic adsorption, and it has been found to be other alternative to enhance H2 adsorption in the carbonaceous materials.
The spillover is expected in some of the graphene based materials with functionalized/doped structures [157–162].
For example, the stability of hydrogenation states of graphene and conditions for hydrogen spillover are described by Han et al.
[161].
The mechanism in metal-doped carbon materials and other solid porous media is explained by George et al.
[162].
In brief, it can be explained in three elementary steps as shown in Fig.
9; (i) the activation or dissociation of H2 molecules on catalytic metal sites, (ii) a transition of atomic hydrogen across the metal–receptor interface, and (iii) the migration of atomic hydrogen throughout the receptor surface.
The whole process is denoted as 1, 2; 3, 4; and 5, 6, respectively.
The receptor, ‘R’ is considered inert towards the unactivated H2 and thus only accepts the activated atomic hydrogen that migrates from the catalyst, ‘C’.
The receptor surface is expected to be the key that simultaneously provides appropriate thermodynamic stability (binding energy) and kinetic mobility (migration).
Fig.
9 also shows stability of the hydrogen chemisorption state of graphene in comparison with the chemical potential of H2 gas.
Effective dissociation of H2 molecules into H atoms is observed with the firmly bound catalytic Pt4 metal cluster on B-doped graphene [160].
The H atoms then migrate from the bridge site of the H-saturated Pt4 metal to the supporting graphene sheet.
By DFT calculations, Cabria et al.
reveal that the adsorption and dissociation of H2 on Pd clusters on the graphene is due to either (a) the adsorption of the H2 in an activated state or (b) the dissociation of the H2 and the chemisorption of the two hydrogen atoms [163].
In an activated state, the H2 bond length is slightly stretched but not broken.
The binding energies of the activated H2 are between (0.4–1)eV/molecule, close to the estimated energies required to obtain reversible H2 storage at room temperature and moderate pressures.
Tight-binding model describes that the C–Pd–H2 interaction is much weaker than the C–Pd–H bonding [164].
The DFT studies also suggest that the graphene-like materials comprising of curved graphene fragments and many edge atoms, such as microporous carbon, strongly interact with the spillover hydrogen atoms [165].
At 300K, Li anchored on the graphene vacancies show 6.2wt% of H2 dissociative adsorption with a strong binding energy of ∼84kJ/mol [166].
Lueking et al.
show that both the B substitutionally doped graphene and hydroxylated graphene satisfies the constraints for reversible room-temperature hydrogenation [167].
A significant enhancement in the binding of H2 on N-doped graphenes is expected through dissociative adsorption and diffusion on the surface with an externally applied field [168,169].
Under an applied electric field, the N-doped graphenes exhibit a relatively low dissociative adsorption energy barrier for H2 and low diffusion energy barrier of H atoms.
Similar to H2 adsorption, the graphenes with ripples, defects, pores, functional groups, metal dispersion or doping and pillaring also show favourable CH4 and CO2 adsorptions [170–181].
The GCMC simulations on fullerene intercalated graphene nano-containers show enhanced CO2 adsorption with increasing fullerene concentration.
The pore with an effective diameter of ca.
0.5nm is found to be optimum for the effective CO2 adsorption at low pressures due to the energetically favourable surface interactions [170].
In a relatively large slit-pore, (1.0–2.4)nm the high CO2 adsorption occurs at relatively high pressures due to multilayer molecular adsorption, similar to the H2 adsorption characteristics discussed above.
In another study, molecular simulations demonstrate the CH4 adsorption in multilayer graphene nanostructures with and without Li-doping (MGNs and Li-MGNs) and function of interlayer distance [171].
The MGNs and Li-MGNs with an interlayer distance of (0.34–0.51)nm (0.34+0.17) show no CH4 adsorption due to steric effect of the adsorption space, whereas an interlayer distance between (0.68–2.04)nm yield a good CH4 adsorption (Fig.
10).
A maximum uptake of 310v(STP)/v of CH4 is obtained at 35bar, with an adsorption energy of 27.2kJ/mol.
The structures also show a higher H2S–CH4 adsorption than CO2–CH4 in case of biogas separation.
Several main influencing factors for CH4 storage and acid gas separation in MGNs and Li–MGNs are identified, including the binding energy between gas molecules and substrates, the optimal layer spacing for CH4 adsorption, and the Li doping strategy.
Li cation is able to enhance the binding of CH4 and H2S molecules with the graphene substrate surface via nonbonding interactions.
The CO2 molecule interacts with Li cation through chemical bond attributed to the high electronegativity of O atoms.
DFT calculations on the graphene layer modified with Ti at high metal coverage (C2Ti) show a dissociative and associative adsorptive behaviour for both CO2 and CH4, respectively at 300K and 1bar [172].
First-principles calculations on the Ti-decorated GO (Ti-GO) show a selective adsorption of CO over CO2, N2, CH4 either in single component or mixture with computed binding energy of ∼70kJ/mol for CO, ∼20kJ/mol for N2 and <5kJ/mol for CO2 and CH4 [173].
The strong interaction between CO molecule and Ti is a result of dative bond, i.e., hybridization between an empty d orbital of Ti and an occupied p orbital of CO.
The plane-wave electronic DFT calculations suggest that CO2 binds stronger to the defective site on graphene surface (with one carbon atom missing, i.e., monovacancy) than a perfect graphene with a physisorption energy of ∼20kJ/mol, approximately 4 times stronger than a perfect graphene surface [174].
The adsorption and reactivity of CO2 on a defective graphene sheet shows strong physisorption energy of ∼136meV [175].
Chemisorption of CO2 is expected by forming a lactone group with the C atoms surrounding the vacancy after overcoming an energy barrier of ∼1eV.
Further dissociation of the CO2 occurs through the formation of stable epoxy groups with ∼614meV.
Ab initio and molecular mechanics calculations show that the periodic nature of the vdW potential energy stored between CH4 and perfect sheet is altered due to the insertion of vacancies (missing lattice atom) and sinusoidal ripples.
The CH4 molecule avoids to be around vacant cites and on top of the peaks [176].
The produced vdW energy surface indicates that around the vacant cites the energy increases.
On the perfect graphene, DFT calculations reveal that the adsorption of CH4 is essentially driven by dispersion interaction with a reasonable binding energy of 0.17eV [177].
First-principles simulations show the large CO2 uptake capacities (∼0.4–0.6gCO2/gsorbent) by decomposition-mediated adsorption in Ca-decorated graphenes [178].
In the CO2 uptake process, decomposition into CO and formation of CaO is found to be thermodynamically favourable.
Thus enhancement in CO2 adsorption is attributed to a secondary process, where further absorption of CO2 in a CaO leads to formation of CaCO3.
The Ca-decorated graphene also shows an improved CO2/N2 selectivity, because of stronger binding of CO2 than N2, e.g.
2.731eV/CO2 against 0.645eV/N2 in case of 12.5% Ca-graphene.
Ohba and Kanoh show the edge effects of graphene on Ar, CH4, N2, CO2, and H2O gas adsorption [179].
The edge sites provide strong Coulombic interactions thus molecules with a strong dipole or quadrupole moment (H2O or CO2) are selectively adsorbed on the edge sites.
The dispersion interaction-dominated molecules (Ar, CH4, and N2) are selectively adsorbed on the basal planes.
Furthermore edge functionalized graphenes with the polar groups, including –COOH, –NH2, –NO2 and –H2PO3, show enhanced CO2 and CH4 adsorption due to strong binding of activating exposed edges and terraces [180,181].
Pristine and porous 2D graphene membranes with and without functionalization have been investigated for gas purification [182].
The pristine graphene is impermeable to He and H2 [183–185].
Calculations by Miao et al.
show that protons can readily pass through a perfect graphene sheet with a low tunnelling barrier compared to a substantially higher barrier for H2 [184].
Fig.
11 shows initial and final energy states for the tunnelling of an H atom through the centre of a hexagonal carbon ring (hollow site).
H2 is physisorbed at a distance of 0.286nm with a high energetic activation barrier of 2.86eV.
The tunnelling of an H atom through the benzene ring causes the ring to expand and then contract between a diameter of 0.285nm and 0.291nm in the initial/final and transition states, respectively.
The strong orbital overlap between the electrons from the H atom and the graphene at −21.62eV and 2.08eV for C-2s and H-1s, and C-2p and H-2s respectively, revealing a strong interaction between the H atom and graphene sheet at the transition state (Fig.
11b).
The penetration barrier decreases exponentially with the size of the defects, expressed by the number of carbon atoms involved in formation of defect.
The potential energy as a function of the distance of an H atom from graphene indicates that a larger defect size result in a smaller penetration barrier.
For example, the barrier is reduced largely to 0.66eV for DV (divacancies) and 1.17eV for 555–777 (one reconstruction from a DV) defects compared to the 2.64eV of a perfect graphene.
For this defect sites the stable states of H atom found at different distances between 0.12nm and 0.15nm.
First-principles DFT calculations estimate that the H2/CH4 diffusion selectivities are on the order of 108 and 1023 for the N- and all H-functionalized pores, respectively [186].
The pores are created by the removal of two neighbouring benzene rings on a single-layer graphene.
A CO2/N2 selectivity of around 300 with a free energy barrier for permeation of 24.7kJ/mol and 9.6kJ/mol for CO2 and N2, respectively, is reported for the same porous graphene using classical molecular dynamics simulations [187].
A sandwich-like model of porous graphene with the dimensions of 10nm×10nm and the nanopore density of 0.04pernm2 shows a linear dependence of flux on CO2 pressure.
The smaller pores of graphene with periodically “one missing” phenyl rings (the polyphenylene unit cell) and line-defect-graphene with missing octagons show a high selectivity for H2 (e.g., 1017 for H2/CO2 and 1023 for H2/NH3) separation from O2, N2, NH3, CH4, CO2 and CO, since CO2, CO and CH4 has larger kinetic diameter and H2 is about width of the pore [188–190].
A controllable selectivity and permeable separation of H2/N2 is demonstrated in a series of porous graphene membranes with different pore sizes and shapes created by removing 11–16 carbons [191].
For example, the pore size of ∼0.65nm (pore-16) show best selectivity of N2 from H2 because of the vdW interactions with graphene membrane that leads N2 molecules to distribute within a single molecule layer around the graphene surface.
Gas permeation in various limits of gas diffusion, surface adsorption, or pore translocation as the rate-limiting step is described by analytical expressions [192].
First-principles DFT and MD simulations on 0.135nm pore membrane show a high selectivity for H2 over other gases with the energy barrier of (0.12, 0.26, 0.25 and 0.82)eV for H2, N2, CO and CH4, respectively [193].
Clearly, it is the electron overlap between the gas molecules and the pore that causes the energy barrier and hinders the molecules from passing through the pores.
Thus the energy barrier for gas molecules passing through the membrane generally increases with decreasing pore size or increasing molecule kinetic diameter.
Lu et al.
demonstrate a high permeable selectivity for H2 relative to CH4, CO, and CO2 in a B or N doped pore rim of (polyphenylene) of porous graphene (PG, one hexagon missing) [135].
The 4B-PG shows a highest selectivity (1.2×1078) for H2/CH4 much higher than the PG (6.0×1058) with respective energy barrier of 0.91eV and 0.68eV.
Substitutional doping leads to a change in the original bond length of PG, where stretching and shortening of the bond is observed by B (BH) and N (NH), respectively.
Thus the 3B-PG and 4N-PG with change in pore size possess high selectivity (5.0×1037 and 2.0×1036) for H2/CO (20 times) and H2/CO2 (5 orders) respectively, compared to the PG.
In the N-doped pore rim graphene (replacement of the unsaturated two C atoms with two N) a Lewis-acid–base type interaction leads to a bound state of CO2 with a binding energy of about 21kJ/mol [194], which show no barrier for N2.
Due to similar kinetic diameters and no electric dipole moments, it is difficult to separate CO2 from N2; and pore functionalization is needed to enhance the electrostatic interaction.
The modified graphene membrane with –CH3, –OH, –NH2, –COOH shows a higher permeability for both N2 and CO2 compared to the unmodified one [195].
The results show that the porous graphene membrane with all-N modified pore-16 exhibits a higher CO2 selectivity over N2 (∼11) due to the enhanced electrostatic interactions compared to the unmodified graphene membrane.
Functional groups also increase the active surface area thereby enhancing the uptake and diffusion onto the membrane surface for more permeation.
The CO2 adsorption ability of functionalized graphene is in the following order: –OH>–COOH>–NH2>–CH3.
The porous membrane with carbon atoms bonding to three carbons and to two carbons with hydrogen generates non-uniform charge surfaces and strongly interacts with polar adsorbate molecules.
A high selectivity for SO2 adsorption (SO2/N2, SO2/CO2) is reported on fluorinated porous graphene due to reduced dispersive interactions, e.g., for CO2 [196].
Furthermore, in a porous graphene the enhanced diffusion rates of H2, O2 and CO2 up to 2, 5, 9 and 7, 13, 20 orders of magnitude are obtained by application of 10% of the uniaxial tensile strain and symmetrical stretch strain, respectively [197].
Clearly theoretical simulations have offered further insights into the mechanisms of molecular interactions with graphene-based structures.
Pristine graphene only show weak binding with most of the molecules.
Enhancement of binding interactions can be achieved by careful design of microporosities, surface doping, defect engineering, and high density of edge/boundary defects.
However, the binding energy needs to be retained at a level to cause “chemical activation” of the molecules, typically (10–40)kJ/mol, rather than their complete decomposition.
Separation of such molecules is another challenge, given the little differences in the kinetic diameters of the molecules of interest.
Modifications that can cause preferential binding of a certain molecules are desirable to promote selectivity of the sorbent materials.
The following section will examine some of the experimental developments along those directions.
Graphenes in the bulk quantities have been synthesised by liquid phase oxidation–reduction/exfoliation of graphite (Fig.
1a).
The degree of oxidation and exfoliation or reduction conditions yield individual single-layer graphenes to loosely packed multi-layer graphenes with a wide range of pore structures, different degree of residual functional groups and accessible specific surface areas (unless otherwise specified, hereafter the term ‘specific surface area (SSA)’, is generally determined using the Brunauer, Emmett and Teller (BET) [198] method from nitrogen gas adsorption–desorption isotherm measurement at 77K).
In many cases, the reduction of GO and agglomeration limits the microstructure and SSA.
For example, in a report by Ma et al., the graphenes obtained by thermal exfoliation of GO followed by hydrogen reduction show a much limited SSA (156m2/g) due to the severe agglomeration of crumpled graphene sheets [199].
The sample adsorbs about (0.4 and 0.2)wt% of H2 at 77K and 290K under 1bar and 60bar, respectively.
The exfoliated graphenes show an enhanced heat of H2 adsorption of ∼6kJ/mol compared to carbon nanofibers and active carbons [200].
A considerably improved SSA and gas sorption in graphenes have been obtained from different post-synthesis treatments of GO.
Srinivas et al.
report an enhanced SSA (640m2/g) in a hydrazine hydrate reduced colloidal suspension of exfoliated GO (Fig.
12) [201].
The sample is characterised with interconnected, wrinkled and disordered graphene sheets with regions of agglomerated multiple layers and a partial restoration of graphitic structure.
The gravimetric H2 uptake measurements at (77–298)K and up to 10bar show a heat of adsorption of ∼6kJ/mol at low coverage, and the maximum adsorption of 1.2wt% at 77K and 0.1wt% (and up to 0.72wt% by extrapolating pressure to 100bar) at 298K (Fig.
12).
A thermally reduced GO by simple out-gassing at 423K overnight shows an improved H2 adsorption of about 1.2wt% at 77K and 1bar with a SSA of 477m2/g due to enhanced microporosity and pore volume of ∼1.0cm3/g [202].
In another case, a mild chemical reduction of GO with glucose yields a highly enhanced SSA up to 1200m2/g with a mesoporous structure [203].
Surprisingly, a high H2 adsorption of 2.7wt% at 298K under 25bar is obtained, a much higher than for most of the carbon nanostructures.
Lee and Park report H2 uptake of up to 1.8wt% at 298K under 100bar (Fig.
13) in a series of exfoliated GO’s by CO2 pressure swing [204].
GO exfoliation is carried out at 373K under high pressure of CO2 up to 40bar followed by rapid release to vacuum to produce graphenes with a maximum SSA of 547m2/g.
Magnesium combustion derived graphene sample shows a more or less linear H2 uptake with increasing H2 pressure up to 300bar with the final capacity of 0.9wt% at 293K [205].
Bulk graphene sample is produced via high temperature combustion of magnesium in a CO2 atmosphere using dry ice black, which exhibits a more ordered carbon lattice than other bulk graphenes derived from GO.
Rao’s group prepare various types of graphenes from thermal exfoliation of GO (EG), pyrolysis of camphor (CG), and conversion of nanodiamond (DG) [206,207], which shows a wide range of SSA: 46m2/g, (280–1013)m2/g and (639–1550)m2/g for CG, DG and EG, respectively.
H2 adsorption at 77K and 1bar reveals an average linear trend with the SSA, in good agreement with other porous carbons.
From this trend, the single-layer graphene, i.e.
for the SSA of 2630m2/g, is estimated to hold about 3wt% of H2, in other words, about 1.2wt% of H2 for every 1000m2/g of SSA, at 77K and 1bar.
In addition, a considerably higher H2 adsorption up to 3wt% at 298K and 100bar is reported.
Guo et al.
obtain a highly improved H2 adsorption of 4.0wt% at 77K and 1bar in a hierarchical pore structure graphene with a SSA of 1305m2/g [208].
Briefly, the sample is synthesised by exfoliation of GO at 423K under vacuum followed by further heat treatment at 873K.
The pore structure analysis from the N2 (at 77K) and CO2 (at 273K) adsorption–desorption isotherm measurements reveal a hierarchical mesopore and ultramicropore structure.
This ultra-microporosity leads to an enhanced H2 uptake at low-pressure as well as a high heat of H2 adsorption, 9.5kJ/mol.
The graphene nanoribbons (GNRs) with in-plane and edge sp2- and sp3-hybridized carbons show a strong irreversible adsorption of CO2 (0.22mmol/g) at 303K and 1bar [209].
GNRs are synthesised through CVD via aerosol pyrolysis using ferrocene/thiophene/ethanol precursors.
Adsorption of 6.4mmol/g of CO2 at 30bar and 298K is obtained in a CO2 pressure swing exfoliated GO with a SSA of 547m2/g and a total pore volume of 2.47cm3/g [204].
Furthermore, a greatly enhanced CO2 adsorption of 21.6mmol/g at 11bar and 298K is reported in a hydrogen exfoliated graphene (HEG) with a wide distribution of mesopores and SSA of 443m2/g [210].
Surprisingly even higher CO2 adsorption of up to 56mmol/g is measured in a vacuum exfoliated GO compared to the maximum of (8, 15 and 39)mmol/g for zeolite-13X, activated carbon and conventional graphene nanosheets, respectively at 298K and 30bar (Fig.
14) [211].
The samples exhibit corresponding SSA and total pore volume of (480, 839, 1453 and 701)m2/g and (1.730, 0.396, 1.382 and 0.727)cm3/g, respectively.
The high CO2 uptake isotherms in graphene samples exhibit unconventional adsorption behaviour than other porous solids.
At low or moderate pressures up to 10bar, the CO2 adsorption in graphenes is not comparatively better than others.
The improvement at high pressures is linked to the higher interior void volume.
The chemical functionalization and surface grafting of graphenes with basic groups is also demonstrated to obtain increased CO2 binding and adsorption capacities through enhanced Lewis base–acid interactions [212,213].
Amine-grafted GO synthesised from different amines, including ethylenediamine, diethylenetriamine and triethylene tetramine and aqueous GO by vigorous stirring and reflux at 353K followed by washing shows about 1.2mmol/g of CO2 adsorption in the breakthrough experiments [212].
An unprecedented CO2 uptake of over 70mmol/g of CO2 (at 10bar and 298K) is obtained in the HEG–polyaniline composites [213].
However due to the very strong chemical binding of CO2, the heat treatment under vacuum is necessary to desorb the CO2.
As described in the theoretical predictions there is an optimum interlayer distance between graphene sheets to maximise the H2 uptake.
Such a range of distance reflects the energetically favourable interaction of one H2 layer between graphene layers.
In order to demonstrate the importance of an interlayer distance for effective H2 storage a variety of hybrid structures have been designed.
Below we summarise the experimental results on graphene layer structures synthesised through intercalating various chemical species starting with simple metals/oxides, molecules to organic ligands and combination of these.
The best example for this case is the well-known graphite intercalation compounds (GICs) [214–217].
GICs are highly ordered periodic layer structures, known for their very large graphene–graphene inter-layer spacing, made up of layers of foreign atoms/molecules sandwiched between single to n-graphene layers, depending on the stage-n compound.
Stage-n represents the n-number of graphene layers present between two intercalated layers.
For example, the stage-1 and stage-2 potassium GICs, KC8 and KC24 contains one and two graphene layers between two adjacent potassium intercalated layers (Fig.
15) [218].
This results in increased interlayer distance over 0.540nm, sufficient to reversibly adsorb H2 at low temperatures.
In particular, the series of stage-2 GICs of stoichiometry MC24 (M=K, Rb, Cs) are reversibly and rapidly adsorb H2 up to ∼2H2/M, without dissociation and behaves as a molecular sieve (Fig.
16) [218,219].
The charge transfer to the graphene planes lead to enhanced binding energy for H2 adsorption (8.4kJ/mol) in KC24 that is about twice to bare graphite.
An enhanced H2 adsorption is observed in the herringbone graphite nanofibers (GNFs) when expanded the graphite lattice using intercalation of concentrated sulphuric/nitric acid mixture followed by a thermal shock [220].
The SSA of 47m2/g in GNFs is increased to (67 and 555)m2/g when treated at 973K and 1273K, respectively.
Thus expanded GNFs show a multifold increase in H2 adsorption (1.2wt% and 0.29wt% at 77K and 300K, respectively at 20bar) compared to unexpanded GNFs.
In order to achieve an optimum interlayer distance, Kim et al.
obtain the H2 isotherms in a series of pillared-GO at 77K and 1bar [221].
Interlayer distance is controlled using pillaring diaminoalkanes (NH2(CH2)nNH2, n=2, 6, 10) and subsequent thermal annealing between (300–598)K. The structures show a maximum H2 uptake for an optimum interlayer distance of 0.63nm, in good agreement with the first-principles calculations for graphitic materials.
In their earlier work [222], the GO itself used to tune the interlayer distance without inserting other pillaring supports.
A careful reduction in a controlled thermal annealing not only retains the layered structure and also shows improvement in SSA, up to 192m2/g and pore volume, 0.689cm3/g, compared to 57m2/g and 0.076cm3/g for the initial GO.
Nuclear magnetic resonance (NMR) measurements confirm a considerable reduction of O– and OH– functional groups in the annealed GO.
As high as 4.8wt% of H2 adsorption at 77K and 90bar is measured in the annealed GO with an interlayer distance of 0.64nm.
A considerable reduction in the H2 uptake is observed with a little increase in layer spacing over 0.64nm.
In another approach, the porous networks of graphenes engineered by bridging the graphene layers and metal-oxide particles through surface functional groups.
For example, the GO-transition metal-oxide composites; GO/V2O5 and GO/TiO2 are developed through binding of C–OH species of GO and the oxygen on the transition metal-oxide via a dehydration reaction [223].
The hydrothermally designed porous graphene–Mn3O4 nanocomposites from the aqueous GO dispersion and MnO(OH)2 colloids yield an enhanced SSA up to 680m2/g [224].
The composite shows ∼2.5mmol/g of CO2 adsorption at ambient pressure and temperature compared to ∼1.75mmol/g in a controlled graphene.
The solvothermally synthesised graphene–iron-oxide hybrid structures reveal an enhanced microporosity with SSA ranging between (418 and 901)m2/g [225].
The nanoporous graphene–polyoxometalate (GPOM) hybrid structures are synthesised from in-situ hydrazine hydrate reduction of GO aqueous solution and cross-linker phosphomolybdic acid [226].
POM is a polyatomic ion, usually an anion that consists of three or more transition metal oxyanions linked together by shared oxygen atoms to form a large, closed 3D framework.
The polynuclear metal-oxo structured POM is a versatile building block cluster for construction of functional hybrid materials.
The hierarchical pore GPOM shows a pore volume of 0.57cm3/g and SSA of 680m2/g, considerably larger than SSA of 23m2/g for GO and 8m2/g for POM.
A highly increased interlayer spacing of 3.2nm and 3.6nm is achieved in GPOM and o-GPOM (oxidation of GPOM by hydrogen peroxide), respectively from 0.4nm in a reduced GO.
H2 adsorption of 0.8wt% and 1.3wt% in GPOM and o-GPOM respectively, is obtained at 77K and 1bar.
From the results, it is worth noting that the (micro)pore size distribution, adsorption enthalpy, exposed metal-sites and functional groups play a critical role in enhancing H2 adsorption.
Using a well-known reactivity between boronic acids and hydroxyl groups, Taner’s group [107,227] develop a novel pillared GO structures, called GO frameworks (GOFs) by linking the various phenylboronic acids between GO layers (Fig.
17).
For the past few years, boronic acids have been considered as versatile building blocks for the construction of a variety of molecular architectures [78,228].
The interaction between boronic acids and diols has been used recently to construct a variety of crystalline porous frameworks, called covalent organic frameworks (COFs) [78].
Thus the rich surface oxy-functional groups on the graphene leads to the development of GOFs by forming strong boronate ester bonds as shown in Fig.
17.
The structures are synthesised through a solvothermal method using methanol solution from various concentrations of GO and phenylboronic acid.
GOFs show a systematic increase in the interlayer distance from 0.75nm to 1.05nm and SSA from 10m2/g to 470m2/g.
About 1wt% of H2 uptake at 77K and 1bar with relatively high (∼9kJ/mol) heat of H2 adsorption at low coverage is achieved.
The H2 adsorption capacity for a given SSA and heat of adsorption appears twice as large as typical porous carbons ([82,199,201], typically about 0.5wt% of H2 uptake at 77K and 1bar for every 500m2/g of SSA and heat of adsorption of 4–6kJ/mol).
GOFs also show a good room temperature CO2 adsorption of ∼2.7mmol/g at 4bar with again high heat of CO2 adsorption of 35kJ/mol.
The micropore structure with slit-like pores and strong surface interactions with functional groups are favourable factors enhancing their capacity for gas adsorption and binding.
Similarly, cross-linked and functionalized graphene sheets from diazonium (product 1) and tert-butylaniline or 4-chloroaniline (product 2) as reagents in chlorosulphonic acid or oleum are synthesised aiming at improved H2 adsorption [229].
Products 1 and 2 show a calculated interlayer spacing of ∼0.96nm and ∼0.76nm and SSA of ∼440m2/g and 350m2/g, respectively.
Enhanced H2 adsorption of up to 1.6wt% at 77K and 2bar is obtained compared to ∼1wt% in the starting exfoliated graphenes.
The enhanced H2 adsorption again suggests importance of tuning the interlayer separation, microporosity, and functionalization.
Zhou et al.
design a porous graphene structures by cross-linking non-planar terpyridine complexes using azide–alkyne click reaction [230].
The structures show up to 440m2/g of SSA and a CO2 adsorption of 2.6mmol/g at 273K and 1atm.
The hybrid monolith aerogels of chitosan (CTS, an environmentally-benign biopolymer) with GO, prepared using freeze-drying, show dramatic increase in both the porosity (from 153m2/g to 415m2/g) and cyclic CO2 uptake (from 1.92mmol/g to 4.15mmol/g at 25°C and 1bar) for a GO loading of 20wt% [231].
The DAB poly(propyleneimine) dendrimer (DAB) cross-linked GO-based hybrids display good CO2 adsorption of ∼2mmol/g (P/P0=0.35) upon wetting [232].
Under humid conditions the sample exhibits ∼3 times the adsorption capacity with fast kinetics, compared with the dry state, due to the presence of amino groups of DAB which can form carbamates through zwitterions (primary amines) and bicarbonates (tertiary amines).
The formation of these groups is enhanced in the presence of a base (hydroxyl groups of water).
In a work by Matsuo et al., the pillared graphenes are synthesised by pyrolysis of various sylsesquioxane bridged GOs [233].
The structures are highly microporous in nature (calculated pore widths of about 0.7nm) and show an interlayer spacing between 1.34nm and 1.6nm and SSA up to 940m2/g, which leads to an H2 excess adsorption of 0.6wt% at ambient temperature and 200bar with a high heat of H2 adsorption of (8–11)kJ/mol.
A further enhanced H2 adsorption of 1.4wt% and 2.6wt% at room temperature and 50bar is obtained in a liquid crystal (LC) route prepared macroscopically ordered layer-by-layer 3D framework of GO and GO-MWCNTs composites, respectively (Fig.
18) [234].
MWCNTs act as 1D spacer between graphene sheets that result in large increased interlayer spacing up to ∼7nm.
The high H2 adsorption is attributed to the synergistic effect of the well-spaced GO sheets and debundling of MWCNTs to enhance the accessible surface area.
It is worth noting that the ∼1.4wt% of H2 in a LC-GO with its interlayer spacing of around 0.8nm is also comparatively higher than an earlier reported value of 0.2wt% at 77K in GO [107].
The enhanced H2 uptake capacities are also realised in graphene incorporated clay materials [235,236].
The samples are prepared by impregnation of the natural clay (montmorillonite and sepiolite) substrates by caramel in aqueous media, followed by a thermal treatment in the absence of oxygen, gives rise to graphene-like materials, which remain strongly bound to the silicate support.
These carbon–clay nanocomposites shows ∼0.4wt% of H2 at 298K and 200bar related to the carbon mass.
These uptakes are still far from the predicted value of (3–4)wt% from model simulations.
A very high isosteric heat of H2 adsorption of 14.5kJ/mol shows a strong stabilization of the H2 molecule within the structure.
The controlled clays without graphene, thermally treated under the same conditions did not show any appreciable H2 adsorption.
The samples also show about (0.6 and 1.7)wt% of H2 uptake related to the total mass of the system and carbon mass, respectively, at 77K.
In another case the enhanced adsorptive gas storage is achieved by pore engineering of highly porous MOFs with GO [237–240].
The MOFs are a type of crystalline and highly porous solids made up of metal clusters linked with organic ligands.
For example, Petit et al.
synthesise the MOF-GO composites by solvothermal method, with the addition of GO up to 50% by weight to the MOF precursors [237].
The successful retention of porosity suggesting that the building process of the composites occur via the reaction/binding of the copper dimers from the HKUST-1 (Cu-MOF, also known as Cu-BTC) with/to the functional groups in GO (epoxy, carboxylic, hydroxylic, sulphonic).
At the optimal GO content, the composites show about 15% and 18% increase in a micro- and total porosity, and SSA up to 1000m2/g compared to a 900m2/g in HKSUT-1.
While the composites with the higher GO contents show smeared SSA (600–700)m2/g due to the formation of nonporous reduced graphene stacks from the excess, unreacted GO.
H2 adsorption of (2.0–2.5)wt% at 77K and 1bar in these structures is consistent with the enhanced microporosity.
Later, Liu et al.
report a further enhancement in SSA and H2 adsorption as well as CO2, CH4 and N2 adsorptive separation in a similar Cu-MOF and GO composites, named as CG (Fig.
19) [238].
The authors also reveal more insights in the growth mechanism of the Cu-MOF crystals on GO substrate in the composite through DFT calculations (in a VASP).
The results show strong interaction energy of Cu to the GO defect of 15.12eV, more favourable than those on the –OH (3.92eV) and –O– groups (1.60eV) of the GO surface.
Furthermore, the epoxy groups and the defects of the GO layers act as seed and termination sites, respectively for formation of nanosized and well-dispersed Cu-BTC on the layers of GO (Fig.
19).
Thus the composites show about a 30% increase in CO2 and H2 adsorption (from 6.39mmol/g of Cu-BTC to 8.26mmol/g of CG-9 at 273K and 1bar for CO2; from 2.81wt% of Cu-BTC to 3.58wt% of CG-9 at 77K and 42bar for H2).
A similar CO2 adsorption is observed in a GO-Cu-BTC with stronger interaction energies (activation energies of (68.6 and 56.7)kJ/mol for CO2 on the composite and Cu-BTC, respectively) and enhanced CO2/CH4 adsorption selectivity, twice that of Cu-BTC for an equimolar CO2 and CH4 mixture [239].
Furthermore, the composites of Cu-BTC and GO modified by urea show enhanced CO2 adsorption of 4.23mmol/g [240].
The incorporation of aminated GO into a MOF structure shows synergistic effect; the porosity modification with new micro-pores at the interface and basic groups on the surface, which are expected to increase the physical adsorption forces and acid–base interactions, respectively.
This would not only increase the adsorption capacity but also the selectivity.
Sandwich-like graphene-silica sheets with incorporation of polyethyleneimine (PEI/G-silica, with about 12wt% graphene and 60wt% PEI, amine based solids for CO2 adsorption) show a high CO2 adsorption of 4.3mmol/g at 348K under 7.4bar with good cyclic stability [241].
The G-silica sheets obtained through the hydrolysis of tetraethylorthosilicate on the surface of GO with the aid of a cationic surfactant (cetyltrimethyl ammonium bromide) followed by pyrolysation at 973K under Ar.
PEI impregnation is made via a solution route using methanol solvent.
PEI/G-silica sheets show a pore size of (0.7–2.0)nm with a SSA of 32m2/g, much smaller than that of G-silica sheets (∼2.0nm and 930m2/g).
Clear advantage of graphene sheets is observed in an enhanced CO2 adsorption at low equilibrium pressures (<1bar), whereas the PEI/G-silica sheets show a significantly higher CO2 adsorption than PEI/silica sheets.
Sui et al.
synthesise 3D cross-linked GO hydrogels with a polyethylenimine (PEI) [242].
As shown in Fig.
20, the structures are developed through gel formation from aqueous GO dispersion with PEI solution at 25°C followed by freeze-drying.
The monoliths possess a low density in the range of (0.02–0.03)g/cm3 due to the open-pore structures interpenetrating the skeleton of GO sheets.
This is also well observed in the XRD patterns.
Compared with GO, an increase in the interlayer spacing up to 1.56nm, indicating successful cross-linking of the GO sheets.
Therefore, the open-structures show enhanced SSA of up to 476m2/g and CO2 adsorption of ∼2.4mmol/g at 273K and 1bar compared to 1.8mmol/g in a controlled GO with a high SSA (876m2/g).
Clearly, the functional basic sites enhance the CO2 adsorption capacity.
As explained in Section 3.1.5, hydrogen spillover is a dissociation of H2 on the catalytic metal nanoparticles, followed by subsequent atomic adsorption onto the substrate surface sites.
It is considered as one of the effective ways to enhance H2 storage capacity in the carbonaceous structures [80,243].
In the study by Hu et al., the H2 uptake by low temperature physisorption and room temperature chemisorption is clearly demonstrated in a high Pd (about 24wt%) dispersed graphene nanocomposite [244].
The chemisorptive H2 uptake in Pd nanoparticles shows a two-phase behaviour of isotherms.
Initially formed solid solution region at low pressures (α-phase), is transformed to a hydride phase (β-phase) at higher pressure through a coexistence of two-phase region (α+β), plateau range, very similar to the typical pressure–concentration–temperature curves of H2 storage materials [245,246].
The isotherms also show a Pd particle size, (4–24)nm, dependence; a clear transition from α- to β-phase is observed with increase in a Pd particle size.
A high H2 uptake of 3.2wt% at 77K and 1.1bar for a given SSA of 230m2/g and a pore volume of 0.3cm3/g suggest a clear spillover effect.
Furthermore, an exfoliated graphene (with a SSA of 755m2/g) decorated by Pd and Pt nanoparticles, (2–5)nm, via an electroless deposition technique also shows enhanced room temperature spillover of H2 uptake [247].
The observed uptake of 4.3 H/Pd is much larger than that of actual chemisorbed hydrogen in the bulk Pd with a stoichiometry of 0.6 H/Pd.
The room temperature H2 spillover capacity in a mixture of the activated carbon (AC, Maxsorb) as “secondary spillover receptor” and Pd-exfoliated graphene (in a 9:1 weight ratio) shows reversible H2 uptake of 0.82wt%, 49% more (0.55wt% of H2) than Pd-free AC-exfoliated graphene under 80bar [248].
The spillover is also revealed by a high isosteric heat of H2 adsorption of ∼14kJ/mol, compared to ∼8kJ/mol for AC.
Ramaprabhu’s group report up to 3wt% and 4wt% of H2 in a Pd (20wt%) nanoparticle decorated graphenes before and after N-doping (ca.
7%), respectively at 298K and 40bar (Fig.
21) [249–252].
The values are comparatively higher than ∼0.7wt% and ∼1.5wt% in a Pd free graphenes before and after N-doping, respectively.
A clear influence of N-doping can be observed.
A combined physisorption and chemisorption process gives a high heat of H2 adsorption over 12.5kJ/mol.
The hydrogen exfoliated GO at 523K is used to synthesise N-doping and Pd-decoration through nitrogen plasma treatment and reduction of PdCl2, respectively.
The Pd/N-G sample accounts 48% of the spillover capacity enhancement in the total H2 storage capacity.
Wang et al.
develop a nonprecious and air stable Ni–B nanoalloy dispersed graphenes to enhance the H2 adsorption [253,254].
As much as 4.4wt% of H2 is achieved at 77K and 1bar in the graphene with Ni (0.83wt%) and B (1.09wt%) (graphene–Ni0.83B1.09) compared to 1.35wt% of H2 in the undoped graphene.
The composites are obtained via NaBH4 chemical reduction of aqueous solutions of GO and Ni(CH3OOH)2.
The H2 capacity seems excellent among above Pd–graphene and all other carbon-based materials.
Up to 12kJ/mol of isosteric heat of H2 adsorption is reported.
Further higher H2 storage capacity of over 5wt% at 298K and 20bar and heat of adsorption, >30kJ/mol at low coverage and ∼14kJ/mol at high surface coverage is reported in the Pd/Hg nanoalloy and Pd nanoparticle loaded GO-like foams (GLFs) [255–257].
Porous GLFs (nominal composition C2OH) are synthesised by simple calcination of the molecular precursor, sodium chloranilate dihydrate in air at 573K [255].
GLFs consist of interconnected bundles of turbostratically stacked few-layer graphenes build-up of aromatic and aliphatic domains with oxygen rich functional groups on the surfaces.
GLFs with SSA of 510m2/g and pore volume of 0.89cm3/g selectively absorb CO2 (about 2mmol/g) over N2 (0.4mmol/g), CH4 (0.7mmol/g), H2 (negligible) and CO (0.7mmol/g) at 10bar and room temperature.
The Pd4Hg particle loaded (12wt%) GLFs (Fig.
22) with its inherent carbon-based radicals near the edge and defect sites yield an enormously enhanced H2 uptake of about 5wt% at room temperature and 20bar compared to a negligible H2 uptake in undoped GLFs [256].
The Pd–Hg alloy loading is obtained through NaBH4 reduction of aqueous GLFs, PdCl2 and HgCl2 mixture.
In other study, Wang et al.
show an important comparison of H2 uptake by spillover in Pd loaded (10wt%) GLFs (Pd/GLFs) and super-activated carbon (Pd/AX-21), and oxygen modified AX-21 (Pd/AX-21-O) [257].
By comparing H2 absorption, the authors conclude that the existence of abundant oxygen functional groups on the surface and edges in Pd/GLFs lead to enhanced H2 uptake with a high heat of adsorption, >30kJ/mol at low coverage and ∼14kJ/mol at high surface coverage compared to those of Pd/AX-21 and Pd/AX-21-O.
The H2 dissociative adsorption is also observed in simple modified graphenic structures without metal dispersions.
The volumetric H2 storage of 5wt% is also reported in exfoliated turbostratic carbon nanofibers (CNFs) at 77K and 100bar compared to 1.5wt% in the as-produced CNFs [258].
This enhancement is attributed to the simultaneous molecular and dissociative H2 adsorption in combination with more defective surface, increased SSA (from 103m2/g to 227m2/g) and pore volume (0.39–0.52cm3/g).
A graphite exfoliation technique using intercalation of acids and thermal shock is employed to expand the CNFs.
Using the bottom-up solution-phase method, graphenes of gram scales are produced from simple laboratory reagents of sodium and ethanol in a 1:1M ratio by solvothermal method [259,260].
The subsequent rapid pyrolysation leads to a porous graphene in relatively large quantities (∼0.1g for 1ml ethanol).
The sample with a high SSA of 2139m2/g shows a high-pressure H2 uptake of 0.9wt% at room temperature with a high heat of adsorption of 12kJ/mol [261].
The adsorption is attributed to dissociation of H2 at the zigzag edges of the graphene, or combined effects of residual oxygen and sodium functionalities.
In other study, the heteroatom (boron, phosphorous, or nitrogen) substituted carbon scaffolds (micrometer sized flake-like few-layered graphene structure assembled by (3–5)nm sized domains) show H2 capacities of (2.2–2.4)wt% per SSA of 1000m2/g at 77K and 2bar relative to 1.8wt% for the pristine carbon scaffold or ∼2wt% for common carbonaceous materials [262].
The heteroatom substituted scaffolds also show H2 binding energies between (5–9)kJ/mol.
As demonstrated above, all the methods; functionalization, exfoliation and pillaring hybrid designs generated a promising gas sorption and storage.
However the structures have limited accessible surface areas and pore volumes.
To obtain a relatively higher SSA with tuneable pore structure, self- or sacrificial-templating from the ordered structures, CVD and chemical activation methods are employed [263–267].
As shown in Fig.
23, the chemical activation of precursor GO with activator KOH yields a very high SSA of up to 3100m2/g graphene-carbons.
The exfoliated GO by thermal shock or microwave irradiation is thoroughly mixed with various amounts of KOH either by solution or dry milling followed by activation at temperatures between 873K and 1173K.
For example, Srinivas et al.
report a range of SSA (up to 1900m2/g), pore volume (up to 1.65cm3/g), pore size and size distribution in GODCs (GO derived carbons) by changing the GO/KOH concentration and activation temperature [263].
The large fraction of smaller pores is transformed to large pores when increasing KOH concentration and/or activation temperature.
As shown in Fig.
24, the GODCs show favourable gas adsorption for CO2 and CH4, compared with other high surface area carbons for a given SSA.
Furthermore, the N-and S-doping in GODCs with pyrrole and poly-thiophene, respectively show an enhancement in ambient CO2 uptake (∼4mmol/g) and the selectivity over CH4 and N2 [264–266], The selectivity is an important factor for the application in flue gas capture.
For example, as shown in Fig.
25, the S-doped graphene carbons show highly enhanced and stable recycling CO2 adsorption capacity of >4.0mmol/g over CH4, N2 and H2 (which show much lower adsorption of <1mmol/g) at 298K and 1bar.
The capacity is twice as high as for the undoped activated graphene carbons (∼2mmol/g) [263].
The CO2 capture capacity in the doped porous graphene seems comparable with or even higher than the various other porous media; N-doped porous carbons, amine functionalized silica and MOFs.
The high microporosity and N-functionality show a high isosteric heat of CO2 adsorption between (30 and 56)kJ/mol.
The CO2 adsorption of 1.82mmol/g at 0.2bar (is a partial pressure of CO2 in the flue gas stream) in the S-doped sample is also comparatively larger than other porous carbons.
The increase in activation temperature yields enhanced microporous samples but same time the N- and S-doping content reduces; reduction from (7 to 2.7)at% is observed at 700°C, whereas the lower activation temperatures lead to a decrease in SSA.
Using porous, layer structure MgO as a template, nanomesh of one- to two-layer graphene in a gram-scale is produced by CVD method using methane as a carbon precursor [267].
After acid treatment to remove MgO template, the graphene sheets show a large corrugations with nanopores of ∼1nm, total pore volume of 2.35cm3/g and SSA of 2038m2/g.
A high H2, CH4 and CO2 adsorption of ∼1.5wt%, 14.5mmol/g (at 90bar) and 36.5mmol/g (at 31bar), respectively at 274K is reported.
The graphene materials with their high electrical conductivity and chemical stability also investigated for electrochemical H2 storage via electro-decomposition of water on a cathodically polarised electrode.
The adsorption–desorption mechanism during charge/discharge phenomenon is simply written in terms of the following equations: [G]+nH2O+ne−↔[CHn]+nOH−, where [G] represents the graphene host.
In steps; water reduces first (H2O+e−→H++OH−) and the nascent hydrogen then adsorbs into the graphene network due to the electric polarisation of the electrode under the external applied potential.
In the reversible process, the hydrogen atoms discharge from the graphene and recombine with OH− to make H2O.
A reversible H2 storage of ∼148mAh/g is reported in a few layer graphene flakes that are synthesised through arc-discharging of pure graphite under hydrogen atmosphere [268].
The graphene working electrodes are prepared on Ni-foam using PVDF binder and press.
The reversible H2 storage seems highly dependent on the structural properties of graphene flakes; crystallite size, wrinkles, defects, and inter layer spacing.
Graphene-porous cobalt-oxide (Co3O4) nanocomposites show a first discharge capacity of 241.9mAh/g which is equal to 0.89wt% of H2 [269].
The nanocomposite on a sheet of nickel foam, Ni(OH)2/NiOOH and Hg/HgO as a working, counter and reference electrode, respectively is employed in a 6M KOH electrolyte at 298K.
As shown in Fig.
26 a further high discharge capacity of 1415mAh/g, equivalent to 5.176wt% of H2 is observed in the fullerene-like orthorhombic-structured Co3C nanoparticles, synthesised by ball-milling of Co and graphene powders [270].
Two obvious plateaus at 1.2V and 0.4V, respectively in the discharge curve of Co3C nanoparticles reveal an existence of two different H2 adsorption sites; first adsorbs into the interstitial sites/pores between Co3C nanoparticles and then diffuses into the interstitial sites between Co3C.
The composition and ball-milling conditions show a profound effect on the cyclic stability of the composites.
The continuous decay in a cyclic discharge capacity is attributed to the formation of β-Co(OH)2.
Overall, the Co3C nanoparticles display a relatively high discharge capacity when compared with the traditional AB5 type H2 storage materials.
The hydrogenated graphene, also called as graphane, a covalently bonded hydrogen atoms to carbon atoms, (sp3 C–H bonds on basal plane) is essentially different from the physical H2 absorbed graphene structures.
Number of synthesis methods; plasma, CVD, liquid and gas phase reactions are developed to obtain graphanes.
Subrahmanyam et al.
demonstrate that it can be used as a chemical H2 storage material [271].
Birch reduction of a few-layer graphene with excess Li in liquid ammonia produce the hydrogenated graphene that contains up to 5wt% of H2 (Fig.
27).
Spectroscopic studies reveal the presence of sp3 C–H bonds and the hydrogen can be released up on heating or irradiation with UV or laser.
Indeed, the hydrogenation of various carbon structures, fullerenes, nanotubes, and graphites via a dissolved metal reduction method with Li and methanol in liquid ammonia has been demonstrated early in 2001 [272].
These structures release about (4–5.4)wt% of H2 upon heating to 873K.
A simultaneous release of H2 and a small amount of methane is observed during the decomposition.
Recently, a hydrogenated graphene with ∼6wt% of H2 (C1.3H)n, is synthesised directly from a Birch reduction of graphite powder [273].
Simplified techniques have also been developed to obtain chemical hydrogenated graphenes in a large scale compared to complexity involved in a hydrogenation of CVD/mechanically cleaved graphene in hydrogen plasma [274] or Birch reduction of graphene/graphite.
For example, a gram scale of hydrogenated graphene directly from the GO is synthesised via in-situ generated atomic hydrogen through the H2 spillover using nickel as an active catalyst under ambient conditions [275].
The hydrogenated graphenes also produced directly by exfoliating GO under high pressures, (60–150)bar of hydrogen at high temperatures, (473–773)K [276].
The hydrogen, fluorine, and oxygen gas phase atomic covalent functionalization to graphene is also explained [277].
Earlier, Orimo and his co-works report a hydrogen concentration up to 7.4wt% (CH0.95) in the ball-milled graphitic nanostructures (Fig.
28) [278–280].
Hydrogenation is attributed to the formation of dangling carbon bonds, the CHx covalent bonds.
Interestingly the milled graphite also shows ∼400m2/g of SSA and H2 physisorption at cryogenic temperature because of random orientation of fragmented small coherent domains that consist of small stacks of 2–3 graphene layers.
Very recently, a large scale (5g for each batch), edge-hydrogenated graphene with 3wt% of H2 (which corresponds to C/H=2.3) is produced by ball-milling of graphite powder in a hydrogen atmosphere [281].
Electron microscopy studies reveal the mechanochemical cracking of graphite into small grain sizes of (0.1–1)μm after milling of 48h correspondingly, the SSA and pore volume increases respectively, (3–437)m2/g and (0.002–0.391)cm3/g.
The edge-selective hydrogenation during the ball-milling process is expected due to the reaction between reactive carbon species (radicals and ions) at the broken edges of graphenes.
A monolayer graphene membrane is impermeable to standard gases including helium [282].
The nanopore graphene membrane as molecular sieves is demonstrated by measuring the transport of a range of gases (H2, CO2, Ar, N2, CH4 and SF6) through the irradiated pores [283].
As shown in Fig.
29, a membrane fixed over predefined 5mm diameter wells on SiO2 and pressurised with H2 gas is etched with ultraviolet light leads to a rapid leakage of H2 but preventing N2 from passing through it.
A size selective permeation of gas molecules is demonstrated by creating different-size pores.
The submicrometer thick, (0.1–10)μm and area ∼1cm2 of GO membranes are found to be completely impermeable to liquids, vapours and gases, including helium, while allowing unimpeded permeation of water (H2O permeated through the membranes at least 1010 times faster than He) [284].
It is attributed to a low-friction flow of a monolayer of water through 2D capillaries formed by closely spaced graphene sheets.
Diffusion of other molecules is blocked due to reversible narrowing of the capillaries in a low humidity and/or by their clogging with water.
Mass-spectrometry measurements show that helium is less permeable in dry GO film than a 1-mm-thick glass.
No permeation is detected for several polar and nonpolar organic liquids through GO membranes of 1μm in thickness.
Reduced GO membranes with an interlayer spacing of 0.4nm are much less permeable to water.
In recent studies, the graphene membranes are also tested for different solvent filtration (desalination) and separation [285,286].
The single layer graphene grown by a CVD method on Ru, Cu and Cu/Ni is found to protect the underneath metal from air oxidation even after heating them in air [287–291].
The surface oxidation of bare copper and oxidation resistance with surface grown graphene is clearly demonstrated in Fig.
30 by controlled CVD growth of graphene directly on Cu foils with partial to full surface coverage [289].
The colour change of bare copper foil indicates a surface oxidation, whereas the fully covered graphene surface shows no sign of oxidation.
In other case, the oxidation resistance of Fe and Cu foils are also demonstrated by coating them with reduced GO layers [290].
Nilsson et al.
investigate the limitations of graphene as an effective corrosion-inhibiting coating on Pt(100) surface for O2 and CO using scanning tunnelling microscopy measurements and DFT calculations [291].
Graphene layer is found to protect Pt(100) surface against O2 and CO exposure up to a certain pressures of 10−4mbar and 10−6mbar, respectively.
At higher pressures, CO is observed to intercalate under the graphene coating layer.
Designing the polymer thin films or membranes to prevent gas or water molecules from permeating through is a major challenge in many applications for food and electronics.
The graphene–polymer composites have been investigated as improved gas barriers due to the large aspect ratio of graphenes [292–296].
For example, a poly(ethylene-2,6-naphthalate)-graphene nanocomposite film with the addition of just 4wt% of graphene shows 60% decrease of the H2 permeability [292].
Up to 90% decrease in N2 permeation is observed in thermoplastic polyurethane films reinforced with isocyanate treated GO of 3wt% [293].
Whereas, the solution filtration synthesised PEI (polyethylenimine) functionalized reduced GO thin films with a brick and mortar structure show a significant decrease in the H2 permeation rate with increasing PEI content due to tight packing and filling up of gallery spacings of the film assembly [294].
A considerably reduced O2 and CO2 permeation is observed in a layer-by-layer assembly of GO and PEI composite (Fig.
31) [295].
A 10 bilayer film (∼91nm thick) with 0.1wt% PEI and 0.2wt% GO mixtures deposited on a 179μm PET shows O2 transmission rate of 0.12cm3/m2/day.
A high H2/CO2 gas separation with a selectivity higher than 383 is observed, could be due to strong binding of CO2 with the amine groups.
A solvent blending prepared graphene-poly(ethylene vinyl alcohol) (RGO/EVOH) thin film (a thickness of about 0.2mm) with only 0.5wt% of RGO loading shows a significantly decreased permeable coefficient of O2 nearly 1671 times lower than that of neat EVOH film [296].
The permeability measurements are carried out by a differential volume-variable pressure method at room temperature with 37% relative humidity and O2 flow at 1bar on 80mm diameter film.
The interesting gas barrier properties are attributed to a tightly packed nanobrick wall structure that creates super tortuosity and diffusion length for gas molecules.
Ultrathin (up to 20nm) graphene and GO based membranes also show excellent gas permeation selectivities [297,298].
CVD grown large-area monolayer graphene films with increasing numbers of graphene sheets deposited on a Poly(1-methylsilyl-1-propyne) (PTMSP) substrate show improved O2/N2 selectivity suggesting that the gas diffusion through irregularly aligned defective graphene pores as well as slit-like interlayer spacings [297].
The GO thin-film membranes deposited on polyethersulphone supports in humidified state show a high selectivity of CO2/N2, which is ideal for postcombustion CO2 capture.
The gas permeance (in gas permeation units, GPUs) measured at a feed pressure of 1bar, using the constant-pressure and variable-volume method shows very different gas transport behaviour (Fig.
32).
Introduction of humidity in the feed stream enhances the CO2 permeance leading to enhanced selectivities of CO2/CH4, CO2/H2 and CO2/N2.
The highly interlocked layer structure of GO membranes shows a gas permeation in the order of CO2>H2⩾He>CH4>O2>N2.
As shown in Fig.
32d the separation performance of GO membrane in its dry and humidified state is higher than that reported for CO2 for polymeric membranes, including thermally rearranged (TR) polymers, polymers of intrinsic microporosity (PIM), and inorganic membranes, such as carbons, silicas, and zeolites.
A more permeable, H2/CO2 selective is demonstrated by creating thermally generated irreversible and small defective pores on the basal plane due to partly released surface oxygen-containing functional groups by thermal reduction of GO membranes.
Overall the enhanced CO2 selectivities are attributed to the CO2-philic permeation behaviour, which is further enhanced by the presence of water.
It is predicted that the incorporation of a carboxylic acid group lead to the highest isosteric heat.
Although CO2 is a nonpolar gas, the polarity of the individual C–O bonds in the molecule allows for interaction with polar groups in GO.
Thus, CO2 can act as a Lewis acid or a Lewis base and can participate in hydrogen bonding.
Carboxylic acid groups on GO provide a preferential site for CO2 adsorption, consequently retarding CO2 transport in the nanopores by strongly trapping CO2 molecules.
This phenomenon is counterintuitive, because strong affinity between penetrant and nanopore walls often leads to surface diffusion of the penetrant preferentially sorbed on the pore wall, resulting in flux enhancement of highly sorbed or condensable penetrants.
As such, these membranes are promising materials for industrial CO2 separation processes related to petrochemical engineering (CO2 removal from natural gas), the environment (CO2 capture from flue gas), and biomass energy (CO2 recovery from landfill gas).
Ultrathin (∼1.8–18nm thickness) GO membranes prepared by vacuum filtration on anodic aluminium oxide (AAO) support from controlled dilution of GO dispersions show unusual H2/CO2 and H2/N2 separation selectivities as high as 3400 and 900 respectively, at 293K [298].
Normally the microporous membranes show low H2/CO2 selectivity (<10) or are selective to CO2 over H2 at temperatures below 373K due to a strong CO2 adsorption and blocking of H2 permeation.
GO membranes with increasing thickness from 1.8nm to 180nm show exponential decrease in H2 and He permeances.
Here the gas permeance is mainly attributed to the selective structural defects within GO flakes.
The GO interlayer spacing has minimal effect on molecular transport.
The H2/CO2 separation selectivity decreased with increasing temperature, resulting from the faster increase of CO2 permeance.
A ∼18nm thick GO membrane on cellulous acetate support show H2/CO2 and H2/N2 separation selectivities of 1110 and 300, respectively.
As shown in Fig.
33, the performance of the ultrathin GO membranes is far above the upper bound for polymeric (black line) and inorganic membranes.
The emissions of NH3, NO2, SO2, and H2S from the industrial plants during number of combustion processes, wastewater treatment, and food and compositing process are toxic, corrosive and malodorous air pollutants.
Due to growing demand for a more economical and improved process, there is always a need of search for new filtrate/adsorptive removal materials.
Recently, the porous inorganic-graphene based materials for adsorptive removal of NH3, NO2, SO2 and H2S pollutants have been in focus of attention.
Many theoretical and experimental studies are conducted for molecular adsorptive detection/capturing on graphene surfaces and GO based functionalised/inorganic hybrid porous structures.
The DFT calculations suggest that adsorption of nitrogen oxides, NOx: NO, NO2, NO3 on GO is generally stronger than that on pristine graphene due to the presence of active defect sites, such as the hydroxyl and carbonyl functional groups [299].
The interaction of NOx with GO is expected to result in the formation of hydrogen bonds OH⋯O (N), weak covalent bonds C⋯N and C⋯O, as well as the nitrous and nitric acid like moieties.
First-principles calculations reveal that the oxygen groups on GO act as strong binding sites and induce dissociation of the NH3 into the NH2 or NH species by the H atom abstractions [300].
This further leads to the removal of surface oxygen species through the hydroxyl group hydrogenation and the ring opening of epoxy group.
The reaction of NH3 with the hydroxyl and epoxy groups is generally exothermic.
From the first-principles calculations adsorption of H2S onto pristine graphene is found to be very weak due to its small binding energy, large bonding distance and small net charge transfer [301].
Whereas, the Pt-decorated graphene bind up to seven H2S molecules to one side of a Pt–graphene system due to large binding energy and a short Pt H2S bonding length.
The adsorption of several acidic gases (CO2, NO2 and SO2) on light metal (Li, Al) decorated graphene oxide (GO) is also investigated with the first-principles calculations [302].
It is found that Li and Al could be anchored stably by hydroxyl and epoxy groups on GO and acts as strong adsorption sites for CO2, NO2 and SO2 with improved binding energies up to (0.59, 2.29 and 1.08)eV, respectively, compared with Ti.
Experimentally, it is also identified that the acidic nature of GO with its carboxyl, hydroxyl, and epoxy groups act as energetic sites for chemical adsorption of gases, especially for polar molecules.
A highly polar NH3 interaction with the GO through gravimetric uptake measurements is demonstrated back in 1962 [303].
About 180mg/g of NH3 adsorption at room temperature and atmospheric pressure is observed.
A physical adsorption at defective sites/or by hydrogen bonding with hydroxyl groups and a chemical interaction with the acidic sites and condensation in pores is proposed at low- and high-pressures, respectively.
Therefore an increase in graphene inter layer distance proportional to the size of the ammonia molecule is observed.
Recently, there are renewed interests and increased investigations on NH3 adsorption in various types of GO based materials, due to the development of promising and efficient new synthesis methods for obtaining GO and its composites.
Bandosz’s group have been actively exploring the NH3, NO2, H2S and SO2 adsorption and interaction properties in various types of GO and its composites with polymers, metal oxides, and MOFs [304–327].
The NO2, NH3 and H2S molecules have a small kinetic diameter between (0.30–0.36)nm but exhibit different chemical properties (e.g., acid–base and redox properties).
The amount of gas adsorption is demonstrated through the breakthrough measurements by passing a gas stream through a sample packed column with the option of either dry or up to 70% relative humidity at room temperature.
The simple schematic of experimental setup is shown in Fig.
34 [304].
The effects of the degree of oxidation, reduction and exfoliation of the GO samples are examined for toxic gaseous adsorption.
For example, a high NH3 breakthrough capacity of 61mg/g is observed in GO, dried at room temperature compared to 28mg/g in GO dried at 393K and 17mg/g in GO exfoliated at 573K [305–307].
The acid–base reactions and intercalations between the GO layers are identified as the main mechanisms of NH3 retention.
The NH3 adsorption is very sensitive to the additional functional groups present in GO when different synthesis methods are employed.
For example, GO prepared by the Brodie method, NH3 is mainly retained via intercalation in the interlayer space of GO and reaction with the carboxylic groups at the edges of the graphene layers.
On the contrary, the NH3 reaction with the epoxy, carboxylic and sulphonic groups is observed in the Hummers GO [305].
In addition, the role played by surface texture, degree of oxidation and interlayer space/or pore structure on the NH3 adsorptive retention is explained [306].
NH3 dissolving in lamellar water as ammonium ions or by forming hydrogen bonding with epoxy and phenolic surface groups is also proposed [307,308].
The interaction of NH3 with carboxylic groups is proposed as either Brønsted: or Lewis acids: The second reaction seams more likely to happen in the dry conditions.
A variety of GO-composites are synthesised with the introduction of surface acidic groups, metal salts or solid acids.
The intercalation of aluminium and aluminium–zirconium polyoxycations (also called Keggin type cations; in case of aluminium oxycation, the cations have one tetrahedral aluminium surrounded by twelve aluminium in octahedral coordinations, [Al13O4(OH)24(H2O)12]7+) in GO layers show a highly increased interlayer spacing (up to 1.5nm) and NH3 sorption due to additional Lewis and Brønsted acidic centres from the Keggin cations [310].
The GO–POM composites also show enhanced NH3 sorption and retention than GO alone due to the interaction with the both bridging and terminal oxygens of POM (hydrogen bonded to the terminal oxygens of the POM could enhance the acidity) [311].
POM is a kind of Keggin polyanion (consists of a central tetrahedron PO4 surrounded by twelve octahedral MO6 (M=W or Mo)) and exhibits a high acidity.
The enhancement of NH3 adsorption in clays–graphene composite is attributed to the synergistic effect of structure as well as graphene edge acidic groups and clay origin Brønsted or Lewis acidic centres [312].
The GO with simple metal-oxide (MnO2) nanocomposites are also tested for NH3 sorption [313].
Interestingly, GO in its dry condition show negligible adsorption for other pollutants; NO2, H2S and SO2.
However, an enhanced H2S sorption (breakthrough capacity of 30mg/g compared to zero in dry GO) is observed in a functionalised GO that is treated with NH3 at 1223K [314].
The NH3 treatment results in decomposition and rearrangement of oxygen functional groups and incorporation of nitrogen mainly in pyridinic and quaternary entities.
These nitrogen functionalities enhance the basicity and attract HS− ions of dissociated H2S via electrostatic interactions.
The oxidation to elemental S and a smaller amount of SO2 is also detected.
The GO-metal oxide/hydroxide composites show enhanced sorption of NO2, SO2 and H2S by forming nitrates, sulphide, sulphite and sulphates through the reactive paths (1) and (2) shown below [315–320]:(1)(2)An iron-oxide (α-Fe2O3 and Fe3O4)-pillared graphene composite with a hydrophobic nanospace shows high activities for O2, NO and NO2 due to the interaction of NO with active γ-FeOOH and α-Fe2O3, and the formation of bridging nitrates [321,328].
Instant reduction of NO2 to NO is noticed.
A favourable adsorption and conversion of NO2, and NO retention is also observed on GO-polyaniline (PANI) composites [322].
GO in its dry state show no NO2 adsorption, however the sorption and retention of NO2 is observed with the presence of N-methylformamide between GO layers due to polar and acid–base interactions.
The MOF-GO composites show an improved NH3, NO2, and H2S sorption compared to the constituents because of synergistic effect related to the new pore space in the interface between MOF units and GO and a reactive interaction of metal sites and ligands either by hydrogen bonding (MOF-5) or coordination and subsequent complexation (HKUST-1) [323–327,329].
For example, the Lewis interactions of NH3 and the Cu sites, (70–80)kJ/mol and the reaction of NH3 with the ligands, (70–100)kJ/mol exhibit stronger binding energy.
The NH3 adsorption in Fe-MIL shows retention of the framework structure and thus could be useful for adsorbent regeneration.
As shown in Figs.
35 and 36 initially dark blue HKUST-1 and its GO composites turned to light blue upon adsorption of NH3, NO2 and H2S [323,329].
With the progress of the adsorption test, the colour of the sample further changed to a light blue or blackish tint during exposure to NH3 and H2S, respectively.
The first colour change is attributed to the coordination to the Cu sites due to the presence of a lone pair of electrons on the adsorbate molecules.
The second colour change is likely caused by the formation of new complexes like CuS.
The reactive decomposition in HKSUT-1 is attributed to the formation of the Cu(NH3)42+, CuS and Cu(NO3)2 for adsorption of NH3, H2S and NO2, respectively.
In Zn-MOF-GO composites, NH3 interact through hydrogen bonding with the oxygen atoms of the ZnO4 units whereas for Fe-MIL-GO composites, the acid–base reaction thought to be the reactive pathway between the NH3 and water coordinated Fe sites.
Long et al.
show an oxidative absorption of SO2 in the porous GO foams, prepared by freeze-drying technique [330].
As shown in Fig.
37 the GO foams exhibit a high porosity and resemble a sponge-like appearance.
The oxidative adsorption of SO2 to SO3 by reduction of GO at room temperature is observed in a dry condition.
Whereas, the reaction is intense with the GO water suspension; the GO is reduced quite fast and SO2 becomes SO42−.
As discussed above and shown in Figs.
1 and 2, there exist a wide range of graphene/GO based materials; starting with expanded graphite, intercalation, exfoliation, chemical reduction, pillared layers, self-assembly, thin-film membranes, functionalization, doping, metal dispersion, porous template and chemical activation.
Those are investigated for gas sorption, storage and separation.
In particular, the experimental H2 and CO2 uptake results are comparatively analysed in various graphene based materials, as indicated in Tables 2–5, along with the SSA and the measurement conditions (particularly, pressure and temperature).
As specific surface area and pore volume are the key porous parameters to determine the physisorption capacity Fig.
38 shows the H2 uptake in a wide variety of graphenes against the SSA and pore volume.
The structure modification effect on the binding, heat of H2 adsorption is shown Fig.
39.
Earlier, the H2 uptake capacity against the SSA of a wide range of other porous carbons is generalised and is normally accepted that for every 500m2/g of SSA, the H2 uptake capacity at 77K is increased by ∼0.5wt% at 1bar and ∼1wt% at high pressures [82,331,332].
Similarly, at room temperature and pressures up to 100bar the H2 storage in a wide variety of porous carbons is found to be well below 1wt% [333].
From Fig.
38 one can clearly understand that the modified graphenes with carefully tuned pore structure or pillaring or chemical modifications (doping, functionalization and metal dispersion) performed much better than simply exfoliated graphenes.
At 77K, most of the exfoliated/reduced graphenes seems to exhibit more or less similar uptake capacities compared to the other porous carbons with respect to the porosity.
Normally the exfoliation leaves very high mesoporosity due to the highly curved nature of graphenes.
However, it is understood that the modified graphenes enhance the microporosity and functional active sites.
Thus comparatively a high H2 uptake is seen in optimised GO structures, with doped, defective, and metal/alloy nanoparticle dispersed graphenes due to the synergistic effect of enhanced microporosity, binding sites or combined molecular and spillover/dissociative atomic hydrogen adsorption.
Most importantly these graphene based materials exhibit very promising H2 uptake capacities between (1–4)wt% at 300K compared to the maximum of up to 1wt% in most porous solid adsorbents, i.e.
MOFs, zeolites and nanostructured carbons.
This is further evidenced by a high heat of H2 adsorption mostly between (8–15)kJ/mol in the modified graphenes, compared to a typical (4–6)kJ/mol in other porous carbon adsorbents.
These facts are clearly encouraging and help to minimise the gap between DOE targets and experimental results.
Similarly, the highest CO2 capacity is reported in functionalised/doped and highly porous activated graphene materials (Table 5).
The graphene based materials are also found to exhibit enhanced CH4 storage that surpasses the DOE target and other traditional highly porous carbons and most of the MOFs.
The graphene-based composites also show enhanced NH3, NO2, SO2 and H2S sorption and retention.
The theoretical and experimental results on the graphene related membranes for gas separations are very encouraging.
Because of multifunctionality of the graphene-based materials many approaches have been adapted to develop various robust interconnected porous network architectures with various functional groups.
Clearly, by controlling the synthesis parameters and tuning the pore parameters; increasing SSA and microporosity, the chemical modification of graphenes could be a very promising in obtaining practicable gas sorption, storage and separation.
There are many possible routes still available to engineer graphene structures.
For example, a number of robust 3D hierarchical porous graphene structures have been developed through self-assembly, sol–gel and cross-linking chemistry [43,334–338].
A vacuum centrifugal evaporation has been introduced as a strategy for a large scale synthesis of GO sponges with 3D interconnected network hierarchical structure [334].
Subsequent thermal annealing of the GO sponges result in recovery of sp2 graphene structure, forming graphene sponges with a large SSA and typical porous structure.
An autoclaved leavening and steaming of GO layer films lead to reduced GO foams with open porous and continuously cross-linked structures [335].
Compared to reduced GO films, the reduced GO foams show greatly improved performance as selective organic absorbents.
Furthermore, the doping of the graphene foams is performed to enhance its functionalizability.
These graphene foams and its thin film forms could be ideal candidates for an efficient gaseous separation.
Optimised pillared graphene structures with amine-based functional groups could produce superior CO2 adsorbents (pillared GOs with NH2 groups like silicas with NH2).
Much stable structures may be developed due to strong nucleophilicity of amines and epoxy groups on GO through a ring opening reaction without any activation.
Using sol–gel method low-density graphene aerogels with tuneable SSA, (584–1200)m2/g and very high pore volume, (2.96–6.4)cm3/g are synthesised [336].
In another method by gelation of a GO suspension under basic conditions, a robust 3D macroassembly of graphene sheets is obtained with SSA>1300m2/g [337].
Furthermore, the chemical activation of graphene aerogels with KOH results in highly increased microporosity (up to 0.69cm3/g) and SSA (up to 1715m2/g) [338].
Similarly, CO2 activation of the mechanically robust, centimetre sized self-assembled interconnected networks of single-layer graphene aerogel monolith increases the SSA up to 3000m2/g compared to the typically 400m2/g for the non-activated foam [339].
Thus, adjusting synthetic parameters allow a wide range of control over surface area, pore volume and pore size, as well as the nature of the chemical cross-links (sp2 versus sp3) making these materials promising candidates for gas sorption and storage applications.
A method for preparing functionalized 3D carbon-based architectures, consisting of high SSA mesoporous carbon spheres (MCS) with a particle size of 60nm intercalated between graphene sheets (GS) is demonstrated by Lei et al.
[340].
The MCS itself consists of graphene sheets in small domains and exhibits a SSA and pore volume of 2396m2/g and 2.9cm3/g, respectively, obtained from silica template and ferrocene by CVD method.
MCS-GS composite shows hierarchical porous architecture with SSA of 1496m2/g and pore volume of 3.36cm3/g with enhanced microporosity.
Further thermal treatment of GMC-GS in NH3 yields N-doped product.
The N-containing basic groups could be very useful in obtaining improved CO2 adsorption at low-pressures due to enhanced acid–base interactions.
In addition, a wide variety of high and tuneable surface area and pore structure assemblies of graphenes with chemical functionalization are demonstrated through template, pyrolysis and chemical activation methods [341,342].
For example, the silica and zeolite based templates are conveniently used to produce a very high SSA and pore volume graphenes and carbons.
In particular, by controlling the pore size of silica one could obtain much more enhanced microporosity than the reported methods of producing mesoporous graphene structures.
Other possible methods, such as soft co-polymer or sacrificial salt or nanoparticle templating, could provide desirable structures as well.
The chemical activation is another promising method in which the tailor made porous graphene structures are highly possible by simply choosing the right activator; KOH, NaOH, Na2CO3, CO2, steam, etc.
together with concentration and temperature [342].
The chemical modification, functionalization and hybrid structures of graphene based highly porous materials [343–368] could have attractive gas sorption and storage properties.
The boron- and nitrogen co-doped and porous graphene like materials should help in obtaining higher gas sorption and storage.
Zhang et al.
show that a very efficient and industrially scalable approach of synthesising defect/wrinkle mesopore structure graphene sheets with a very high SSA up to 3523m2/g and total pore volume up to 2.4cm3/g [366].
The material obtained through the following two standard industry steps: (1) in-situ hydrothermal polymerisation/carbonisation of the mixture of cheap biomass or industry carbon sources with GO to get the 3D hybrid precursors; and then (2) a KOH chemical activation step.
An enhanced surface area and pore volume is also obtained with the addition of GO to mesoporous carbon.
Tuneable surface area and porosity parameters are also demonstrated in mesoporous carbons with the controllable addition of GO.
Recently, graphenic type fragments are synthesised from zeolites/MOFs as templates, with either self- or addition of small molecular carbon precursors [369–375].
Upon carbonisation, the organic linker ligand in MOF transforms into single layers functionalised graphene fragments with new hierarchical high slit-like micropores and mesopores while retaining the MOF framework porosity.
These carbons with new pore types show ultrahigh porosities i.e., simultaneous high surface area (up to 3300m2/g) and micro- and total-pore volumes (up to 5.53cm3/g) [369,372].
These functionalised carbons yield exceptionally high H2, CH4 and CO2 uptakes compared with the capacities of those benchmark microporous materials under identical conditions.
In particular, a gravimetric (volumetric) H2 uptake of 3.25wt% and 7.3wt% (50g/l) are achieved at 77K under 1bar and 20bar, respectively [369,370].
At 100bar and 298K, these carbons show ∼0.94wt% of H2 storage.
The functionalised MOF carbons also show record high CH4 and CO2 uptakes [372–375].
For example, the room temperature uptake of up to 4.6mmol/g at 1bar and over 27mmol/g (119wt%) at 30bar are again some of the highest values reported in the literature for porous carbons.
The overall synthesis and gas uptake mechanism of such graphenic type MOF carbons is schematically shown in Fig.
40.
The novel ultrathin, robust nanocomposite membranes by incorporating GO sheets into silk fibroin matrix through heterogeneous surface interactions in organised layer-by-layer manner show outstanding mechanical properties, with a tensile modulus of 1.45×106bar (145GPa), an ultimate stress of more than 3000bar (300MPa), and a toughness of above 2.2MJ/m3 [376].
These nanocomposite membranes in their support and free-standing state could be valuable for potential applications in protective molecular coatings, permeable membranes for separation and delivery.
A new avenue for graphene-based ultra-films anchored on solid supports with a high stability and controlled thickness via a layer-by-layer assembly is demonstrated using amino-substituted π-conjugated compounds, including 1,4-diaminobenzene, benzidine, etc., as cross-linkages [377].
A thermal annealing leads to a reduction of the films thus could yield suitable pore space for molecular sieving.
Furthermore, the amine groups could show selective gas permeations.
A simple and effective method of preparing functional nanoporous graphene/carbon composite membranes from MOFs is also demonstrated and reviewed [378,379].
The resulting symmetric dense or asymmetric composite membranes exhibit good performance in gas separation and liquid separation via pervaporation.
In summary, the rich chemistry of graphene nanosheets have offered great potential for the design and development of many combinations of interconnected porous graphene-based materials as promising molecular adsorbents, with high thermal/chemical stability, tuneable specific surface area and pore structure.
The other advantages are the relatively low cost and easy scale-up associated with the manufacturing of the porous graphene carbons.
Graphene and its composites have been used as adsorbents for low temperature physisorption and high temperature chemisorption of H2, CO2, CH4, etc., showing promising results, especially when compared with conventional high surface area activated carbons.
As highlighted in this report, the interest in gas sorption and storage by physical adsorption has grown considerably over the last few years.
While many computational studies prove strong potential of graphene and its derivatives, practical developments have advanced markedly in molecular pillaring, metal dispersion, doping, functionalization and 3D interconnected networks to enhance gas sorption and storage.
Clearly, there still exist several challenges in practice, such as the effective control of the desirable binding sites, porosities and functionalities; the balance of functional properties with structural integrity; and further tuning of molecular selectivities in gas mixtures for purification and sensing.
More specifically, there have been many attempts to use carbon adsorbents as reversible H2 storage materials.
Most of the measurements so far have been conducted at 77K, which limits the practicality.
Similarly, carbon capture at ambient conditions is not satisfactory.
As summarised in Tables 2–5, and Figs.
38 and 39, the remarkable synergistic properties from combining GO hybrid structures showed promising room temperature H2 storage and high pressure CH4 and CO2 adsorption capacities.
For the given rich chemistry and functionalizability of graphene, the number of new compounds are continuously being tailor-made.
Thus if carefully tuned the microporosity and the high surface area of the chemically modified graphene materials could be very promising for H2 and CH4 storage and CO2 capture.
Similarly, the gas permeation separation on graphene and GO based membranes is highly dependent on GO sheet size and thickness, layer structure and assembly, residual lamellar water content, and deformation of thin-layered structures during dewetting can all effect.
There is much scope in tuning and controlling the parameters to develop practically useful membranes.
This work was supported by EPSRC Grant Nos.
(EP/L018330/1 and EP/G063176/1).
Large Eddy Simulations of turbulent particle-laden channel flow This paper scrutinises the Large Eddy Simulation (LES) approach to simulate the behaviour of inter-acting particles in a turbulent channel flow.
A series of simulations that are fully (four-way), two-way and one-way coupled are performed in order to investigate the importance of the individual physical phenomena occurring in particle-laden flows.
Moreover, the soft sphere and hard sphere models, which describe the interaction between colliding particles, are compared with each other and the drawbacks and advantages of each algorithm are discussed.
Different models to describe the sub-grid scale stresses with LES are compared.
Finally, simulations accounting for the rough walls of the channel are compared to simulations with smooth walls.
The results of the simulations are discussed with the aid of the experimental data of Kussin J. and Sommerfeld M., 2002, Experimental studies on particle behaviour and turbulence modification in horizontal channel flow with different wall roughness, Exp.
in Fluids, 33, pp.
143–159 of Reynolds number 42,000 based on the full channel height.
The simulations are carried out in a three-dimensional domain of 0.175m×0.035m ×0.035m where the direction of gravity is perpendicular to the flow.
The simulation results demonstrate that rough walls and inter-particle collisions have an important effect in redistributing the particles across the channel, even for very dilute flows.
A new roughness model is proposed which takes into account the fact that a collision in the soft sphere model is fully resolved and it is shown that the new model is in very good agreement with the available experimental data.
Particle-laden turbulent flows can be found in various industrial and environmental processes.
Examples of such processes are pneumatic transport of particles; energy conversion of fossil fuels; movement of soot particles in the atmosphere; the flow of particles in cyclones and many more.
Understanding the effects of particle–fluid interactions is of utmost importance because this will result in a more accurate implementation of these processes.
Additionally, applications such as sediment transport, where the direction of gravity is perpendicular to the flow, particle–particle and particle–wall collisions become very important.
Therefore, the need to understand the effects of these additional physical phenomena is of fundamental importance.
Thus robust numerical simulations will therefore help the optimisation and better design of industrial processes and provide a more reliable prediction of environmental processes involving particles.
There are various frameworks in which the continuous phase for gas–solid flows can be predicted, i.e.
Direct Numerical Simulation (DNS), Large Eddy Simulation (LES) and the Reynolds Averaged Navier–Stokes (RANS) method.
DNS methods offer high accuracy in resolving all scales without ad hoc modelling at the expense of huge computational time.
Currently, DNS can only solve flows of relatively low Reynolds (Re) numbers, which are outside of most engineering and industrial interests.
Although the computational effort for LES is still very high, it is considerably lower than for DNS and it has therefore become very fashionable for analysing flows in academia and it is also an emerging tool in industry.
LES solves the Navier–Stokes equations up to a particular length-scale due to the application of a filter.
Length-scales smaller than the cut-off filter width (Δ) are modelled with a so-called sub-grid scale (SGS) model.
The cut-off width is an indication of the smallest size eddies that are retained in the computations and eddies smaller than Δ, are filtered out.
Due to the filtering of the Navier–Stokes equations, models are required to provide closure for the SGS stresses, which account for the effect of the unresolved scales on the convective momentum transport.
In this paper, the well known model proposed by Smagorinsky (1963) with van Driest damping near the wall is used to model the SGS stresses.
Moreover, the model proposed by Germano et al.
(1991) and Lilly (1992) is also adopted.
The results of the two LES models are compared with each other in order to verify that the solutions are independent from the SGS models.
There are various frameworks to model the collisions between particles; via stochastic or deterministic methods.
Stochastic methods, such as the one proposed by Sommerfeld (2001), generate fictitious collision partners with a given size and velocity and as a result no information regarding the real position and velocity of the particles and the corresponding fluid environment is required.
Stochastic collisions are therefore performed via the use of a probability density function which is based on kinetic theory.
The benefit of this method is the speed of computation of the collisions because no collision pairs are searched within the domain.
However, the downside of these methods is that the particle and fluid velocity fluctuations need to be assumed (e.g.
Gaussian) and this may prevent the prediction of clustering.
Deterministic methods on the other hand, determine collision pairs by using the particles actual position and velocity.
The actual collisions can be performed either by the soft sphere or hard sphere model.
In this work the particle interactions are modelled according to the soft sphere and the hard sphere models, which are both deterministic methods, in order to investigate their main differences.
In the soft sphere model, first applied by Cundall and Strack (1979), the collisions are approximated by the elastic and plastic deformation on the particle–particle contact area occurring during a collision.
Such a deformation can be mathematically described by a spring-dashpot-slider model (Tsuji et al., 1992).
On the other hand, the hard sphere model, which was first proposed by Maw et al.
(1976) and further developed by Louge (1994), uses the conservation of momentum of the particles and approximates the collisions as instantaneous and binary.
In other words, the hard sphere collisions are event-driven, as opposed to the soft sphere model which uses a fixed time-step, and hence this makes the large scale simulations potentially faster.
Note, however, this is only valid for fast flowing dilute flows where the errors of the hard sphere model approximations are negligible.
In addition, in these type of simulations, a large number of particles are required.
The soft sphere approach is computationally more intensive compared to the hard sphere model because the collision of each particle is fully resolved.
On the other hand, the soft sphere model potentially has a higher accuracy as no empirical data, besides the material properties, are required to compute the collisions.
In this work, the point-particle or point-mass approach is used to approximate the presence of particles as seen by the fluid.
The effect of the particles on the fluid phase is modelled as an inter-phase momentum exchange source term.
Elghobashi and Truesdell (1992) mention that the point-particle approach is valid if the particle diameter (dp) is smaller than the Kolmogorov scale (ηκ).
This implies that dp must be smaller than the grid size (Δx).
Although accounting for the volume fraction effects on the drag force is probably not of large importance in the test-cases simulated in the current research work, it might have an effect on the particle-clustering.
Individual particles are tracked by solving Newton’s second law.
Moreover, the drag force of the fluid acting on the particles is added via the correlation proposed by Wen and Yu (1966).
Therefore the simulations performed in this work are fully coupled, or four-way coupled.
The purpose of this paper is to examine the particle behaviour in a horizontal channel flow with the gravity acting perpendicular to the main flow direction.
This paper first compares the hard sphere and soft sphere methodologies and evaluates their differences as opposed to other studies that primarily use the hard sphere methodology, such as Sommerfeld (2003).
The results of the two models are compared to the experiment of Kussin and Sommerfeld (2002), who investigate the particle behaviour and turbulence modification of a horizontal channel flow.
Finally, this article investigates the differences and effects of simulations that are one-way coupled, two-way coupled and four-way coupled on the particle statistics.
Moreover, the effect of the wall roughness on the particle statistics is investigated and compared to the available experimental data and a novel wall roughness model that is used in conjunction with the soft sphere methodology is proposed.
This paper is organised in six sections.
Section 2 describes how the fluid-phase is solved and how the fluid sub-grid scales are modelled.
Section 3 describes how the equation of motion for each particle is solved, the method used for particle tracking and the models used for the wall roughness for both the soft sphere and hard sphere models are discussed.
Section 4 describes the simulation set-up and Section 5 compares and discusses the numerical results with the available experimental data and various set-up conditions.
Section 6 summarises the main conclusions of this work.
The filtered momentum equation for the fluid phase is(1)∂(αfρfṽf,j)∂t+∂(αfρfṽf,jṽf,i)∂xi=-αf∂p̃∂xj+∂(αfτ̃ij)∂xi-∂αfτija∂xi+Sf,j+∑p=1phases≠fβ(f,p)[ṽf@p,j-vp,j]where αf is the fluid volume fraction, ρf is the fluid density and ṽf,i is the filtered fluid velocity.
The last two terms on the right hand side of Eq.
(1) are source terms; Sf,j is an additional source term; and ∑p=1phases≠fβ(f,p)[ṽf@p,j-vp,j] is the inter-phase momentum exchange between the two phases respectively; the subscript f@p indicates the undisturbed fluid at the location of the particle.
For more details and validation see Electronic Annex A in the online version of this article.
As the domain is periodic in the direction of the flow an additional source term is required to drive it.
This source term is equal to the integrated wall shear stress.
This additional source term is added in the filtered momentum equation, analogously to the pressure drop.
Furthermore, there are two ways to implement this: (a) by fixing the mass flow rate ṁ, which will be corrected by adjusting the forcing term in the momentum equation at every time-step; and (b) by specifying a constant pressure gradient (dp/dx), which can be applied when the required wall shear stress is known.
The former has been used in the current simulation which results in(2)Sf,1=ṁo-ṁnAcrossΔtnwhere ṁo is the specified mass flow rate at a given cross-section; ṁn is the computed mass flow rate at current time step; Across is the cross-sectional area; and Δtn is the current time-step.
Sf,1 has the units of pressure gradient; i.e.
kgm2s2.
Note that this is only implemented in the x-direction, Sf,2 and Sf,3 are zero, as there is no net flow in these directions.
Newton’s 2nd law for a particle in a gas is(3)mpdvp,idt=βVpαp(vf@p,i-vp,i)+mpgi+Fpw,i+Fpp,iwhere mp is the mass of the particle, vf@p,i is the undisturbed fluid velocity along the particle, vp,i is the particle translational velocity and β is the drag function as proposed by Wen and Yu (1966), where the reciprocal of the Eulerian fluid-particle timescale is given by(4)β=34CDαpαfρf|vf@p,i-vp,i|dpαf-2.65and CD represents the coefficient of drag for an individual particle and αf represents the fluid volume fraction.
The detailed equation of motion of a particle is provided in Electronic Annex B in the online verison of this article.
The coefficient of drag, CD, is defined as (Rowe, 1961)(5)CD=241+0.15((1-αp)Rep)0.687Rep(1-αp),if(1-αp)Rep<10000.44,if(1-αp)Rep⩾1000 Modelling the particle motion in the Lagrangian framework involves tracking the properties of individual particles and the fluid properties at the particle’s location.
MultiFlow (van Wachem et al., 2012), which is an in-house multiphase code, achieves this by creating a particle mesh.
This mesh is isotropic and homogeneous in all Cartesian directions and completely overlaps with the corresponding fluid mesh, see Fig.
1.
The particle grid spacing is directly proportional to the mean particle diameter (constant C in Fig.
1, where dp is the particle diameter).
The particle mesh is used to determine the interpolation properties from the fluid phase to the particle phase, as well as to enable collision-neighbour finding lists.
The fluid effects on the particles are modelled using Eq.
(3).
Including only this effect and neglecting the effect of particles on the fluid and particle–particle interactions is referred to in the literature as one-way coupling.
Note that the fluid velocity in Eq.
(3) strictly represents the undisturbed fluid velocity of a particle’s centre along its trajectory.
Strictly speaking this velocity does not exist because of the particle’s presence at that point.
In the point-particle approach, the fluid velocity at the location of the particle is simply found by interpolation.
Note that the fluid velocity is in the Eulerian framework and the particles are in the Lagrangian framework.
Therefore, it is required to transform the Eulerian fluid properties to Lagrangian at the particle’s centre by an interpolation technique.
In this work, spline interpolation has been used to interpolate the fluid properties from the fluid mesh to the particle mesh.
Yeung and Pope (1988) perform a study on the interpolation schemes in homogeneous turbulence and report that spline (or third order Lagrangian polynomial) interpolation has the least effect (or minimum error) on the fluid energy spectrum.
Balachandar and Maxey (1989) investigate the effect of interpolation methods on one-particle and two-particle dispersion in homogeneous turbulence.
They also report that spline interpolation offers the highest accuracy and least computational time when compared to other methods, which is important for two-particle dispersion (or coagulation).
The particle effects are included in the fluid momentum equation (Eq.
(1)) as a source term approximated by the Wen and Yu (1966) drag function (Eq.
(4)).
The inclusion of this source term is referred to as two-way coupling.
For two-way coupling, the Lagrangian particle properties must be transformed to Eulerian because all properties must be continuous.
Interpolation from the particle centres to the particle mesh (and consequently to the fluid mesh) is performed on a volume basis.
This is because one particle cell might have several particles and/or several fractions of particles and this leads to different weighting of each particle within each particle cell.
The contribution of each particle to the particle mesh (and vice versa) is determined by the fraction of the volume of the particle present in each particle cell and, consequently, in each fluid cell.
Therefore, the two-way coupling is the total contribution of all particles and fractions of particles in the fluid cell.
The model has two distinct timesteps, corresponding to the particle and the fluid.
The two-way coupling term is determined at every particle timestep which is always smaller than the fluid timestep.
Thus, the total contribution of the two-way coupling term at the fluid timestep is updated by the cumulative contribution at each particle timestep.
When collisions are neglected, the particle time-step is set to a small and fixed value, so it is much smaller than the fluid time-step.
Loth (2000) discusses the assumptions required for this approach.
Eaton (2009) discusses the relevant difficulties of the point-mass approach in the LES framework.
The main assumption of the point-particle approach is that the particle diameter must be smaller than the Kolmogorov micro-scale (ηκ) and smaller than the grid size.
Bagchi and Balachandar (2003) investigate the effect of turbulence on the drag and lift of a particle via DNS of an isotropic field.
They report that when the particle diameter is within the range 1.5ηκ<dp<10ηκ, the drag law is accurately predicted.
Moreover, Vreman et al.
(2009) mentions that the drag force acting on the particles is reasonably predicted when dp<4ηκ.
Based on the experimental data concerning this study, the Kolmogorov length scale at the centre of the channel and near the wall are ηκ,centre=9.35×10−5m and ηκ,wall=2.911×10−5m.
Therefore, throughout the channel in this study this ratio does not exceed dp/ηκ<7.
Furthermore, Yamamoto et al.
(2001) show that for large particle Stokes numbers (St≫1) the dispersion of particles is not affected by the subgrid scales.
Hence, in this study it is not expected that the particle statistics to be affected significantly by the unresolved scales.
Moreover, because Δy⩾ηκ everywhere in the domain, including in the near-wall region, the assumption that the particle is much smaller than the mesh spacing is also satisfied in this study.
The particle source terms have a different impact on the flow since interpolation is also performed between the particles (their centres) and the particle mesh.
Cubic spline interpolation is used to interpolate properties from the vertexes of the particle mesh to the particle centres.
The particle mesh also enables the efficient tracking of inter-particle and particle–wall collision pairs (or four-way coupling).
The particle mesh significantly reduces the search of possible collision pairs, either between particles or between particles and wall-segments, and hence the computational run-time.
The interactions of particles with other particles and walls are of dynamic nature.
This is because the particle movements are essentially defined by the particle–particle interactions, particle–wall interactions, particle–fluid interactions and/or body forces.
Newton’s 2nd law is solved for each particle, accounting for these interactions and thus obtaining the individual trajectories (i.e.
Lagrangian framework).
The integral for the soft sphere model is approximated with the Verlet algorithm (Allen and Tildesley, 1989), whereas for the hard sphere model with an explicit scheme.
The major distinction between the soft sphere and hard sphere models is that the soft sphere collisions are fully resolved.
The deformation of the particles undergoing a collision is approximated and the resulting repellent force is determined.
This implies that the soft sphere model needs a very small time-step, much smaller than the fluid time-step.
In other words, the soft sphere model computes the actual deformation of the particles and the corresponding contact forces which depend on the contact time of the collision.
On the other hand, the hard sphere algorithm performs each collision only once since it is approximated as instantaneous.
Collisions are treated by evaluating the potential collision time between each pair.
Therefore, this framework is so-called event-driven.
The hard sphere collisions are resolved by satisfying the global conservation of momentum and only depend on the direction of motion of each particle and their corresponding elapsed collision time.
Hence, the operation of the hard sphere algorithm is significantly faster compared to the soft sphere algorithm in fast flowing dilute flows.
The effect of rough walls has shown to be important in a number of gas-particle flows (Sommerfeld and Kussin, 2004) because the particles that collide with a rough wall have a tendency to be suspended into the flow.
In horizontal channel flow simulations, neglecting the effect of wall roughness, a large number of particles grazing the wall are predicted.
It was shown experimentally by Kussin and Sommerfeld (2002) that the wall roughness strongly enhances the transverse dispersion of the particles and their fluctuating velocities throughout the channel.
The measurements have also revealed that the wall roughness causes a significant reduction of the mean horizontal velocity of the particles.
The most obvious approach to model a rough wall is a deterministic approach, where the wall roughness is resolved.
However, because of the rapidly changing normal of the wall or the small length scale required to describe the wall roughness, a fully deterministic approach is very costly.
Therefore, a stochastic approach to model wall roughness is adopted.
There are a number of stochastic approaches described in the literature (for example see Tsuji et al., 1987), the most applied model is of Sommerfeld (1992) and later corrected by Sommerfeld and Huber (1999) for the so-called shadow effect.
A stochastic model usually works with a virtual wall concept, which changes the orientation of the wall with a randomly chosen angle roughness γ, see Fig.
2.
The angle γ is sampled according to the following algorithm (Sommerfeld and Huber, 1999):1.Sample a roughness angle, γ, from a normal distribution.
The standard deviation for this distribution is given by the actual roughness of the wall as experienced by the particle.
If a negative roughness angle with an absolute value larger than the pre-collision angle, α is sampled, the roughness angle is rejected, as this is a non-physical collision; the so-called shadow-effect.
Rotate the local solid wall with the random roughness angle, γ and so it has normal nγ.
This fictitious wall replaces the actual solid wall in determining the collision dynamics.
The above algorithm has been further refined by Konan et al.
(2009), by realising that the above algorithm only accounts for a single collision with a rough wall.
In the original algorithm of Sommerfeld and Huber (1999), when the post collision angle is very small, a so-called grazing particle is predicted, e.g.
a particle which remains close to the wall.
However, in reality it is very likely that such a particle will endure a second wall collision very soon after the first collision.
This effect decreases the likelihood of random rough wall angles leading to very small post collision angles.
So far, all the employed rough wall models from the literature have dealt with hard sphere type collision models, where the actual collision is assumed instantaneous.
The rough wall model can then be used as a black box; using a direct probability density function using the pre-collision angle to predict a post-collision angle.
In this work the roughness model of Sommerfeld (1992) with the shadow wall effect is used for the simulations were the particles are considered as hard spheres.
However, the algorithm needs to be refined when the particles are considered as soft spheres.
This is because the collisions are fully resolved, allowing for a realistic collision time and multiple collisions to occur at the same time.
The important consequence from resolving the collision as it occurs, is the assumption that walls have an infinite size.
For instance, a particle colliding with the virtual wall depicted in Fig.
2 might leave the domain at the bottom.
In reality, this would not occur because of two reasons.
The first reason is that a real rough wall has an amplitude, which is assumed zero in the virtual wall method.
The second reason is that a real rough wall segment is of finite length; usually small compared to the particle size.
To overcome these two shortcomings in a soft sphere framework, a variation of existing virtual wall procedure is employed:1.When the shortest particle–wall distance is the wall roughness amplitude (taken to be 10% of the particle diameter) one virtual wall is generated at the point of the particle which is closest to the wall.
The virtual wall is generated with the original algorithm (Konan et al., 2009; Sommerfeld and Huber, 1999) as outlined above.
If the shortest particle–wall distance becomes half of the distance at which the virtual wall was inserted, i.e.
the particle has moved closer to the wall, a second virtual wall is introduced, with a newly randomly sampled angle.
This is shown in Fig.
3.
The addition of new virtual walls is repeated until the particle is moving away from the wall.
The required standard deviation for the normal distribution is taken from the experimental data provided by Kussin and Sommerfeld (2002).
In the analysed flow, up to three virtual walls are required to deal with the rough wall collision, although almost all collisions are dealt with by application of the first rough wall.
The large-scale simulations are performed in the Eulerian–Lagrangian framework and the predictions are compared to the experimental work of Kussin and Sommerfeld (2002).
In their work, a horizontal channel with a height of 35mm, a width of 175mm and a length of 6m, corresponding to approximately 170 channel heights, is used.
A flow of an air-particle mixture with various particle sizes and mass loadings is introduced in the horizontal direction.
This paper focuses on the results obtained for the single phase flow and the two-phase flow with mass loading ϕ=1.0, which is based on the experimental conditions.
At this mass loading both fluid-particle as well as particle–particle interactions are expected to be important.
The experimental Reynolds number considered based on the channel height is 42,585, arising from the average air velocity of Uav=19.7m/s, air density of ρf=1.15kg/m3 and a viscosity of μf=18.62Pas.
The friction Reynolds number based on the half channel height is Reτ=600.
The particles considered are glass beads, ρp=2500kg/m3, with an average diameter of 195μm and a narrow particle size distribution as described in Kussin and Sommerfeld (2002).
In the simulations, particles are tracked for 47 TL, where TL is the integral time scale of turbulence at the centre of the channel.
In the Electronic Annex C in the online version of this article the various particle properties and Stokes numbers based different fluid timescales are presented.
It is important to note that the Stokes number of the particles for all definitions is greater than one.
The domain used for the simulations is sketched in Fig.
4.
The simulations are carried out with our in-house code MultiFlow (van Wachem et al., 2012; Bruchmüller et al., 2011), which is a fully coupled parallel computational fluid dynamics code based on finite volume discretisation.
The simulations are carried out in a three-dimensional domain of 0.175m×0.035m×0.035m, where the X direction corresponds to the direction of the flow and the negative Y direction is the direction of gravity.
The X and Z directions are taken to be periodic.
The flow is initialised by setting a mean velocity corresponding to the mass flow rate of the experimental data provided by Kussin and Sommerfeld (2002).
On top of the mean, synthesised turbulence is added as randomly sampled from a von Karman spectrum, using the Fourier modes of a fully developed turbulent spectrum.
The initial condition does not impose a flow profile; the flow profile is formed as a result of solving the Navier–Stokes equations and enforcing the no-slip condition for velocity at the wall.
The boundary conditions at the wall are set as no-slip.
For the simulations involving particles, the particles are introduced uniformly in the domain with a small random slip velocity compared to the local fluid velocity.
The number of particles in the domain, which is determined from the mass loading of ϕ=1.0, given in the experimental set-up, is 24,500, leading to a particle volume fraction of αp=4.7977×10−4.
The experimental data provided by Kussin and Sommerfeld (2002) have a slightly different mass flow rates for the single phase and particle laden cases.
The forcing term (see Eq.
(2)) keeps a constant mass flow rate of ṁ=0.028175kg/s for the single phase and ṁ=0.027044kg/s for the particle laden cases, computed from the data provided by Kussin and Sommerfeld (2002).
The resulting pressure drop equals the integrated wall shear stress in the channel.
In addition, the pressure is fixed to a reference value on one arbitrary cell face inside the domain.
In the Electronic Annex A, spectra for the one dimensional spanwise and streamwise velocities are included in order to ensure that the LES simulations in this study resolve most of the energetic lengthscales.
Two computational meshes are used to carry out the single-phase simulations in order to show that the solution is grid independent.
The coarse geometry is then used to carry out the gas-particle simulations.
The coarse mesh contains a total of 870,000 computational cells and the finer mesh contains a total of 1,299,000 computational cells.
The refinement is achieved by refining the nodal spacing equally in all directions.
Both meshes resolve the wall boundary layer, and contain 5 and 12 mesh points within the y+=10 layer, respectively.
Near the wall a DNS resolution is obtained by using(6)y=ymax121+tanhRyymax-12tanh12Rwhere R is a constant set to 7.0 and defines the amount of refinement near the wall.
ymax=35.0mm, is the channel height.
In addition, in every x+=50 and z+=30, 1 mesh point is uniformly added.
The discretisation of the Navier–Stokes equations is done using a finite volume approach, combined with a second order accurate three point backward Euler time discretisation for the temporal terms and a second order accurate central differencing scheme for the advection term.
The pressure velocity coupling is done in a fully coupled framework, using one outer iteration per time-step (van Wachem et al., 2007).
Single-phase simulations are performed in order to validate the performance of the LES models.
The Smagorinsly model with van Driest dampening and the dynamic model are compared with the experimental data.
The single-phase results are in very good agreement with the corresponding experimental data.
Additionally, in order to verify that the numerical solution is grid independent, mesh refinement is performed.
The results show that the solution is indeed grid independent.
The interested reader is refered to the Electronic Annex A in the online version of this article.
Particle laden simulations are carried out and compared to the experimental work of Kussin and Sommerfeld (2002).
The effects of wall roughness, one-way, two-way and four-way coupling are investigated and compared.
The purpose of this section is to investigate the effect of the subgrid scale models when used in conjunction with the fully coupled simulations.
Fully coupled simulations with the Smagorinsky model with van Driest dampening and the Dynamic Germano model are performed and compared to the particle-laden experimental data.
Yamamoto et al.
(2001), who perform LES simulations of a vertical particle-laden channel flow, question the suitability of the Dynamic Germano model.
The Dynamic Germano model, which utilises plane averaging in order to be numerically stable, can be affected erroneously by the anisotropy caused by the particles.
This violates the assumptions of the model.
Fig.
5 compares the mean horizontal velocity predicted by the two subgrid scale models.
Fig.
5 shows that the Smagorinsky model with van Driest dampening is closer to the experimental particle-laden mean horizontal velocity.
Additionally, the Dynamic Germano model modulates turbulence by a higher percentage compared to the Smagorinsky model with van Driest dampening, see Fig.
6.
Note that Kuerten (2006) investigates the effects of subgrid scale models on the particle statistics and reports that the mean particle wall-normal (i.e.
horizontal) velocity is least accurate compared to the DNS results.
Based on these results, the Smagorinsky model is chosen for the remaining sections of this paper.
Fig.
7 compares the horizontal mean fluid velocity with the experimental data both for smooth and rough walls.
Similarly to the single phase simulations the results for the rough walls are slightly over-predicted at the centre of the channel compared to the experimental data.
One possibility for this small discrepancy is that the experimental data have a small mean velocity in the vertical direction, which implies that the flow in the experiment is not fully developed.
The mean vertical velocity is small, on the order of 1.6% compared to the mean horizontal velocity.
Another possibility is that the effect of the sub-grid scales is ignored and this may have a small effect on the flow as well.
It is interesting to note that when the walls are treated as rough the mean fluid velocity is similar to the particle-free experimental results.
Fig.
8 shows the dimensionless mean horizontal velocities as a function of dimensionless height.
This means that the average mean fluid velocity shape is not influenced by the presence of the particles, although the simulations predict a small effect of the particles on the flow.
Additionally, the simulated flow profile for the particle laden cases without wall roughness shows a slight asymmetry.
This is because more particles are found in the bottom half of the channel, lowering the fluid velocity in this region due to the effect of two-way coupling.
Although there is no experimental data for this precise case, a similar observation was made by Lain et al.
(2002), who experimentally and numerically investigate the four-way coupling of a particle-laden horizontal channel flow for similar experimental conditions.
The experimental particle-free and particle-laden RMS velocities have small differences between them, as shown in Fig.
9, which compares the fluid horizontal velocity fluctuations.
On the other hand, the simulation results show larger differences in RMS velocities.
The simulations predict a large attenuation of the turbulence in the channel flow due to the addition of particles.
Therefore, the predicted particle-laden velocity fluctuations are lower compared to the corresponding experimental results.
This shows that the two-way coupling over-dampens turbulence by 25% at the centre of the channel.
This is opposite to the findings of Eaton (2009), who reports that the two-way coupled simulations do not attenuate turbulence sufficiently.
In fact, Eaton (2009) mentions that by ten-folding the mass loading (i.e.
by adding more particles) the correct turbulence attenuation is achieved.
Eaton (2009) has not provided a physical explanation for this.
Yamamoto et al.
(2001), who perform LES simulations on a vertical channel, mention that for large Stokes numbers their computations for turbulence attenuation do not agree with the experimental results.
For small Stokes numbers, however, Yamamoto et al.
(2001) mention that the predicted turbulence attenuation is in good agreement with the experimental results.
The shape of the predicted horizontal RMS velocity of the particle-laden case compared to the single phase case is also somewhat different.
The shape of the single-phase horizontal RMS velocity, as predicted by the simulations, are symmetric.
On the other hand, the corresponding particle-laden case shows a slight asymmetry in the RMS fluid velocity.
This is because without the wall roughness more particles tend to remain near the bottom of the channel, and most graze in a layer near the bottom.
This influences the fluid RMS velocity profile due to the two-way coupling, which makes it asymmetric as opposed to the symmetric profile as predicted for the rough wall case.
In the smooth wall case, the rebound angle of the particles is smaller compared to the rough wall case.
Thus less particles are found at the centre of the channel which would then be dispersed by the turbulence to other parts of the channel, e.g.
the top wall.
The smooth walls fail to do this and due to the action of gravity, the particles tend to remain near the bottom wall.
Turbulence is more suppressed at the bottom half of the channel due to the high particle volume fraction and two-way coupling, thus creating an asymmetric RMS velocity profile.
As also mentioned by Lain et al.
(2002), this effect is more pronounced with increasing mass loading.
Additionally, this behaviour has important consequences on the shear stresses, which are plotted in Fig.
10.
The shear stresses on the wall for the smooth wall case are asymmetric, opposed to the rough wall case and the single phase case, e.g.
at y/H=0.05, uf,rmsvf,rms=−0.45 but at y/H=0.95, uf,rmsvf,rms=0.65.
This is also illustrated in Fig.
11, which compares the particle concentration obtained by the simulation with the rough and smooth walls with the experimental data.
For both the soft and hard sphere models simulating smooth walls, the concentration of the particles is much higher at the bottom wall.
However, when the walls are treated as rough, the particle concentration is almost homogeneous and is in very good agreement with the experimental data.
The wall roughness, therefore, is important as it helps to redistribute the particles into the main flow.
The redistribution of particles is also driven by particle–particle collisions, even in this dilute case; where the particle volume fraction is αp=4.7977×10−4.
To illustrate this, Fig.
12 compares the particle concentration for rough walls with and without inter-particle collisions.
When inter-particle collisions are not taken into account, the particle concentration at the bottom wall is 16.6% higher, despite accounting for wall roughness.
In addition, the particle concentration gradient without particle–particle collisions at the centre of the channel is steeper by about 37%.
This illustrates the importance of particle–particle collisions even at very low particle volume fractions.
Fig.
13 shows the instantaneous distribution of particles for simulations with: (a) with particle–particle collisions and wall roughness, (b) with no particle–particle collisions and wall roughness (c) with particle–particle collisions and smooth walls, and (d) with no particle–particle collisions and smooth walls.
Fig.
13a–c illustrate that the particles remain suspended in the channel, however with a different concentration profile.
On the other hand, when particle–particle collisions are ignored and the walls are treated as smooth, particles with time slowly migrate to the bottom wall and remain there.
In fact the flow now resembles sediment transport because the particles are now sliding across the bottom wall.
The importance of the inter-particle collisions in redistributing the particles in the channel becomes apparent.
Vreman et al.
(2009) numerically investigate the effect of particle–particle collisions for a vertical channel, but at a much lower Re number and much higher mass loading.
They report that the collisions affect the statistics of both the fluid and the particles.
They conclude that it is important to include the particle–particle collisions in order to correctly predict the modification of the fluid and particle statistics.
Yamamoto et al.
(2001) investigate the particle–particle collisions at lower mass loadings and reach similar conclusions.
On the other hand, particles remain distributed across the channel when inter-particles collisions are taken into account even for the smooth walls (see Fig.
11), whereas this is not true when inter-particle collisions are ignored.
Therefore, inter-particle collisions act as an extra distributive mechanism, even for dilute flows.
This is an important finding because many simulations in the literature ignore inter-particle collisions because of the low mass loading.
Lain and Sommerfeld (2010) investigate the transport of particles in a cylindrical elbow of mass loading ratio 0.7 and illustrate that by ignoring the particle–particle collisions, particles do not preferentially concentrate at the exit of the elbow.
Therefore the roping effect is not observed, however, when Lain and Sommerfeld (2010) perform fully coupled simulations indeed observe this effect.
This indicates that particle–particle collisions are especially important in all wall-bounded flows.
As already discussed, in dilute flows the choice between the hard sphere and soft sphere models largely depends on the computational time spent to solve the particle equation of motion.
For very dilute flows, the hard sphere model is the most natural choice.
However, when the collisions can no longer be assumed as binary and instantaneous, the soft sphere model is the only realistic option.
It is interesting to know whether the choice of the collision model affects the statistics.
Fig.
14 compares the mean velocity obtained from both models with the experimental data.
The same comparison is performed for the smooth walls.
The differences between the hard and soft sphere models for the smooth walls are almost negligible.
However, the differences between the hard and soft sphere models for the rough walls are minor.
This is because the rough wall treatment in the soft sphere implementation adds extra virtual walls during the collision of a particle with a wall, which is a more realistic representation of a rough wall compared to the hard sphere rough wall treatment where one random wall is considered.
This is because, a soft sphere collision is not instantaneous and occurs over a finite amount of time.
Similarly, the same effects are observed on the fluid statistics.
However, Fig.
15, which compares the particle velocity fluctuations, shows that the differences are somewhat larger.
Additionally, the differences in both particle mean and RMS velocity profiles are because the hard sphere collisions are unfortunately heavily dependent on the tangential coefficient of restitution (ψ); the effects by varying this quantity are shown in Figs.
16 and 17.
This tangential coefficient of restitution is empirical and difficult to evaluate experimentally.
Therefore, a sensitivity analysis is essential to determine an appropriate value of ψ in order to obtain good agreement with the experimental data.
Konan et al.
(2011), who perform an investigation based on DES; use particles with a diameter of 100μ m with similar flow settings, report that similar results can be obtained by increasing the experimental roughness angle (γ) from 5.3 to 6.5.
This is another way of changing the effect of ψ because it affects the particle rebound angle directly.
Konan et al.
(2011), however, neglect the effect of the rotation of particles and use the collision model proposed by Sommerfeld and Huber (1999).
This model does not split the coefficient of restitution into normal and tangential coefficients but has a single coefficient of restitution which depends only on the impact angle of the particles and it is this relationship that is determined empirically.
Moreover, the coefficient of friction is treated the same way, i.e.
it is a function of impact angle.
In this work the roughness angle used is γ=5.02o estimated from the experimental measurements and reported by Sommerfeld and Huber (1999) and Lain et al.
(2002).
The soft sphere parameters rely on the properties of the solids and no empiricism is required.
Most importantly, the coefficient of restitution is related to the parameter α (see Tsuji et al., 1992) and automatically depends on impact velocity and angle.
Tsuji et al.
(1992) heuristically find a relation for the coefficient of restitution which is independent of the constants used in the soft sphere model, which is not required to be empirically specified.
To investigate the effect of one-way coupling on the fluid and particle statistics the particle sources in the fluid momentum equation are “turned off”; i.e.
by making the last term on Eq.
(1) equal to zero and setting the fluid volume fraction as αf=1.0.
One-way coupled simulations are performed under the same conditions for both rough and smooth walls.
The particle mean velocity profile, presented in Fig.
18, is not affected.
The differences are statistically insignificant because the difference, for example, of the mean centre-line particle velocity is less than 0.8%.
This is because the particle volume fraction is very low, so the two-way coupling force does not affect the average particle velocity.
Therefore, the absence of the two-way coupling does not significantly affect the average particle statistics.
The particle concentration profiles, not presented, are almost identical compared to the four-way coupled simulation.
On the other hand, the particle velocity fluctuations exhibit different trends.
Fig.
19 compares the particle RMS velocity fluctuations in one-way coupled simulations and fully coupled simulations, for both types of walls.
The one-way coupled RMS velocity values are higher compared to the respective fully-coupled simulations.
In particular, the fluctuations predicted by the one-way coupled simulations and by considering the walls as rough are 10% higher.
This work compares different models and their implications in the framework of LES and compares the findings to the experimental results for turbulent particle-laden channel flow of Kussin and Sommerfeld (2002).
The Reynolds number of the simulations is approximately 42,000 based on the full channel height and the mass loading of the simulations is 1.0, as set by the experiment, corresponding to about 24,500 particles.
Mesh refinement studies show that the fully converged solutions are in very good agreement with the single-phase experiments.
Also, the results obtained from employing the standard Smagorinsky LES model are very similar to the dynamic Germano-Lilly model.
For the multiphase cases, the results of fully-coupled simulations are compared to one-way coupled and two-way coupled simulations and their differences are physically interpreted.
In addition, the soft sphere and hard sphere particle collision algorithms are compared.
The effect of the wall roughness on the particle statistics is also investigated and a new roughness model is proposed in order to be used with the soft sphere methodology.
The predicted particle-laden results for average fluid and particle velocity are in very good agreement with the experimental findings.
The predicted particle-laden fluid fluctuations are slightly lower compared to the particle-laden fluid fluctuations of the experimental data.
Although the gas–solid flow is relatively dilute, there is a major difference between the results obtained by the one-way coupled and the two-way coupled models, having a particularly large effect on the particle velocity fluctuations.
The particle velocity fluctuations show an almost 10% difference between the one-way and two-way coupled approach.
The results obtained with the soft sphere (discrete element model) and hard sphere (event-driven) models are also compared.
The simulation results show that the two models yield almost identical fluid velocity statistics.
The particle mean and RMS velocity profiles are somewhat different, which is attributed to the dependence of the hard sphere model on empirical properties.
In particular, a parameter sensitivity analysis on the tangential coefficient of restitution for the hard sphere model is performed in order to obtain good agreement with the available experimental data.
On the other hand, the soft sphere model, which is independent of empirical parameters, does not require a sensitivity analysis.
Additionally, the particle statistics may be different due to the treatment of the rough walls in the two models.
In the soft sphere methodology, a new model is proposed to account for the wall roughness, as the collision of a particle occurs over a finite amount of time.
The results show that the newly proposed model for treating the rough walls in the soft sphere methodology are in very good agreement with the experimental data.
The wall roughness in the case researched has a very big effect on the gas-particle flow.
In cases where wall roughness is accounted for, the average rebound angle of the particles colliding with the bottom wall is slightly larger than in the case considering fully smooth walls.
This slightly larger angle enables the particles to re-entrain the bulk of the flow, instead of remaining near the bottom; a so-called grazing particle.
Simulations without considering the rough walls show a much steeper particle concentration profile compared to particles in the channel including the effect of rough walls.
Due to two-way coupling, the fluid velocity fluctuations and shear stresses are strongly affected because of the large number of particles in the lower part of the channel.
In particular, the fluid RMS velocity profiles are asymmetric for the simulations without considering wall roughness, as opposed to the simulations with rough walls.
The latter show almost symmetrical flow profiles.
Moreover, the large concentration of particles in the bottom half of the channel suppresses the turbulence.
This paper also shows the importance of particle–particle collisions in the relatively dilute gas-particle laden flow.
Including the effect of particle–particle collisions increases the re-distribution of particles into the flow, having a similar, although slightly less pronounced, effect as the rough walls.
The simulations results in this paper show the importance of four-way coupling and including a model to account for the wall roughness.
The overall comparison with the experimental results is very good.
The authors are grateful to the Engineering and Physical Sciences Research Council (EPSRC) for their financial support (Grant No.
EP/G049262/1).
The simulations have been performed on the MECTOR computer cluster of Imperial College London.
The authors thank Prof. M. Sommerfeld for providing the experimental data.
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.ijmultiphaseflow.2013.02.007.
Supplementary material
Information impact on transportation systems With a broader distribution of personal smart devices and with an increasing availability of advanced navigation tools, more drivers can have access to real time information regarding the traffic situation.
Our research focuses on determining how using the real time information about a transportation system could influence the system itself.
We developed an agent based model to simulate the effect of drivers using real time information to avoid traffic congestion.
Experiments reveal that the system's performance is influenced by the number of participants that have access to real time information.
We also discover that, in certain circumstances, the system performance when all participants have information is no different from, and perhaps even worse than, when no participant has access to information.
With a larger distribution of personal smart devices and navigation tools, there are several novel sources for real time data collection and better means for information transmission.
At the same time, Intelligent Transportation Systems (ITS), applying information processing, communication, sensing, and control technologies [21], have become more advanced and play a key role in improving transportation [20].
In this context, large amounts of data are processed and presented to the participant vehicles through their navigation systems.
Surveys show that, in most of the cases, drivers trust real time information and follow the navigation recommendations [6].
However, the consequences of providing real time information to drivers, who are themselves participants in the data collection process, has not been investigated in much detail.
Information dissemination with feedback loops is a fundamental topic in all human complex systems where people make decisions by accessing real time information.
Knowing details of future problems modifies people's behaviours and this possibly affects the entire system.
This effect has been studied in several areas of human activity.
For example, in financial markets, has been analysed the effect of private and public information.
In [8], the market dynamics is explained by phases: boom, euphoria (with informational cascades), trigger and panic (with information avalanches).
Another example is analysing the effect of transaction costs on the overall market efficiency when aggregating private information [3].
In this paper, we investigate the effect of information dissemination on transportation systems.
The annual Traffic Report released in 2014 by the navigation device maker, TomTom, after analysing real world traffic data, reveals that the travel time is increased by 50% because of the common traffic shortcuts drivers take to avoid congestion [16].
The effect can be empirically observed, for instance, during daily commutes, when multiple drivers make the simultaneous decision to take the same alternative route, thus simply moving the congestion to the new road.
Unlike in existing research, [14,15,1,11] discussed in more detail in Section 2, we are particularly interested in investigating how the traffic is affected by the amount of drivers that receive information about the traffic situation.
Intuitively, the more drivers are informed, the better the traffic situation should be.
We investigate and analyse the conditions under which, drivers having knowledge of the current situation of the traffic system is detrimental to the system as a whole.
For this, we built an experimental set-up based on a microscopic traffic simulation.
The transportation system and the information dissemination in it is modelled and analysed in this paper through an agent based simulation.
This paper is organised as follows: Section 2 introduces related work done on the effect of information dissemination over transportation networks.
Section 3 and Section 4 describe the computational model, experimental set-up and the numerical results.
Section 5 presents the conclusion and the significance of our study.
There are some relevant studies on information dissemination in transportation systems using simulations.
One category of studies look at how either local information (only about the neighbours) or global information (about the entire network) affects the global network performance.
Our approach is different in the sense that we investigate the impact of information on the global network performance depending on the fraction of people that receive information.
We analyse what is the effect of real time information dissemination and explain why this effect appears.
Information is disseminated in real time and contains global details about how congested the roads are.
This approach is important as it gives insights on the impact that massive use of real-time information can have on traffic.
This can be useful for building more intelligent traffic control mechanisms where information is a steering tool.
Models of information dissemination have also been studied for networks with congested and uncongested nodes [14,15].
The information (details such as congestion, flow or occupancy) was either local or global.
Information is used to control the node's outgoing traffic flow, influencing this way the routing choice for vehicles.
In [15], urban street models were implemented for various topologies ranging from naturally evolved ones such as Bologna or London to grid-like cities such as Los Angeles or Washington.
Both [15] and [14] show that the best performance is achieved when local information is used.
Information control systems for traffic planning in the presence of congestion has been researched by [1,9–11].
In [1], a fleet of taxi drivers from Singapore used a Web based application to specify trip origin, destination and departure time and receive route recommendations.
Congestion was modelled as a relationship between flow and delay, model proposed by the Bureau of Public Roads (BPR).
Congestion is estimated using traffic data from loop detectors, GPS location and time data from a roving fleet of taxis.
The learned congestion model is used in multi-agent system (computing socially optimal paths) and also in a single agents route planning (computing greedy path).
The study proposes an experimental comparison between actual taxi paths, with socially optimal and greedy path congestion-aware planning.
The results show that socially-optimal congestion aware routing achieves 15% reduction in travel time.
Our approach is similar because we also select a fraction of drivers to receive recommendations.
However, in the previous studies, the number of informed drivers is fixed to the taxi fleet.
We investigate in more detail what happens when different percentages of traffic participants receive information.
Another difference is that we do not estimate congestion, because, the fact that traffic participants use information makes the congestion prediction invalid.
Other similar studies analysing the effect of information on a traffic simulation are inspired from biological ants systems [4,5].
Information consists of route recommendations.
There are infrastructure agents (roads) and vehicle agents.
The vehicle agent knows the destination of the car, asks the environment for routing options and informs the road agent of its intention.
Based on these details, the road agent estimates the future traffic intensity and gives a recommendation to the vehicle agent.
This mechanism provides a better routing choice to drivers.
Understanding congestion is an important aspect in our study.
In transportation systems, there are two parameters that are usually taken into consideration when defining congestion: the amount of traffic flow and the strength or the degree of congestion [17].
There are multiple causes for congestion: high traffic flow, bottlenecks (local reduction of the road capacity) and local disturbances of individual drivers in the flow [17,19].
While bottlenecks (caused by road obstructions or lane narrowing) are considered to be spatial and deterministic, local disturbances in the flow are stochastic (spontaneous and unpredictable).
They can be triggered, for instance, by an abrupt break or by two trucks overtaking each other at different speeds or several other factors.
In our experiments we create congestion using local disturbances.
Network performance is defined by transportation engineers combining the analysis of individual traffic elements.
The most common variables used are speed calculated usually as travel time or the delays defined as additional travel time experienced by the traffic participants.
The global network performance is then obtained by aggregating the individual travel times across the entire network [12].
A similar performance indicator was selected for our study.
Another approach would be to calculate the fundamental diagram of the network mean flux and a function of traffic load [15].
In other studies network performance is defined as the relation between the filled fraction of the total network capacity and the jammed population of nodes [14].
We investigate the effect of information dissemination on transportation systems when different fractions of drivers are informed.
In order to gain an understanding of this effect, we use a computation model of the traffic flow, congestion formation and information dissemination.
The transportation system is simulated using an agent-based simulation.
The system consists of agents (vehicle driver units) operating and interacting in a shared environment (road network).
The behaviour of the entire system is the emergent behaviour of all its interacting elements.
The agents know the road network, perform route calculations and move forward on their route with a certain speed and acceleration determined by a time-stepped car following model [2].
For this, the Intelligent Driver Model (IDM) is used [18,7].
A road Y, is characterised by a tuple with road length, minimum speed and maximum speed: RoadY=〈vYmin,vYmax,LY〉.
Fig.
1 illustrates a typical IDM scenario.
A vehicle i follows the car in front vehicle i+1 at a speed less than the desired speed of the road vd, which is a value between vmin and vmax.
The current speed of car i, vi is adapted to the speed of car i+1, vi+1 in order to maintain a gap distance greater than Dgap.
Where Dgap is a parameter of the IDM model that specifies the preferred distance between cars.
IDM calculates a realistic instantaneous acceleration (or deceleration) and displacement of vehicle i for a time step δt by taking into consideration its current speed and position (vi and xi), the desired speed (vd), the current speed and the position of the car in front (vi+1 and xi+1).
In addition, there are parameters that specify vehicle length (Lvehicle), time headway (th) for safe acceleration and deceleration (to avoid collisions), and maximum acceleration and deceleration (amax, dmax).
We analyse the effect of traffic information dissemination in the presence of congestion.
For this, we introduce stochastic disturbances in the traffic flow to create a controlled scenario where congestion is persistent.
Congestion is produced naturally as an emergent behaviour of cars interacting on roads (for example it can create self-organised stop-and-go waves, as described in [7]).
This congestion naturally appears and disappears through the evolution of the traffic.
For our study we need to regulate congestion and for this reason we artificially introduce disturbances.
Information dissemination is simulated by sending updates to only a fraction of the agents.
Information contains details about current travel times on links and is used by informed agents to estimate the fastest path (as defined by travel time).
Route calculation is done using Dijkstra's algorithm.
We therefore have two types of agent, informed and uninformed.
The only difference between these two agents is that one calculates the best route using a road network of current travel time (these are the informed agents).
The other agents calculate the best route assuming free flowing traffic, i.e., they estimate travel time by dividing the length of the road by the maximum speed of that road: LY/vYmax.
This way, the congested roads may have a lower priority in the informed driver's choice.
We are particularly interested to evaluate the effect caused on traffic by real time information dissemination in the presence of congestion (generated by local disturbances).
For this, we define the experimental setup with the network graph, parameters, metrics and indicators.
For the experiments we consider a simplified scenario using a road network as shown in Fig.
2.
Every agent starts at the origin and moves towards the destination.
The agents have 2 choices in terms of the routes they take: RoadA=〈11[m/s], 19[m/s], LA〉 and RoadB=〈11[m/s], 19[m], LB〉, where LA is fixed at 500[m] and LB≥LA.
Both roads are single lane.
As described in Section 3, each car applies the IDM model and the parameters used are: Lvehicle=3[m], amax=3[m/s2], dmax=5[m/s2], th=1.5[s], δt=250[ms].
Agents are created by a Poisson process with a mean inter arrival time of 1700[ms].
This value is chosen as the minimum inter arrival time so as to maximise traffic, while not causing congestion on Road C (i.e., before the decision point, marked in Fig.
2).
Each informed agent receives updates about the traffic situation every 2[s].
At the decision point, the agents select either Road A or Road B, whichever gives the fastest route time.
We simulate 40min which implies about N=1000 agents in total that finish trips.
From this amount, we consider the last 800 vehicles to complete their trip Nc, giving a warm-up period of 10min.
Congestion is created by introducing disturbances in the disturbance area (the last 150[m] of Road A), marked on Fig.
2.
In order to create a disturbance, a random vehicle i driving on the disturbance segment of the road is chosen every 2[s] and forced to brake immediately (vi=0[m/s]).11We use a simplification of instantaneous deceleration.
The car takes some time to accelerate and once again reach full speed, thus causing a temporary congestion on the link.
Each experiment is characterised by two parameters: LB, the length of Road B and p, the percentage of informed agents.
LB varies from 500[m] to 1250[m], while LA is fixed to 500[m].
p varies from 0% (no agent is informed) to 100% (all agents receive information).
Each experiment is repeated 10 times.
In order to quantify the effect of information dissemination, we characterise network performance as T, the average travel time of all agents in one experiment.
(1)T=1Nc∑i=0Ncti,where ti is the trip duration of an agent i and Nc is the last 800 agents to complete their trip.
T is analysed for different groups of agents.
We calculate a global T over all agents and TU, TA and TB for the different groups of agents: uninformed, informed agents on Road A and informed agents on Road B.
For each LB we can calculate the maximum improvement across all levels of informed agents, i.e., p∈(0, 100].
The information impact indicator for a road length LB, ILB, quantifies the maximum improvement (negative change) on T when compared to the case of no information.
This is actually the value of p at which we see the smallest value of T, which we define as pmin.
(2)ILB=max(T0,LB−Tpmin,LB),where Tpmin,LB is the minimum T, obtained for pmin.
T0,LB is T for p=0%.
As we increase p, for some road lengths LB the average trip time will initially decrease (i.e., performance improves) to a minimum at pmin, beyond which average trip time will increase.
We define this degradation in performance (increase in average travel time) as DLB.
(3)DLB=max(Tpmax,LB−Tpmin,LB),where pmax gives the longest average trip time and pmax>pmin.
Note that for LB=625[m], pmax=100% and pmin=60%, I625=98.1−85.8=12.3[s] and D625=91.06−85.85=5.21[s].
It is interesting to note that T is influenced both by p and by LB, shown in Fig.
3.
We illustrate in Fig.
3(b) the impact of information on T as we vary LB.
For this we calculate the improvement indicator ILB (defined in Eq.
(2)) and the degradation indicator DLB (defined in Eq.
(3)).
We see that the information produces the biggest improvement, ILB=12.3[s], for LB=625[m] and the most significant degradation, DLB=6.7[s], for LB=875[m].
Next, we explain why there is a significant improvement on T for LB=625[m] and a significant degradation on T for LB=875[m].
For this, we define FY as the fraction of informed agents that select Road Y:(4)FY=NY/NI,where NY is the number of informed agents that select Road Y and NI is the total number of informed agents.
Fig.
4 shows how p affects FA and FB for the two values of LB.
In Fig.
4(a) we observe that, for LB=625[m], FA<FB.
This happens because Road A is congested (we introduce artificial disturbances) and therefore Road B, with LB=625[m], is the fastest option.
The agents that select Road B don’t experience a significant increase in distance and therefore no severe degradation on travel times.
In this case T has a big improvement.
On the other hand, in Fig.
4(b) we see that, for LB=875[m], FA≥FB.
The reason is that LB is long enough that it is better to choose Road A, even though this road is congested.
The informed agents that select Road B experience longer trip durations, producing a significant degradation on T. Based on the results illustrated in Figs.
3 and 4, we focus further investigation in the case of LB=875[m].
This case is interesting as it contains a significant improvement (I875=6[s] for pmin=40%) but also significant degradation (D875=6.7[s] for pmax=100%).
Even though intuitively, one would expect that more agents using information would improve traffic conditions, our results show the opposite in some cases.
Fig.
5(a) shows T as a function of p in the case of LB=875[m].
We notice that by increasing p, first T improves and afterwards it decreases to almost the same level as if no agent is informed.
In order to explain this effect, we analyse the average speed for roads A and B, and we evaluate TU, TA and TB for uninformed, informed agents that select Road A and informed agents that select Road B.
We notice in Fig.
5(a) that TU and TA have smaller values than TB.
The informed agents select Road B because it is recommended by Dijkstra's algorithm as the best route at that moment.
After a while, the situation improves on Road A, the informed agents find out about this change.
The agents from Road B are not able to return on Road A and their trips take significantly longer, causing a severe degradation on the global T. We define SA and SB as the average speed on Road A and on Road B for one experiment.
In Fig.
5(b) we see that SB decreases for higher values of p. This is caused by the increasing number of agents that select Road B as illustrated in Fig.
4(b).
SA remains almost the same, regardless of the fact that the number of agents on Road A is getting smaller as p increases.
This happens because on Road A we introduce disturbances to regulate congestion.
The average speed on roads influences the way Dijkstra's algorithm produces route recommendations.
In Fig.
6 it is shown that SD (standard deviation) of NA, NB, SA and SB becomes bigger when we increase p. This means that the recommendations from Dijkstra's algorithm change more frequently for bigger p. Some informed agents are recommended to select Road B, even though very soon afterwards, the recommendation is no longer valid.
Nevertheless, the agents that selected Road B are not able to move Road A, even though they may receive new recommendations later.
The congestion indicator CY for a Road Y is defined as follows:(5)CY=(1−vYavg/vYmax),where vYavg is the average speed on Road Y at the moment when agents select their route (at decision point marked on Fig.
2) and vYmax is the maximum speed for the road.
Fig.
7 illustrates FA and FB (defined in Eq.
(4)) depending on CA.
We notice in Fig.
7(a) and (b) that for higher p, agents experience CA∈(0, 1), while for smaller p, CA interval is narrower.
Road A is selected when CA has smaller values (the congestion is smaller on Road A), while Road B is selected when CA is bigger (the congestion on Road A is higher).
It is important to notice that traffic is influenced both by p and by LB.
Unlike the common intuition, there are cases when more information becomes detrimental.
Also, we show that among the informed agents some experience worse T than others, even though one would expect to have similar performances.
This phenomenon is explained in an abstract manner as follows: the transportation system provides data coming from fixed or mobile sensors.
Information is processed from data and sent back into the system in real time.
People make new routing decisions and change their behaviour.
These new routing decisions, result in the original model of the transportation system, which produced the routing recommendations, to be invalid because the participants changed their behaviour.
We present our experimental results involving information dissemination in transportation systems.
These results show that informing drivers about congestion in the transportation system will affect the overall performance.
In some cases, if most of the traffic participants receive information (for example trough navigation tools and applications on personal smart devices) traffic can become worse, contrary to common expectation.
There are cases when giving more information does not make a difference on the network performance and cases when it gets improved.
It is important to note that our model of disseminating information consists in selecting specific numbers of traffic participants to receive details about congestion.
We calculated the network performance when varying the amount of informed agents and the length of the alternative road selected to avoid the congested area.
For this study, it was assumed that all agents are rational and decide to use the information about congestion to improve their travelling time.
Future work will aim to extend the existing models of information dissemination by introducing among others time delays and information errors.
Also, we plan to use more realistic city networks and human behaviour models in order to determine how agents decide to use the real time congestion awareness information.
The findings of our study are relevant in the context of information based solutions for ITS [21], involving information processing, advanced communication and sensing.
There are significant amounts of money that governments and private industry invest in developing such systems.
ITS are expected to play even a more important role in the future [20].
It is useful to anticipate the impact that the massive use of real time information can have on traffic.
Particularly, understating the effect of real time information disseminated in traffic can help solving problems related to congestion.
A practical solution to improve congestion should take into consideration not only the travel time but also reliability, predictability, recurrence,peak-spreading or the geographic extent [13].
This will be analysed in future work.
For planning efficient future ITS, it is necessary to consider the negative and the positive effects that real time congestion awareness information can have.
Our study illustrates that the real time information has a great impact on a transportation system.
Understating what the effect is and why it occurs can help decreasing the likelihood of congestion, therefore it is worth exploring further details.
Room temperature ferromagnetism and half metallicity in nickel doped ZnS: Experimental and DFT studies The nickel doped nanocrystalline ZnS thin films were deposited onto glass substrates by chemical bath deposition (CBD).
Also ZnS:Ni nanoparticles were synthesized by CBD/co-precipitation method.
Powder X-ray diffraction (p-XRD) studies demonstrate that both thin films and nanoparticles correspond to sphalerite (cubic) phase of ZnS with slight shift towards higher 2θ values due to incorporation of nickel in the ZnS lattice.
The crystallite sizes estimated by Scherrer equation were 4 and 2.6 nm for ZnNiS thin films and nanoparticles, respectively.
Scanning Electron Microscopy (SEM) images reveal that the morphology of thin films is based on quasi-spherical particles with nano scale dimensions.
Energy Dispersive X-ray (EDX) spectroscopy confirms that the as-deposited thin films have a stoichiometry consistent with the nickel doped ZnS.
Full-potential linearized augmented plane wave (FP-L/APW) method based on spin-polarized density functional theory (DFT) was employed to investigate the electronic and magnetic properties of ZnNiS for the doping concentration.
Exchange-correlation functional was studied using generalized gradient approximation (GGA + U) method.
Electronic band structures and density of states (DOS) demonstrate 100% spin polarization (half metallicity) with ferromagnetic exchange interactions.
Superconducting quantum interference device (SQUID) analysis confirms the theoretical observation of ferromagnetism in nickel doped ZnS.
These ZnS based half metallic ferromagnets seem to have virtuous applications in future spintronic devices.
Half metallic ferromagnets (HMF) have attracted enormous interest due to their applications in spintronic devices [1].
Dilute magnetic semiconductors (DMSs) are considered to be the best materials to show half metallicity.
These materials have two components, one being a semiconducting material with diamagnetic properties while the other is a magnetic dopant such as transition metal having un-paired d electrons [2].
The major advantage of these materials is utilization of electron's spin as information carrier since advanced functionalities in spintronic devices can be viable by the use of spin degree of freedom along with the charge of electrons [3].
The major issue regarding the applicability of these materials is to enhance the Curie temperature above room temperature.
That's why the research interest shifted towards large band gap materials.
A lot of work has been reported on DMSs with different II–VI and III–V semiconductors as host material such as, ZnS, CdS, GaN, ZnO, ZnSe, ZnTe, TiO2, SnO2 [4–12].
ZnS is an important II–VI semiconductor which exists as two polymorphs, zinc blende (cubic) and wurtzite (hexagonal) both having wide band gap i.e.
3.54 eV and 3.91 eV respectively [13].
Being an optical material, pure and doped ZnS have been extensively studied to develop optical and optoelectronic devices [14–18].
Although there appeared many reports regarding theoretical investigations on magnetic properties of doped ZnS [19,20]; however, there is very little information available on experimental evidence of room temperature ferromagnetism in Ni doped ZnS [21–23].
To the best of our knowledge, there are no previous reports of the combined magnetic and half metallic studies of Ni-doped ZnS in the literature.
Consequently, in the present study, we have investigated the structural and magnetic properties of nickel doped ZnS thin films and nanoparticles obtained from CBD method.
Theoretical investigations on half metallicity in Ni doped ZnS are also presented in the current work to correlate the experimental results with the magnetic properties obtained for this material through DFT calculations.
All reagents, zinc chloride (≥98%), nickel chloride (>99%), thioacetamide (≥99%) and urea (≥99%) were purchased from Sigma–Aldrich and used as received.
De-ionized water was used as solvent in all the experiments.
Acetone and ethanol were used for cleaning of the substrate.
CBD experiments were carried out in jacketed beaker connected to thermostat water bath.
Mettler Toledo pH meter calibrated against standard pH (2.00, 4.01, and 7.00) buffers was used to record pH.
X-Ray powder diffraction measurements were performed using Bruker D8 Advance Diffractometer with Cu-Ka radiation.
p-XRD data was recorded across a 2θ range of 20–80° for three hours and twenty minutes scan using a step size of 0.02°.
SEM and EDX analyses of thin films were carried out using Philips XL 30 microscope after carbon coating by precision etching coating system.
Transmission Elecron Microscopy (TEM), High Resolution (HR) TEM and Selected Area Electron Diffraction (SAED) images were collected using Tecnai 20 F30 transmission electron microscope with accelerating voltage of 200 kV.
Superconducting quantum interference device (SQUID) was used to study the magnetic properties of ZnNiS nanoparticles and thin films.
Elk-code was employed to solve Kohn–Sham equation within Density Functional Theory (DFT) using Full Potential Linearized Augmented Plane Wave (FPLAPW) approach.
The nickel doped ZnS thin films were deposited onto glass substrates from acidic chemical bath containing solutions of zinc chloride (0.15 M), nickel chloride (0.15 M), urea (5 M) and thioacetamide (1 M).
All the solutions were prepared in de-ionized water separately.
Urea solution was added to the solution of zinc chloride to start the hydrolysis followed by addition of nickel chloride solution in appropriate amount for 6.25% doping.
Finally solution of thioacetamide was added and the mixture stirred vigorously to achieve homogeneous mixing of precursors.
The pH of solution was adjusted to 3.8 by drop wise addition of 0.2 M HCl.
The reaction mixture was then transferred to a jacketed beaker attached with a thermostat bath and maintained at a temperature of 80 °C along with uniform magnetic stirring.
After ten minutes of reaction at 80 °C, the transparent solution starts turning milky turbid.
Glass substrates (1.5 × 2.5 cm2) degreased with ethanol and ultrasonically cleaned with acetone were immersed vertically in the chemical bath with the help of copper wire and clip.
After thirty minutes of reaction in chemical bath, the color of solution changes from milky to blackish as an evidence of nickel incorporation in the ZnS matrix.
Substrates were removed from the bath after 3 h, washed with de-ionized water.
Precipitates and loosely adhered particles were removed by sonication.
As-deposited ZnS thin films were white in color while the nickel doped ZnS thin films were blackish.
The films were allowed to dry under nitrogen stream before further characterization.
ZnS nanoparticles doped with nickel were prepared by CBD/co-precipitation technique.
The deposition of thin films was carried out under magnetic stirring as mentioned in previous section.
The uniform magnetic stirring helps to obtain smooth and uniform thin films and avoid precipitation throughout the reaction time.
After the deposition of thin films, the reaction mixture was maintained at same temperature (80 °C) without magnetic stirring for 30 min to allow precipitation.
The precipitates so formed in CBD of nickel doped ZnS thin films were collected from the beaker, washed and centrifuged several times with methanol to remove the by-products of reaction.
Pure nickel doped ZnS nanoparticles were then dried in nitrogen atmosphere.
Powdered nanoparticles thus obtained were then subjected to characterization and magnetic studies.
In order to investigate the electronic and magnetic properties of Zn0.9375Ni0.0471S alloy (composition estimated by XRD and EDX), DFT calculations were performed.
Full Potential Linearized Augmented Plane Wave (FPLAPW) approach employed in Elk-code was used to solve Kohn–Sham equation within DFT formulation [24,25].
Generalized Gradient Approximation (GGA) along with hybrid functional ‘U’ was used to optimize the exchange correlation energy [26].
In (FPLAPW) method, wave function, charge density and potential are expanded into two different basis sets in the two regions of the unit cell.
Inside the non-overlapping spheres surrounding the atomic sites (muffin-tin spheres) the wave functions are expanded into spherical harmonics with angular momentum quantum number lmax = 10 and in the interstitial region, wave functions are expanded into plane wave basis.
A plane wave cut off of Gmax = 7/RMT was used for the expansion of wave functions inside the interstitial regions.
In the above expression, G is a wave vector and Gmax is its maximum value while RMT is the average muffin-tin radius.
RMT's were chosen so that there is no charge leakage from the core and total energy convergence is ensured.
A dense uniformly distributed mesh of 70 k-points was used in the irreducible part of Brillouin Zone.
Core electrons were treated fully relativistically by solving the Dirac equation, whereas the valence electrons were treated non-relativistically.
Stoichiometry (elemental composition) and incorporation of nickel in ZnS was investigated by EDX analysis which showed that 4.71% of nickel was incorporated into the ZnS lattice (Fig.
1).
This incorporated amount is less than the amount added (6.25%) as expected.
The amount of the dopant added in the crystal lattice and the amount of dopant added in the chemical reaction rarely match.
The amount of the dopant in the crystal lattice as compared to the added amount in the reaction depends on various factors such as the conditions of the chemical reaction, type of material and the type of dopant etc.
Glass constituents such as, silicon, sodium, calcium, magnesium, potassium and aluminum were also detected in EDX spectrum because of the thin nature of the films.
XRD patterns (Fig.
2) show the polycrystalline nature of Ni doped ZnS thin film and nanoparticles.
The observed diffraction peaks corresponds to (111), (220) and (311) lattice planes and are well matched with the standard pattern of cubic ZnS (ICCD # 01-005-0566).
Effect of Ni incorporation in ZnS is evident from shift of diffraction peak corresponding to (111) plane from that of un-doped ZnS thin films i.e.
2θ increases from 28.86° to 29.62°, since the ionic radii of Ni (0.83 Å) is smaller than that of Zn (0.88 Å).
The shift of diffraction peak (111) is shown in inset of Fig.
2.
A slight change in lattice constant was also observed.
Substitution of Ni (dopant) into the ZnS lattice is also confirmed by the absence of any diffraction peaks corresponding to Ni, NiS or any other impurity.
The average crystallite size was estimated by using Scherrer formula as:(1)D=kλ/βcosθwhere D is the average crystallite size, k is the geometric factor, λ is the X-ray wavelength of Cu Kα radiations (1.5405Å), β is the full width at half maximum of diffraction peaks, and θ is the Bragg diffraction angle.
Crystallite sizes calculated by Scherrer's equation for Ni doped thin films and nanoparticles are 4 and 2.6 nm, respectively.
The lattice parameter ‘a’ after doping is found to be 5.307Å which is relatively smaller than that of pure ZnS (5.406 Å).
The comparison of observed d-values and lattice constant for Ni doped ZnS with the standard pure ZnS and un-doped ZnS thin films deposited under the same conditions [27] is given in the Table 1.
SEM representative image (Fig.
3) of the ZnNiS thin film shows distribution of almost spherical nanoparticles on the surface of the substrate.
It was observed that agglomeration of nanocrystallites leads to the formation of spherical clusters having an average diameter of 80 nm onto the substrate surface.
The uniform layered growth of ZnS is associated with the increase in size of particles instead of more pronounced nucleation [28].
Size distribution of particles was analyzed by an image analysis program (ImageJ) and average diameter of the particles was found to be 80 ± 5 nm.
Microstructure of Ni doped ZnS thin film and nanoparticles was further studied in detail with the help of TEM, HRTEM and SAED.
The results are shown in Figs.
4 and 5, respectively.
TEM image of ZnNiS thin films reveals the clusters of nanoparticles as shown in Fig.
4.
HRTEM image confirms the crystalline nature of the ZnNiS thin film.
Inter planner spacing (0.30 Å) calculated experimentally is in fair agreement with that expected for (111) plane of cubic phase of ZnS.
SAED pattern shows a set of three circular rings obtained due to diffraction of electrons from (111), (220) and (311) planes of cubic ZnS.
SAED pattern thus indicates polycrystalline nature of ZnNiS thin film.
TEM and HRTEM images with SAED pattern for ZnNiS nanoparticles collected from the CBD bath precipitate are shown in Fig.
5.
TEM image shows the chain of spherical crystallites composed of nanoparticles.
HRTEM shows that the lattice spacing obtained for ZnNiS nanoparticles and thin films are exactly same i.e.
0.30 Å. SAED pattern shows spots instead of diffused rings due to he highly crystalline nature of nanoparticles.
SQUID magnetometry was used to study the magnetic properties of ZnNiS nanoparticles and thin films.
Variation in magnetization as a function of applied magnetic field at room temperature was observed.
Hysteresis loops obtained at room temperature demonstrate the ferromagnetic nature of both thin films and nanoparticles as shown in the inset (magnified image) of Fig.
6.
The room temperature values of coercivity are 79 Oe and 70 Oe whereas remnant magnetizations (M) are 12 × 10−3 emu/g and 9.4 × 10−3 emu/g for nanoparticles and thin films, respectively.
The observed ferromagnetism at room temperature in nickel doped ZnS nanoparticles and thin film are consistent with the recently published reports [21,23].
We suggest that in absence of any impurity and substitution of nickel ions for zinc in the ZnS lattice, exchange interaction (RKKY) between the local spin-polarized electrons of dopant (Ni) ions and conductive electrons is responsible for room temperature ferromagnetism.
There are few reports showing room temperature ferromagnetism in nanoparticles and nanowires of pure ZnS [22,29].
The zinc and sulfur vacancies produced at the surface of nanomaterials were suggested as origin of ferromagnetism since surface of nanomaterials is active having ferromagnetically ordered domains.
To further investigate the effect of nickel doping, origin of magnetism and half metallicity, we have performed DFT calculations using Elk-code.
The DFT calculations on nickel doped ZnS were performed by using 128 atoms supercells with 64 atoms each for zinc and sulfur.
For comparative study the doping concentration selected for DFT calculations was almost same as observed experimentally.
Three zinc atoms were replaced with nickel atoms yielding 4.69 percent doping of nickel in cubic ZnS lattice.
The lattice constants were also optimized after geometry relaxation.
A little change in lattice parameters (5.40 Å) was observed in comparison to the experimentally reported value of 5.416 Å [19,30], since there is small difference in ionic radii of zinc (0.88 Å) and nickel (0.83 Å).
In order to compare the energetic stability of nickel doped ZnS, we have calculated the formation energy of both un-doped and nickel doped ZnS.
It was observed that the formation energy of nickel doped ZnS (−4.1641 eV/atom) is a bit less than that of the un-doped ZnS (−4.0217 eV/atom) due to exothermic nature of reaction.
Energy minimization was done by slight variation in lattice parameters of ZnS after nickel incorporation.
Energy versus volume curve is shown in Fig.
7.
Lattice parameter ‘a’ is computed directly from minimum of energy–volume curve.
The total magnetic moment of nickel doped ZnS was found to be 6 μB.
This magnetic moment is believed to be induced in the host semiconductor due to nickel doping.
In transition metals doped semiconductors, the role of hybridization between s, p orbitals of host and d orbitals of dopant element is very significant to study the induction of magnetic moments in host semiconductor and splitting of d states due to local magnetic moments [31,32].
The nickel ions substituted for zinc, delocalize the magnetic moments and induce ferromagnetic ordering between zinc and sulfur [33].
Due to incorporation of nickel, the number of density of states for one or both spin channel in the valence band changes.
In the present work, nickel doping, introduces empty or partially filled minority states at the Fermi level keeping the majority states occupied due to double exchange (RKKY) interactions.
This suggests that nickel doping leads to 100% spin polarization and results in half metallicity in ZnS.
The asymmetry in DOS after nickel doping in ZnS is shown in Fig.
8.
Fig.
9 shows the band structure of the ZnNiS at its equilibrium volume.
The majority and minority bands are plotted separately and conclusion made that nickel doped ZnS are half metals with direct band gap of 2.1 eV for majority spin.
The DOS appeared at the Fermi level for minority spin due to exchange interaction of impurity d orbitals can be seen in Fig.
9 (b).
The nickel doped ZnS thin films and nanoparticles have been successfully obtained via CBD and CBD/co-precipitation methods, respectively.
The crystallinity and incorporation of nickel was confirmed by p-XRD and EDX analysis.
The results of electron microscopy (SEM/TEM) are consistent with p-XRD data.
The M−H loops indicate that nickel doping in ZnS gives rise to room temperature ferromagnetism.
Half metallic ferromagnetism in nickel doped ZnS have been observed with magnetic moment of 6 μB by DFT calculations.
Based on ferromagnetic and half metallic properties nickel doped ZnS have the potential applications in future spintronic devices.
One of the authors (Muhammad Saeed Akhtar) would like to acknowledge the Higher Education Commission (HEC) of Pakistan for providing financial support as indigenous scholarship (Grant no.
17-5-4(Ps4-264)/HEC/Sch/2007) in Batch-IV and IRSIP scholarship (Grant no.
1-8/HEC/HRD/2013/2503).
Azad Malik thanks EPSRC for funding of instruments under grant number EP/K039547/1 for characterization of the compounds.
Characterisation of nanosized oxides in ODM401 oxide dispersion strengthened steel Changes to the microstructure of oxide dispersion strengthened (ODS) alloy ODM401 in response to high temperature heat treatments have been observed.
Extensive analysis of thin foil and extraction replica specimens has been performed using transmission electron microscopy (TEM), convergent beam electron diffraction (CBED) and energy dispersive X-ray spectroscopy (EDS).
A distribution of 2nm (mean diameter) particles, with a number density of 2×1023m−3, was observed in the extruded material.
Following a heat treatment of 1h at 1300°C the number density of the dispersoids was diminished and their mean diameter increased to an average size of 5nm.High angle annular dark field scanning transmission electron microscopy (HAADF-STEM) and chemically sensitive, high resolution, phase contrast, transmission electron microscopy has been used to positively identify nanometer scale pyrochlore oxides.
Identification and a complete characterisation of the A2B2O7 structures, including the lattice positions of trivalent (A) and tetravalent (B) species is presented.The composition of the oxide particles was shown to be complex and the role of cations other than yttrium and titanium, in particular aluminium, must be considered in future analysis of the oxide phases and their nucleation and growth behaviour.The majority of the cube shaped oxide particles displayed one of two orientation relationships (OR) with the ferritic matrix.
Cube on cube and the Baker and Nutting, cube edge on cube, relationships were identified.
ODM401 is a 14wt% Cr ferritic ODS alloy made with additions of Mo, Ti and yttria (Table 1) [1].
The material was produced by Dour Metal s.r.o.
and is similar to MA957 steel produced by the INCO Company upon which many scientific papers have been published.
Ferritic oxide dispersion strengthened alloys are candidate structural materials for applications in future fusion and fission nuclear reactors.
As a group, these alloys show low susceptibility to irradiation damage, excellent high temperature strength and creep properties and good corrosion and oxidation resistance.
Key to the mechanical performance of ODS alloys is the dispersion of ultra-fine oxide particles which should be distributed homogeneously throughout the matrix.
It is imperative that these oxide particles, which also trap helium and serve as obstacles to dislocation glide, are retained during thermal processing and in service.
These nano-features, show remarkable stability under simulated irradiation tests [2,3], creep conditions [2] and accelerated aging experiments [4–7].
Studies by Miao et al.
[4], have shown that the oxide particles remain stable at temperatures below 900°C with only modest changes observed after 3000h at 950 and 1000°C.
Alinger et al.
[8], published similar findings and revealed that a more significant coarsening regime is observed at temperatures of 1200°C and above.
Whilst there are some slight variations, most authors agree that below 1000°C the oxides are resistant to coarsening but at higher temperatures they coarsen more rapidly.
There is still some debate about the crystal structure and composition of the fine oxides found in ODS steels and a number of different phases have been both proposed and identified.
A complete characterisation of the oxide particles, including crystal structure and composition, is needed as different phases and chemical variants of a single structure have been shown to respond differently to high temperatures and irradiation.
Ribis and de Carlan [6] have studied the coarsening characteristics of Y2O3 and Y2Ti2O7 oxides at high temperatures.
They show that the increase in particle size is greater for the non-Ti containing phase.
Similarly, Ratti et al.
[9], although they do not allude to specific oxide phases, have shown that small Ti additions to an 18%Cr ODS alloy dramatically reduces the coarsening rates of dispersoids when compared to an equivalent alloy without titanium.
For example, Ribis indicates that coarsening rates may be controlled by interfacial energy between the secondary phase particles and the matrix; he points out that the resistance to coarsening observed in the Y, Ti, O system is probably the result of a very low interface energy and this would differ from one phase to another.
Whittle et al.
[10] have shown that pyrochlore and structures closely related to the pyrochlore structure respond in different ways to irradiation.
They revealed that oxide structure and variations in composition can affect their ability to withstand and recover from irradiation induced damage.
With this knowledge and the ability to tailor the oxide phases that form, i.e.
their compositions and distribution, it may be possible to further develop this group of alloys for specific applications and service environments.
It is therefore of great importance that nucleation and growth mechanisms of oxide phases that form in ODS alloys are better understood.
This requires an in depth study of the oxides, the matrix and their interactions.
However, due to their diminutive size, characterisation of the crystallographic structure of the oxides which are found in these ODS steels presents a significant challenge.
Experimental techniques which include X-ray diffraction (XRD), HRTEM including fast Fourier transform analysis and atom probe tomography (APT) have been used widely to identify secondary phases in ODS steels.
Identification by X-ray diffraction of matrix oxide particles is extremely difficult due to their very small volume fraction.
Kasada et al.
[11] and Sakasegawa et al.
[12], both present the results of XRD analysis performed on residue extracted from ODS alloys indicating the presence of a pyrochlore structure.
Analysis in the TEM enables the characterisation of single oxide particles.
Selected area electron diffraction from larger crystals can be achieved but whether the structures are representative of the finer oxides is not known.
A potential route to satisfactory electron diffraction analysis may lie within generation of Debbye–Scherrer ring patterns from extracted particles.
As the oxides are often very small it might also be assumed that diffraction occurs entirely within the kinematic range; hence, the intensities within each discrete diffraction ring could provide additional structural information.
Alternatively, convergent beam electron diffraction can be used to analyse single crystals.
Assuming again that kinematical conditions prevail, the intensities of diffraction discs would hold additional information regarding the atomic structure of the nano-scale oxides.
The use of extraction replica specimens alleviates many of the issues associated with the ferritic matrix and Wu et al.
[13], have recently shown promising high angle annular dark field imaging results using this method; the process of producing extraction replicas from 14Cr ODS is detailed in the same paper.
Historically, concerns have been raised over the use of replication as a means of providing true representations of particle distributions in an alloy.
To achieve quantitative results the technique must not alter the precipitates in any way, including morphological, crystallographic or chemical changes, and must simply remove the matrix leaving behind a representative distribution of secondary phase particles which is picked up by the carbon film.
It has been shown that the fine dispersoids formed in yttrium and titanium containing ferritic ODS alloys, which do not have intentional additions of Al, are primarily composed of yttrium, titanium and oxygen.
The precise composition of these oxides remains unclear.
Wu et al.
[13], list 6 known oxides of Y+Ti, although only three of these, Y2Ti2O7, Y2TiO5 and YTiO3 are widely reported, the latter being less prevalent, in ferritic ODS steels.
Within the same work, the crystal structures of 5–6nm diameter oxide dispersoids were shown to be consistent with a pyrochlore structure.
They also reported larger oxide particles displaying the orthorhombic Y2TiO5 structure; however, it was stated that the FFTs were also consistent with YTiO3, hence there remain some ambiguities.
The Y/Ti ratios of oxide clusters, in various ODS alloys, have been reviewed by Sakasegawa et al.
[12] where it was shown that Y/Ti was often neither unity or 2; hence fits neither Y2Ti2O7 or Y2TiO5 compositions.
Sakasegawa et al.
[14], have themselves analysed particles extracted from MA957, Fast Fourier Transforms of HRTEM images were used to identify crystal structures and compositional analysis was carried out by EDS.
Although the crystal structure appears to be consistent with that of pyrochlore, it was reported that the Y/Ti ratio was not unity; hence did not agree with that expected for Y2Ti2O7.
Interestingly, they mentioned that non-negligible concentrations of Al were detected; however, it was not made clear if the Al was dissolved in the pyrochlore crystals or its presence was due to the coexistence of Y3Al5O12.
Millar et al.
[15], analysed three ferritic ODS alloys of slightly differing compositions within the range of 12–14%Cr.
Ultra fine particles, rich in yttrium, titanium and oxygen were detected by APT in 12YWT, 14YWT and MA957 alloys subsequent to extrusion.
These alloys were subjected to heat treatments at 1300°C and the fine dispersoids showed a high resistance to coarsening.
The ferritic grain structure displayed some evidence of recovery but did not coarsen significantly.
Millar and Hoelzer [2], investigated creep behaviour and the effects of neutron irradiation on nanoclusters formed in MA957.
Their findings reveal the excellent stability of the oxide particles under simulated service environments.
Number densities of dispersoids in the 2nm diameter size regime remained in the order of 1 to 2×1024m−3 in as extruded, irradiated and crept specimens.
They report no significant change in the compositions of the nanooxides but state that compositions were not consistent with TiO2, Y2TiO5, YTiO3 or Y2Ti2O7.
Millar and his co-workers [5] have also reported on the stability of nano-oxides in MA957 at high temperatures.
In that study it was shown that the Guinier radius of the particles increased from 1.2±0.4nm in the as received (AR) material to 1.7±0.4 and 4.6±1.1nm during 1 and 24h heat treatments at 1300°C respectively.
Number densities decreased from 2×1024m−3 in the AR alloy to 2×1023m−3 and 8×1022m−3 during the 1 and 24h treatments respectively.
This work gives an in depth overview of the microstructure of ODM 401 and its high temperature stability.
The findings also clarify a number of issues surrounding the crystal structure, composition and morphological trends of oxide dispersoids in ferritic ODS steels.
Scanning electron microscopy was performed in a JEOL 7001F field emission microscope.
Specimens were prepared by standard metallographic polishing techniques; the final polish was carried out using 0.25μm diamond paste followed by colloidal silica.
Samples were etched using Vilella’s reagent.
Hardness measurements were recorded using a Matsuzawa Seiki micro-hardness tester model DMH-2 using a 1kg load and a dwell time of 10s.
Thin foil, 3mm diameter, specimens were punched from ODS material which had been sectioned across the bar diameter, perpendicular to the extrusion direction.
Each foil was mechanically ground to a thickness of less than 50μm; the final polish being carried out using a 1μm diamond lapping film.
Electropolishing was performed in a TENUPOL 3 twin jet polishing unit using an electrolytic solution of 5% perchloric acid in 95% methanol (%vol).
A potential of 10 volts was applied to the specimen at a temperature of −42°C; the resultant current was measured at approximately 50mA.
Once specimen perforation was detected the thin foils were rinsed, alternately, in cooled (<−50°C) and room temperature methanol; the cooled methanol rinse is reported to reduce contamination levels on the specimen surface [16].
Extraction replicas were prepared by depositing a thin carbon layer on polished and etched samples.
The carbon replica film was then released by etching in Vilella’s reagent before being rinsed in methanol.
The films were subsequently floated off in distilled water and captured on Cu support grids.
An aberration corrected JEOL 2100FCs transmission electron microscope, operating at 200kV, equipped with a 2048×2048 pixel CCD camera, was used to capture high resolution phase contrast TEM images over exposure times of 0.6s.
The same microscope was used, in STEM mode, to record z-contrast images.
Scattered electrons were collected by a HAADF detector over a semi-angle ranging from 75 to 150mrad.
A 300kV JEOL 3010 microscope, equipped with a LaB6 emitter, was used to perform a combination of tasks including HRTEM imaging, convergent beam electron diffraction and EDS analysis using an Oxford Instruments detector (model: 6636); EDS data was analysed using Link ISIS software.
ODM401 ODS alloy was received in the form of a round bar approximately 25mm in diameter.
Electron channeling contrast images, recorded in the SEM, revealed a grain structure typical of extruded bar.
The average diameter of cross sectioned grains, viewed along the extrusion axis was approximately 0.3μm; the maximum grain size observed in this orientation was 0.8μm.
Longitudinal sectioned specimens revealed evidence of grain elongation in the direction of extrusion.
There was no evidence of bimodal grain size distributions, as have been reported in similar alloys [17,18], within the samples analysed here.
A number of coarse stringers were observed, most of which were oxides of either silicon or aluminium.
Etched specimens revealed a dispersion of titanium rich precipitates, probably carbides, within the size range of 20–100nm (diameter).
The grain size distribution subsequent to 1h heat treatment at 1300°C showed little evidence of grain coarsening and a uniform grain diameter of approximately 0.3μm was observed (Fig.
1).
A number of coarse cube shaped Ti rich secondary phase particles were apparent in the TEM thin foils; often as large as 0.3μm.
There was evidence of recovery in the heat treated (HT) sample but a large proportion of the grains retained high dislocation densities; there was no evidence of recrystallisation.
Microhardness tests revealed a loss of hardness from 367HV in the AR material to 281HV after the heat treatment.
A dispersion of extremely fine precipitates was observed in thin foil specimens in the TEM.
In addition to the very small facetted particles, larger spherical particles, approximately 10nm diameter, were also present but less numerous (Fig.
2a).
Good contrast between the oxide particles and matrix was introduced to the images by aligning the ferrite grains to a low index zone axis and inserting a small objective aperture.
At high magnifications the particles displayed a cubic or rectangular morphology and were faceted parallel to 100 and 110 ferrite directions (Fig.
2b).
Number densities of the small particles were calculated from TEM images.
In AR material the average particle size was 2nm (Fig.
3) and a number density of 2×1023m−3 was measured.
Subsequent to heat treatment, the particles coarsened to an average size of 5.1nm.
The chemical make-up of the nanofeatures observed in both AR and HT material was determined by energy dispersive X-ray spectroscopy.
All of the sub 10nm diameter particles analysed contained both yttrium and titanium.
On average, the atomic ratio of Y/Ti was approximately 1 but varied significantly.
In addition to Ti and Y, aluminium was detected in the nanoparticles in both AR and HT conditioned specimens.
Extraction replicas, taken from as received alloy, confirmed that the aluminium was associated with the particles, as opposed to the surrounding matrix.
Atomic ratios of Y:TI:Al are presented as a ternary plot in Fig.
4.
The significant spread in data was due to the low signal to noise spectra captured from the nanosized particles but on average aluminium contributed 20% of the combined Y+Ti+Al concentration.
Silicon and chromium were also detected in, or around, extracted particles during EDS analysis; silicon concentrations sometimes approached that of Al.
According to Hadraba et al.
[1], the presence of aluminium in ODM401was probably the result of contamination picked up from the ball mill which had previously been used in the production of Al containing ODM751 ODS alloy.
Sakasewaga et al.
[14], report non-negligible concentrations of aluminium detected in Y2Ti2O7 oxides and aluminium contamination in MA957 and 14Cr ODS alloys has been reported elsewhere [5,12,19–22].
EDS spectra presented in other publications [5,11,23] show evidence that aluminium is present in the proximity of small oxides although it is not always alluded to within the text.
Miller et al.
[5], revealed the presence of large Al2O3 oxide particles subsequent to high temperature exposures of MA957.
In the same work, an EDS spectrum from small grain boundary features enriched in titanium and yttrium was also presented.
There was clear evidence, from that spectrum, that aluminium was also present in the sampled region, however, the atom probe maps presented did not include aluminium.
In a more recent publication [22], Miller acknowledges that aluminium was detected in some of the coarser particles including ‘Y2Ti2O7’.
Yamashita also detected aluminium in EDX spectra recorded from nanoscale oxides.
He attributed the presence of Cu to the copper support grid.
However, his explanation of Al coming from a “surface oxide” requires clarification as the specimen was an extraction replica and an XRD plot revealed no evidence of alumina.
The XRD plots show peaks consistent with the ordered Y2Ti2O7 and Y2TiO5 structures but the measured Ti/Y ratio of 0.7 fits with neither phase.
However, if aluminium were considered as a substitutional element for the trivalent yttrium species then the chemical formula might well have approached (Y+Al)2TiO5 which would have been consistent with the orthorhombic phase.
Ohtsuka et al.
[24], carried out a study which investigated the effects of minor additions of aluminium to a 9%Cr ODS steel.
They reported that aluminium replaced titanium in the oxides with levels as high as 28at% (of the cations) detected in particles in an alloy containing only 0.085wt% Al.
There was a clear correlation between Al level in the alloy and that in the oxide particles.
Unfortunately the crystal structure of the oxides, hence phase identification, was not determined in their work.
According to Cotton and Wilkinson [25], yttrium rarely adopts anything other than a trivalent oxidation state.
Titanium has a tendency to form compounds in the 4+ oxidation state but 3+ compounds are known.
The most common oxidation state of aluminium is 3+ hence both aluminium and yttrium commonly adopt trivalent oxidation states and for titanium the tetravalent state.
Therefore it might be expected that in the situation where yttrium is deficient it would be substituted by aluminium.
However, in the present study the mean Y/Ti ratio of fine oxides was approximately 1 (0.97).
Assuming the 3+ cation sites were occupied by a mixture of Al and Y then (Y+AL)/Ti>1; hence, a tetravalent species must substitute for Ti on 4+ cation sites to maintain charge neutrality.
Miller et al.
[26], have shown that Fe and Cr are also associated with Y–Ti–O nano-features, although Marquis [20] indicates the high Cr levels are due to a Cr rich shell.
It is not thought that Cr is dissolved in ODS oxides as Matteucci et al.
[27], indicate Cr has only a very low solubility in Y2Ti2O7 but it would be reasonable to expect this may vary depending on other elements present in the phase.
Marquis [20] also reports traces of Mn, Si, Al and C partitioned to small oxides in ODS MA957 and Eurofer alloy.
In the present study, silicon, which readily adopts the tetravalent state, was detected by EDS in some of the yttrium–titanium oxides extracted from AR ODM401.
The implications of these findings are that the pyrochlore structured oxide which forms in ODS alloys has a complex chemical formula not just Y2Ti2O7 which probably incorporates Al and possibly Si.
Fig.
5a and b shows a high resolution TEM phase contrast image of an oxide particle at the perforation edge of the thin foil sample.
The particle remains in intimate contact with the ferritic matrix with which there is a cube on cube orientation relationship.
The ferrite grain and pyrochlore structured crystal are both oriented with their 011 zone axes parallel to the electron beam.
The ferrite lattice was used as a means of internal calibration for the measurement of oxide d-spacings in high resolution TEM images.
The value of aα=2.865Å, as presented by Pearson [28], corresponds to a 14%Cr, Fe balance, binary alloy, measured at 20°C.
The lattice parameter of the oxide was calculated by measuring both the 222 and 004 planes, averaged over 16 and 25 planes respectively, and was in good agreement with the value of 10.098Å reported for Y2Ti2O7 in the literature [27].
Toward the centre of the oxide particle, where uniform thickness was assumed, there were three distinct intensities associated with the positions of atomic columns in the image.
Fig.
5c and d shows the integrated intensities of line profiles recorded directly from the unmodified TEM image shown in Fig.
5b.
Line intensity profiles, calculated using Digital Micrograph software, were integrated over widths of approximately 2Å.
The areas analysed represent the projected electrostatic potentials of atomic columns which form two neighbouring 004 type planes.
Plane a consisted of columns which displayed alternating intensities, the minimum and maximum peak intensities were approximately 3500 and 4500 counts.
Plane b did not show the same periodic variation as plane a but a constant intermediate intensity of approximately 4000 was observed.
Results were reproducible and similar intensity distributions were observed in images recorded of AR material and extracted oxides on carbon support films.
The difference in peak intensities correlate well with a pyrochlore structure viewed along the 011 axis which has columns of cations composed of species A, B and A+B.
The pyrochlore crystal structure has the space group Fd-3m z=8 and adheres to the formula A2B2O7 where A and B are trivalent and tetravalent atomic species respectively.
An equivalent phase can also form with A2+ and B5+ species so long as charge neutrality is retained.
The superstructure of pyrochlores can be considered as cubic cells comprised of 8 sub-cells, each of which is formed by alternating A3+ and B4+ cations located at face diagonally opposed corners (Fig.
6a).
Each of the 8 sub-units contains 7 oxygen atoms hence can be considered as distorted oxygen deficient fluorite cells with ordered trivalent and tetravalent cations.
The cation arrangement in the superstructure leads to a doubling of the fluorite unit cell dimensions.
Fig.
6b shows a simulated unit cell of the Y2Ti2O7 structure.
The cell is aligned showing the 110 plane almost parallel to the plane of the page and the oxygen atom positions are depicted by open spheres with the cation positions filled.
When viewed along a 110 zone, neglecting oxygen atoms, the projected image is formed of atomic columns which consist of either species A, B or an equiatomic mix of both species.
It is apparent that each alternate {004} plane is comprised of either columns of mixed A and B species or of alternating single species columns.
For example (Fig.
6b), the plane designated ‘plane 1’ has alternating atomic columns which contain only a single species, in this case either Ti or Y.
The adjacent plane ‘plane 2’ contains columns comprising a mixture of the trivalent and tetravalent species in equal quantities (atomic).
It was assumed that the oxide particles observed in TEM mode were behaving as weak phase objects; i.e.
they were so thin that on average only a single interaction occurred between the incident electron and the specimen.
The intensities of the projected potentials in the image could then be directly related to the atomic columns, and hence the average atomic number of the species in each column [29,30].
Columns in the particles with high intensity would then be yttrium rich and the least intense columns would be titanium rich.
The columns showing an intermediate intensity would be composed of yttrium and titanium atoms.
This assumption of weak phase object behaviour appears plausible, since the positions of the bright columns shown near the edges of the particles, where the thickness would be much smaller, are in registry with those near the centre of the particles, which is very unlikely if multiple scattering is occurring.
Also several different particles showed the same intensity variations from column to column when viewed along [011].
In order to confirm the findings of the TEM studies, HAADF-STEM experiments, in which images reveal z-contrast [31], were performed on carbon extraction replica specimens.
Oxide particles viewed along [011] directions revealed similar intensity distributions to those observed in the TEM.
Fig.
7a is a HAADF-STEM image showing a 5nm diameter oxide particle extracted from heat treated material.
The corresponding bright field (BF) STEM image (Fig.
7b) shows atomic columns, which appear dark under BF conditions, at identical locations to those in the dark field image.
In the HAADF image, which shows clear z-contrast, variations in atomic column intensities were observed.
Intensity profiles were measured along adjacent {004} atomic planes which are consistent with the cationic sublattice of the pyrochlore crystal structure (Fig.
7c and d).
An inverse FFT image, which has been mask filtered to remove noise, is presented in Fig.
6e with a model of the cation column positions superimposed on the image.
The arrows in the image depict planes consisting of, plane a, alternating Y and Ti columns and in plane b the columns are mixed Y+Ti.
The results of the STEM study are complementary to the TEM results and confirm the yttrium–titanium oxides posses the ordered pyrochlore structure.
Were the cation species distribution disordered, on the lattice i.e.
a fluorite type structure, then intensity variations in HAADF-STEM images would not be observed.
However, due to the rapid contamination rates, thickness variations towards the edges of the particles and the possible substitution of Al for Y on the lattice, quantitative HAADF analysis could not be achieved.
The structural information gained from HAADF-STEM and TEM images was further supported by electron diffraction studies.
Convergent beam electron diffraction patterns were generated from sub-10nm, Y-Ti rich particles, on extraction replica specimens.
Solutions to diffraction patterns were in good agreement with the published crystal structure of Y2Ti2O7.
Patterns recorded from oxides oriented on [110] type zone axes displayed large variations between the intensities in diffraction discs (Fig.
8b).
The intensities of the reflections were a good fit with those of simulated patterns which display disc diameters relative to the calculated intensities for a given diffraction experiment (Fig.
8c).
All of the reflections in the experimental patterns were consistent with those in the patterns which were simulated using SingleCrystal™ software.
There was no detectable increase in intensity at the 002 position; this is a forbidden reflection and was expected to be absent under kinematic conditions.
During the analysis of the extraction replica specimens, oxides in the sub-5nm size regime were not observed in great numbers.
The high number density of the nano-features observed in thin foils was not apparent in replica specimens.
The faceted cubic and rectangular particles, which dominated fine precipitation in thin foil specimens, were largely absent from the extractions; and spherical shaped oxides dominated within that size range.
It was concluded that the replication technique applied in this study, although useful for phase identification, did not give a true representation of the oxide size distributions and precipitate number densities found in ODM401 alloy.
Oxides with a pyrochlore crystal structure, which were rectangular or lozenge/coffin shaped when viewed in 011 oriented ferrite grains, displayed a cube on cube orientation relationship with the matrix (Fig.
9):011α//011A2B2O7and100α//100A2B2O7 A second orientation relationship was also observed (Fig.
10); this too was apparent in both AR and HT material.
The structure of the oxide was consistent with the pyrochlore structure but the second orientation relationship, the Baker and Nutting OR, was described by:011α//001A2B2O7and100α//100A2B2O7 The measured spacings of 2.51 and 1.79Å (±1%) of the planes separated by 45° is in good agreement with the calculated {004} and {044} planes of the Y2Ti2O7 phase.
Although no chemical analysis was made of particles which displayed this particular OR with the matrix, it was assumed that they were of the Y2Ti2O7 type.
No oxides displaying the cube edge OR were observed in 100 oriented ferrite; hence, 110 orientated particles were not observed and detailed atomic column image intensity analysis could not be performed.
This orientation relationship was far less prevalent than the cube on cube OR.
Ribis and de-Carlan [6] reported the existence of yttria particles which displayed both cube on cube and the Baker and Nutting cube edge on cube orientation relationships with the matrix.
The lattice parameter for yttria is approximately 10.61Å (ICSD CC-153500), hence the measurements recorded in the present work, which were internally calibrated using the ferritic matrix, are in better agreement with the Y2Ti2O7 pyrochlore structure.
High number densities of nano-scale oxides were observed in as received ODM401 alloy.
The particles coarsened from 2nm to 5nm during heat treatments of 1h at 1300°C but no phase change or detectable change in their chemistry was observed.
The microstructure was relatively unchanged by the heat treatment and a fine ferritic grain structure, in the order of 0.3μm diameter, was retained.
Only partial recovery was observed and no instances of recrystallisation could be seen.
Nanoscale oxides in ODM401 ODS alloy have been shown to display the pyrochlore crystal structure.
Oxide particles, which were sufficiently thin, behaved as weak phase objects in HRTEM phase contrast images.
Variations in the electrostatic potentials of atomic columns, which can be directly associated with the atomic species within each of the atomic columns, were observed.
A direct relationship between image intensities and the predicted positions of atomic columns in pyrochlore structured oxides which contain trivalent only, tetravalent and mixed trivalent/tetravalent species has been established.
Oxide crystals aligned with a 〈110〉 direction parallel to the incident electron beam show intensities directly related to the A23+B24+O72- crystalline structure.
Chemical analysis shows that aluminium and possibly other species including Si are associated with the oxide particles.
Hence, this work clearly demonstrates that aluminium must be considered in future models for the formation of oxides in ODS steels.
This work was carried out as part of the Materials for Fusion and Fission Power project.
The authors wish to acknowledge the support and funding which was gratefully received from the Engineering and Physical Sciences Research Council (Grant No.
EP/H018921/1).
We are also grateful to Bohumil Kazimierzak, of Dour Metal s.r.o., for the provision of ODM401 alloy samples.
Comparison and calibration of a real-time virtual stenting algorithm using Finite Element Analysis and Genetic Algorithms In this paper, we perform a comparative analysis between two computational methods for virtual stent deployment: a novel fast virtual stenting method, which is based on a spring–mass model, is compared with detailed finite element analysis in a sequence of in silico experiments.
Given the results of the initial comparison, we present a way to optimise the fast method by calibrating a set of parameters with the help of a genetic algorithm, which utilises the outcomes of the finite element analysis as a learning reference.
As a result of the calibration phase, we were able to substantially reduce the force measure discrepancy between the two methods and validate the fast stenting method by assessing the differences in the final device configurations.
Endovascular devices (stents, stent grafts, flow diverters) are widely adopted for restoring the patency of arteries and veins.
Over the last decade, the development in stent technology has been rapid and has revolutionised the treatment of many cardiovascular diseases by offering a minimally invasive alternative to complex open heart surgical interventions  [1].
Progress on different designs, materials and surface processing allowed the use of stents for treating a wide variety of conditions, including stenosis and aneurysms of coronary  [2], carotid  [3], cerebral  [4], thoracic  [5] and peripheral arteries  [6] and, more recently, cardiac valve dysfunction  [7,8].
As a consequence, interventional cardiologists currently have an extensive range of products at their disposal that enable them to reach complex cardiovascular districts from different access sites.
This variety of available devices also aims at targeting diverse patient populations who might suffer from acquired or congenital heart diseases.
In addition, every stent is associated with its own mechanical implantation performance, characterised by different foreshortening, dog-boning, deliverability and elastic recoil  [9].
Such increasing availability can make the choice of the most suitable device more complicated and, therefore, makes it desirable to accurately predict optimal stent placement, especially under patient-specific conditions  [10–12].
Computational models, like finite element (FE), have been widely employed in the past to study stent implantation.
FE simulations supported the inexpensive testing of novel designs, materials, etc., and contributed towards better understanding of the mechanisms of stent expansion  [13,14].
More recently, thanks to the advances in cardiovascular imaging and computational power, it has also been possible to include patient-specific anatomical models in the simulations, to study the interactions with complex geometries  [15–17] and the risk of structural failure in these environments  [18].
However, the complexity of such simulations still requires relatively high computational times, which limit the use of FE simulations to support daily clinical activities.
In this context, there have been a number of attempts to develop different computational methods to simulate stent deployment in “real-time”.
These methods feature fast computational times, necessary for their usefulness in clinical settings; however, they still display considerable simplifications, especially in stent modelling, which is often based on fitting a generic cylinder inside the vessel, on the surface of which stent struts are subsequently drawn  [19,20].
Even the approach proposed by Larrabide et al.
[21] that models the stent design explicitly is challenged by complex vascular geometries.
Therefore, to the best of our knowledge, no virtual stent deployment method has yet been proposed, that is able to combine fast turnover times with sufficient accuracy.
Against this background, we developed a novel fast computational method to simulate stent deployment under patient-specific conditions, specifically designed to be subsequently used in clinical practice  [22].
This fast method is based on a spring–mass model and can potentially take into account different device designs as well as emulate deformable vasculature.
Although the spring-mass method offers a convenient way to model mechanical behaviour in real-time due to its simplicity, the search for the appropriate springs constants is not straightforward.
Moreover, the fast computational speed inevitably comes with a cost of reduced accuracy; therefore, the error has to be thoroughly estimated.
Hence, the goal of this study is to perform a rigorous comparison of our fast method against FE analysis and to use FE outcomes as a learning base for fitting the required parameters.
For that purpose, we conducted a series of experiments of increasing complexity, in order to evaluate the discrepancy in final device configurations and residual force estimation.
Thereafter, we performed an optimisation step, in which the parameters of the fast method were calibrated with the help of a genetic algorithm which was guided by the FE results.
We conclude the paper by presenting the post-calibration results and the outlook on further improvements.
This study focuses on the analysis of a novel numerical fast method (FM) to simulate the process of stent expansion, as compared to detailed Finite Element (FE) analysis.
The device employed in this comparison resembled a self-expanding stent graft that is widely used for the treatment of aortic aneurysms and dissections (Fig.
1(a)).
The device consists of an external self-expanding nitinol wire structure (i.e., stent struts) that is helically attached along the entire surface of the ePTFE graft.
The stent is characterised by 10 crowns, where the 2 terminal crowns are attached to the central spiral crown by 2 vertical bars.
The device diameter is 38 mm and its length is 92.25 mm.
The wire of the stent struts is 0.381 mm thick.
The assessment of FM was based on the comparative analyses of a stent graft expansion, with subsequent optimisation step, and is divided into four phases:1.Verification of the numerical methods: a simple case of stent free expansion was tackled with both FM and FE methods in order to verify the convergence of the numerical solutions (Section  2.3).
First comparison of FM against FE: a comparative analysis of both numerical methods was performed by simulating the stent implantation within six models of different vessel sites of increasing complexity.
For this initial comparison, the FM was implemented using standard stiffness parameters derived from literature data (Section  2.4).
Optimisation of FM: based on the results obtained during the first round of comparative analyses, the parameters of the FM were calibrated by means of a genetic algorithm (Section  2.5).
Second comparison of FM against FE: the post-calibrated FM was finally tested by repeating the six simulations of stent implantation and by comparing the new results against the unmodified FE analyses (Section  2.4).
In the next sections, a detailed description of both numerical methods and each procedural step is reported.
The proposed FM operates on a simplified stent representation, in which the strut wires are approximated by straight lines (centrelines), that is, the struts are modelled without an explicit thickness.
Additionally, the modelled struts are supplemented with a background mesh, that emulates the graft of the device.
The two meshes are merged by creating an anisotropic grid using triangulation constraints, that is, the background mesh incorporates the elements of the stent struts (see schematic representation in Fig.
1(a)).
The mechanics of the device in FM is based on so-called spring–mass models that are widely used in surgery simulations and many other engineering applications  [23].
In this approach, the mesh is replaced by fictitious springs, as schematically depicted in Fig.
1(b).
Each spring then behaves according to Hooke’s law: the restoring force F acting on the node i from its neighbours is determined in the following way (1)Fi=∑j=1nikij(δj−δi), where δi is the displacement of node i, kij stands for the stiffness of the spring (i−j) and ni indicates the number of vertices directly connected to the node i.
Application to stent deployment.
The crimping of the stent creates spring forces acting in the structure, which allows the nodes of the mesh to recover their original configuration driven by these forces.
The nodal movement takes into account its intertwinement with the neighbours by means of the nodal stiffness, the value of which is set to be equal to the sum of stiffness values of all springs connecting it to its immediate neighbours.
In the first instance, we have set the stiffness of springs to be inversely proportional to the segment length before the deformation, as originally proposed by Batina  [24].
This setting takes into account the fact that mesh vertices located in close proximity to each other should exert stronger restoring force onto each other.
Subject to restoring forces, the new nodal displacement can be calculated at every time step as the restoring force divided by the stiffness of the node Ki,nodal: (2)δi=−FiKi,nodal, where the stiffness of the node is set to the sum of the stiffness of all edges emanating from this node Ki,nodal=∑j=1nikij.
Eq.
(2) is solved for displacements for all nodes of the structure on the vertex-to-vertex basis.
The new nodal coordinates are determined by adding the calculated displacement δ to the old coordinates.
The deployment process starts with the crimping of the device which simulates its positioning in the catheter.
In clinical practice, the chosen device is compressed into a 24FR (24×0.333 mm=8 mm) catheter system which results in an 80% reduction in diameter.
Thereafter, the crimped device is aligned along the centreline of the target vessel at the relevant position for the start of the expansion process.
The stent expansion procedure is performed in an iterative way on a vertex-to-vertex basis, according to Eq.
(2).
The coordinates of a vertex are updated after each iteration, as long as no contact between the vertex and the vessel occurred.
The contact is defined to occur when the Euclidean distance between the vertex of the stent and any of the vessel vertices becomes smaller than a chosen parameter ϵ.
To detect contact, a contact check is performed in every iteration of the deployment loop before the update of the coordinates of stent vertices (step “Check contact”).
After the contact was detected, the displacements of the stent vertex in contact are still calculated in future iterations, to enable possible readjustments resulting from the tension of the neighbouring nodes.
However, its position can only change if it is located within the ϵ-boundary of the vessel inner surface.
The detailed pseudocode is shown below.
1.Calculate initial displacement of stent vertices by subtracting the load-free (lf) coordinates of vertices from the current crimped coordinates (cr).
For a vertex vi=(xi,yi,zi)′ initial crimping delta δicr is found by: δicr=(δxicrδyicrδzicr)=(xicryicrzicr)−(xilfyilfzilf).
Iterate until there is almost no change in expansion delta δexp, that is, stop when max{|δxiexp|,|δyiexp|,|δziexp|}≤ϵ, where i ranges over all nodes (ϵ=10−6) •Calculate restoring forceFi for each node i from Eq.
(2)Fi=∑j=1nikij(δjcr−δicr).
Calculate expansion deltaδiexp for each node i from force and nodal stiffness δiexp=−Fi∑j=1nikij.
Check contact between the newly determined stent coordinates and the vessel: for each stent vertex vi and each potential vessel contact vertex pj∈Pi if they are closer than ϵ: ‖(vi+δiexp)−pj‖<ϵ add i to set C of nodes in contact: C≔C∪{i} Update stent nodal coordinates for each node i not in contact, i∉C(xicryicrzicr):=(xicryicrzicr)+(δxiexpδyiexpδziexp).
Update crimping delta values δicr for each node i∉C, by adding expansion displacement δiexp to the previous δicrδicr=(δxicrδyicrδzicr):=(δxicrδyicrδzicr)+(δxiexpδyiexpδziexp).
Output stent nodal coordinates The FM was implemented in Matlab and executed on an Intel Core 2 Duo with a 2.66 GHz CPU with 4 GB of memory and without using any parallelisation.
All the FE analyses were performed using the FE code Abaqus 6.11/Explicit (Dassault Systemes Simulia Corp., Providence, RI, USA) on an Intel Xeon X5690 with 2 processors of 3.46 GHz and 24 GB of RAM; all simulations ran in parallel using 4 cores.
The CAD model of the stent was meshed with beam and shell elements to discretise the stent struts and the stent graft, respectively.
Following sensitivity analysis, the average length of the beam elements was 0.9 mm with a final mesh resulting in 2371 elements, while the average size of shell elements was 0.8 mm for a total of 13004 elements.
The constitutive models used to characterise both the Nitinol stent struts and polyester graft were based on a previously validated study by our group  [25].
Each experiment consisted of three steps: crimping, positioning within the vessel model and deployment.
1.Crimping.
The device was crimped to the size of the delivery catheter (8 mm diameter) by applying a radial displacement (15 mm) to a coaxial cylindrical surface.
Such cylindrical surface mimicked the presence of the membrane sheath that constrains the stent into the delivery catheter (see Fig.
2(a)).
Specific boundary conditions were assigned to the stents, in order to avoid rigid translation during the analysis: the device was constrained in its terminal section in both circumferential and axial direction (see Fig.
2(b)).
Positioning.
In order to position the crimped stent and its cylindrical sheath within each virtual implantation site, we used the respective centrelines.
Since the stent and the sheath were coaxial, their centreline (Cs) is unique.
Such centreline was then aligned to the vessel centreline (Cv).
We therefore subdivided the lines through a discrete number of points (n=11).
A kinematic constrain was applied between the points of Cs and the nodes of the cylinder laying on the plane passing through that point and cutting Cs perpendicularly.
Hence, the coordinates differences of the points of Cs and Cv were calculated.
The resulting displacements and rotations were applied onto the points of Cs, causing the bending of the device (see Fig.
2(c)).
Deployment.
The deployment of the stent was achieved by replicating the release from the delivery catheter and was implemented by gradually retrieving the virtual membrane sheath.
To test the validity of the models and their overall performance, we set up the free expansion experiment with both FM and FE methods.
In case of FM, from the initially crimped configuration (8 mm in diameter), the stent graft was subject to free expansion until it reached its load-free configuration.
Since the released stent graft is supposed to recover its known load-free configuration, the expansion process can be tracked by the average distance to the target load-free stent.
Therefore, the dynamics of the free expansion process was evaluated by analysing the convergence of four parameters: 1.Mean nodal distance metric is the average distance from the vertices of the current expanding stent to their counterparts in the load-free device.
Such distance is defined as d¯nodal=1N∑i=1N‖vi0−vi‖, where vi0 is the position of the node i in the load-free stent and vi — in the current expanding stent.
Mean angle difference between the load-free and the current expanding stent is measured as ε¯angle=1M∑j=1M|θj0−θj|, where θj0 is the value of the angle j in the load-free stent and θj — in the current expanding stent, and M stands for the number of angular elements in the stent structure.
Mean strut length difference provides quantification of the error related to the stent strut length as a relative percentage of their initial length.
It is defined as ε¯length=1S∑k=1S|lk0−lk|lk0, where lk0 is the length of the strut k in the load-free stent and lk — in the current expanding stent and S stands for the number of struts (expressed in %).
Mean nodal force takes into account the reaction forces causing the expansion process.
It is measured as F¯=1N∑i=1N‖Fi‖, where Fi is the reaction force at the node i, with the index i running from 1 to number of nodes N. To set up the FE free expansion we used quasi-static conditions, in which the contribution of the kinetic energy to the total energy should be equal to zero, assuming that there are no inertial effects.
Therefore, to verify the convergence of the FE analysis, the ratio Kinetic Energy/Internal Energy had to reach a value <5%.
As in the free expansion case, the device was crimped to 8 mm and then deployed, this time until it reached the vessel walls.
The comparison was based on the analysis of the expansion of the stent graft under 6 different conditions: 1.deployment within a 34 mm straight cylindrical vessel; deployment within a 20 mm straight cylindrical vessel; deployment within a bent vessel (diameter=34 mm;curvature=0.01); deployment within a C-shaped vessel (diameter=34 mm;curvature=0.02); deployment within an W-shaped vessel (diameter=34 mm); deployment within an anatomical model of a patient-specific dissected aorta.
As it would be done in clinical practice, the graft was placed inside the true lumen to cover the initial tear entry and prevent blood from flowing into the false lumen.
The vessel diameter was varying at the deployment site: in the distal location (with respect to the dissection entry) it was approximately of a round shape with 29 mm in diameter, whereas proximally the true lumen resembled an oval with dimensions of 13 and 28 mm.
Hence, the geometry in the proximal end is quite irregular, exhibiting sharp corners that might be problematic to comply with for a stiff device.
In order to quantify the differences in outcomes of the two methods, the following parameters have been measured: 1.Residual distance is measured between the corresponding nodes of the deployed stent that was expanded with the two different methods: FM and FE.
The difference has been calculated by comparing these distances to the vessel diameters.
The distribution of residual forces in stent vertices is determined in the two different methods, FM and FE.
While FM can directly output nodal forces, the residual forces (RF) in the FE method can be calculated only if the structure is constrained in all directions.
For this reason, we derived the RF in the stent vertices through the reaction forces measured in each node of the vessel, which is always fully constrained.
For each stent vertex in contact with the vessel, it was possible to find the 4 closest nodes of the vessel with the help of a specifically implemented automated method; these 4 nodes belong to the element of the vessel in contact with the vertex of the stent.
The sum of these 4 reaction forces is equal and opposite of the RF in the stent’s vertices (see Fig.
3).
Relative error in struts length is calculated in relation to the original length of struts, i.e., in the load-free stent configuration.
In this section we aim at finding the optimal stiffness values for our fast stent model of the analysed stent device.
In general, stiffness setting has significant implications on the expansion process.
Without any modification to other parts of the expansion algorithm, alterations to the stiffness setting can lead to substantial degradation or even a complete loss of expanding properties of the modelled device.
On the whole, instantiation of the stiffness values for spring–mass models is not a straightforward task.
Several attempts have been reported in the literature, describing ways of finding the best stiffness parameters for spring–mass models for different applications; for the most part, they are of a heuristic nature  [26,27].
We have chosen the framework of Genetic Algorithms (GAs) as a means to optimise stiffness setting of the given stent device.
GAs draw inspiration from natural evolution  [28].
They simulate populations of individuals, each individual being represented by a certain number of genes that encode parameters of the investigated problem.
The fitness of an individual is determined by a cost or fitness function that is formulated for a given problem setting.
At the beginning of the optimisation process, the population is randomly initialised by assigning to individuals random values of genes that define them.
In the course of an iteration, the fitness is assessed, and the new population is formed based on the fitness/cost function and the generic operators of mutation and crossover applied to selected individuals.
Mutation means that when copying the genes from the current to a new generation, random changes might occur with a certain probability.
In contrast, crossover combines genes from selected individuals to form an offspring.
The aim of the algorithm is to make the population converge to a final population in which the best performing individuals will be as close as possible to the optimal solution of the problem.
As a rule, GAs do not guarantee a convergence to the global optimum; however, they are able to find good approximative solutions by reaching local optima.
In our case, we used the known final FE configurations and reported residual forces as a reference for optimising the stiffness parameters of FM.
Thus, the cost function for the GA formulation should take into account the discrepancy between the FM and FE outcomes in terms of the nodal distances and the value of forces.
Our goal is to find stiffness parameters resulting in better alignment between the two methods.
Setup.
In theory, the spring constant of an “ideal” spring can be found as k=EA0L0, where E is the Young’s modulus of the material, A0 is the unstretched cross sectional area and L0 is the unstretched length of the spring.
This indicates that, all else being equal, k is inversely proportional to the original length of the spring (L0), scaled by EA0.
Therefore, in our model, we keep the inverse relation to the length, as was the case before, and will be only looking for the optimal scaling constant with the help of the GA. Additionally, we have differentiated between the scaling values for the background springs (wBg) and the springs representing stent struts (wStr).
Hence, the springs constants in our setup will be calculated in the following way for a vertex vi=(xi,yi,zi)′ and vj=(xj,yj,zj)′: (3)kijBg=wBg‖vi−vj‖andkijStr=wStr‖vi−vj‖+kijBg=wStr+wBg‖vi−vj‖.
Therefore, the space our GA is operating within can be visualised as a 3D function defined by the two stiffness weights (wStr and wBg).
Fig.
4 illustrates an approximative extract from this space for case 3 (bent vessel with diameter = 34 cm) for the intervals wBg=[0.1…1] and wStr=[0…1] along abscissa and ordinate axes, respectively, and the corresponding cost value along the z-axis.
The graph shows the behaviour of the nodal (a) and force (b) difference as parameters vary.
The squared error of the nodal distance ranges between 80 (blue) and 170 (red), whereas the squared force error lies between 0.015 (blue) and 0.2 (red) in these intervals.
Note the difference of more than 3 orders of magnitude between the error measures.
All the simulations in the initial comparison (pre-calibration) section were performed with the stiffness setting wStr=0 and wBg=1, which is located at the bottom right corner of the error space, indicated by the red arrows in Fig.
4.
With the help of the graphs, we can see that this stiffness setting corresponds to a relatively low value with respect to the nodal difference; however, it maps to a fairly high point on the force difference space, compared to other values of the analysed interval.
More precisely, the baseline squared error values are e¯nodal=82.7447 and e¯F=0.1196.
Implementation.
As already mentioned, we distinguish between two classes of springs: springs belonging to the stent struts and those of the background sheath.
Population: In each generation, there are 7 individuals in the population.
Each individual is defined by a pair of genes that represent the two scalars we are looking for: the weight for the background sheath (wBg) and the (additional) weight for the struts stiffness (wStr), that is, an individual is represented as ind=[wStr,wBg].
The population is randomly initialised and then evolved for roughly 100 generations using the genetic operators described below.
Cost function: In each generation, for each of the 7 individuals in the population, the FM simulation with the stiffness values corresponding to the genes of the individual (ind=[wStr,wBg]) is run and then compared to the FE reference to determine the fitness of the individual.
Both the nodal distances and the force values will be optimised simultaneously.
Firstly, we compute the mean squared error of both values calculated from the convergence metrics defined in the previous section.
More precisely, the mean squared difference in residual force magnitude is measured ase¯F=1N∑i=1N(‖FiFE‖−‖FiFM‖)2, where Fi is the reaction force at the node i, with the index i running from 1 to number of nodes N. The mean squared difference in nodal distance is calculated as e¯nodal=1N(1002R)2∑i=1N‖viFE−viFM‖2, where vi is the position of the node i in the final stent obtained with the fast (viFM) and FE (viFE) method, respectively; R indicates the radius of the vessel wall, that is, the error is estimated with respect to the vessel wall diameter.
Thereafter, we combine two error measures into a single cost function in the following way: (4)fcost=12(e¯nodal)+5⋅1032(e¯F) to be able to optimise both metrics simultaneously.
In order to bring them to relatively the same importance, we had to scale the force error to 5⋅103 due to the difference in magnitude of the two values.
Selection: At each generation, all individuals are sorted according to their performance with respect to the cost function.
Two best performing individuals are kept for the next generation without alterations.
The rest of the population is subject to generic operators of mutation and crossover.
Mutations: At each generation, the 3 best performing individuals undergo random mutations when creating new offsprings.
The mutation operator is adding a random variable from the interval [−1.5…1.5] to each of the two genes.
However, since stiffness cannot be negative, the value is corrected if the result of the addition turns out to be negative; the positive counterpart is taken instead.
Crossover: At each generation, the 2 fittest individuals are selected and crossed to create two offsprings by swapping the defining genes.
To ensure the diversity in the population, at the end of the population forming, we perform an additional check to ensure that all individuals are different.
This enables to explore the space of possible solutions in a better way and speed up the convergence.
The optimisation is performed once on the case of the bent vessel (diameter=34 cm) to capture the bending behaviour, and the resulting stiffness values are subsequently used in all post-calibration comparisons.
In this section we report the outcomes of the numerical experiments described above: (1) verification with free expansion, (2) pre-calibration comparison, (3) FM optimisation using genetic algorithm and (4) post-calibration comparison.
Fig.
5 highlights the phases of the free expansion process simulated with both numerical methods.
In particular, results of FM simulations are presented as the snapshots of the obtained stent configuration after a certain number of iterations, while FE analyses are shown throughout the simulation (30 s).
Dynamics of the expansion process are plotted in Fig.
6, where the four metrics of convergence and the internal energy are plotted for FM and FE, respectively.
The evolution of all plotted functions confirm the convergence of the results and the ability of both methods to expand the device to its load-free configuration.
Specifically for the FM case, the nodal displacement d¯nodal (Fig.
6(b), upper left corner) converges completely after 200 iterations, while the measure of angular distance ε¯angle (Fig.
6(b), upper right corner) and the measure of the residual nodal forces (Fig.
6(b), bottom right corner) exhibit a slightly faster convergence, being almost zero during the last 100 iterations.
The trend related to the error of strut lengths (Fig.
6(b), bottom left corner) shows a different tendency, with the error first increasing up to 20% after few iterations and then decreasing in the course of the iterative process, until complete convergence is reached.
The analysis of the ratio between internal and kinetic energies of the FE model confirms the absence of any dynamic processes at the end of the free expansion phase when the stent graft completely recovers its original shape, as displayed in Fig.
6(a).
To assess the differences between the FM and FE methods, the stent graft configurations obtained with both methods were compared in six different deployment scenarios.
Both methods are able to realistically capture the self-expansion of the stent graft until it reaches equilibrium in contact with the arterial wall.
The process of expansion is visualised in Fig.
7 which displays two examples of different complexity.
In order to mimic clinical practice, the deployment of the device was either initiated at the centre and progressed to both extremities (first row) or was started at the one end and progressed towards the other extremity (second row), as common for this type of stent grafts.
In the case of FM, the final configuration was obtained after approximately 200 iterations of the virtual deployment algorithm and required around 20 s. The device was in a reasonably good opposition to the vessel wall and covered the initial tear completely.
In both FM and FE simulations, the stent graft successfully covered the virtual vasculature lesion.
These results confirmed the possibility to use both the techniques also in case of complex anatomical cases.
One of the most important outcomes of the comparative analysis is the fact that in all tested cases the use of FM is associated with a dramatic reduction in computational time when compared with FE, generally being in the order of seconds for FM and in the order of hours for FE.
Table 1 reports the timings of the simulations for both methods.
Free expansion is the fastest case, where FM reaches the load-free configuration in just 2 s, while simulations inside the vessels with the diameter of around 30 mm take approximately 30 s. Most of the execution time of the FM deployment algorithm is dedicated to the contact check and calculations of the implications the vessel wall has on the stent structure.
Interestingly, in both methods, the highest computational time (i.e., curved vessels) is not associated with the most complex geometry (i.e., patient-specific case of aortic dissection).
Another fact worth mentioning is the relation of the computational time to the diameter of the vessel in both methods.
While the computational time of FM appeared to be directly related to the diameter of the vessel, no immediate relation was found for the FE simulations.
Such outcome is probably related to the simplified contact model used by FM, which makes the stent-graft expansion terminate once the nodes come in contact with the vessel wall.
On the contrary, it is well known that the contact algorithm used in the FE analyses increases the computational cost of the simulations.
The detailed results of all six simulations are reported in Figs.
8 and 9, which illustrate the final deployed configurations for both numerical methods.
The visual comparison enables the qualitative assessment of the differences of the two methods by means of overlaying the final deployed configurations of the devices (last column FM+FE).
The visual comparison showed an overall good agreement in terms of device positioning.
However, in the cases where the curvature is higher (i.e., curved C-shaped vessel and curved W-shaped vessel), it is possible to observe a larger difference in the arrangement of the extremities of the stent.
Importantly, it is possible to observe that such differences seem to be associated with a different grade of vessel curvature rather than with a different opening size.
In order to quantify resulting differences, nodal distance and residual stent forces have been computed for each vertex of the stent struts.
In Fig.
10, the difference between the two methods is reported in the form of box plots.
In the simple straight geometries, the average nodal distance discrepancy is about 4%; however, it is increasing with more complex geometries, reaching about 10% for the patient-specific case.
The smallest residual nodal distance is reported for the straight vessel (d=34mm) with the mean of the nodal errors μ=3.78% and the standard deviation σ=1.20%, when fitted to the normal distribution.
For the 20 mm straight vessel, the discrepancy between FM and FE is more pronounced (μ=7.13%, σ=2.60%).
In the narrow vessel the FM device exhibited a configuration very similar to the crimped one.
The difference for the more narrow vessel is larger because the FM device does not have a chance to sufficiently open up before hitting the vessel wall and is still exhibiting a configuration very close to the crimped one, due to the unsophisticated FM contact model.
Since the crimping procedures in both methods are different (and hence different crimped configurations), we are effectively comparing the difference in crimping as well.
In fact, in a wider vessel, the difference in crimping is being gradually elevated in the course of the expansion, when the device is moving to its load-free configuration, resulting in a smaller difference.
Overall, the nodal difference tends to increase with the increasing complexity of the vessel model, reaching its maximum in the case of the model of dissected aorta.
Under patient-specific conditions, the highest average nodal distance of μ=10.50%, with respect to the vessel diameter (which is taken to be 2.5 cm as the result of averaging the round distal and oval proximal diameters) and σ=4.64%.
Such differences can be explained by observing that the middle part of the device was not completely complying to the vessel wall in the case of FM.
The reason for the lack of opposition in the FM device is the fact that the FM contact is detected too soon in the complex oval vessel geometry, which shows the limitation of the FM contact model employed.
With respect to the residual forces, the difference between the two methods is displayed in Fig.
10(b), with an average force difference lying below 1 N and an absolute maximum of 4 N highlighted in the case of the C-shaped vessel.
The values were in the same order of magnitude as the ones reported by FE method.
Even in the case of the narrow 20 mm vessel, the relation remained the same, which means that the forces reported by the fast method were consistent with the increase of crimping in the device.
The general trend is that the FM reports force values larger than the FE method for all simulated cases.
This outcome indicates that the stiffness of the springs used in the FM were initially set to a value which was too high for the investigated problem.
The final comparison metric was the discrepancy in struts’ length.
As we noted in free expansion case, FM tended to distort the struts length at the beginning of the iterative process, correcting it in the course of the subsequent iterations (see Fig.
6(b), bottom left graph).
Hence, in cases where the device deployment is terminated beforehand, there is usually a small error in struts’ length for the FM cases.
For example, in the case of the 34 mm vessel, the majority of struts display an error of less than 3% of their original length.
The aforementioned FM convergence dynamics leads to the higher error rate in the narrower 20 mm vessel, where the average is approximately 6%.
On the contrary, the FE method does not distort the struts length in such extent; however, for the 20 mm vessel there is a non-zero error of about 2%.
On the whole, the error in struts length is negligible and is indirectly incorporated in the nodal distance metrics, which was already discussed above.
Bearing in mind these initial results, the next sections report on the effect of modifying the FM spring stiffness setting, following the GA calibration.
With the aim of reducing the differences between FM and FE and modifying the stiffness settings, we have performed multiple runs of the GA resulting in similar outcomes.
As a rule, the convergence was achieved after 50 generations.
The results of one typical run are reported here in detail.
The very first, randomly assigned, population and the final population are captured in Table 2, together with the corresponding values of the cost function.
The cost function fell from the initial best value of 275.6908 to 49.4124 during the simulation, which is displayed in Fig.
11.
All individuals that were sampled during the evolution process can be seen in Fig.
12, which displays them as red crosses on the cost space (a) and as red circles on the nodal (b) and force (c) differences spaces.
The evolution of the fittest individuals from each generation are plotted as a black line.
The convergence towards the most optimal exemplar can be noticed by means of the increase in the concentration of red crosses and circles in the lowest interval of both parameters.
In the course of the evolution, the fittest candidate emerges, defined by the pair [0.0688, 0.0874] with the e¯nodal=80.7917, e¯F=0.0036 and f¯cost=49.4124.
Note that wStr was defined as the additional stiffness weighting that would be added to the background stiffness weight (see Eq.
(3)).
Thus, the overall struts stiffness weight is the sum of the two values (0.0688+0.0874=0.1562), whereas the weight for the background stiffness remains 0.0874.
The optimal individual that emerges in the course of the GA is very sensitive to the definition of the cost function, in particular to the weightings that indicate importance of the two error measures.
The value of weightings that was employed in our cost function was identified using testing and seems to result in both good simulation outcomes and stable convergence behaviour of the genetic algorithm.
Fig.
13 shows the results of the simulations with the new stiffness values obtained with the GA in the previous section, with the strut stiffness weight set to 0.1562 and the weight for the background stiffness set to 0.0874.
Compared to the pre-calibration result (Fig.
10), there is a considerable improvement in the force measurement: from an average difference in magnitude of 1 N to 0.18 N. However, the nodal distance exhibits only a small change, with inconsistent tendency: in some cases there has been a slight improvement, whereas others exhibit minor increase of the resulting discrepancy.
This is of no concern, however, since the error in nodal distance was small in the first place.
These results indicate that the stiffness scaling is not sufficient by itself.
Further improvements in the FM algorithm could be achieved by modifying the contact model and possibly implementing a more sophisticated stiffness mechanisms, which potentially takes into account 3D stiffness or different settings for the background mesh, such as anisotropy.
These will require further investigation.
This paper reported a comparative analysis of a novel fast virtual stenting method with the widely-used finite element analysis.
A set of six in silico experiments were conducted and evaluated, resulting in the assessment of the novel FM.
Further, using FE results as a learning base, we were able to optimise FM for the chosen stent device by calibrating stiffness parameters with the help of a genetic algorithm.
Such calibrations of the FM have to be performed for each new stent device that is to be included in FM deployment simulations.
In general, each stent features its own optimal FM stiffness coefficients that have to be previously determined with the help of the demonstrated method.
The obtained stiffness setting substantially improved the force measures reported by the FM and reduced the force discrepancy between the two methods.
Overall, the results of all performed experiments indicated a good agreement between the FM and FE methods.
More precisely, the mean difference in the nodal positioning ranged between 4% and 10% depending on the complexity of the case, being the highest for the patient-specific case of the aortic dissection with a very irregular geometry.
Another quantitative measure of the validity of the FM was the mean force discrepancy, which after the calibration phase, lay below 0.2 N. The obtained results are promising, especially considering the current limitations of the FM.
In particular, the simplified contact model, which is likely to be responsible for high errors, especially in the case of narrow vessels and complex anatomical features.
Improving the contact model algorithm would help reduce the errors, most probably at the cost of more expensive computation and longer execution time.
Another limitation of the study is in the assumption of rigid vessel walls.
Although the FM is already able to model the flexible vasculature, we decided to focus on the process of stent expansion, in order to eliminate possible interdependencies.
In conclusion, the computational fast method proposed here can be explored as an additional tool to facilitate clinical practice.
While FE can provide more detailed and accurate analysis, the much faster computational time (in the order of seconds) and acceptable error (<10%) associated with FM, bring high hope for its potential usefulness as an integral part of clinical practice.
The authors thank Prof. J. Byrne (University of Oxford) and Dr. H. von Tengg-Kobligk (Inselspital Bern, University of Heidelberg) for valuable clinical input.
KS acknowledges the Centenary Year Graduate Scholarship, Department of Engineering Science, University of Oxford.
CC, GMB and SS acknowledge Heart Research UK, British Heart Foundation, the Royal Academy of Engineering/EPSRC and Rosetrees Trust.
YV acknowledges the Welcome Trust/EPSRC, Centre of Excellence in Personalised Healthcare, (grant number WT 088877/Z/09/Z) for support.
Direct normal irradiance related definitions and applications: The circumsolar issue The direct irradiance received on a plane normal to the sun, called direct normal irradiance (DNI), is of particular relevance to concentrated solar technologies, including concentrating solar thermal plants and concentrated photovoltaic systems.
Following various standards from the International Organization for Standardization (ISO), the DNI definition is related to the irradiance from a small solid angle of the sky, centered on the position of the sun.
Half-angle apertures of pyrheliometers measuring DNI have varied over time, up to ≈10°.
The current recommendation of the World Meteorological Organization (WMO) for this half-angle is 2.5°.
Solar concentrating collectors have an angular acceptance function that can be significantly narrower, especially for technologies with high concentration ratios.
The disagreement between the various interpretations of DNI, from the theoretical definition used in atmospheric physics and radiative transfer modeling to practical definitions corresponding to specific measurements or conversion technologies is significant, especially in the presence of cirrus clouds or large concentration of aerosols.
Under such sky conditions, the circumsolar radiation—i.e.
the diffuse radiation coming from the vicinity of the sun—contributes significantly to the DNI ground measurement, although some concentrating collectors cannot utilize the bulk of it.
These issues have been identified in the EU-funded projects MACC-II (Monitoring Atmospheric Composition and Climate-Interim Implementation) and SFERA (Solar Facilities for the European Research Area), and have been discussed within a panel of international experts in the framework of the Solar Heating and Cooling (SHC) program of the International Energy Agency’s (IEA’s) Task 46 “Solar Resource Assessment and Forecasting”.
In accordance with these discussions, the terms of reference related to DNI are specified here.
The important role of circumsolar radiation is evidenced, and its potential contribution is evaluated for typical atmospheric conditions.
For thorough analysis of performance of concentrating solar systems, it is recommended that, in addition to the conventional DNI related to 2.5° half-angle of today’s pyrheliometers, solar resource data sets also report the sunshape, the circumsolar contribution or the circumsolar ratio (CSR).
active cavity radiometer aerosol optical depth European cooperation in science and technology concentrating photovoltaic circumsolar normal irradiance concentrating solar power circumsolar ratio concentrating solar technologies direct normal irradiance International Energy Agency Lawrence Berkeley National Laboratory Monitoring Atmospheric Composition and Climate Monte Carlo for the physically correct tracing of photons in cloudy atmospheres optical properties of aerosols and clouds photovoltaic power systems rotating shadowband irradiometer solar (or system) advisor model Solar Facilities for the European Research Area Solar Heating and Cooling simple model of the atmospheric radiative transfer of sunshine slant optical depth solar power and chemical energy systems weather intelligence for renewable energy World Meteorological Organization broadband sky radiance solar zenith angle angular distance from the center of the sun azimuth angle of a given point in the sky in the orthogonal spatial system of axes defined by the direction of the sun (see Fig.
1) broadband DNI penumbra function or acceptance function half-angle of the sun disk sunshape non-scattered direct normal irradiance (non-scattered radiant flux from the sun disk only) extraterrestrial normal irradiance of the sun opening half-angle viewing angle acceptance half-angle concentration factor slope angle limit angle ideal DNI DNI of the sun (radiant flux from the sun disk only) CSNI ideal CSNI circumsolar ratio circumsolar contribution aerosol optical depth Ångström exponent slant aerosol optical depth at 550nm The direct irradiance received on a plane normal to the sun over the total solar spectrum is defined as direct normal irradiance (DNI).
DNI is an essential component of global irradiance, especially under cloudless conditions, and represents the solar resource that can be used by various forms of concentrating solar technologies (CST), such as concentrating solar power (CSP) systems—also called solar thermal electricity systems, including parabolic dish, parabolic trough, linear-Fresnel, or solar tower, or concentrating photovoltaic (CPV) systems.
For that reason, the characterization of the solar resource in terms of quantities related to DNI is of particular importance, and presently corresponds to one of the primary research topics in the domains of solar radiation modeling, satellite-based retrievals, and radiometric ground-based measurements.
In the fields of electromagnetic scattering, radiative transfer and atmospheric optics, many decades of theoretical developments in so-called “directional radiometry” are noteworthy, as recently reviewed by Mishchenko (2011, 2014).
Such highly fundamental studies have reached the solar energy community only indirectly, however.
Despite the fact that the term “direct normal irradiance” and its corresponding acronym DNI have been widely used for a long time in the fields of solar energy and solar resource assessment, this quantity may actually correspond to different definitions, interpretations or usages, which can lead to confusion.
These issues have been identified in two EU-funded projects, namely MACC-II (Monitoring Atmospheric Composition and Climate-Interim Implementation) and SFERA (Solar Facilities for the European Research Area).
Moreover, experts participating in the International Energy Agency Solar Heating and Cooling Programme (IEA SHC) Task 46 “Solar Resource Assessment and Forecasting” extensively discussed these issues at two international workshops.
This collaborative IEA Task is also coordinating with both the IEA SolarPACES (solar power and chemical energy systems) and IEA PVPS (photovoltaic power systems) implementing agreements.
Thus, international experts from all relevant solar technologies—most particularly CSP and CPV—have been involved to reach a consensus on these definitions.
DNI is defined as follows in the ISO-9488 standard (ISO-9488, 1999): “Direct irradiance is the quotient of the radiant flux on a given plane receiver surface received from a small solid angle centered on the sun’s disk to the area of that surface.
If the plane is perpendicular to the axis of the solid angle, direct normal solar irradiance is received”.
This definition is simple to understand from a theoretical perspective, even though it remains vague due to the lack of specification about what a “small solid angle” actually is.
This issue will be examined further in Section 2.
The rest of this contribution is mainly concerned with the experimental side of the question.
Historically, the interest in accurate measurement of DNI started decades ago.
Early studies (e.g., Linke, 1931; Linke and Ulmitz, 1940) identified the difficulty of separating the measurement of DNI from that of the diffuse irradiance in the immediate vicinity of the sun, hereafter referred to as circumsolar irradiance.
Pastiels (1959) conducted a detailed study of the geometry of pyrheliometers, and how that geometry interacted with circumsolar radiance, using simplified representations of the latter.
Various communications were then presented at a WMO Task Group meeting held in Belgium in 1966 (WMO, 1967) to improve the accuracy of pyrheliometric measurements, including estimates of the circumsolar enhancement.
Ångström (1961) and Ångström and Rohde (1966) later contributed to the same topic, followed years later by Major (1973, 1980).
The whole issue of instrument geometry vs. circumsolar irradiance was complex and confusing at the time because different makes and models of instruments had differing geometries.
This was considerably simplified after WMO issued guidelines about the recommended geometry of pyrheliometers, which led to a relatively “standard” geometry used in all recent instruments.
The experimental issues related to the measurement of DNI are discussed in Section 3.2.
After the theoretical background related to DNI in Section 2, a review of its multiple definitions, measurements and applications are reviewed in Section 3.
Section 4 summarizes the expert consensus on clear definitions and terminology related to DNI.
Section 5 gives examples based on simulations and ground measurements demonstrating the important role of circumsolar radiation in DNI, notably due to aerosols and thin clouds.
Finally, Section 6 provides recommendations for a better mutual understanding of the possible definitions of DNI and how to reconcile them.
Let L(ξ,φ) be the broadband sky radiance—usually expressed in Wm−2sr−1—for an element of sky whose angular position is defined by the angular distance ξ from the center of the sun and its corresponding azimuth angle φ.
The angle ξ is the angular distance of the considered point in the sky with respect to the angular position of the sun (Fig.
1).
If the sun happens to appear within the sky patch considered, its radiance is included in L. The red surface is the plane perpendicular to the direction of the sun.
The corresponding solid angle with aperture half-angle of ξ is represented by the grey cone.
Here, the term “broadband” refers to the shortwave part of the extraterrestrial solar spectrum that is received at the surface of the Earth, typically ranging from 290 to 3000nm (WMO, 2010).
This energy-rich part of the solar spectrum is covered by the spectral responses of pyrheliometers, which covers the range 300–4000nm (e.g.
EKO, 2011; Kipp and Zonen, 2008; Hukseflux, 2011).
However, some CSP technologies with selective receiver coatings only use the spectral range from about 350 to 2500nm (Benz, 2004).
Similarly, PV and CPV collectors have a very different—narrower and uneven—spectral response than pyrheliometers.
It should be noted that DNI is implicitly considered as broadband for this discussion.
The ISO definition of DNI, noted Bn, can be expressed by the following fundamental formula:(1)Bn=∫02π∫0αlP(ξ,φ)L(ξ,φ)cos(ξ)sin(ξ)dξdφ.where P(ξ,φ) is the “penumbra function” that is sometimes also called “acceptance function”.
The penumbra function is equal to 0 for ξ greater than a limit angle αl (see Section 3.2).
Sometimes, the aperture solid angle precisely defined by the penumbra function can also be simply or roughly characterized by an equivalent angular half width that may be called opening—or acceptance, aperture, viewing—half-angles.
The value of the penumbra function P(ξ,φ) is defined by the fraction of parallel light rays incident on the aperture from the angles (ξ,φ) that reach the pyrheliometer’s sensor element.
The penumbra function can be calculated from the pyrheliometer’s geometric specifications.
For angles ξl less than 5°, the deviation from 1 of the cos(ξ) term can be considered negligible, so that Eq.
(1) may be simplified into:(2)Bn=∫02π∫0αlP(ξ,φ)L(ξ,φ)sin(ξ)dξdφ.
Under the assumption of radial symmetry of the sky radiance in the vicinity of the sun position for clear skies (Gueymard, 1995, 2001; Buie and Monger, 2004), which can be considered reasonable when the sun is not too low over the horizon, Eq.
(2) simplifies into:(3)Bn=2π∫0αlP(ξ)L(ξ)sin(ξ)dξwhere P(ξ) and L(ξ) are the azimuthal averages of P(ξ,φ) and L(ξ,φ), such that:(4)P(ξ)=12π∫02πP(ξ,φ)dφandL(ξ)=12π∫02πL(ξ,φ)dφ.
Gueymard (2001) provides an expression for L(ξ) as a function of the aerosol optical mass, spectral Rayleigh and aerosol optical depths, aerosol phase function, and other variables.
Eq.
(3) outlines the way the DNI can be calculated from the azimuthal averages of the sky radiance and the penumbra function, in the vicinity of the sun, usually referred to as “circumsolar region” or “aureole”.
The mathematical formulation of the definition for DNI, per Eq.
(1), is ambiguous because neither a limit angle ξl nor a penumbra function is specified.
This ambiguity is the main source of the multiple definitions of DNI found in the literature, since each of them explicitly or implicitly refers to different limit angles and penumbra functions, which inherently leads to varying amounts of integrated radiance in the vicinity of the sun.
When seen from outside the atmosphere, the sun appears basically as a disk whose angular radius can be quantified by the angular distance δs between the visible edge of the disk and its center.
Considering the visible diameter of the sun (≈1.392×106km) and the varying sun-Earth distance during a year (≈1.496×108km ±1.7%), δs is equal to 0.2666° ±1.7%, using the set of constants from Liou (2002).
In other words, at the top of atmosphere, the angular extent of the sun to be considered is defined by a limit angle equal to δs.
At the ground level, due to scattering effects occurring within the atmosphere, the circumsolar region for angles greater than δs should be considered since its radiance is added to the radiance from the solar disk, typically up to 5° or more, depending on the application.
The direct radiance can be described as the radiance emanating from the circumsolar region and the sun.
It is expressed as a function of the angular position relative to the center of the sun.
The term sunshape, or Ss(ξ), refers to the broadband azimuthal average radiance profile, normalized with respect to the radiance at the center of the sun, i.e.
:(5)Ss(ξ)=K∫02πL(ξ,φ)dφwhere the normalization constant K is determined so that Ss(0)=1 (Biggs and Vittitoe, 1977).
As an example, Fig.
2 shows several sunshapes derived from the Lawrence Berkeley National Laboratory (LBNL) circumsolar telescope (Grether et al., 1975; Noring et al., 1991).
Even without any influence of the terrestrial atmosphere, the solar disk radiance decreases with increasing angular distance from the center of the sun.
This effect is referred to as limb darkening and varies with wavelength.
While the radiance at the edge of the solar disk is approximately 55% of the radiance in the center of the sun at 1000nm, the radiance decreases to approximately 20% at 370nm.
From the various limb-darkening models proposed in the literature, the radiance profiles presented by Pierce and Slaughter (1977) are recommended options to describe the region between 300nm and 2400nm.
The objective of this section is to review the multiple definitions and common acceptances related to DNI in different scientific fields, including radiative transfer in the atmosphere, radiometry, and solar energy conversion.
The strict definition of the DNI refers to photons that did not interact with the atmosphere on their way to the observer.
The mathematical formulation of this fundamental definition of DNI, noted as Bnstrict, makes use of the broadband transmittance of the atmosphere T and the top-of-atmosphere—or extraterrestrial—normal irradiance Entoa for the actual sun-Earth distance (WMO, 2010):(6)Bnstrict=EntoaT The broadband transmittance T depends on the altitude, solar zenith angle and parameters describing the optical state of the atmosphere related to aerosols, water vapor and other gases.
The definition of DNI described above is conceptually useful for atmospheric physics and radiative transfer models, but brings along a complication for ground observations or even for concentrating solar systems.
It is not possible to identify whether or not a photon was scattered before it ultimately reaches an observing instrument.
For the same reason, this strict definition also does not fit the ISO definition of DNI, since the ISO definition does not distinguish between scattered and non-scattered radiation.
Besides this fundamental problem, there are practical implications to consider too.
Pyrheliometers have to track the apparent sun position along its path through the sky.
As this cannot be done with perfect accuracy, pyrheliometers are designed such that they receive light from a greater angular aperture than the solar disk.
In the domain of numerical modeling of radiative transfer in the atmosphere with codes such as MODTRAN (Berk et al., 1998), SMARTS (Gueymard, 2001, 2005), or the publicly available solvers included in libRadtran (Mayer and Kylling, 2005), the direct normal irradiance is generally considered as a Dirac or delta function with no angular extent.
DNI at the surface is modeled as the attenuation of the extraterrestrial radiation originating from the strict direction of the sun considered as a point source, without taking into account the scattered photons that may re-enter the beam or the angular extent of the solar disk.
The very concept of numerical radiative transfer modeling that distinguishes non-scattered beam from scattered photons is currently evolving.
For instance, within the EU-funded SFERA project, described by Reinhardt (2013) and Reinhardt et al.
(2014), the authors have devised a special version of libRadtran, and modified the Monte-Carlo based radiative transfer equation solver named MYSTIC (Mayer, 2009) to account, inter alia, for a more realistic angular variation of the normalized extraterrestrial radiance over the solar disk.
This solar radiance, hereafter referred to as “extraterrestrial sunshape”, decreases from the center of the solar disk towards its edges, due to the phenomenon called “limb darkening” in solar physics, as mentioned above in Section 2.
The extraterrestrial sunshape can also be modeled by other means to evaluate errors in spectral direct irradiance measurements obtained with sun photometers (Kocifaj and Gueymard, 2011).
Fig.
3 exhibits examples of simulated radial radiance profiles using either the extraterrestrial radiance modeled as a delta function (blue solid line) or with a realistic extraterrestrial sunshape (red dotted–dash line).
In contrast, the green (dashed) curve represents the diffuse part of the radiance profile with a realistic extraterrestrial sunshape.
Because of constraints related to tracking accuracy and design-based limits on concentration factor, solar concentrating conversion systems also have different angular apertures, generally smaller than for pyrheliometers.
Therefore, DNI is interpreted differently in the realm of solar energy and irradiance measurements than in the realm of radiative transfer modeling.
Both interpretations of DNI are discussed in more detail in Sections 3.2 and 3.3.
The measurement of DNI is defined in the WMO CIMO Guide (WMO, 2010): “Direct solar radiation is measured by means of pyrheliometers, the receiving surfaces of which are arranged to be normal to the solar direction.
By means of apertures, only the radiation from the sun and a narrow annulus of sky is measured, the latter radiation component is sometimes referred to as circumsolar radiation or aureole radiation”.
This experimental definition is in a perfect agreement with the ISO definition of Section 3.1, and considers that DNI is logically related to the specific measurement device being used.
The amount of measured circumsolar scattered irradiance depends on the state of the atmosphere and on the specific penumbra function of the instrument (Pastiels, 1959).
In most cases, this penumbra function can be approximated in a geometrical way by means of three angles: the opening half-angle α, the slope angle αs, and the limit angle αl (Fig.
4), defined as(7)α=atanRLαs=atanR-rLαl=atanR+rLwhere R, r and L are characteristic dimensions of the instrument.
Since these angles are usually small, their definition implies that:(8)α≈(αl+αs)2.
Such a straightforward definition results in what is sometimes called the geometric penumbra function (Major, 1994).
In practice, the numerical value of the penumbra function is given by the fraction of collected radiant flux by an optical aperture depending on the off-axis angles.
The effective penumbra function is somewhat different from the geometric penumbra function, and obtained by taking into account effects such as the spatial inhomogeneity of the sensor in addition to its geometry (Major, 1994).
Penumbra functions for diffusometers—instruments consisting of a pyranometer with a shading disc or ball meant to shade the solar disc, so as to measure diffuse irradiance—are defined correspondingly, but refer to the fraction of rays that is blocked by a shading structure such as a tracking shade or a shadow ring, and thus does not reach the sensor (Major, 1992).
For off-axis angles between a minimum value of 0° and a maximum value equal to the slope angle αs, the penumbra function is equal to 1.
For angles greater than the limit angle αl, the penumbra function is equal to 0.
Finally, a continuously decreasing penumbra from 1 to 0 characterizes the range [αs,αl].
The opening half-angle α is the main characteristic for the description of the field of view of a pyrheliometer (Gueymard, 1998), and it corresponds approximately to the center of the transition range [αs,αl].
The viewing angle Ω is defined as the solid angle of a cone with apex angle 2α:(9)Ω=2π(1-cosα) Of course, the opening half-angle—or the viewing angle—alone does not fully describe the penumbra function.
Table 1, based on Gueymard (1998), gives the value of such angles for common pyrheliometers.
The WMO CIMO guide (WMO, 2010) recommends an opening half-angle of 2.5° and a slope angle of1°.
Nevertheless, there is a large variety of pyrheliometers in current use (Rüedi, 2000; Gnos, 2010), which might have other geometries.
Ångström and Rohde (1966) and Ångström (1961) studied a set of pyrheliometers with opening half-angles up to 10° and evaluated their typical circumsolar enhancement effects.
Of course, different opening half-angles result in differing measured values.
The instrument-to-instrument differences that the circumsolar effect generates are far from being negligible, especially in the presence of cirrus clouds.
These differences must be seriously considered if the desired relative accuracy is better than 1.5%.
In addition, several instruments have a circular aperture whereas others—however quite rare and old—have a rectangular aperture.
The quasi-equivalence between the rectangular and circular field of view is valid ideally only for clear skies with low aerosol content (Willson, 1969).
The authors listed in the previous paragraph have underlined the need for further standardizing the acceptance conditions of pyrheliometers within meteorological networks.
As a consequence, in 1978, the World Meteorological Organization (WMO) made the important decision that the radiometric definition of the Watt would have to be directly related to the electric scale through the World Radiometric Reference (WRR).
The WRR is maintained experimentally by a group of stable instruments called active cavity radiometers (ACRs) located at the World Radiometric Center in Davos, Switzerland (WMO, 2010).
This decision became effective on January 1, 1981 (Rüedi and Finsterle, 2005).
The practical usage of WRR is described elsewhere (Gueymard and Myers, 2008).
Every five years, an International Pyrheliometer Comparison (IPC) is conducted in Davos where the WRR reference instruments are used to transfer their calibration to other primary standards belonging to diverse countries.
This process eventually trickles down and propagates in a way so that all calibrated field instruments in the world are ultimately traceable to the WRR.
During the 11th IPC held in Davos in September–October 2010, an episode of Saharan dust occurred, which produced periods of slightly elevated dust aerosols in the air, in contrast with the normally very pure atmosphere at this elevation (1596m).
This circumstance provided an opportunity to study the effects of high-altitude aerosols on the transfer of calibration between instruments of different viewing geometries (Finsterle et al., 2012).
Even though the necessary corrections were small (<0.1%), they were of similar magnitude as the stated WRR precision (≈0.1%) and uncertainty levels (≈0.3%).
More details about the effect of circumsolar irradiance are provided in Section 5.
In the last few decades, other types of radiometer have started to be used for the purpose of measuring DNI as an alternative to conventional thermopile pyrheliometers.
At the cost of an expected moderate loss of precision, these instruments can have some advantages, such as (i) lower first costs or investments; (ii) lower operation and maintenance costs; and (iii) decreased risks of misalignment, soiling or perturbation due to meteorological events, thus also decreasing down periods and maintenance costs.
Examples of these alternative systems are:•a pair of thermopile pyranometers, one unshaded to measure global irradiance and the other one equipped with a tracking shade (or, less desirably, a shadow ring), to measure diffuse irradiance; a thermopile radiometer resembling a pyranometer, but equipped with a system of shades inside the glass dome to separate the global and diffuse components (Wood, 1999); a rotating shadowband irradiometer (RSI) that alternatively senses the global and diffuse components at rapid intervals (Michalsky et al., 1986; Geuder et al., 2003, 2008, 2011; Vignola et al., 2012).
The determination of the DNI with these systems results from specific processing consisting of intermediate steps, such as:•computation of the direct horizontal irradiance from the difference between the measured global and diffuse horizontal irradiances; computation of DNI from direct horizontal irradiance by dividing it by the cosine of the solar zenith angle; corrections for RSI systems to compensate for systematic errors (e.g.
King et al., 1998; Geuder et al., 2003, 2008, 2011; Vignola, 2006); corrections needed to compensate for the shaded part of the sky when using a shadow ring or other system of shades (e.g.
WMO, 2010; López et al., 2004; Batlles et al., 1996).
The opening half-angles that would characterize the measured DNI when using these alternative pyranometric systems are not always easy to define.
Indeed, they depend on the sensor-sun geometry and on the specific procedure used to calibrate the instrument against a reference pyrheliometer or ACR.
Additionally, the variability of the irradiance measured during the rotation of the shadowband of a RSI has to be considered (Wilbert et al., 2012; Wilbert et al., 2013c).
The accuracy of some of these instruments or methods is limited and may not even be adequate to successfully distinguish the effects and differences stated above.
An inter-comparison of such alternative sensors from different manufacturers has been made in Payerne (Switzerland), in the frameworks of the European COST program (cooperation in science and technology) ES1002 WIRE (weather intelligence for renewable energy) and IEA SHC Task 46.
The results of this DNI inter-comparison will be published soon.
Sensor soiling has an additional impact on the experimental determination of DNI.
It should be avoided as thoroughly as possible during measurement campaigns since its impact may easily outweigh the effects of circumsolar radiation or of differing instrument geometries.
Since soiling cannot always be avoided, its effect on the uncertainty of DNI measurements should be quantified along with the measurements.
The effects of instrument soiling are strongly dependent on site, instrument type, season and corresponding weather and environmental conditions (Geuder and Quaschning, 2006).
Mitigation measures require a meticulous record of the sensor cleanings with their exact times and the potential increase in the corresponding sensor signal along with signal coincidence in the case of redundant measurements.
Such methods are described in (Geuder and Quaschning, 2006; Pape et al., 2009; Wolfertstetter et al., 2012; Wolfertstetter et al., 2013).
Circumsolar radiation and the corresponding sunshape play a role in determining the efficiency of concentrating solar systems, and are always somehow—and sometimes implicitly—included in common solar performance models.
Such models include ray tracing tools, analytical optical performance models, and models that determine the optical performance with look-up tables or parameterizations of the solar position relative to the collector.
To better understand the use of the term DNI in power plant models we briefly introduce different types of optical performance models and tools.•Ray tracing models The available solar radiation can be described as a multitude of solar rays transmitted from the sun to the concentrators and finally to the receiver.
Ray tracing tools such as STRAL (Belhomme et al., 2009), SolTRACE (Wendelin, 2003), MIRVAL (Leary and Hankins, 1979), or SPRAY (Buck, 2010) calculate the path of the sun’s rays from the sunshape to the receiver by application of physical laws.
Monte Carlo techniques are often implemented to allow for tractable calculation times.
For the sake of illustration, one method for the description of the sunshape that is available in SPRAY is explained in the following.
The method selects one concentrator element after another and traces a given number of rays from the current element.
After the calculation of the vector to the center of the sun, the appropriate sunshape is included.
This is done by calculating an angular deviation of the ray vector from the center of the sun based on the probability density function corresponding to the sunshape as defined in Section 2.
To do so, a user-defined sunshape has to be provided as an input to SPRAY.
The radiance is determined both by the specified DNI and the user-defined sunshape.
The specific ray under scrutiny is then related to a power calculated as the product of the incident DNI and the projected area of the current concentrator element divided by the number of rays per element.
Then the path of the ray is followed until it reaches the receiver.
This ray tracing method can be based on actual measurements of the plant geometry.•Analytical optical performance models The Bendt–Rabl model (Bendt et al., 1979; Bendt and Rabl, 1981) is another type of calculation method that uses an analytical approach.
To accelerate calculations, analytical equations are derived and solved to describe the ray’s path through the optical system.
For instance, the model suggested by Bendt and Rabl can be used for parabolic troughs and solar dishes.
In a first step, an angular acceptance function is determined from the design geometry.
The angular acceptance function Pacc(α) is defined by the fraction of rays incident on the aperture at an angle α that reaches the receiver.
This is equivalent to the definition of the penumbra function given in Section 2.
The second step of the Bendt–Rabl method is to determine an effective source that includes both the user-defined sunshape and the deviations from the design geometry.
The optical errors of a CST collector are described as Gaussian-distributed independent uncertainties.
Their combination is also a Gaussian distribution with standard deviation σopt, which is often called optical error.
The function that describes the optical errors is then combined with the sunshape using convolution.
For line-focusing systems, such as parabolic troughs, a further integration step is required because the effect of circumsolar radiation on the incident irradiance depends strongly on angle φ (Eq.
(1)).
Finally, the intercepted radiation can be determined by integrating the product of the effective source and the acceptance function, similar to Eq.
(3).
Bendt and Rabl (1981) also describe an alternative order of the calculation steps that combines the angular acceptance function and the optical errors to the so-called “smeared acceptance function”, which is then combined with the sunshape.
Similar analytical methods are used in HELIOS (Vittitoe and Biggs, 1981), DELSOL (Kistler, 1986) and HFLCAL (Schwarzbözl, 2009).•Look-up tables-based optical performance models The fastest way to determine the optical performance of a CST collector uses only parameterizations or look-up tables that describe the change of the optical performance with solar position.
The necessary parameters can be derived from experimental data, or the aforementioned analytical performance models or raytracing tools.
Experimental measurements are obtained for a given time series of sunshapes.
In contrast, results from the aforementioned models always have to make assumptions concerning the sunshape.
Hence, even such simple performance models indirectly include an assumed sunshape.
Only one constant sunshape is typically described by these simple models, which may constitute a serious limitation.
As a rather extreme example that can occur when thin clouds mask the sun, Grether et al.
(1977) found 15% and 24% reduction of the intercept factor for two solar towers using a broad sunshape with a CSR of 0.4.Such look up tables or parameterizations are used in SAM (Gilman et al., 2008) and Greenius (Quaschning et al.
2001; Dersch et al., 2011).
The first two types of model mentioned above—ray tracing and analytical models—need the sunshape and DNI as input variables.
The third model type (look-up tables) only requires DNI as input, whereas assumptions on the sunshape are included as fixed settings in the model.
In CST applications, the term DNI is commonly interpreted as the experimental DNI that is measured with a pyrheliometer.
This does not always lead to the right interpretation of the plant performance analysis software, however.
For the ray tracing tool SPRAY, for instance, the input DNI value is related to rays that are distributed over the complete interval up to the angle ξl over which the sunshape is defined by the user.
The input DNI must be computed by the user from the chosen sunshape and experimental DNI, provided that the angle ξl is greater or equal to the limit angle of the pyrheliometer.
Users must pay attention to the adequacy between the limit angle of their user-defined sunshape and the limit and slope angles of the pyrheliometer considered as the source of the experimental DNI.
For instance, the experimental sunshapes presented by Neumann et al.
(2002) all have the same limit angle of 1.72°, which makes them incompatible with experimental DNI data obtained with common pyrheliometers having a 2.5° opening half-angle.
In contrast, the standard solar scan (Rabl and Bendt, 1982) is defined up to 3.2°.
Hence, the error due to the angular incompatibility is smaller in the later case, since only the angular interval from 3.2° to the limit angle is affected.
The same holds for the sunshapes proposed by Buie et al.
(2003b).
Rabl and Bendt (1982) discuss the interaction between the specified DNI and the outer limit angle (up to an angle of 3.2°) of the sunshape data used in their study.
They define the optical performance of a solar system for the specific penumbra function of the Eppley Normal Incidence Pyrheliometer (NIP), which has an opening half-angle of 2.9° (Table 1).
The optical performance that is obtained after the above-explained convolutions and spatial integration is hence only a preliminary result.
This preliminary result refers to an angle of 3.2° that coincides with the limit of the LBNL circumsolar radiance data and not to the penumbra function of the pyrheliometer.
Rabl and Bendt (1982) derived the experimental optical performance by multiplying this preliminary result with the ratio of DNI computed with the sunshape with a 3.2° opening half-angle and perfect penumbra function and the measured DNI.
For performance models that use look-up tables or experimental data, DNI has to be defined in correspondence with the source of the parameters or look-up table.
For such models, however, errors may occur if the sunshape deviates from the indirectly included sunshape of the parameters or look-up table.
Although two of the three aforementioned types of performance model allow calculations with arbitrary user-defined sunshapes, typically only constant standard sunshapes are used.
Nevertheless, Wilbert (2014) has processed time series of sunshapes using a software add-on for SPRAY.
For the processing of DNI time series from typical meteorological years, however, no approach for the corresponding sunshape data is published so far.
Further research is thus required to alleviate the lack of site-specific sunshape data for such applications.
The availability of site-specific time series of both the experimental DNI and corresponding sunshape is the ideal case in CST modeling.
Approximate descriptions of the collector performance including the variation of circumsolar radiation might be achieved by specifying a collector-specific DNI (Lemperle, 1982) that only includes radiation up to a collector-specific outer boundary angle, which is called acceptance angle.
Such approximations and their shortcomings are not the topic of this paper.
What is described here is rather an overview of definitions and values of acceptance angles that might be adequate for such approximations, as an illustration of the contribution of circumsolar radiation to collector performance.
Several concentrating solar systems exist with different types of collectors having diverse apertures corresponding to different limit angles ξl and penumbra functions.
These are generally not the same as the apertures characterized by pyrheliometers or other DNI measurement devices.
In the domain of solar energy conversion, the term “acceptance half-angle” is generally used instead of “opening half-angle”.
The nominal acceptance angle is defined in (Rabl, 1985) as “the largest incidence angle for which all or almost all rays on the aperture reach the receiver” and can be roughly considered as analog to the slope angle for pyrheliometers.
Other approaches can be cited to define an equivalent acceptance angle, such as the one also proposed by Rabl (1985), which is obtained as twice the standard deviation of the radiation angular distribution incident on the absorber.
The acceptance half-angle can be also defined as the off-axis angle at which the sensitivity of the instrument decreases below a given fraction of the incident irradiance, e.g.
50% or 90%.
The nominal acceptance half-angle of a concentrating collector and its concentration factor are closely related.
This relation depends on the quality of the non-imaging collecting optical system.
Calling upon the second law of thermodynamics, Rabl (1976) demonstrates the existence of an upper bound of the concentration factor C for a given acceptance half-angle θc.
For a refraction index of the solar concentrator equal to 1, one obtains:(10)C⩽1sin(θc)dwhere d equals 2 for point focusing collectors (e.g.
parabolic dish or solar tower) and 1 for line focusing collectors (e.g.
parabolic through or linear Fresnel).
As stated above, we do not discuss the application of the acceptance angles for CST modeling or the quality of such models, but only state the upper bounds for acceptance angles as an illustration.
To illustrate the diversity of the possible acceptance angles in CST systems, we have collected the concentrating factors and acceptance angles of several systems from the literature.
Table 2 shows typical concentration factors of various CSP technologies (IRENA, 2012) and the corresponding upper bound of acceptance half-angles from Rabl (1976).
These upper bounds are within the typical range 0.7–2.3°, and are typical of concentrating collectors such as parabolic through, solar tower or parabolic dish (Meyen and Lüpfert, 2009).
In parallel, Table 3 provides data on the concentration factors and their corresponding upper bounds of acceptance half-angles for several CPV systems, assuming a refraction index of the solar concentrator equal to 1.
The need for clear and specific definitions and terminology related to DNI was raised by MINES ParisTech (Blanc et al., 2013) in the name of the EU-funded MACC-II project, in the framework of two expert meetings and dedicated workshops under IEA SHC Task 46: “Solar Resource Assessment and Forecasting”.
Approximately 70 experts from 12 countries participate in Task 46.
As an outcome of discussions held at Task 46 Expert Meetings and workshops during 2012–2013, various definitions related to DNI were specified with their corresponding acronyms.
The following is a synthesis of these conclusions.
In the ideal case where the penumbra function is a perfect rectangular function with respect to the off-axis angle (i.e.
no transition range), an ideal DNI for the opening half-angle α, named Bnideal(α), is defined as follows:(11)Bnideal(α)≈2π∫0αL(ξ)sin(ξ)dξ.
The direct irradiance from the sun, noted Bnsun, is defined as the solar radiant flux collected by a surface normal to the direction of the sun, within the current extent of the solar disk only (half-angle δs) with a perfectly rectangular penumbra function.
This constitutes a special case of Eq.
(11), with α=αs=αl=δs and:(12)Bnsun=Bnideal(α=δs)≈2π∫0δsL(ξ)sin(ξ)dξ.
With this definition, used by Buie et al.
(2003a) for instance, the small fraction of diffuse radiance within the solar disk is included.
Theoretically, Bnsun is different from Bnstrict because the latter consists only of non-scattered radiant flux.
In practice, however, the difference between the two quantities corresponds to the spatial integration of diffuse radiance within the solar radius angle, which can be considered negligible.
In the more general—and concrete—context of DNI measurements or solar conversion systems, the penumbra function P is generally not a perfect rectangular function.
The limit angle αl is defined as the angle above which the acceptance function is null or considered as negligible:(13)∀ξ⩾αlP(ξ)≈0.This limit angle αl is the greatest angular distance from the center of the sun that is considered to be part of the circumsolar region, for a given acceptance function P. The angular extent of this circumsolar region cannot be defined in a universally valid way.
This is due to the fact that different pyrheliometers and different concentrating collectors are sensitive to radiance up to specific angular distances from the center of the sun.
For example, the limit angle αl is 3.2° for the measured circumsolar ratio (CSR) in the LBNL data base (Noring et al., 1991).
For pyrheliometer measurements following the WMO recommendations (WMO, 2010) a limit angle greater than 4° would be necessary.
The slope angle αs is defined as the angle below which the penumbra function equals 1, or its deviation from 1 can be considered negligible:(14)∀ξ⩽αsP(ξ)≈1.
The experimental definition of DNI for an acceptance function P and its limit angle αl defined by Eq.
(3) can then be related to the ideal DNI for the opening half-angle αs:(15)Bn≈Bnideal(αs)+2π∫αsαlP(ξ)L(ξ)sin(ξ)dξ.
Under the (reasonable) assumption that the acceptance function P is equal to 1 for off-axis angles less than the solar disk half-angle δs, it is clear that δs⩽αs, hence the following relationship is obtained:(16)Bn≈Bnsun+2π∫δsαlP(ξ)L(ξ)sin(ξ)dξ.
The experimental circumsolar normal irradiance, CSNI, noted CSn, is related to the acceptance function P, and is defined as the part of the corresponding DNI that is incident from the annular angular region defined by the two half-angles α0 and α1 verifying the constraint δs⩽α0⩽α1⩽αl:(17)CSn(α0,α1)≈2π∫α0α1P(ξ)L(ξ)sin(ξ)dξ.
Following Eq.
(16), a fundamental closure relationship is obtained:(18)Bn=CSn(δs,α1)+Bnsun.
Similarly to the ideal DNI, the ideal CSNI, noted CSnideal, is defined as the part of ideal DNI coming from the annular angular region defined by the two half-angles α0 and α1 verifying the order constraint α0⩽α1:(19)CSnideal(α0,α1)≈2π∫α0α1L(ξ)sin(ξ)dξ.
Similarly to Eq.
(18), the following relationship is then obtained:(20)Bnideal(α)=CSnideal(δs,α)+Bnsun.
The circumsolar ratio (CSR) for the opening half-angle α, noted CSR(α), is then defined as the ratio between the ideal CSNI for α0=δs and α1=α and the ideal DNI Bnideal(α):(21)CSR(α)=CSnideal(δs,α)Bnideal(α).Eq.
(21) is used in the literature to describe the circumsolar ratio (Grether et al., 1977; Schubnell, 1992; Neumann and Witzke, 1999).
Depending on the authors and the measurement systems, other angles are used for δs.
For instance, Neumann and Von Der Au (1997) use the current solar disk angle plus 0.0206° (0.36mrad) for sunshape measurements, in order to avoid the effect of imperfect image sharpness of the experimental images.
Alternatively, in Neumann et al.
(2002), the average solar disk angle 0.2664° (4.65mrad) is used for average sunshapes, thus introducing a small error—less than 1.7%—caused by neglecting the annual variation of the solar disk angle due to the elliptic path of the Earth around the sun.
LBNL used the current solar disk angle increased by 0.013° as the inner limit angle to avoid instrumental errors that could cause an overestimation of the radiance close to the solar disk edges (Grether et al., 1975).
Buie et al.
(2003b) showed that CSR characterizes the sunshape to some extent.
However, there is no bijective relation between the sunshape and CSR.
Indeed, a specific value of CSR can be obtained for different sunshapes, as discussed by e.g.
Wilbert et al.
(2013b).
Finally, and in a similar way, the circumsolar contribution (CSC) from a given annular region can be defined as(22)CSC(α0,α1)=CSnideal(α0,α1)Bnideal(α1).
The circumsolar irradiance and its angular distribution are dependent on the physical characteristics of aerosols and thin clouds—most particularly optically thin cirrus clouds—in the atmosphere.
Whenever thin clouds obscure the sun, important modifications to the underlying clear-sky circumsolar irradiance will occur, because clouds have different optical characteristics than aerosols.
This section uses both real measurement-based and simulated examples to demonstrate that the amount of circumsolar radiation and its relative contribution to the measured or simulated DNI varies strongly with sky condition.
SMARTS simulations for clear-sky conditions with various loads of rural aerosols, composed of small aerosol particles, have shown that, for relatively low θs, the relative proportion of the circumsolar irradiance in the measured DNI is less than 1% for opening half-angles ranging from 2.5° to 2.9° (Gueymard, 2010a,b).
Increasing the air mass from 1 (θs=0°) to 3 (θs=70.7°) roughly results in the doubling of the circumsolar contribution.
For the specific conditions of the ASTM G173 reference spectral standard, which is used by the PV and CPV communities for rating purposes, the combination of an air mass of 1.5 (θs=48.24°), a rural aerosol with an aerosol optical depth (AOD) of 0.084 at 500nm, and other atmospheric conditions defined in the standard, similar SMARTS calculations indicate a circumsolar contribution of only 0.25% for a 2.9° aperture half-angle.
For many aerosol types composed of small particles and/or not too large AOD, the circumsolar contribution is essentially proportional to the slant optical depth (SOD) defined as the product of AOD and air mass, so that an AOD of 0.084 at air mass 15, or an AOD of 0.84 at air mass 1.5, would induce a circumsolar contribution 10 times greater than before, or approximately2.5%.
In the specific case of small aerosol particles and opening half-angles α less than 10°, the circumsolar contribution to DNI is found to vary almost linearly with α (Gueymard, 2010a,b).
These SMARTS results have been validated against more rigorous calculations of spectral circumsolar irradiance (Gueymard, 2001; Kocifaj and Gueymard, 2011).
In addition to the SOD, the scattering phase function is the other dominating factor that governs the magnitude of the circumsolar contribution.
The scattering phase function can be defined as the intensity of electromagnetic radiation at a given wavelength that is scattered for a given angle from the original direction of the incident beam (Zdunkowski et al., 2007).
The scattering phase function depends on the type of aerosol or cloud particles (material, particle size and shape).
Large particles, such as desert dust particles or ice crystals, tend to scatter more strongly in the close vicinity of the forward direction than smaller particles such as rural aerosol particles, thus yielding comparatively larger circumsolar contributions.
Under cloudless skies, and for a given value of the SOD, this means that the circumsolar contribution would be normally larger over arid/desert areas than over rural areas.
The principle just stated that “larger particles induce greater circumsolar radiation” only holds for aerosols.
In the case of cirrus clouds, ice particles may be so large that the forward scattering peak becomes extreme and the scattering angles for most photons can be smaller than the angular extent of the sun disk itself.
This increases the diffuse radiation coming from the part of the sky occupied by the sun disc, but may lead to smaller circumsolar radiance values than in the case of smaller particles.
In such extreme cases, a brighter sun disc but dimmer circumsolar region leads to smaller CSR values.
For the sake of illustration, simulations of the circumsolar radiation for an opening half-angle of 2.5° have been performed with a specifically modified version of the MYSTIC Monte Carlo radiative transfer model that allows, notably, a precise description of the extraterrestrial sunshape to be considered (Reinhardt et al., 2014; Mayer, 2009).
To cover the full range of expected circumsolar radiation values while maintaining conciseness and legibility, only results for sky conditions resulting in specifically high or low circumsolar radiation values are shown in the following.
Cirrus clouds are represented using the optical properties of Hong-Emde-Yang (Reinhardt et al., 2014) for rosettes and solid columns with effective radius of respectively 90μm and 15μm.
The former size yields low circumsolar radiation values in the considered 2.5° field-of-view, whereas the latter yields high circumsolar radiation values.
As far as aerosols are concerned, three aerosol types have been chosen from the optical properties of aerosols and clouds database (OPAC) proposed by Hess et al.
(1998):•The continental polluted aerosol type is composed of mostly small particles and causes low circumsolar radiation values even for high AOD values.
This aerosol type is typical over areas highly polluted by anthropogenic activities; The desert aerosol type contains a significant amount of large aerosol particles and therefore induces considerably more circumsolar radiation; A coarse aerosol type, obtained as the pure coarse mineral dust component from OPAC minus the smaller particles.
This third type has been considered because it may induce circumsolar radiation with the same magnitude as cirrus clouds, even though situations where only large particles exist are highly unlikely.
This somewhat unrealistic type of aerosol is introduced here to demonstrate that, over desert regions, it is not easy to define an upper limit for the circumsolar radiation caused by aerosols since local uptake may cause a particle size distribution that contains considerably more large particles than in the average OPAC desert dust mixture.
The different simulations were performed for five different values (0.1, 0.2, 0.3, 0.4 and 0.6) of the vertical optical depth at 550nm τ550, and for different sun zenith angles from 0° to 80° in steps of 10°.
These simulations are derived with the same methodology as that presented by Reinhardt et al.
(2014).
The SOD at 550 nm, noted d550, is defined by:(23)d550=τ550cosθs.
Fig.
5(a) and (b) respectively show the ideal CSNI and the corresponding ideal CSR for an opening half-angle of 2.5°, as a function of d550.
It is found that CSR is a strong function of d550.
However, Fig.
5(a) shows that CSNI also depends on the sun zenith angle, since there are several combinations of vertical optical depth and sun zenith angle that yield similar values of d550.
Using the same MYSTIC simulations, Fig.
6 shows the correspondence between CSRideal(2.5°) and Bnideal(2.5°).
For the same magnitude of simulated DNI, different circumsolar ratios can be observed with respect to the aerosol or ice-cloud types, which means that the circumsolar contribution to DNI is not just a function of turbidity.
Small circumsolar ratios correspond to aerosols under clear-sky conditions, whereas large ratios correspond to cloudy-sky conditions with ice clouds.
Fig.
7 presents a combined two-dimensional (2D) histogram of measurements of the circumsolar ratio and DNI performed at the Plataforma Solar de Almeria (Spain) using the so-called SFERA sunshape measurement system.
This SFERA system consists of Visidyne Sun and Aureole Measurement System (SAM), a Cimel sun photometer, a CHP1 pyrheliometer, and appropriate post-processing software (Wilbert et al., 2013a).
The 2D histogram in Fig.
7 has been created from 337,701 measurements taken between April 1st 2011 and December 20th 2012.
These measurements corroborate the MYSTIC simulations presented in Fig.
6.
The histogram contains a “bend”, represented by a black-dashed line in Fig.
7 for lower DNI values, which is interpreted as a signature of the transition between cloudless and cloudy conditions.
Measurements below this ‘bend” for lower CSR values correspond to clear-sky measurements with typically small aerosol particles.
A very practical analysis of these simulation- or measurement-based figures shows situations or conditions with high experimental DNI together with high circumsolar ratios.
For example, for an experimental DNI around 600Wm−2, for approximately 7% of the situations, the circumsolar ratios reach 20–30%, due to the effect of thin clouds.
This implies that a solar energy conversion system with a narrow acceptance angle—just slightly greater than δs—would be able to convert only 70–80% of the experimental DNI under such circumstances.
Aerosols also cause an increase in the relative difference between Bnideal(α) and Bnsun, but, conversely to the effect of cirrus clouds, their effect may strongly decrease Bnideal(α).
This difference in how aerosols and thin clouds impact DNI is caused by the Ångström exponent η that characterizes the variation of spectral AOD with wavelength in Ångström’s law:(24)τ(λ)=τ(1μm)λ-ηwhere τ(λ) is the AOD at wavelength λ (in μm).
For instance, rural aerosols have an exponent that usually ranges between 1 and 1.5, which means that the extinction in the visible band—where the spectral irradiance is high—is much stronger than in the near infrared (where the spectral irradiance is much lower).
In comparison, the scattering effect of clouds is roughly wavelength independent (η≈0), and is therefore generally of lower magnitude in the ultraviolet and visible parts of the spectrum than that of aerosols for a similar optical depth.
However, this difference would become almost non-existent under sand storm conditions, because the overwhelming presence of large particles would make the aerosol Ångström exponent η reach values close to 0.
Depending on atmospheric conditions, a more or less significant fraction of solar radiation that is scattered by atmospheric constituents emanates from the circumsolar region.
Whereas a large part of the circumsolar radiation is measured by pyrheliometers, concentrating collectors can only use a part of it, depending on concentrator technology, among other things.
Therefore, this circumsolar effect has to be considered for yield assessment and performance evaluation of concentrating solar technologies.
Given these circumstances, circumsolar radiation measurements or estimates should be included in solar resource assessment, plant design, yield assessment, plant operation, or power plant performance tests for concentrating technologies.
Otherwise, an additional uncertainty is introduced.
In parallel, standard DNI measurements using procedures that follow the WMO-recommended geometry—in terms of slope and limit angles—always need to be carried out for solar resource assessment and performance monitoring.
An early study (Lemperle, 1982) suggested that a pyrheliometer with a modified opening angle similar to the CST system’s acceptance angle could be used as an alternative to sunshape measurements.
Although there could be a market for “special geometry” pyrheliometer, such an approach cannot be recommended for five reasons.
First, the acceptance angle is specific to each CST system technology.
Hence the DNI assessment should be system specific, too, which is difficult and costly to implement in practice.
Second, such an approach is only an approximation even for a single specific system.
Third, the sensitivity of a CST to circumsolar radiation varies with solar position for all types of systems except for parabolic dishes.
Fourth, any deviation from the WMO recommendation for the geometry of pyrheliometers brings along a complication when comparing data from these measurements to data obtained with conventional, WMO-compliant instruments.
Fifth, the calibration of pyrheliometers of unusual geometry becomes more uncertain since this calibration is obtained by comparing their reading to reference instruments (ACRs) that have a mandated 2.5° opening angle, with no adjustment possible.
The last two reasons, related to the geometry of radiometers, also apply to the case of modeled data, since radiation models are usually validated, and sometimes also partially calibrated, against standard ground-based measured DNI data.
The authors strongly recommend that standard DNI measurements following the WMO-recommended field of view be conducted at all radiometric stations.
Such measurements can be collected with pyrheliometers, of course, but also e.g.
with RSIs or dual pyranometers, since these are calibrated against common pyrheliometers.
If possible, circumsolar radiation measurements should also be carried out in addition to the common DNI observations.
In the best-case scenario, circumsolar radiation measurements can be reduced to sunshape functions.
However, the measurement of the circumsolar ratio or of the circumsolar contribution may be sufficient, depending on conditions and applications.
As a guideline for the proper usage of the term “DNI” we conclude with the following:•Different interpretations of the term DNI are required depending on the topic and scientific field.
A decision to establish a single interpretation of the term DNI is neither necessary nor possible.
To allow for the correct interpretation of published results, and in order to avoid introducing additional errors, it is necessary to explain clearly which definition of DNI is used.
In the case of experimental DNI data, this involves the specification or the characterization of the penumbra function.
This paper has been done in the framework of the IEA SHC Task 46 “Solar Resource Assessment and Forecasting”, in collaboration with both the IEA SolarPACES and IEA PVPS implementing agreements.
The research leading to these results has partly received funding from the European Union’s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement No.
283576 (MACC-II project, Monitoring Atmospheric Composition and Climate-Interim Implementation) and grant agreement no.
228296 (SFERA project, Solar Facilities for the European Research Area).
The Cimel sun photometer at DLR was calibrated at AERONET-EUROPE, supported by the FP7-funded ACTRIS (Aerosols, Clouds, and Trace gases Research InfraStructure Network) project.
The support provided by the AERONET, PHOTONS and RIMA staff with the sun photometer calibration and data evaluation is much appreciated.
Product analysis during the thermo-oxidation of amorphous deuterated hydrocarbon films with NO2 Graphical abstractImage, graphical abstract Thermal partial desorption and thermal oxidation have been routinely used in the industry and laboratories for purifying materials (like steel in metallurgy) and/or remove undesirable deposits (like petroleum coke in refineries).
In a nuclear fusion device both techniques are easy to apply as they do not require any special equipment [1–16].
Furthermore, opposed to other tritium removal techniques—as plasma, laser desorption and ablation, etc.—baking and thermo-oxidation can treat hidden parts, like sub-divertor areas, pumping ducts, castellation gaps, etc.
[1–3].
In the case of carbon co-deposits, those areas are precisely where fuel-rich and more chemically reactive films develop [17,18]: therefore making them easier but paramount to remove.
The main drawback of thermo-oxidation in most actual devices and ITER is its limitation to maintenance periods, when the vessel walls can be heated up around 300–400 °C by hot helium injection through the cooling system [19,20], and also because of the required reconditioning of the walls before plasma operation to remove the absorbed oxygen [10].
However, the temperature achieved is not homogeneous over the vessel, as it is limited to the distance to the cooling tubes, and thus to the device design.
The analysis of this study is a continuation of previous works done for the treatment of ITER carbon co-deposits [1–3], so the temperatures studied are in the range of 350 °C for divertor and 200–275 °C for main wall and remote parts.
At present, due to budget restrains as well as due to tritium trapped in co-deposited carbon layers, ITER will not use carbon materials at the divertor strike points in spite of their excellent resilience against large heat loads.
Nevertheless, many present experimental nuclear fusion devices (DIII-D, TCV, etc.)
and new ones (JT-60SA, KSTAR, Wenderstein-7X) use carbon elements, so the removal of carbon co-deposits is still necessary for a better device operation—plasma density control, dust events, etc.
The temperatures used in this work are not very different from the ones achievable in present devices, such that the results can be extrapolated to them.
Moreover, even for ITER this study could be useful if carbon materials have to be eventually installed in the case that operation with tungsten tiles at the strike points is precluded by unexpected reasons.
Compared to other carbon removal techniques, thermo-oxidation efficiency is not significantly affected when treating real tokamak co-deposits compared to laboratory ones, even when contaminated with oxygen getters as beryllium or boron [3,5–9,15], except at very large quantities of boron [8].
Even more, treatment of co-deposits in-situ seems to be more effective than ex-situ [10], probably caused by the deleterious effects of exposition to the atmosphere.
Being a volume reaction, thermo-oxidation is highly suited for the porous tokamak carbon co-deposits, as it acts on the whole co-deposit [4,7], i.e.
it is independent of co-deposit thickness [4].
Water has been found to be the main hydrogen product from thermo-oxidation in O2, usually 99% [11–14], and CO2 has been found as the main carbon product with ratios to CO from 10 to 2.5 [12–14].
However, the generation of products like tritiated water has been predicted but not confirmed in NO2 [1,2].
With respect to the reactive gas, previous studies have shown that oxygen, the most common oxidant, can be effective at the usual maximum allowed temperature of 350 °C given sufficient time [7], but it is not adequate at the lower temperatures prevailing in remote parts and main wall [1,7].
On the contrary, NO2 has shown a huge removal rate of tokamak [1] and laboratory films [2,4] at 275–350 °C, and it is efficient even at 200 °C [4].
In any case the investigation of the possible drawbacks of thermo-oxidation with NO2 is necessary.
This work focuses on the detection of the hydrogen-containing gas products: molecular hydrogen, hydrocarbons and mainly water.
While molecular hydrogen and hydrocarbons are the best products in terms of hydrogen isotopes recovery and recycling, large quantities of tritiated water would be more expensive and more dangerous to treat.
Therefore, an identification and quantification of the thermo-oxidation products is paramount for the implementation of this technique in nuclear fusion devices.
In order to do this, a laboratory reactor has been fully baked to reduce the natural water and its walls have been coated with a thick a-C:D film to distinguish the produced deuterated water from natural one as explained in Section 2.
Afterwards, the reactor has been baked in a NO2 and O2 atmosphere.
The gas analysis by mass spectrometry is given in Section 3, and the quantification of the products, especially water, is discussed in Section 4.
Finally the conclusions are exposed in Section 5.
The setup of this work is focused on the detection of deuterated water from the thermo-oxidation reactions, and it is presented in Fig.
1.
The setup can be fully baked to reduce the residual water content, up to 160 °C, and the tube where plasma is generated can be heated up to 500 °C by means of a thermocoax for thermo-oxidation.
The setup is pumped down by means of separate turbomolecular pumps in series with roughing pumps, and it is baked during a few days to improve the residual vacuum and minimize the isotopic exchange between the produced deuterated water (and other possible products) and the natural, protonated water.
The pressure in plasma and diagnostics section is around 1-3 × 10–6 Pa, while the analysis chamber reaches 3–5 × 10–7 Pa.
Plasma section is routinely cleaned after each experiment by a He/20%O2 at 1 Pa Direct Current (DC) glow discharge (GD) plasma during 30 min to remove any residual carbon layer, followed by a D2 DC-GD plasma at 1 Pa during 1 h to remove the oxygen by-products and to promote isotopic exchange with residual H. These combined treatments are effective as no film has been detected in any vacuum part when the equipment was disassembled.
Then a He/20%CD4 DC-GD plasma at 1 Pa is produced to cover the inner walls of the plasma section (377 cm2) with a 300 nm a-C:D layer of hard type and D/C ratio of 0.4 analyzed by Nuclear Reaction Analysis (NRA).
Thickness, and thus deposition rate, was measured by profilometry from a masked silicon piece placed at the bottom of the plasma section during preparation experiments.
After that, the film is left to stabilize overnight, but only a few minutes should be necessary [21].
The following day, prior to each thermo-oxidation, the setup is baked at 150 °C for 3 h to reduce water sticking during the experiment.
Then the tube coated with the a-C:D film is slowly heated (10 °C/min) to the experiment temperature, and allowed to stabilize during 30 min (no deuterium release was detected).
Finally, the communication to the pumping system of the plasma (thermo-oxidation) chamber is closed by a gate valve, and pure (99.9999%) O2 or NO2 is introduced by means of leak valve 3a in Fig.
1 up to the desired pressure and left to react during ∼3 h. Ideally, it would be desirable to operate with a continuous injection of O2 or NO2 to pump out the products in order to be able to detect them in real time.
This operation mode was tested but, among other problems, the contribution of the products to the gas mixture was very low compared to the large excess of reactants, so their signals could not be separated from noise.
Both plasma and thermo-oxidation processes were monitored by differentially pumped mass spectrometry with a SRS Residual Gas Analyzer (RGA-100) in the analysis chamber.
As the pressure necessary for thermo-oxidation is very high (in the order of kPa) the pressure drop is controlled by means of an all-metal leak valve in series with a 1 mm diameter pinhole.
This allows the pressure drop between them to be varied by factors from 104 to 109 to find an optimum balance between signal-to-noise ratio for the reaction products and the desired low pumping speed.
High purity Ar was used to adjust the pressure of the analysis chamber prior to each experiment to a few mPa to enhance sensitivity of minority products.
The gaseous products from thermo-oxidation are carbon oxides, CO and CO2, but from hydrogen isotopes they could be many: molecular hydrogen, water and hydrocarbons.
In the case of thermo-oxidation with oxygen only water has been detected, with residual concentrations of other hydrogenated products [11–14].
The main strategies to distinguish hydrogenated molecules (mainly water) produced during the thermo-oxidation from natural, protonated H2O and H2 molecules have been the use of tritiated films [11], and deuterated films [13,14], with also 18O isotope [12].
In this work deuterated hydrocarbon films are used.
An example of the mass spectra measured during the thermo-oxidation in 20 kPa of NO2 is shown in Fig.
2.
As it can be observed, in spite of the long conditioning of the vacuum chamber there is an important quantity of background, protonated water (H2O), as can be deduced from the large m/q 18 peak and its cracking pattern, m/q 17, the 20% of it.
This water most probably was dragged out from the walls during reactive gas injection, as no background water is seen before, see Fig.
2.
Deuterated water is also detected: HDO m/q 19, and a lower quantity of D2O m/q 20 (around 4 times less), which indicates an important isotopic exchange between the generated D2O and the protonated background H2O molecules.
Water molecules easily adsorb on the stainless steel wall and a-C:D film, which facilitates the isotopic exchange as has also been described in other works [14].
This relatively large quantity of deuterated water is generated during the thermo-oxidation of the a-C:D film, as the comparison with the generated carbon products yield, CO and CO2 in the following point demonstrates.
The low RGA resolution at low m/q ratios, 1–6, makes impossible the measurement of the small quantities of HD and D2 (at m/q 3 and 4 respectively), which will be probably produced.
Nonetheless, the yield of HD and H2 have to be far smaller than that of HDO and D2O as their m/q 19 and 20 peaks intensities are larger than the background at m/q 3 and 4, so HD and D2 yield has to be similar or lower to the one found in O2 thermo-oxidation by other authors (∼1%) [11–14].
Other products like partially deuterated methane and ammonia have also a cracking pattern between m/q 12 and 20, in the range showed in the inset of Fig.
2.
However, the analysis in this range is complicated because of the overlapping of the cracking pattern of the multiple products and reactants: m/q 16 due to O22+ and O from O2 and NO2; m/q 14 due to N+ from NO2; and m/q 12 due to C+ from CO2 and CO. Other products like superior hydrocarbons, HCN, DCN and CNOHx are not considered, as no m/q 25, 26, 27 or 42 peaks from their cracking pattern are detected.
A cold finger with liquid nitrogen could not be used in these experiments to solve the mass spectra peak overlapping as in other previous works of our group [22,23].
In this experiment the reactant NO2 would condense very easily as its condensation temperature is close to that of water (by 10 K) hindering the products detection, specially water.
Nevertheless, other deuterated products different from water are very unlikely as in thermo-oxidation by oxygen almost only water was found [11–14], and NO2 is more oxidant, and thus, less prone to methane or ammonia production.
In Figs.
3–6 the evolution of the mass spectra with time for the NO2 thermo-oxidation at 200, 275 and 350 °C, and from O2 at 350 °C are presented.
For a better understanding, in Table 1 it is presented which m/q value is used to follow the evolution of the different molecules, considering the main peak of their cracking pattern.
In those figures point 1 is the reactive gas injection, and point 3 is the pump-out.
Point 2 indicates the moment of the modification of the flux into the analysis chamber by means of the needle valve between pumping-diagnostic and analysis sections (point 3b at Fig.
1), which causes a variation in the chamber pressure as can be seen in the abrupt step of the signals.
This modification is done to enhance sensitivity towards minority products.
The complete removal of the film should be detected as a maximum in CO signal, m/q 28, as C is usually removed after D [4–7].
However, no clear maximum can be easily appreciated in these figures because of the absence of a strong pump-out of gases to allow the detection of products, as explained previously in the experimental section.
Notwithstanding, the film removal is confirmed during the subsequent He/O2 plasma done.
After thermo-oxidation at 350 °C in NO2 and O2 the production of CO and CO2 in the plasma lasted for 1–2 min, and after 275 °C in NO2 about 4 min.
These residual films are probably on non-heated parts as the anode and metallic net (points 4 and 6 in Fig.
1 respectively).
After thermo-oxidation at 200 °C in NO2, CO and CO2 are produced during 7 min, about a third part of the time required for the full 300 nm film removal.
In NO2 thermo-oxidation, some O2 can be produced in the first moments from the decomposition of NO2 into NO and O2 at the hot (> 150 °C) stainless steel walls [24–26].
This is confirmed by the slight decrease along all NO2 experiments of m/q 46, related to NO2: approximately 13%, 6% and 4% for 350, 275 and 200 °C respectively; and the parallel slight increase of m/q 30, related to NO, about 9%, 10% and 13% for 350, 275 and 200 °C respectively.
Both trends are specially strong at first moments.
This initial fast decomposition suggests that the reaction slows down when the stainless steel surface is fully oxidized.
Furthermore, in O2 and NO2 experiments at 350 °C and NO2 at 275 °C (Figs.
6, 3, 4 respectively) the m/q 32 signal (O2) decreases steadily, 12%, 40%, 20% respectively, and it is linked to a fast increase in m/q 44, 650%, 100%, 90% respectively, indicating that part of the O2 is consumed in a post-oxidation of CO into CO2.
Oppositely, at 200 °C (Fig.
5) the increase of m/q 44 (CO2) is much slower, ∼30%, but instead m/q 32 increases along all the experiment, ∼25%, suggesting that at 200 °C the post-oxidation of CO is just done by NO and/or NO2, as the O2 generated from the decomposition of NO2 is not consumed.
Consequently, an initially fast thermal decomposition of NO and NO2 into O2, and the post-oxidation of CO into CO2 seems to take place at all temperatures studied here: 200–350 °C.
But a fast post-oxidation of CO by O2 takes place only at temperatures greater or equal to 275 °C, especially evident in O2 thermo-oxidation at 350 °C.
In a vacuum system, water production is very difficult to quantify due to its easy sticking to the walls and its slow pumping.
Furthermore, the large isotopic exchange found in this work with background water toward HDO would make any quantification very uncertain.
Therefore, in this work deuterated water measurement in the RGA has been compared to the yield of other products.
The general thermo-oxidation reactions for both O2 and NO2 are:
NO2+C:D(film)→NO+CO2+CO+D2O(unbalanced)O2+C:D(film)→CO2+CO+D2O(unbalanced) CO, CO2 and D2O have been considered as the only products from thermo-oxidation of the a-C:D film in accordance with previous experiments in O2 [11–14].
Consequently, the total production of CO and CO2 (carbon products) can be related to HDO and D2O (deuterium products) yield and then compared with the D/C ratio of the a-C:D film.
The relative quantification of the products is not straightforward when operating on stationary mode as this work.
The reaction products should slowly accumulate in the vacuum chamber as they are produced, so quantifying the mass spectra once the film has been completely eroded, ideally, should be enough.
However, the slow pump out of gas to the analysis chamber to measure its concentration in real time causes a steady loss of products at the end of the experiment.
For this reason the accumulated values from the spectra measured along the experiment have been used.
The relative quantification process is as follows:
1.HDO, D2O, CO and CO2 instant quantities were calculated for each mass spectra using the calibrated mass cracking pattern given by the RGA software.
Background is subtracted to all signals considering their respective mean value in vacuum.
But in the case of CO m/q 28, the intensity in blank experiments (reactive gas injection at walls at 150 °C) is also subtracted to account for the N2 from air contamination during gas injection and NO2 decomposition at the stainless steel walls.
It results in 5–15% of the m/q 28 signal during thermo-oxidation.
These instantaneous quantities are averaged by normalizing to the measurement time of each mass spectra and to the analysis chamber pressure, as both parameters were varied once during each experiment to improve RGA sensitivity for minority products.
Measurement time is controlled by the RGA program, and the pressure by a needle valve to the analysis chamber, see valve 3b at Fig.
1.
Finally, the relative, averaged yield rates are calculated by the sum of all the previously averaged quantities for each product divided by the thermo-oxidation duration (∼3 h).
The results of this quantification are presented in Table 2.
As the averaged yield rates cannot be compared due to the different sampled flux from the thermo-oxidation chamber to the analysis one during each experiment, their ratios will be used.
About 2–4 times more HDO is produced compared to D2O due to its isotopic exchange with background water.
The CO2/CO ratio in Table 2 indicates that CO2 production is larger than CO, and it depends on the temperature and the gas, as at 275 °C and 200 °C in NO2 the ratio is lower than in the rest, and at 350 °C the ratio in O2 is about half than NO2.
The CO2/CO ratio values obtained agree with previous results: 10–2.5 [12–14], except NO2 at 350 °C, which in this work is larger: 16.
As the post-oxidation of CO into CO2 at 350 °C was much stronger in O2 than in NO2 (m/q 44 increase of 650% versus 100%), then most of the CO2 during the thermo-oxidation with NO2 has to be generated directly during the removal of the a-C:D film.
Nevertheless, the relative large water yield of the experiments of this work (the film has a larger D/C ratio than previous works) can also have an effect as water acts a catalyst for the reaction of CO into CO2 [14].
Total quantity of D atoms divided by the total quantity of C atoms is also given in Table 2.
Except at 200 °C, the D/C ratio obtained in the gas is similar (0.06–0.07), because the a-C:D film is completely removed, except for the few remaining films not heated during the experiment as indicated by the subsequent He/O2 plasma.
Furthermore, at 200 °C the D/C ratio in the gas is lower, 0.043, which indicates that at this temperature NO2 seems to remove more C than D during the first part of the thermo-oxidation, as detected also on silicon samples in a previous work [4].
The initial ratio of the film is C = 2.5 D, so this can be physically possible.
Notwithstanding, as the a-C:D film is completely removed then the D/C ratio detected in the gas should be 0.4 like the film, but the obtained values are between a factor 5–10 lower.
The reasons for this lower detection are the water condensation in colder areas, slow water pumping velocity, and mainly excessive dilution toward HDO by the H2O dragged out, especially critical in NO2 as its HDO/D2O ratio is between 2 and 3 times than that of O2, see Table 2.
Nevertheless, the quantity of deuterium products found in the experiments (estimated as D/C ratio) is sufficient to demonstrate that the detected deuterated water comes from a-C:D thermo-oxidation.
As previously commented, about two-thirds of the film volume is completely eroded at 200 °C, which is another indicator of the possibility of using thermo-oxidation with NO2 at mild temperatures in nuclear fusion devices.
It is necessary to note that 200 °C is the working temperature of some actual devices without active cooling like JET, so a long shutdown would not be required and can be used at weekends or similar periods (plus the required recovering procedures).
In general, thermo-oxidation, especially with NO2, is suggested to be used to remove in a fast way all kind of carbon co-deposits irrespective of their location or their thickness.
However, the large production of water during thermo-oxidation will advise against the use of this technique in nuclear fusion devices operating with tritium.
But it could be used after most co-deposits have been removed by other techniques, like maintenance plasma, laser, etc.
In this way it will only remove co-deposits at remote places where no other technique can access, and the produced tritiated water could probably be handled.
Thermo-oxidation in NO2 is a very fast technique to remove carbon co-deposits in a nuclear fusion device irrespective of their location or thickness.
The main gas products have been identified as the same as in O2: CO and CO2 from carbon removal, and water from hydrogen isotopes removal.
Other products from hydrogen isotopes could not be identified, but their production is expected to be very low.
The total calculated deuterium products related to carbon products is around one order of magnitude lower —0.04 to 0.07— than the D/C ratio of the film 0.4.
This is probably caused by the inherent difficulties in quantifying water in a vacuum system and by the isotopic exchange with the relatively large quantity of background, protonated water found.
Other findings of previous works have also been confirmed.
One of them is the possible use of thermo-oxidation to remove carbon co-deposits even in remote parts at mild temperatures, as even at 200 °C the reaction velocity is acceptable.
In 3 h it has removed about two-thirds of a 300 nm a-C:D film of hard type, much less reactive than the soft films expected in remote parts.
Also at 200 °C some indications of a relatively faster C than D removal have been detected in agreement with previous works.
On the other hand, NO2 has been found to slowly decompose in stainless steels walls to NO and O2, and it also allows the post-oxidation of CO to CO2.
Moreover, at > 275 °C the previously formed O2 also enhances the post-oxidation of CO, especially evident in the thermo-oxidation in O2 at 350 °C.
The production of water will complicate its utilization in devices operating with tritium due to the expensive treatment of tritiated water.
However, it can be used to remove just the co-deposits in remote parts that other techniques cannot remove, precisely if the rest of the co-deposits are removed first by these techniques (like maintenance plasma, laser, etc.)
Nonetheless, thermo-oxidation can be used to remove carbon co-deposits in any other device where carbon materials are used (like DIII-D, TCV, JT60-SA, etc.)
to improve the plasma density control, reduce dust events, etc.
D. Alegre acknowledges the financial support from the FPI grant awarded by CIEMAT (BOE resolution n171, 24/06/2008).
Benchmark calculation for tunnelling through a multidimensional asymmetric double well potential A benchmark calculation is presented for the quantum dynamics of tunnelling through a multidimensional asymmetric double well potential.
A model Hamiltonian is used with a 1-dimensional tunnelling mode coupled to an (M−1)-dimensional harmonic bath, a system-bath problem.
The benchmark calculation uses a basis set expansion of the wavefunction, with separate basis functions for the system and bath.
Indistinguishability of configurations is exploited to greatly reduce the expense of the calculation, and a fully converged result is achieved.
Comparison is offered to existing quantum dynamical methods that have tested this model problem, and further benchmark results not previously studied are presented.
Tunnelling is a fundamentally quantum feature, absent from classical dynamics calculations and only partially treated by semiclassical ones [1].
Tunnelling events are vital for many processes in biology, chemistry and physics, including hydrogen tunnelling in enzyme catalysis [2,3], proton transfer in proteins [4], tunnelling through a reaction barrier [5], and atomic tunnelling of a Bose-Einstein condensate in a double well trap [6,7].
In order to correctly treat the dynamics of tunnelling problems, fully quantum techniques must be used.
Over the previous few decades there has been a growth in the number of time-dependent quantum dynamics methods appearing in the literature.
Early examples include powerful integrators capable of solving the time-dependent Schrödinger equation exactly, such as the split-operator [8,9], Chebyshev expansion [10] and short iterative Lanczos [11] methods.
However, these are only capable of treating a few degrees of freedom due to the exponential basis scaling effect.
More recently we have schemes that are capable of treating a greater number of degrees of freedom.
The multiconfigurational time-dependent Hartree (MCTDH) method [12] in particular has emerged as a very accurate method of wavepacket propagation.
It still suffers from exponential scaling, but with a smaller base to be exponentiated than the integrators mentioned above.
Methods capable of scaling more favourably with dimensionality utilise Gaussian wavepacket basis sets; examples of which include the full multiple spawning (FMS) [13] and matching pursuit split-operator Fourier transform (MP/SOFT) [14] methods, along with our own coupled coherent states (CCS) method [15].
Due to reliance on random basis sets, these methods suffer from noise and slow convergence, but they are all capable of treating problems in a fully quantum manner and have been applied to multidimensional tunnelling problems [16–19].
For any quantum dynamical method, existing or emerging, reliable benchmarks are required to assess their accuracy.
A model Hamiltonian exhibiting tunnelling dynamics through a multidimensional asymmetric double well potential has been used as a test by the MP/SOFT [18] and CCS methods [19] mentioned above, and also more recently by a configuration interaction (CI) expansion method [20] and two-layer version of CCS (2L-CCS).
[21] The Hamiltonian consists of a 1-dimensional tunnelling mode coupled to an (M−1)-dimensional harmonic bath, hence it is a system-bath problem which bears some similarity to the Caldeira-Leggett model of tunnelling in a dissipative system [22,23].
This Hamiltonian is non-dissipative, however and the harmonic modes all have the same frequency.
System-bath models play an important role in physics, being used to describe superconductivity at a Josephson junction in a superconducting quantum interface device (SQUID) [24], for which the Caldeira-Leggett model provides a theoretical basis, and magnetic and conductance phenomena in the spin-bath regime [25].
No standard reference result has thus far been proposed for the model problem, and instead comparison to other methods and indication of any sort of tunnelling has been used to evaluate their effectiveness.
In this letter a benchmark result will be presented for the model Hamiltonian to evaluate its treatment by existing methods and provide a point of comparison for any future methods that wish to use it.
Previously a 20-dimensional case has been considered that will also be presented here, alongside more challenging cases of 40 and 80-dimensions and 20-dimensions with a stronger system-bath coupling.
We state from the outset that although the calculations performed in this work are relatively straightforward and trivial, they have not been published before and serve as a useful standalone reference to a problem tackled a number of times.
Atomic units are used throughout, with ℏ=1.
The model Hamiltonian consists of a 1-dimensional tunnelling mode coupled to an (M−1)-dimensional bath.
It is given by(1)Hˆ=pˆ(1)22−qˆ(1)22+qˆ(1)416η+Pˆ22+1+λqˆ(1)Qˆ22where (qˆ(1),pˆ(1)) are the position and momentum operators of the 1-dimensional tunnelling mode and (Qˆ,Pˆ) are the position and momentum operators of the (M−1)-dimensional bath modes, with Qˆ=∑l=2Mqˆ(l) and Pˆ=∑l=2Mpˆ(l).
Previous studies have considered the case of a 20-dimensional problem, M=20, system-bath coupling constant λ=0.1 and potential parameter η=1.3544.
We shall also use these parameters initially before moving onto more challenging cases of M=20, λ=0.2 and M=40 and M=80 with λ=0.1.
The wavefunction is represented as a basis set expansion(2)|Ψ(t)〉=∑j=1Nbth∑n=1Nsyscjn(t)|ψjb〉|ψns〉,in which cjn(t) are complex, time-dependent amplitudes, |ψjb〉 is a time-independent basis function for the bath modes and |ψns〉 is a time-independent basis function for the system mode.
The number of bath and system basis functions are given by Nbth and Nsys respectively.
Substitution into the time-dependent Schrödinger equation leads to an equation for the time-dependence of the amplitudes(3)dcim(t)dt=−i∑j=1Nbth∑n=1NsysHimjncjn(t),where Himjn is the Hamiltonian matrix(4)Himjn=〈ψibψms|Hˆ|ψjbψns〉=〈ψmspˆ(1)22−qˆ(1)22+qˆ(1)416ηψns〉δij+〈ψibPˆ22+Qˆ22ψjb〉δmn+λ2〈ψibQˆ2ψjb〉〈ψmsqˆ(1)ψns〉.
The bath and system basis functions are orthonormal (see below), a fact that has been exploited in the above.
The basis functions for the system are those of a particle in a rectangular box(5)〈q(1)|ψns〉=2LsinnπL(q(1)−qbox),where L is the size and qbox the lower coordinate of the box.
Both these values may be adjusted to ensure a large enough area of coordinate space is sampled by the system basis functions.
The bath modes are nearly harmonic, therefore they can be represented by harmonic oscillator basis functions.
A complete description of the bath would involve all excited state harmonic oscillator configurations, however in practice we can simply add on configurations until a converged result is achieved.
For an (M−1)-dimensional bath, an excited state is comprised of the product of (M−1) single particle harmonic oscillator functions, ∏l=2M|χ(l)〉, with different permutations of this product yielding different configurations.
As the coupling of system and bath modes is proportional to Qˆ2 and all bath modes are initially in the ground state, only even excitations are involved.
The size of the bath basis can be reduced further by exploiting the effective indistinguishability of the bath modes.
The amplitudes of the harmonic oscillator excited state configurations, which correspond to similar vibrational excitations but differ only by the bath modes involved, will be identical for a given excited state.
This means that configurations corresponding to the same excited state can be grouped together and associated with a single amplitude.
This simplification reflects the permutational symmetry of the Hamiltonian in Eq.
(1), with the harmonic bath modes all having the same frequency.
For example, if we include all even excitations up to a total quanta of 8 (the reasons for this choice will become apparent later), then the bath basis functions obtained by grouping configurations are:(6)|ψ1b〉=|0000…0000〉|ψ2b〉=(|2000…0000〉+…+|0000…0002〉)1/M−1|ψ3b〉=(|4000…0000〉+…+|0000…0004〉)1/M−1|ψ4b〉=(|2200…0000〉+…+|0000…0022〉)×2!/(M−1)(M−2)|ψ5b〉=(|6000…0000〉+…+|0000…0006〉)/M−1|ψ6b〉=(|4200…0000〉+…+|0000…0024〉)×1/(M−1)(M−2)|ψ7b〉=(|2220…0000〉+…+|0000…0222〉)×3!/(M−1)(M−2)(M−3)|ψ8b〉=(|8000…0000〉+…+|0000…0008〉)/M−1|ψ9b〉=(|6200…0000〉+…+|0000…0026〉)×1/(M−1)(M−2)|ψ10b〉=(|4400…0000〉+…+|0000…0044〉)×2!/(M−1)(M−2)|ψ11b〉=(|4220…0000〉+…+|0000…0224〉)×2!/(M−1)(M−2)(M−3)|ψ12b〉=(|2222…0000〉+…+|0000…2222〉)×4!/(M−1)(M−2)(M−3)(M−4)with relevant normalisation factors included.
The square of the normalisation factors is simply equal to the number of configurations grouped; in this case there are 8855 bath configurations governed by only 12 distinct bath basis functions, and hence 12 distinct amplitudes.
Such reduction of parameters due to indistinguishability of modes/particles and permutational symmetry of the Hamiltonian is well known and exploited by the second quantization approach.
Here the idea is used in a more straightforward fashion.
Now the basis functions have been defined, the matrix elements of the Hamiltonian may be evaluated.
Firstly, the system elements(7)〈ψmspˆ(1)22−qˆ(1)22+qˆ(1)416ηψns〉=n2π22L2δmn+2L∫qboxqbox+LsinmπL(q(1)−qbox)×sinnπL(q(1)−qbox)q(1)416η−q(1)22dq(1),are the particle in a box energy levels, plus an additional potential term.
Secondly, the bath elements(8)〈ψib|Pˆ22+Qˆ22|ψjb〉=δij∑l=2Mϵi(l)+M−12are simply the harmonic oscillator eigenvalues for the excited states, where ϵi(l) is the number of quanta in one mode.
Finally, the system-bath interaction elements are comprised of a system term multiplied by a bath term, and are given by(9)〈ψms|q(1)ˆ|ψns〉=2L∫qboxqbox+LsinmπL(q(1)−qbox)×sinnπL(q(1)−qbox)q(1)dq(1).
(10)〈ψib|Qˆ2|ψjb〉=Aij2(ϵi(l)+2)(ϵi(l)+1)if   ϵi(l)=ϵj(l)−2   in   only   one   mode   and   ϵi(l)=ϵj(l)   in   all   other   modesAij2ϵi(l)(ϵi(l)−1)if   ϵi(l)=ϵj(l)+2   in   only   one   mode   and   ϵi(l)=ϵj(l)   in   all   other   modes∑l=2Mϵi(l)+M−12ifϵi(l)=ϵj(l)in   all   modes0if   states   differ   by   more   than   two   quanta   in   one   mode,   or   two   quanta   in   more   than   one   modeFor the bath term, Aij is a constant that depends upon the normalisation factors and the number of configurations that differ by only two quanta in one mode between excited states.
Returning to the example of including all even excitations up to and including a total quanta of 8, we may evaluate the 〈ψib|Qˆ2|ψjb〉 matrix elements to clarify what these Aij constants are:(11) As with previous studies, the initial wavepacket is defined by(12)〈q|Ψ(0)〉=1πM4∏l=1Mexp−12q(l)−q(l)(0)2,where the initial tunnelling coordinate q(1)(0)=−2.5 is located in the lower well, and initial bath coordinates q(l)(0)=0.0 for l>1.
The initial momenta for all modes is p(l)(0)=0.0.
Thus, the initial conditions for all bath modes are identical, which along with their identical Hamiltonian parameters makes them indistinguishable.
The initial amplitudes are calculated via projection onto the initial wavepacket, with all bath modes in the ground level at t=0(13)cim(0)=〈ψibψms|Ψ(0)〉=δ1m〈ψms|Ψ(0)〉=δ1m2L∫qboxqbox+LsinmπL(q(1)−qbox)1π14×exp−12q(1)−q(1)(0)2dq(1).
The quantity of interest is the cross-correlation function (CCF) between the wavefunction at time t and the mirror image of the initial wavepacket, |Ψ¯(0)〉.
The mirror image of the initial state is located in the upper well of the asymmetric potential, therefore non-zero values of the CCF are indicative of tunnelling.
Rather than express |Ψ¯(0)〉 as a Gaussian wavepacket in the CCF, it is simpler to represent it as the basis set expansion instead, with initial amplitudes calculated using the mirror image coordinates (i.e.
q¯(1)(0)=+2.5)(14)CCF(t)=〈Ψ¯(0)|Ψ(t)〉=∑i,j=1Nbth∑m,n=1Nsysc¯im*(0)cjn(t)〈ψibψms|ψjbψns〉=∑i,j=1Nbth∑m,n=1Nsysc¯im*(0)cjn(t)δijδmn=∑j=1Nbth∑n=1Nsysc¯jn*(0)cjn(t).
To aid with comparisons made later in the text, the spectra of the CCFs are presented via a Fourier transform (FT):(15)I(ω)=∫0TRe(CCF(t))exp(−iωt)dt.The FT also makes it simpler to identify the long-time propagation accuracy of a quantum dynamical method, due to a small number of sharp peaks as opposed to the highly oscillatory nature of the CCF.
Total propagation time is T=120 a.u for all results that follow, with step size δt=0.001 a.u.
The calculation can be converged with respect to the system box length L (to ensure sufficient coordinate space sampling of the tunnelling mode and allow correct representation of the initial wavepacket), number of system basis functions Nsys, and number of bath basis functions Nbth (to ensure sufficient basis functions, and hence amplitudes, are included to represent the system and bath modes and their time-dependence over the timeframe of the calculation).
The calculation is fully converged when the CCF and FT show no observable change upon increasing L, Nsys and Nbth in turn, whilst the other two parameters are held fixed at their fully converged value.
Figures illustrating this are not included in the main body text of the letter as they are trivial, however they are included in Supplementary Material (Figs.
S1–S3).
The result of the fully converged benchmark calculation is shown in Figure 1, with the CCF in panel (a) and the FT in panel (b).
The calculation can be converged with respect to the system box length L (to ensure sufficient coordinate space sampling of the tunnelling mode and allow correct representation of the initial wavepacket), number of system basis functions Nsys, and number of bath basis functions Nbth (to ensure sufficient basis functions, and hence amplitudes, are included to represent the system and bath modes and their time-dependence over the timeframe of the calculation).
The calculation is fully converged when the CCF and FT show no observable change upon increasing L, Nsys and Nbth in turn, whilst the other two parameters are held fixed at their fully converged value.
Figures illustrating this are not included in the main body text of the letter as they are trivial, however they are included in Supplementary Material (Figs.
S1–S3).
The result of the fully converged benchmark calculation is shown in Figure 1, with the CCF in panel (a) and the FT in panel (b).
In the fully converged calculation the system box length is L=12 and the lower coordinate of the box is qbox=6, meaning the system wavefunction at t=0 a.u.
until t=120 a.u.
samples coordinate space in the region [−6:6].
The number of system basis functions Nsys=50, and although this is a relatively small number only a single mode is being treated by them and they cover a large amount of coordinate space.
This indicates a significant amount of delocalisation, as may be expected from a tunnelling mode, hence accurate treatment of the system by other methods may not be as trivial as for this benchmark calculation.
The range of momentum values required to be sampled can be estimated from the wavelengths of the particle in a box basis functions and the De Broglie relationship.
The longest wavelength basis function occurs when n=1 in Eq.
(5) and smallest when n=Nsys.
This gives a range of wavelengths from λ=2L and λ=2L/Nsys, leading to momenta of p=π/L and p=πNsys/L.
Therefore as well as a significant amount of coordinate space, a large amount of momentum space must also be sampled.
The number of bath basis functions Nbth=12, corresponding to adding on even excited harmonic oscillator states up to and including 8 quanta.
The explicit form of these basis functions has been demonstrated in Eq.
(6).
Whilst this may not seem like a very large amount, we can recall the simplification we made earlier: all configurations for a particular excited state are governed by the same amplitude.
So we have 8855 configurations governed by only 12 bath basis functions – a significant reduction.
Taking into account that the number of system basis functions Nsys=50, the total wavefunction is a superposition of 50×8855 configurations, described by only 50×12 amplitudes.
For calculations where this trick is not possible, a large amount of basis functions may be required for accurate modelling of the wavefunction, requiring a large amount of phase space to be sampled.
Comparison of our benchmark calculation to previous works on this Hamiltonian [18–21] is shown in Figures 2 and 3.
Evaluating each of the methods in turn, MP/SOFT in panel (a) has done a reasonable job for short time propagation, although there is a loss of structure and amplitude in the CCF at longer times.
This suggests that the calculation is less able to treat tunnelling as the propagation progresses, producing a more semiclassical result.
In panel (b) the CCS calculation from Ref.
[19] does not reproduce the converged result, missing the large peak splitting in the FT at ω=9.5, as well as the smaller splittings at ω=10.8 and ω=11.6.
Additionally, there is a peak at ω=9.0 which does not appear in the benchmark calculation.
Some indication as to why this is the case may be found in Ref.
[19], where it was noted that the bath modes were sampled from a narrow distribution.
Based on the results of this benchmark calculation, where a number of excited harmonic oscillator states were required for the bath, a broader distribution may be required.
In Ref.
[21] corrected CCS calculations with better and broader sampling of the bath has been reported, and this is shown in panel (c).
The re-calculated CCS result performs much better, with the CCF and FT more closely resembling the benchmark calculation.
The CI expansion performs best out of all three methods in panel (d), with a CCF that is accurate with respect to the benchmark for a longer time than MP/SOFT and CCS, leading to a FT that is also more accurate.
This is to be expected as the CI expansion is similar to our benchmark calculation, with a basis set expansion based on excited levels used for modelling of the modes.
Furthermore, the CI expansion method uses a regular basis unlike CCS and MP/SOFT, and is therefore free from the problem of random noise.
The λ=0.2 case has not been explored by any previous work, but it would present a more stringent test for a quantum dynamical method as the increase in coupling between system and bath will cause greater perturbation of the bath by the system.
One would therefore expect an increased number of bath basis functions required for convergence in the calculation.
The fully converged result is shown in Figure 4, with the CCF in panel (a) and the FT in panel (b).
Figures illustrating how this calculation converges are included in Supplementary Material (Figs.
S4–S6), as with the 20D, λ=0.1 case.
The λ=0.2 case has not been explored by any previous work, but it would present a more stringent test for a quantum dynamical method as the increase in coupling between system and bath will cause greater perturbation of the bath by the system.
One would therefore expect an increased number of bath basis functions required for convergence in the calculation.
The fully converged result is shown in Figure 4, with the CCF in panel (a) and the FT in panel (b).
Figures illustrating how this calculation converges are included in Supplementary Material (Figs.
S4–S6), as with the 20D, λ=0.1 case.
The system box size L is the same as for the λ=0.1 case, and the number of system basis functions required is also the same.
Therefore the tunnelling mode is as delocalised as for the λ=0.1 case, requiring no further sampling of phase space over the timeframe of the calculation.
The increase in complexity arises with the modelling of the bath, as a much greater number of bath basis functions are required with Nbth=45.
This corresponds to involving excited levels up to and including 14 quanta.
Without exploiting the indistinguishability of the excited state configurations, an extremely large number of basis functions would be required; in this result there are 657800 bath configurations governed by only 45 basis functions.
The total wavefunction is a superposition of the 50×657800 configurations, which can be described by only 50×45 independent amplitudes.
As expected, this strongly coupled system and bath illustrates the significant perturbation of the bath by the system due to the large number of bath configurations required for convergence.
Calculations with even larger λ turned out to be impossible because increasing coupling between system and bath modes makes the asymmetric potential unbound.
We have also performed calculations for 40D and 80D cases, i.e.
with 39 and 79 bath modes in the Hamiltonian in Eq.
(1).
There is no obvious computational scaling with dimensionality for the benchmark calculation as the bath basis functions represent excited states of the entire system rather than individual modes.
However, a greater number of excited states may be required due to the increase in dimensionality of the bath.
We revert to the weak coupling case of λ=0.1 for this reason, as an increase in the number of bath modes and their coupling may result in a calculation that is prohibitively expensive to converge, even when exploiting mode indistinguishabilities.
The fully converged result for the 40D case is shown in Figure 5, with the CCF in panel (a) and the FT in panel (b).
Once more, figures illustrating how this calculation converges are included in Supplementary Material (Figs.
S7–S9).
It can be seen that the CCF oscillates at a higher frequency than the 20D, λ=0.1 case, which is demonstrated in the FT with a shift to higher frequencies.
This is due to the tunnelling coordinate q(1) being coupled to all of the bath modes, and there is a greater number of bath modes for the 40D case than the 20D case.
As the dimensionality increases, so does the separation between the two wells [18], therefore one would expect a decrease in the amount of quantum tunnelling.
By comparison of the CCF's for the 20D and 40D case in Figures 1a and 5a, we see a small decrease in the amplitude for the 40D case, indicative of a small decrease in the amount of tunnelling.
The fully converged result for the 40D case is shown in Figure 5, with the CCF in panel (a) and the FT in panel (b).
Once more, figures illustrating how this calculation converges are included in Supplementary Material (Figs.
S7–S9).
It can be seen that the CCF oscillates at a higher frequency than the 20D, λ=0.1 case, which is demonstrated in the FT with a shift to higher frequencies.
This is due to the tunnelling coordinate q(1) being coupled to all of the bath modes, and there is a greater number of bath modes for the 40D case than the 20D case.
As the dimensionality increases, so does the separation between the two wells [18], therefore one would expect a decrease in the amount of quantum tunnelling.
By comparison of the CCF's for the 20D and 40D case in Figures 1a and 5a, we see a small decrease in the amplitude for the 40D case, indicative of a small decrease in the amount of tunnelling.
For the 80D case, the fully converged result is shown in Figure 6, with the CCF in panel (a) and the FT in panel (b).
Illustration of convergence is shown in Supplementary Material (Figs.
S10–S12).
There is a large decrease in the amplitude of the CCF compared to the 20D and 40D cases, indicating a large decrease in the amount of quantum tunnelling due to the increase in separation of the wells.
As with the 40D case, the frequency of tunnelling increases because of coupling to a larger number of bath modes which can be observed from the CCF, or more directly from the FT. For the 80D case, the fully converged result is shown in Figure 6, with the CCF in panel (a) and the FT in panel (b).
Illustration of convergence is shown in Supplementary Material (Figs.
S10–S12).
There is a large decrease in the amplitude of the CCF compared to the 20D and 40D cases, indicating a large decrease in the amount of quantum tunnelling due to the increase in separation of the wells.
As with the 40D case, the frequency of tunnelling increases because of coupling to a larger number of bath modes which can be observed from the CCF, or more directly from the FT. For both the 40D and 80D cases, the number of system basis functions required for convergence does not increase from the 20D case.
Therefore, even though the tunnelling mode is coupled to more bath modes, more system basis functions are not required.
The size of the box required for the system basis functions does not increase for the 40D case relative to 20D; however there is a small increase for the 80D case, meaning a small increase in the amount of coordinate space required to be sampled by the tunnelling mode.
As the increased dimensionality will result in bath modes that cover a larger amount of coordinate space, and the fact that the tunnelling mode is coupled to all of them, this can be explained.
The most significant change for both the 40D and 80D cases when compared to the 20D case is the number of bath basis functions required.
For the 40D case Nbth=30, corresponding to the bath basis functions involving excited levels up to and including 12 quanta.
For the 80D case Nbth=45, corresponding to the bath basis functions involving excited levels up to and including 14 quanta, the same as required for λ=0.2.
As expected, the increased dimensionality of the bath has required more excited levels to converge.
The total wavefunction is a superposition of the 50×177100 and 50×657800 configurations for the 40D and 80D cases respectively, which is described by sets of only 50×30 and 50×45 independent amplitudes.
A benchmark calculation has been presented for tunnelling through a multidimensional asymmetric double well potential.
The model Hamiltonian, previously used by the MP/SOFT [18] CCS, [19,21] and CI expansion [20] methods, consists of a 1-dimensional system tunnelling mode coupled to an (M−1)-dimensional nearly harmonic bath; a system-bath problem.
The dynamics were computed via a basis set expansion of the wavefunction, comprising of separate time-independent basis functions for the system and bath and associated time-dependent amplitudes.
The basis functions for the system were those of a particle in a rectangular box, and those for the bath were ground and excited state harmonic oscillator configurations.
The number of bath basis functions required to converge the calculation was reduced by noting two useful properties of this problem.
Firstly, the coupling of bath and system is proportional to the square of the bath coordinate; therefore, as initially all modes are in the ground state, only even excited state harmonic oscillator functions were required.
Secondly, and more significantly, the indistinguishability of the excited state configurations was exploited so that only one amplitude was required to be associated to each excited state and not one amplitude per configuration for each excited state.
A fully converged result for the 20D, λ=0.1 problem has been presented, with comparison to the methods that have previously studied this Hamiltonian.
The MP/SOFT and CI expansion methods compared well to the benchmark, whereas the CCS calculation from Ref.
[19] did not due to insufficient sampling of the bath.
However, a re-calculated CCS result in Ref.
[21] with improved sampling performed much better.
Guidance for sampling this problem has also been presented, with the tunnelling mode being highly delocalised and requiring a considerable amount of phase space to be sampled, as may be expected.
Ranges for sampling the coordinates and momenta of the system have been given in Section 3.1.1.
The bath required a large number of configurations for convergence, although it heavily benefited from the exploitation of indistinguishability to reduce the number of basis functions required in this calculation.
A stronger coupling case, not previously studied, of λ=0.2 was computed and it was observed that the system did not need additional basis functions to accurately represent it, although the bath did due to increased perturbation by the system.
Higher dimensional cases of 40D and 80D have also been presented, in the λ=0.1 regime once more.
As with the stronger coupling case, no increased treatment of the system was required, but additional basis functions were required for the bath.
However, this is most likely due to the increased size of the bath rather than increased perturbation by the system.
The fully converged CCFs and FTs for each of these calculations has been presented, providing a point of comparison for future tests on tunnelling/system-bath problems using this model Hamiltonian.
The latter 40D and 80D cases, as well as 20D λ=0.2, may present a challenge for the most advanced methods of multidimensional quantum dynamics.
All data produced by the benchmark calculation in this letter has been included in Supplementary Material so that it may be used in future work.
A stronger coupling case, not previously studied, of λ=0.2 was computed and it was observed that the system did not need additional basis functions to accurately represent it, although the bath did due to increased perturbation by the system.
Higher dimensional cases of 40D and 80D have also been presented, in the λ=0.1 regime once more.
As with the stronger coupling case, no increased treatment of the system was required, but additional basis functions were required for the bath.
However, this is most likely due to the increased size of the bath rather than increased perturbation by the system.
The fully converged CCFs and FTs for each of these calculations has been presented, providing a point of comparison for future tests on tunnelling/system-bath problems using this model Hamiltonian.
The latter 40D and 80D cases, as well as 20D λ=0.2, may present a challenge for the most advanced methods of multidimensional quantum dynamics.
All data produced by the benchmark calculation in this letter has been included in Supplementary Material so that it may be used in future work.
The success of exploiting the indistinguishability of the bath basis function excited state configurations has provided motivation towards further studies in which indistinguishabilities are used to reduce the dimensionality of a problem.
Using this model Hamiltonian as an example once more, if it were second quantized then the modes themselves would be indistinguishable rather than the excited state configurations of the bath basis functions.
Investigations of this using the CCS method are currently underway.
This work was supported by the EPSRC grants No.
EP/J019240/1 and EP/I014500/1.
J.
A. G. is supported by the University Research Scholarship from the University of Leeds.
D.V.S.
gratefully acknowledges V. Batista and S. Habershon for providing their data.
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cplett.2015.10.073.
The following are the supplementary data to this article:
Application of corrosion inhibitors for steels in acidic media for the oil and gas industry: A review This review summarizes the corrosion inhibition of steel materials in acidic media.
Numerous corrosion inhibitors for steels in acidic solutions are presented.
The emphasis is on HCl solutions, lower-grade steels, and elevated temperatures.
This review is also devoted to corrosion inhibitor formulation design – mixtures of corrosion inhibitors with (mainly) surfactants, solvents, and intensifiers to improve the effectiveness of individual compounds at elevated temperatures.
The information presented in this review is useful for diverse industrial fields, primarily for the well acidizing procedure, and secondly for other applications where corrosion inhibitors for steel materials are needed.
American Petroleum Institute corrosion inhibitor corrosion inhibitor formulation corrosion rate cold rolled steel cold rolled mild steel carbon steel electrochemical impedance spectroscopy electrochemical frequency modulation inhibition effectiveness low carbon steel current density current linear polarization resistance linear sweep voltammetry mild steel polarization resistance temperature weight loss scanning electron microscope Human demand for fossil fuels is still growing even though alternatives to such energy are currently being sought.
Oil and natural gas account for 60% of all global energy demands [1].
It is thus not expected that the conventional method of extracting fossil fuels will disappear within the next few decades.
The extraction of geothermal water for use as an energy source is also of paramount importance and its usage is increasing.
The methods required to maximize production typically comprise formation stimulation and subsequent well cleaning, both of which can induce a corrosive environment for the steel involved, as it is the main construction material of wells.
Corrosion is worth investigating in oilfield applications, because corrosion problems represent a large portion of the total costs for oil and gas producing companies every year worldwide.
Moreover, appropriate corrosion control can help avoid many potential disasters that can cause serious issues including loss of life, negative social impacts, and water resource and environmental pollution.
Corrosion in oilfields occurs at all stages from downhole to surface equipment and processing facilities.
It appears as leaks in tanks, casings, tubing, pipelines, and other equipment [2–4].
Corrosion problems are usually connected with operating problems and equipment maintenance, leading to recurrent partial and even total process shutdown, resulting in severe economic losses [5].
Moreover, Garcia-Arriaga et al.
[5] reported that the economic costs linked to the corrosion of natural gas sweetening (CO2 corrosion) and oil refining plants range between 10% and 30% of the maintenance budget.
In the petroleum industry, general and localized corrosion are the most common types of corrosion occurrences.
The other large problem in operating pipe flow lines is internal corrosion [6], mainly due to stress corrosion cracking.
Martinez et al.
[7] claim that the combination of corrosion and erosion is the main problem in pipe deterioration.
Also noted recently is an increase in the occurrence of galvanic corrosion problems associated with the use of different dissimilar materials, which has garnered much attention.
Wilhelm [8] reported that the most common situation of coupling dissimilar materials in wells consists of a tubing string made of corrosion-resistant alloy in contact with lower-grade steel casing.
Moreover, the metal contacts also cause crevice corrosion in the occluded area between tubing and casing.
The primary focus of this review is to summarize different research relating to corrosion and its inhibition regarding mild, carbon, and low-alloy steel – lower-grade steels – in different acidic solutions encountered in the crude oil and natural gas sector.
These materials are used in well construction.
In the petroleum industry, one facet of the development of new oil and gas production is the stimulation process.
Overall, the stimulation process involves many different aspects, including the acidizing portion utilized to stimulate the carbonate reservoir or for dissolving fines.
Typically, highly concentrated acids, between 5 and 28wt.%, are used which make the environment corrosive to mild, carbon, and low-alloy steels.
Hydrochloric, hydrofluoric, acetic, or formic acids are injected into the well during the acidizing stimulation process and cause serious corrosion issues.
In the absence of corrosion inhibitors (CIs), the general CR (corrosion rate) can be extremely high (>100mm/y) and can increase exponentially with increasing temperatures and acid concentrations [9].
Due to the extreme corrosion conditions of this process, developed technology can then be translated to other industries.
In particular, this can be relevant for acid pickling, industrial cleaning, and acid descaling, where corrosion conditions are usually milder.
This may be a secondary source of information for readers of this review.
It has to be pointed out that the petroleum industry is the largest consumer of CIs.
This review only addresses individual CIs for application in HCl mediums with different steels because HCl is the most prevalent acid used in stimulation.
An effort has been made herein to combine different works by the same authors in a single paragraph, even though not all authors of different articles or patents appear together all the time.
In this review, when steel materials in general are written about, lower-grade steels are being referred to.
All concentrations in % are always reported as a mass fraction if not stated otherwise.
Moreover, when concentrations in various articles were reported in parts per million (ppm), herein they are converted to mg/L.
This work discusses the well acidizing procedure in general so that readers of this review can gain an impression of the severe corrosion conditions during that process.
Moreover, the steel materials used for well construction and associated with corrosion problems are discussed.
The corrosion of these steel materials and previously tested CIs for HCl solutions are reviewed.
This review also explains aspects of a corrosion inhibitor formulation design in order to increase the success of these CIs at elevated temperatures or under other well environmental conditions.
Furthermore, it also presents environmental concerns in corrosion inhibition processes, environmental friendly methanesulphonic acid, and some recommendations for correct test methods regarding acid CIs.
Limestone formations or carbonate-bearing sandstone carry many hydrocarbon reservoirs [10].
A very important step in the oil, gas, and geothermal water drilling industry is the well acidizing procedure, which is a rock reservoir (the origin of the natural resource or water – a geological subterranean formation) stimulation technique used to improve productivity.
Acids are forced under high pressure through the borehole into the pore spaces of the rock formation, where they react chemically with rocks to dissolve them (usually calcite, limestone, and dolomite), which enlarges the existing flow channels and opens new ones to the wellbore [11–15].
Acidizing is used in conjunction with hydraulic fracturing techniques and matrix acidizing techniques [16].
In fracture acidizing treatments, one or more fractures are produced in the formation and acidic solution is introduced into the fracture to etch flow channels in the fracture face.
The acid also enlarges the pore spaces in the fracture face and in the formation [12,13].
The fractures are then filled with sand or other material in order to prevent the fractures from closing and allow the penetration of natural resources or water.
Acids are often also employed for scale removal treatments (pickling of the well tubing) and for the removal of drilling mud damage in newly drilled wells before being brought into production [17].
For example, the combination of fluorosilicate with metal ions such as Na+ may cause the precipitation of gelatinous compounds, which need to be removed [10].
Scale removal treatments are usually done with 15% HCl at temperatures up to 60°C in order to remove iron oxides and carbonated minerals [18].
Acidizing steps are frequently repeated.
All these procedures involve the injection of acids into the well system made of steel tubes.
In deep wells the downhole temperature may exceed 200°C [19,20].
During the acidizing process metallic materials can also come into contact with acid solution and sometimes with H2S and CO2 at elevated temperatures.
Due to the above listed problems, the acidizing process requires a high degree of corrosion protection of tubular materials and other equipment employed.
Different acids are employed depending on the underground reservoir characteristics.
The treatment normally involves the injection of acid at 15% concentration (sometimes from 5% up to 28%) [11,21–23].
A standard 15% acid concentration had been chosen before 1960 due to the insolubility of arsenic inhibitor, the primary inhibitor of the industry at the time, because it was not soluble in HCl concentrations higher than 17% [24].
The most common conventional acids are HCl, HF, acetic, and formic acids.
It has also been noted that mixtures of these conventional acids with sulphamic, sulphuric, phosphoric, methanesulphonic, nitric, citric, and chloroacetic acids are employed [9,12,13,15,25–28].
The majority of acidizing treatments carried out utilize HCl at concentrations of 5–28% [24].
HCl has an advantage over the other mineral acids in the acidizing operation because it forms metal chlorides, which are very soluble in the aqueous phase.
Other acids have been employed historically, but they were not so successful compared with HCl.
One of the reasons was that, for example, sulphate, nitrate, and phosphate salts have lower solubility compared with chloride salts in aqueous media [29].
HCl is widely used for stimulating carbonate-based reservoirs such as limestone and dolomite.
Alternatively, sandstone formation can occur and for a successful stimulation process HF is needed.
Sometimes a combination of HCl with HF, also called mud acid, is employed (typically 6% HCl/1% HF and 12% HCl/3% HF) [20,23].
For such application three treatment procedures are typically used: a pre-flush, main-flush, and after-flush.
The pre-flush is done with 15% HCl.
The main-flush is done with 12% HCl and 1.5% HF.
The after-flush is performed for rapid formation clean-up.
This can be done using ethylene glycol monobutyl ether (EGMBE) or methanol.
Moreover, HCl represents the most economical acid for dissolving CaCO3 in pickling applications.
However, the fast reaction rate with rocks, the CR, and the pitting tendency of materials vary considerably with HCl concentration, which can cause problems.
Additionally, another disadvantage of using HCl is its high corrosiveness to steel, aluminium, or chromium-plated equipment.
One important factor that is considered in acid stimulation is the reaction rate of the acid in the formation.
This rate is dependent on the type of acid, acid concentration, temperature, fluid velocity, and the type of formation material.
Acidic solutions are introduced into the formation and can only travel a short distance before they become spent.
The acid pumped down the well is usually called the live acid.
The acid that is produced from the well after completion of the treatment is usually called spent acid.
Due to the reaction of the acid with rocks, the pH of the spent acid goes up, commonly reaching a value near 1.
An example of a spent acid simulation in the laboratory is the use of HCl with 150g of dissolved CaCl2 and a pH adjusted to 1 [10].
It is desirable to maintain the acidic solution in a reactive state for as long as possible to maximize the permeability enhancement [12,13,28].
On the other hand, in order to achieve the desired deeper penetration of the stimulation fluid into the rock formation, the acid needs to be emulsified by an appropriate agent.
In such a manner the acid reaction rate (especially HCl) with the rocks is significantly retarded, but still effective, and the acid is spent more slowly, allowing deeper penetration (“unretarded” 15% HCl at ∼160°C would penetrate only approximately 10cm) [19].
In some applications formic and acetic acids can be used in conjunction with HCl since they react more slowly compared with HCl by itself.
For example, combinations of 9% formic acid and 13% acetic acid with 15% HCl are employed [19].
This combination of acids helps to improve the fracture geometry.
However, Ali et al.
[15] reported that formic acid and other short-chain aliphatic acids and their related aldehydes are still as corrosive as HCl to pipelines and/or other equipment.
Fortunately, they are easier to inhibit compared with HCl by itself [19].
Unfortunately, the solubility of carbonates in these acids is much lower than in HCl.
Formic and acetic acids may be used instead of HCl in limited applications at very high temperature [24].
In particular, this is relevant in wells completed with Super chrome-13 tubulars [30].
At higher formic and acetic acid concentrations, potential precipitation of reaction products of the acid with the rock formation is expected (mainly calcium formate or acetate) [30].
Consequently, this will lead to deposit problems and disturb or even prevent the flow of fluids.
In summary, the corrosion of pipelines and other equipment involved in the oil and gas industry represent a large problem in the acidizing process and consequently a large part of the total cost and potential danger to the personnel involved.
Thus, the selection of non-corrosive or low-corrosive inhibited acid solution is crucial.
The materials used for pipeline construction play an important role in the petroleum industry as they carry liquids and gases over long distances from their source to the ultimate consumers.
Corrosion problems associated with the transportation process can exist at every stage of production, from the initial extraction to refining and storage prior to use.
The steels used in well construction can range from mild steel API N80 (API – American Petroleum Institute), L80, and J55, to high Cr-content corrosion resistant alloys, such as austenitic-ferritic steel, e.g.
duplex 22% Cr steel, and modified martensitic 13% Cr steel, sometimes called Super-13 [18,20].
Moreover, an important property of pipe corrosion resistance is how the weld of the pipe is made and the acid corrosion at that place [1,31].
Over the past few decades there have been many developments in new corrosion resistant alloys (CRA), however CSs (carbon steels) are still the most commonly used materials for downhole tubulars, flow lines, and transmission pipelines in the gas and oil industry, most likely due to their low cost [4,6,32,33].
For example, the cost of austenitic stainless steels (AISI 304 and 316) is currently around 8-times higher than that of CSs [1].
A combination of CS and chemical treatments is the most cost-effective method for corrosion control [34].
Without the usage of chemical treatments, in particular CIs, CS materials are highly susceptible to corrosion in most acids [35].
More corrosion resistant alloys may also be employed, e.g.
austenitic or duplex stainless steels [23].
However, high-grade alloys significantly increase the capital cost and are susceptible to corrosion in media containing large amounts of chloride ions [18,34].
API N80 CS has generally been used as the main construction material for downhole tubulars, flow lines, and transmission pipelines in the petroleum industry [13,17,36,37] and consequently most acid CI data exist for that steel type [24].
API L-80 grade CS tubing is H2S-resistant steel [23].
Moreover, Torres-Islas et al.
[38] reported that micro-alloyed API X80 steel was specially designed for sour gas transport and is intended to be applied as pipeline steel in Mexico.
API P-110 was reported to be used as the production liner, whereas API G95 steel has been used for the tubing [19].
Steel metallurgy is one of the most important criteria for acidizing CIs in laboratory testing when simulating real field conditions.
A problem sometimes arises because the standard materials commonly employed, e.g.
API N80 steel, may vary considerably from one manufacturer to another or from one lot to the next, which leads to confusion in comparative corrosion testing.
In order to clearly show the differences in steel composition, even though these steels are sometimes named the same, Table 1 was constructed for the materials found in the literature (presented below).
Primarily for steel producers, tubing must meet tensile stress requirements, and that is why the CR of steel types with the same name can vary considerably.
Smith et al.
[24] claim that the variance in the CR of different steel materials (most likely they meant carbon or low-alloy) can be up to 35% at temperatures above 93.3°C.
It is almost impossible to prevent corrosion, however it is possible to control it [23].
To control the corrosion damage of well tubulars, mixing tanks, coiled tubings, and other metallic surfaces, acids need to be inhibited by the use of an effective CI solution (now commonly organic compounds) [18,21].
A CI is a chemical substance that is effective in very small amounts when added to a corrosive environment to decrease the CR of the exposed metallic material.
Unfortunately, CIs are effective only for a particular metallic material in a certain environment.
Minor changes in the composition of the solution or alloy can significantly change the inhibition effectiveness (η).
For example, many good inhibitors that in the past worked for 15% HCl did not perform well for 28–30% HCl.
On the other hand, Smith et al.
[24] also reported that some inhibitors developed for concentrated acid are not as effective in less concentrated HCl.
However, this should rarely occur.
For stimulation applications, inhibitors are added to the acid fluids in batch-wise fashion; batch-wise refers to the single addition of the CI into the holding tank of the acid before the acid is used in the stimulation process.
The use of CIs is one of the most cost-effective and practical methods of corrosion protection.
The employment of an appropriate inhibitor can allow the use of lower-grade CSs, which significantly reduces the capital costs of a well construction project when compared with the use of high-grade alloys in the same project [39].
The selection and the amount of CI used depend on the acid type and its strength, the steel type, the desired protection time, and the expected temperature [23].
The maximum temperature limit is one of the key roles in CI selection, because some components are sensitive to thermal decomposition, i.e.
when they lose their inhibition effectiveness [10].
Smith et al.
[24] claim that the introduction of arsenic acid as a CI in 1932 was responsible for the development of well acidizing.
However, for arsenic compounds it is known that they produce poisonous arsine gas under acidic conditions [40] and numerous persons died in the past due to arsenic poisoning [41].
Subsequently, the majority of CIs used were inorganic salts or acids such as arsenate or arsenic acid until the mid-1970s, when they were replaced by organic molecules, which generally contain N, O, P, or S heteroatoms, of an aromatic and/or unsaturated character [9,42–58].
The corrosion environments in oil and gas production wells can be highly variable, which makes the selection and application of inhibitors complicated.
Frequently it happens that a CI that works in one well may not work in another well [59].
Numerous compounds used as CIs are discussed below.
Because the mechanism of how CIs work is usually not known, empirical testing is still unavoidable, in spite of some proposed models for forecasting η.
The scientific community and the industry do not fully understand the mechanism or the role of CIs and it is difficult or sometimes impossible to predict if a particular compound will work or not [55].
Walker [12,13,28] claims that in general CIs are effective for ferrous materials only at temperature levels below 121–149°C.
A variety of organic compounds act as CIs for steels during the acidizing procedure, including acetylenic alcohols, aromatic aldehydes, alkenylphenones [25,27,60–62], amines [13], amides, nitrogen-containing heterocycles (e.g.
imidazoline-based [3,35,63]), nitriles, imminium salts, triazoles, pyridine and its derivatives or salts [14,16,64], quinoline derivatives, thiourea derivatives, thiosemicarbazide, thiocyanates, quaternary salts [14,16,64], and condensation products of carbonyls and amines ([11,18,65–67] and the refs.
therein).
Molecules containing nitrogen and acetylenic alcohols are claimed to form a film on the metal surface and can retard the metal dissolution process (an anodic reaction) as well as hydrogen evolution (a cathodic reaction) [9].
However, it has been reported that propargyl alcohol is soluble in acids, but the solubility of other acetylenic alcohols decreases with increasing carbon chain length.
On the other hand, the solubility of such acetylenic alcohols can be increased when combined with quaternary ammonium surfactants [9] (explained below).
Acetylenic alcohols are widely used because of their commercial availability and cost effectiveness.
Propargyl alcohol is usually taken as a standard CI for acidization [65] and sometimes it has a significant synergistic effect with other compounds.
In our experience, the most commonly used CIs in the natural resource exploitation industry are propargyl alcohol and its derivatives, cinnamaldehyde, and nitrogen aromatic-based compounds such as pyridinium benzyl quaternary chloride.
In 1984, a comprehensive review article was published by Schmitt [66] which presented the application of CIs for acid media.
Due to this reason, the focus of this review is the literature published subsequently, most of which is summarized in Table 2, where the investigated corrosive medium, steel types, pH (if reported), the concentration of CI used, the method of corrosion testing, and the minimum and maximum reported η or CR are summarized (because the different experimental techniques used sometimes resulted in quite large differences).
This review includes corrosion studies on HCl mostly related to the petroleum industry for low Cr content steels (e.g.
MS – mild steel – and CS).
Moreover, only studies published for HCl solution are reviewed below, because the majority of acidizing jobs are performed by this acid.
As seen in Table 2, the most common techniques for the evaluation of CI performance are weight-loss (WL), linear sweep voltammetry – LSW (polarization resistance (Rp) or even more frequently Tafel plot measurements), and electrochemical impedance spectroscopy (EIS).
The main focus below is on η evaluation and not on reporting the manner of inhibitor bonding or examining adsorption isotherms.
As mentioned above, among acid solutions, HCl (at 5–28% [24]) is the most widely used for the acidizing procedure and that is why the main focus is on this acid (see below and Table 2).
However, not all CIs tested in HCl solutions (given below) have already been used for oilfield applications, but the emphasis herein is on summarizing what kind of compounds have already been tested in HCl and could potentially be used.
That is why HCl concentrations lower than the minimum commonly employed (5%) are also included.
It must be pointed out that the following review cannot cover all aspects of CI use in HCl solutions.
Ita and Offiong [68] studied benzoin and benzil compounds as CIs for MS in HCl solution at 30 and 40°C.
It was reported that the η of these compounds has the following order: benzoin>benzoin-(4-phenylthiosemicarbazone)>benzil>benzil-(4-phenylthiosemicarbazone).
They related this trend to the compound solubility and to the strength of the inhibitor–metal bond.
Flores et al.
[69] showed that the effectiveness of sodium N-alkyl-phthalamates (alkyl=n-C6H13, n-C10H21, n-C14H29) as CIs for SAE 1018 CS in 0.5mol/L HCl is dependent on the alkyl chain length and concentration, i.e.
by increasing them η increased.
All there inhibitors acted as mixed-type inhibitors.
The authors suggested a physisorption type of adsorption and that phthalamates form complexes and chelates with iron, which prevents iron oxidation and consequently corrosion.
Baddini et al.
[18] and Cardoso et al.
[70] studied 23 compounds as CIs for UNS-G4130 CS in 15% (w/V) HCl at 60°C.
Along with active inhibitor compounds, they employed formaldehyde to minimize hydrogen penetration into steel.
Baddini et al.
[18] found that tributylamine, some alcohols (Table 2), aniline, n-octylamine, diphenylamine, dodecylamine, di-n-butylamine, cyclohexylamine, 1,3-dibutyl-2-thiourea are the most effective CIs among the compounds studied.
Vishwanatham and Haldar [36] reported that furfuryl alcohol is an effective mixed-type CI, with the predominant effect on the cathodic reaction, for N80 steel in 15% HCl.
Its η increased with increasing CI concentration, but decreased with increasing T (temperature) from 30 to 110°C.
Jayaperumal [29] reported that octyl alcohol and propargyl alcohol are excellent inhibitors for MS in 15% HCl at 30 and 105°C.
Especially in solution containing propargyl alcohol, as the CR was reported to be 0.4mm/y (at 30°C) and 3mm/y (at 105°C).
However, according to the EIS spectra, the charge transfer resistance is relatively low (290Ωcm2 for the highest inhibitor concentration), indicating quite a high CR, which could raise some doubts regarding the inhibitor performance.
Moreover, no explanation of how the electrochemical measurements were carried out at 105°C is given in the text.
Babic-Samardzija et al.
[71] investigated 2-butyn-1-ol, 3-butyn-1-ol, 3-pentyn-1-ol, and 4-pentyn-1-ol as CIs for iron in 1mol/L HCl at ambient temperatures.
They reported that all compounds act as mixed-type inhibitors and that their η depends on the chain length and the position of the triple bond.
Popova et al.
[72] investigated benzimidazole, 2-aminobenzimidazole, 2-mercaptobenzimidazole, 1-benzylbenzimidazol, and 1,2-dibenzylbenzimidazole as CIs for MS in deaerated 1mol/L HCl solution.
They reported that all five diazoles have pronounced corrosion inhibition properties, whereas the latter three were particularly effective.
Subsequently, Popova et al.
[73] reported that 8 benzimidazole derivatives exhibit a corrosion inhibition effect for MS in 1mol/L HCl at 20°C.
Their η increases with increased CI concentration and they mainly act as mixed-type inhibitors.
The η of these compounds has the following order: 5(6)-nitrobenzimidazole<benzimidazole<2-methylbenzimidazole<5(6)-carboxybenzimidazole<2-hydroxymethylbenzimidazole<2-aminobenzimidazole≈2-benzimidazolylacetonitrile<2-mercaptobenzimidazole.
However, the reported Rp values from the LSW measurements are very low (in all cases less than 1Ωcm2), which raises some doubt about the effectiveness of the compounds studied.
Most likely the measured Rp values were in kΩcm2 and there is simply a typographical error in the article.
Furthermore, Popova et al.
[74] investigated 5 different azole compounds as CIs for MS in 1mol/L HCl at 20°C.
The η trend of these compounds was reported to have the following order: indole≈1H-benzotriazole≈benzothiazole>benzimidazole.
On the other hand, the addition of benzothiadiazole to the HCl solution even promoted CR compared with the non-inhibited solution.
The authors also reported that the η of these compounds, except benzothiadiazole, increases with increased CI concentration and that they act as mixed-type inhibitors by predominantly reducing the rate of the anodic reaction.
Aljourani et al.
[75] showed that the η trend of different CIs has the following order: 2-mercaptobenzimidazole>2-methylbenzimidazole>benzimidazole for MS in 1mol/L HCl at 25°C.
The η of all 3 CIs decreased with increasing T from 25 to 55°C.
Moreover, it has to be pointed out that their η calculation from the experimental results obtained by means of LSW and EIS techniques gave quite large difference (up to approximately 20% for the highest CIs concentration).
Khaled [76], Khaled and Hackerman [77], and Khaled et al.
[78] studied the corrosion inhibition of pure Fe in 1mol/L HCl at 25°C.
In the first study, wherein Khaled [76] examined different benzimidazoles, he showed that the η trend of these compounds has the following order: 2-aminobenzimidazole>2-(2-pyridyl)benzimidazole>2-aminomethylbenzimidazole>2-hydroxybenzimidazole>benzimidazole.
In a study on o-substituted anilines [77] they showed that the η trend has the following order: 2-chloroaniline>2-fluoroaniline>2-methoxyaniline≈2-ethoxyaniline>2-ethylaniline>2-methylaniline.
For piperidine and its 6 derivatives [78], the authors reported that the inhibition performance has the following order: cis-2,6-dimethylpiperidine<3,5-dimethylpiperidine<2-methylpiperidine<3-methylpiperidine<piperidine<4-benzylpiperidine<4-methylpiperidine.
Finally, it was shown that in each case all compounds acted as mixed-type inhibitors.
Cruz et al.
[79] investigated 2-aminomethylbenzimidazole and bis(benzimidazol-2-ylethyl)sulphide as CIs for CS in deaerated 0.5mol/L HCl.
They reported that the former acts as a cathodic-type inhibitor and the latter as a mixed-type inhibitor.
Ait Chikh et al.
[80] studied the adsorption and inhibition properties of 1,12-bis(1,2,4-triazolyl)dodecane for CS in 1mol/L HCl.
This compound acted as a good cathodic-type inhibitor.
They suggested that adsorption of this compound occurs via synergistic effect between chloride ions and the positive quaternary ammonium ion moiety present in the inhibitor molecule.
However, the inhibition effect diminished in aerated compared with deaerated solution and with prolonged immersion time of the CS in HCl solution.
Abiola [81] reported that 3-(4-amino-2-methyl-5-pyrimidyl methyl)-4-methyl thiazolium chloride effectively prevents hydrogen evolution and corrosion of MS in 0.5mol/L and 5mol/L HCl at 30°C.
Elachouri et al.
[82] employed known surfactants, i.e.
some 2-(alkyl(CnHn+1)dimethylamonio)butanol bromides (n=11–15) as CIs for Fe (purity 99.5%) and showed that they are effective cathodic-type CIs.
Their η increased with an increased number of C atoms in the side chain and an increased CI concentration.
Tang et al.
[83] reported that 1-(2-pyridylazo)-2-naphthol is an effective mixed-type CI, with a predominant inhibition effect on the anodic reaction, for CS in 1mol/L HCl at 25–50°C.
Moreover, its η decreases with increasing T. Cruz et al.
[84] showed that the trend of the η has the following order: 1-(2-ethylamino)-2-methylimidazoline≫N-[3-(2-amino-ethylaminoethyl)]-acetamide>1-(2-ethylamino)-2-methylimidazolidine for CS at room T in deaerated 0.5mol/L HCl.
Ita and Offiong [85] reported that α-pyridoin is more effective than 2,2′-pyridil as a CI for MS (only a composition of 98% iron was specified) in 0.5mol/L HCl at 30 and 40°C.
The author also showed that these compounds prevent hydrogen evolution in 8mol/L HCl.
Moreover, the η of these CIs increases with increased concentration and with increasing T. Researchers attempt to increase the environmental acceptability of potential CI compounds by synthesizing new chemicals.
Some examples are given below.
Abd El Rehim et al.
[86] synthesized 4-(2′-amino-5′-methylphenylazo) antipyrine and tested it as a CI for MS in 2mol/L HCl at 20–60°C.
They reported that this compound is an effective mixed-type inhibitor and its η increases with increased concentration, but decreases with increasing T. Quraishi and Jamal have published numerous studies on this field.
They synthesized 3 fatty acid triazoles [21], i.e.
3-undecane-4-aryl-5-mercapto-1,2,4-triazole, 3(heptadeca-8-ene)-4-aryl-5-mercapto-1,2,4-triazole, and 3(deca-9-ene)-4-aryl-5-mercapto-1,2-4-triazole, and tested them as CIs for CRMS (cold rolled mild steel) and N-80 steel in 15% HCl at 28±2°C and boiling T (105±2°C).
The authors claim that these compounds are environmentally benign and have low toxicity.
They showed that all compounds act as effective mixed-type CIs and their η increases with increased inhibitor concentration.
Subsequently, Quraishi and Jamal [65] synthesized another compound, i.e.
4-salicylideneamino-3-hydrazino-5-mercapto-1,2,4-triazole, which they claim is eco-friendly and cost efficient.
The η of this compound was compared with propargyl alcohol in 15% HCl at 28±2°C and 105±2°C for CRMS and N-80 steel.
Propargyl alcohol was used as a standard CI for the acidization process.
The authors reported that the triazole compound acts as a mixed-type inhibitor, whereas propargyl alcohol acts as an anodic-type inhibitor for CRMS and as a cathodic-type inhibitor for N-80 steel.
Moreover, propargyl alcohol was more efficient for both steel materials at concentrations above 750mg/L.
On the other hand, the authors stated that the triazole compound does not produce toxic vapours like propargyl alcohol during the acidization process.
Furthermore, Quraishi and Jamal [87] synthesized three oxadiazoles, i.e.
2-undecane-5-mercapto-1-oxa-3,4-diazole, 2-heptadecene-5-mercapto-1-oxa-3,4-diazole and 2-decene-5-mercapto-1-oxa-3,4-diazole and tested them as CIs for MS in 15% HCl at 28±2°C and 105±2°C.
The authors reported that all these compounds act as effective mixed-type CIs and that 2-undecane-5-mercapto-1-oxa-3,4-diazole is the most effective among them.
Moreover, it was shown that 2-undecane-5-mercapto-1-oxa-3,4-diazole under the same experimental conditions is an effective CI for N-80 steel.
Next, Quraishi and Jamal [67] synthesized different dianils as condensation products of aromatic aldehydes and p-phenylenediamine, i.e.
2,4-didimethyl aminobenzyledene aminophenylene (DDAP), 2,4-divanilledene aminophenylene (DVAP), 2,4-disalicyledene aminophenylene (DSAP), 2,4-dibenzyledene aminophenylene (DBAP), and 2,4-dicinnamyledene aminophenylene (DCAP), and tested them as CIs for CRMS 15% HCl at 105±2°C.
They reported that the η at 3000mg/L CI concentration has the following order: DCAP>DBAP>DVAP>DDAP>DSAP.
Moreover, all these CIs acted as mixed-type inhibitors at 28±2°C.
DCAP was also tested for N-80 steel in the same solution and the authors reported that it behaves predominantly as an anodic-type CI at 28±2°C.
At 105±2°C, DCAP exhibited a high η, whereas its η decreased with increased immersion time from 0.5h to 6h.
Quraishi and Sardar [63] also synthesized different dithiazolidines, i.e.
3,5-diphenyl-imino-1,2,4-dithiazolidine (DPID), 3-phenyl-imino-5-chlorophenyl-imino-1,2,4-dithiazolidine (PCID), 3-phenyl-imino-5-tolyl-imino-1,2,4-dithiazolidine (PTID), and 3-phenyl-imino-5-anisidylimino-1,2,4-dithiazolidine (PAID), and tested them as CIs for MS in 1mol/L HCl.
The η of all CIs increases with increased concentration and has the following order: PAID>PTID>PCID>DPID at 25°C.
Furthermore, Quarishi et al.
[88] synthesized different thiosemicarbazides of fatty acid, i.e.
1-undecane-4-phenyl thiosemicarbazide, 1-heptadecene-4-phenyl thiosemicarbazide, and 1-decene-4-phenyl thiosemicarbazide, and tested them as CIs for CRMS in 1mol/L HCl.
They reported that the η of the CIs increases with increased concentration at 35°C and does not chance significantly by prolonging the immersion time and by raising the T from 35°C to 65°C, when a 500mg/L concentration of each CI was employed.
Quraishi et al.
[22] also synthesized dicinnamylidene acetone, disalicylidene acetone, and divanillidene acetone, and tested them as CIs for N-80 steel in 15% HCl.
The first two compounds were more effective than the latter at 105±2°C.
For all three compounds, the presence of KI as an intensifier increased their η.
The authors also reported that all three compounds act as mixed-type inhibitors in 15% HCl at 40±2°C.
Yildirim and Cetin [89] synthesised different acetamide, isoxazolidine, and isoxazoline derivatives with a long alkyl side chain and tested them as CIs for cold rolled low CS DIN EN 10130-99 in 2mol/L HCl containing 10% acetone at room T. They showed that newly synthesized compounds act as very effective CIs and almost all exhibit the highest η at 50mg/L concentration.
The disadvantage of the presented results for practical use in oil-field applications is that the inhibitors were introduced into the acid medium in 10% acetone and that corrosion products after the test were removed by emery paper (even though this was done gently).
Wang et al.
[90] studied corrosion inhibition of MS at 25°C in 1mol/L HCl by four synthesized mercapto-triazole compounds, which acted as efficient mixed-type inhibitors.
Ali et al.
[91] synthesized different isoxazolidines.
In 1mol/L HCl solution at 60°C they examined the corrosion inhibition properties of these compounds for two MS types and found good performance.
They claim that the presence of adjacent heteroatoms (N–O) with three lone pairs of electrons in isoxazolidine moiety invariably plays a dominant role in corrosion inhibition.
However, even though the η values are high, the reported corrosion current densities in inhibited solutions are high and consequently CR is suspected to be high as well.
Mernari et al.
[92] synthesized different 3,5-bis(n-pyridyl)-4-amino-1,2,4-triazoles (n=1, 2, 3) and tested their η for MS in 1mol/L HCl at 30°C.
They showed that these compounds act as effective anodic-type inhibitors whose η increases with increased inhibitor concentration.
Subsequently, some members of this research group showed that 2,5-diphenyl-1,3,4-oxadiazole, 3,5-diphenyl-1,2,4-triazoles 2,5-di(n-pyridyl)-1,3,4-oxadiazoles [93], and 1,4-bis(2-pyridyl)-5H-pyridazino [4,5-b]indole [94] could act as CIs for MS in 1mol/L HCl at 30°C.
Furthermore, Bentiss et al.
[95] and Lebrini et al.
[96] used 2 compounds considered to be non-cytotoxic substances as CIs for MS in 1mol/L HCl, i.e.
3,5-bis(2-thienyl)-1,3,4-thiadiazole and 3,5-bis(3-thienyl)-1,3,4-thiadiazole.
They showed that these compounds are mixed-type inhibitors with a predominant inhibition effect on the cathodic reaction.
Moreover, their η increases with increased concentration and T. Qui et al.
[97] synthesized 3 gemini surfactants (these contain two hydrophilic groups and two hydrophobic groups in the molecule), i.e.
1,2-ethane bis(dimethyl alkyl (CnH2n+1) ammonium bromide) (n=10, 12, 16) and tested them as CIs for A3 CS in 1mol/L HCl at 25°C.
The authors showed that these compounds are effective in preventing corrosion and their η increases with increased surfactant concentration, reaching the maximum value near the critical micelle concentration.
Badr [33] synthesized 3 thiosemicarbazide compounds and tested them as CIs for CS in 2mol/L HCl at 30°C.
They reported that the η of these compounds has the following order: 1-ethyl-4(2,4-dinitrophenyl) thiosemicarbazide>1,4-diphenylthiosemicarbazide>1-ethyl-4-phenylthiosemicarbazide, that they all act as mixed-type inhibitors, and that their η increases with increased concentration.
Saliyan and Adhikari [98] synthesized quinolin-5-ylmethylene-3-{[8-(trifluoromethyl)quinolin-4-yl]thio}propanohydrazide and tested it as a CI for MS in 1 and 2mol/L HCl at 30–60°C.
They reported that this compound acts as an anodic-type CI, and that its η increases with increased concentration and slightly decreases with increasing T. Yadav et al.
[17] synthesized 1-(2-aminoethyl)-2-oleylimidazoline and 1-(2-oleylamidoethyl)-2-oleylimidazoline and tested them as CIs for N80 steel in 15% HCl.
They showed that both compounds act as mixed-type inhibitors at 25°C and that the CR of N80 steel increases with increasing T from 25°C to 50°C, when both compounds are present at a concentration of 150mg/L.
Sathiya Priya et al.
[99] synthesized 1-cinnamylidine-3-thiocarbohydrazide and 1,1′-dicinnamylidine-3-thiocarbohydrazide and tested them as CIs for CS in 15% HCl.
They showed that these compounds act as mixed-type inhibitors at 30°C.
When T increased from 30°C to 110°C, the CR increased, but the η remained between 98.2% and 99.1%.
The authors also showed that these compounds effectively decrease the hydrogen permeation current compared with non-inhibited solution.
The hazards of most synthetic organic inhibitors is commonly known and the restrictive environmental regulations of many countries forced researchers to focus on developing cheap, non-toxic, and environmentally acceptable products.
Due to this reason, some researchers suggest using plant extracts as CIs, however the resulting corrosion inhibition effectiveness is usually found to be very low.
Raja and Sethuraman [100] examined some of these compounds in their review article.
However, they concluded that a phytochemical investigation of the extract is rarely carried out and seldom is it known which active ingredient present in the plant extract is responsible for corrosion inhibition.
Therefore, it is also likely that a mixture of constituents are acting as a CI [100].
Some examples are given below.
Ostovari et al.
[101] investigated the corrosion inhibition performance of henna extract (Lawsonia inermis) and its main constituents (Lawsone, gallic acid, α-D-glucose, and tannic acid) for MS in 1mol/L HCl.
Henna extract is considered a low cost, eco-friendly, and naturally occurring substance.
The authors showed that this extract is effective in preventing corrosion (also pitting), however, as the T increased from 25 to 60°C the η of henna extract decreased.
Moreover, they tested the extract’s constituents and concluded that all compounds act as mixed-type inhibitors and some of them also as oxygen scavengers.
They also reported that the η of these compounds has the following order: Lawsone>henna extract>gallic acid>α-D-glucose>tannic acid.
Satapathy et al.
[102] tested Justicia gendarussa plant extract as a CI for MS in 1mol/L HCl at 25–70°C.
They claim that the major components in this extract are β-sitosterol, friedelin, lupenol, phenolic dimmers, o-substituted aromatic amines (2-amino benzyl alcohol, 2-(2′-amino-benzylamino) benzyl alcohol), and flavonoids (6,8-di-C-α-L-arabinopyranosyl-4′,5,7-trihydroxyflavone, 6,8-di-C-α-L-arabinosylapigenine, 6-C-α-L-arabinopyranosyl-4′,5,7-trihydroxy-8-C-β-D-xylopiranosyl-flavone, and 6-C-α-L-arabinosyl-8-C-β-D-xylosylapigenine).
The authors reported that this extract acts as a mixed-type inhibitor.
With increasing concentration, the η increases; on the other hand, the η decreases with increasing T. They also reported that at 80°C it has little or no corrosion inhibition effect due to the decomposition of the extract’s compounds.
Ashassi-Sorkhabi et al.
[103] studied the corrosion inhibition effect of 3 amino acids, i.e.
alanine, glycine, and leucine for steel (the type of steel was not mentioned; see the composition in Table 1) in HCl solutions.
The authors used these compounds as they are non-toxic, relatively cheap, and easy to produce with purities greater than 99%.
These 3 amino acids acted as efficient CIs, but only if the inhibitor concentration was 1mmol/L or higher for alanine and glycine and 10mmol/L or higher for glycine.
Otherwise, the authors observed a corrosion promotion effect, most likely due to complexation with Fe.
One of the most important aspects in formulating the acid system is the need to ensure adequate corrosion inhibition while providing the desired reactivity with the formation material for stimulation purposes [19].
Organic compounds alone are usually not effective enough for corrosion control and a proper mixture containing additional intensifiers, surfactants, solvents, and co-solvents is needed [39].
This mixture is then called the corrosion inhibitor formulation (CIF).
Some authors refer to CIF as an inhibitor cocktail [9] or corrosion inhibitor package – CIP [104].
Hereinafter, the CIF term will be used.
On the other hand, most of the literature concerns single compounds as CIs for steel materials (see Table 2), however as single compounds, they are usually not effective enough in industrial applications.
In general, the CIF has several criteria for application.
As an acidizing inhibitor, the CIF must be stable (dispersive – not separated) in the acid for at least 24–72h, which is the duration of time the acid/CIF is stored onsite.
Additionally, it needs to be liquid over a wide temperature range for field use in both cold and hot climates and there should not be any separation or solidification issues.
Other criteria that are often required include the pour point (−20°C), shelf life (1year), viscosity, and other additive and H2S compatibility requirements.
The most important factors that have to be taken into account in CIF design are performance criteria, these include performance at various temperatures and pressures, exposure time, steel metallurgy, acid type and concentration, and the surfactant used.
The performance cost of a particular CIF can also be a very important decision factor.
Smith et al.
[24] reported that the CI cost needed in the acidizing process can be compared with the cost of the pipe, therefore care should be taken when deciding which and how much of the CI is necessary.
Commonly, the corrosion inhibition effectiveness is judged from the material mass loss after a certain time at a given temperature.
The authors would like to note that sometimes a data comparison can be misleading due to the different testing procedures employed [24].
Finding an effective CIF is a difficult task.
Usually this is done by determining the corrosion inhibition effectiveness of numerous single compounds.
If they perform as effective CIs, they are often used to develop a complex mixture together with other chemicals.
The goal is to improve the CIF’s η as compared with a single CI.
Therefore, in most cases this is performed by trial and error experiments on the basis of previous knowledge.
Moreover, CIF development must include a balance between environmental impact, cost, safety, and technical requirements [20,105].
In order to follow industrial recommendations strictly, no more than 2% w/V of active components is allowed for matrix acidification operations [18,30].
However, sometimes a CI concentration of up to 4% is used [27].
A commonly employed acceptable CR limit of the tested materials is 0.243kg/m2/test period and the pitting index of the tested material should not be higher than 3 (Table 3) [9,19,28,40].
Brondel et al.
[23] reported that a CR of 2.4–9.8kg/m2/year without pitting is acceptable.
A comprehensive study in 1978 of how CIF should be designed was given by Smith et al.
[24].
In general, a typical composition of a formulated inhibitor package contains all or some of the following components: active inhibitor substance(s), surfactant, solvent, and intensifier.
Depending on the application, other additives are also sometimes added.
Active inhibitor substances are mainly responsible for the inhibition of metal corrosion.
As discussed below in Section 5.6, the most common compounds in effective CIFs are acetylenic alcohols, α,β-unsaturated aldehydes, α-alkenylphenones, quaternary amines, and derivatives of pyridinium and quinolinium salts.
However, numerous compounds used as CIs for lower-grade steel materials are presented above and in Table 2, which could potentially be used as active corrosion inhibitor substances and formulated with surfactants, solvents, and intensifiers in order to develop an effective CIF (see below).
A surfactant is a surface active agent.
In this work a surfactant term will be used for compounds which improve the dispersability of the CI in the acid (as emulsifiers providing dispersed emulsion – not separated) while wetting the surface of the metallic material [14,20,24].
However, surfactants can offer corrosion protection themselves.
Some examples when the same compound was used as a surfactant or active corrosion inhibitor ingredient are given below.
Typical surfactants in the oilfield services industry are alkylphenol ethoxylates, e.g.
nonylphenol ethoxylate (NPE) [14,15,30,106,107].
However, NPEs have been banned from use in the North Sea because of their toxicity.
On the other hand, ethoxylated linear alcohols are more acceptable [20].
The quaternary ammonium salts and amines (when protonated) are the most used compounds of the cationic surfactants class, where the cation is the surface active specie.
As the amines only function as a surfactant in the protonated state, they cannot be used at high pH.
On the other hand, quaternary ammonium compounds, frequently abbreviated as “quats”, are not pH sensitive.
Long-chain quaternary ammonium bromides were also reported to work as efficient CIs for steel materials [106].
A frequently employed surfactant was N-dodecylpyridinium bromide (DDPB) [9,60,61,108,109].
Anionic sulphates, anionic sulphonates, alkoxylated alkylphenol resins, and polyoxyethylene sorbitan oleates are also useful surfactants.
Ali reported that a particularly useful surfactant is a blend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols [15].
Several examples of the surfactants used are given below in Section 5.6.
Solvents are mainly used for two purposes: to reduce viscosity for ease of handling and to ensure formulation stability in various environments.
Moreover, solvents have a similar purpose as surfactants, but with a different mechanism, i.e.
to improve CIF solubility and dispersability in the acid, and wettability on the acid–steel interface.
Flammability is an important factor when selecting a solvent for CIF in some regions, but not all.
On the other hand, we note that cost is a larger factor than flammability in solvent selection.
Hill and Romijn [20] reported that the usually employed solvents are toluene, xylem, and other aromatic solvent mixtures, however they are classified as products that cause tainting.
Therefore they would need to be replaced in the future.
We note that methanol and isopropanol are commonly used solvents.
Methanol is a very cost effective solvent, however it is a cumulative poison [20].
OSPARCOM (Oslo Paris Commission, see below) [110] accepts it for application as it poses little or no risk to the environment.
Isopropanol is an excellent solvent, but it has a low flash point and it needs to be labelled as flammable [20].
Moreover, low-molecular weight alcohols, glycols, dimethylsulphoxide, dimethylacetamide, 1-methyl-2-pyrrolidone, tetramethylene sulphone [15,111], formic acid, and formic acid derivatives such as dimethylformamide were also applied (the latter is classified as a mammalian mutagen).
Sometimes co-solvents are also employed.
An example of a co-solvent is HAN – heavy aromatic naphtha (a mixture of mainly C9 and C10 aromatic hydrocarbons – predominantly trimethyl benzenes, diethyl benzenes, and dimethyl ethyl benzenes) [112], which has oil-wetting characteristics [28].
Occasionally, alcohols or glycol ethers (ethylene–glycol-monobutyl ether – EGMBE, sometimes called a mutual solvent [30,39]), are added to the acid to improve acid penetration and clean-up [19].
However, they can reduce the CIs effectiveness, but lower parasitic consumption.
A mutual solvent is usually described as a chemical additive soluble in oil, water, and acid treatment fluids.
An intensifier (sometimes called an inhibitor aid [19]) is usually added to the CIF, because organic CIs frequently cannot provide adequate protection to steels at high temperatures and long exposure times [113].
Common intensifiers include formic acid (used from 0.5 to 10wt.% [62]; Brondel et al.
[23] reported 9% for deep sour wells), methyl formate, KI [22,114] (which can be used from 0.1 to 2.0wt.% [62]), CuI [15,16,40,64,111], CuCl [15] (when the η of the CI is low, Cu plating on the tubular occurs [10]), and metals ions (e.g.
from Sb2O3, SbCl3, Sb2O5, K4Sb2O7, K2H2Sb2O7, Sb2S3, SbCl5, K2Sb2(C4O6H4)2, Sb[(CH2OH)2]3 [13,16,28,64,111], BiCl3, BiI3, BiOCl, Bi2O3, BiOI3, BiF3, bismuth tartrate, bismuth adduct of ethylene glycol and bismuth trioxide, bismuth subsalicylate [14,62,64,111,115], SnCl2 [40], As3+, Cr6+, Cu2+, Ni2+, Sn2+, Hg2+ [116], calcium salts [64], and MgCl2 [111]).
It was reported that a 50:50 mixture of CuI and CuCl was much more effective than the use of individual components as an intensifiers [15].
Moreover, the reaction of insoluble Sb2O3 and Bi2O3 with HCl leads to the formation of soluble SbCl3 and BiCl3, respectively [64], which can then act as intensifiers.
It was also reported that Bi2O3 is an especially effective intensifier in combination with KI [40,62].
Sometimes calcium chloride or bromide and zinc bromide may be used at concentrations starting at 0.1% up to saturation [27].
Williams et al.
[111] also reported the possible use of Ca, AI, Mg, Zn, and Zr ions.
Furthermore, formamide or formic ester have also been employed [62].
Frenier [25] reported that propionic, propiolic, acetic, and chloroacetic acids, HI, and NaI can be used as intensifiers.
Hill and Jones [19] employed an antimony salt intensifier for SM25Cr steel.
Hill and Romijn [20] emphasized the need for a formic acid intensifier at T above 93°C.
However, lately it has been recommended to avoid formic acid in the CIF design due to the pipeline corrosion problems associated with its use [15].
Keeney and Johnson [40] claim that CuI significantly increases the η of ethyl octynol.
Williams et al.
[14,64] claim that the function of the metal compound is to produce metal ions which form complexes (a coordination or association of the metal) with, for example, the quaternary ammonium compound, and form a protective deposit on the metal tubulars and equipment.
On the other hand, they claim that Sb compounds are toxic, whereas Bi compounds have lower toxicity.
Moreover, Williams et al.
[64] claim that the concentration of the metal ions as intensifiers is preferred to be in the range of 1–1.5% of the total acid solution (with CIF inside the acid), due to economic reasons.
It is also interesting that Hill and Jones [19] used less CI (1.2%) compared with the intensifier (5%) to protect steel at 163°C.
The authors also claim that formic acid, antimony salts, and KI are acceptable intensifiers for the CIF design to be used in the North Sea.
Commonly, also other additives, along with the above-mentioned substances, are added to the acidizing fluids.
They do not have the purpose of inhibiting corrosion, but they can influence the corrosion inhibition performance of the known CIFs and increase the CR significantly.
These include iron control agents, water wetting agents, anti-sludge agents, non-emulsifiers, stabilizers, and viscoelastic surfactants.
Ferric iron forms a gelatinous mass precipitate, which prevents or slows down the flow through the channels.
This precipitated iron plug therefore decreases production.
Iron control agents are used to prevent this issue.
The source of iron comes from iron minerals, scale, and rusty tubular goods.
Iron control agents isolate or chelate the iron and therefore prevent the formation of the iron precipitate.
Nash-El-Din et al.
[30] suggested the use of iron control agents, which are added to HCl solutions to prevent the precipitation of ferric hydroxide once the acid is spent.
One way to prevent precipitation is to use reducing agents such as erythorbic acid.
Another approach is to use the chelating agents mentioned above or citric acid [10] and nitrilotriacetic acid and its sodium salt [30].
Sometimes a non-corrosive chelating solvent applicable for dissolving carbonate scale is added to the CIF, such as the ammonium salts of EDTA (ethylenediaminetetraacetic acid), HEDTA (N-(hydroxyethyl)-ethylenediaminetriacetic acid), and DTPA (diethylenetriaminepentaacetic acid) – an amount from 0.1% to 15% may be present in the CIF [19,27,117].
Wetting agents are employed to facilitate the penetration of the acid into the cracks and fissures in the scale, which helps remove the scale.
They are known as pickling accelerators and usually do not have a corrosion inhibition effect [118].
Some hydrocarbons may form acid sludge in the presence of live or spent acid mixtures.
Due to this reason, anti-sludge agents, which are surfactants, are added to the acidizing fluid to prevent sludge formation [24,119].
An anti-sludge agent is usually used in low concentration (no higher than 1.0wt.%).
Some anti-sludge agents can also act as non-emulsifiers.
Non-emulsifiers such as dodecylbenzylsulphonic acid (DDBSA) are employed to prevent the mixing of the acid and the extracted crude oil, therefore to prevent oil–acid emulsions.
Stabilizers are also added to the CIF to reduce the precipitation of the CIs on the rocks (some examples of stabilizers are given below by Walker [12]).
Sometimes different dispersing agents are needed to disperse the solution better.
Examples of dispersing agents are aromatic amines, aliphatic amines, and heterocyclic amines, such as aminophenol, aniline, chloroaniline, toluidine, diphenyl amine, picoline, alkyl pyridine, and n-octylamine [15,64].
Sometimes a viscoelastic agent is used to gel the system at intermediate pH levels [15], which eliminates the need for multiple stages.
The acid treating fluid is initially at a low pH and the viscoelastic agent has a very low viscosity, which makes the acid treating fluid easy to pump and flow into the pores and channels of the formation.
Upon acid reaction with the rock formation, the viscosity of the fluid increases due to the increase in the calcium ion content and pH, thus causing in situ gelling of the acid.
The higher viscosity of the gelled viscoelastic agent temporarily blocks the wormholes and channels formed in the formation, allowing the acid to divert to other untreated areas.
The viscosity of the gelled acid can be completely reduced by the introduction of a mutual solvent or by the produced hydrocarbons during flow-back.
Glycol and methanol are often added to flowing systems to decrease the corrosion activity of the aqueous solutions.
It was also assumed that consequently a change in the CO2 corrosion mechanism occurs [120].
As mentioned, the inhibitors frequently used for CIF design in acidizing procedures include acetylenic alcohols, α-alkenylphenones, α,β-unsaturated aldehydes, quaternary amines, and derivatives of pyridinium and quinolinium salts.
They are commonly formulated with solvents, surfactants, and intensifiers.
Some examples are given below.
Beale and Kucera [121] tested different combinations of acetylenic alcohols (preferably those in Fig.
1) as CIFs for C1010 MS in HCl, H2SO4, sulphonic, phosphoric, and acetic acids at 93.3°C.
These combinations allowed the use of smaller total amounts of the inhibitor.
They claim that there is an advantage in mixing more than 2 compounds and that the most pronounced effect was observed when the acetylenic alcohols were present in substantially equal amounts.
The preferred combinations comprise a lower molecular mass compound containing 3–6 carbon atoms and a higher molecular mass compound containing about 7–11 carbon atoms.
Keeney and Johnson [40] claim that a CIF consisting of nitrogen-containing compounds or acetylenic alcohol compounds or their mixtures, and CuI (at 25–25,000mg/L by weight) is effective for ferrous materials corrosion protection in HCl, H2SO4, HF, acetic acid, and mixtures thereof at 65.5–232.2°C.
Among the acetylenic alcohols, they suggested hexynol, dimethyl hexynol, dimethyl hexynediol, dimethyl hexynediol, dimethyl octynediol, methyl butynol, methyl pentynol, ethynyl cyclohexanol, 2-ethyl hexynol, phenyl butynol, and ditertiary acetylenic glycol, butynediol, 1-ethynylcyclohexanol, 3-methyl-1-nonyn-3-ol, 2-methyl-3-butyn-2-ol, 1-propyn-3-ol, 1-butyn-3-ol, 1-pentyn-3-ol, 1-heptyn-3-ol, l-octyn-3-ol, 1-nonyn-3-ol, 1-decyn-3-ol, and 1-(2,4,6-trimethyl-3-cyclohexenyl)-3-propyne-l-ol.
Instead of acetylenic alcohols, acetylenic sulphide-type molecules may also be employed, with the general structure given in Fig.
2, such as dipropargyl sulphide, bis(1-methyl-2-propynyl) sulphide, and bis(2-ethynyl-2-propyl) sulphide.
For nitrogen-containing compounds, they suggested amines such as mono-, di-, and tri-alkyl amines having 2–6 carbon atoms in each alkyl moiety, such as ethylamine, diethylamine, triethylamine, propylamine, dipropylamine, tripropylamine, mono-, di-, and tri-butylamine, mono-, di-, and tri-pentylamine, mono-, di-, and tri-hexylamine, and isomers of these, such as isopropylamine and tertiarybutylamine.
They also suggested the six-membered N-heterocyclic amines, e.g.
alkyl pyridines, having 1–5 nuclear alkyl substituents per pyridine moiety, with alkyl substituents having from 1 to 12 carbon atoms and preferably those having an average of six carbon atoms per pyridine moiety.
Growcock and Frenier [108] tested trans-cinnamaldehyde as a CI for API J55 steel in HCl solution at 65°C.
They also tested the synergistic effect of trans-cinnamaldehyde with 3 surfactants, i.e.
N-dodecylpyridinium bromide (DDPB), the adduct of trimethyl-1-heptanol with 7mol of ethylene oxide (THEO), and Polystep A18 (commercial name), which is a sulphonate.
The authors proposed that trans-cinnamaldehyde adsorbs onto protonated active sites to form a tenacious surface species, which serves as a primary barrier to mitigate corrosion.
Moreover, they claimed that trans-cinnamaldehyde subsequently polymerizes on the surface and that the time-dependent polymerisation may be initially assisted by the surfactants.
This was confirmed in later studies [122,123].
Subsequently, Growcock et al.
[109] performed a similar study using different derivatives of cinnamaldehyde and showed that these compounds can act as effective CIs, especially when formulated with the above-mentioned surfactants.
Growcock [60] also showed that the mixture of α-alkenylphenone and DDPB effectively protects API J55 steel from corrosion in HCl solution at acid concentrations up to 28.3% and 95°C.
In addition, Frenier et al.
[27] also observed that this class of alkenylphenones is effective in 15–28% HCl at 65°C, which have the following structure: where in R1 is an unsubstituted or inertly substituted aryl of 6–10 carbon atoms, and R2 and R3 are the same or different and each can be hydrogen, halogen, or an inertly substituted aliphatic of about 3–12 carbon atoms, and R2 may also be an alkanol, ether, or unsubstituted or inertly substituted aryl of 6–10 carbon atoms, provided that the total number of carbon atoms in the alkenylphenone does not exceed 16.
Inert substituents means that they do not have an effect on the corrosion inhibition of the corresponding unsubstituted alkenylphenone.
The formulation containing alkenylphenone preferably includes a surfactant at concentrations up to 2%.
They suggested surfactants such as THEO, DDPB, 4-decylated oxydibenzenesulphonate, and coco beta-amino propionate.
Furthermore, Frenier et al.
[61] demonstrated that octynol without surfactant protects J55 CS effectively at T up to 93°C.
Moreover, they reported that CIF containing 2-benzoyl-allyl alcohol, 2-benzoyl-3-methoxy-1-propene, 2-benzoyl-1,3-dimethoxy-propene, and 5-benzyol-1,3-dioxane in combination with the surfactants DDPB and THEO protect J55 CS effectively at T up to 93°C.
Frenier [25] also observed that mixtures of alkenylphenones (Fig.
3) and N-substituted quinolonium salts (Fig.
4) are effective CIFs for iron and steel corrosion protection over a broad range of HCl concentrations and at T up to 200°C in HCl, HF, H2SO4, H3PO4, formic acid, acetic acid, citric acid, and their mixtures.
R4 in Fig.
4 is an alkyl group of about 4 to about 16 carbon atoms, or an alkylaryl of about 7 to about 20 carbons, and X is chlorine or bromine.
The author preferred quinolinium salt composed of 1-(α-naphthylmethyl)-quinolinium chloride.
This CIF may also contain the same surfactant as used before [27] and an intensifier such as propionic, propiolic, formic, acetic, and chloroacetic acids or halide ions.
Moreover, this CIF may also contain EDTA, an ammonium salt of EDTA, HEDTA, and DPTA chelating agents.
Jasinski and Frenier [62] suggested a CIF designed for steel with Cr content higher than 9%, composed of phenyl ketone, phenyl ketone with a quaternary salt of a nitrogen-containing heterocyclic aromatic compound, or cinnamaldehyde (cinnamaldehyde can be employed substituted or unsubstituted) with a quaternary salt of a nitrogen-containing heterocyclic aromatic compound and an acid soluble metal from antimonium or bismuth (such as Bi2O3) salts.
Moreover, HCOOH or its derivatives may be employed to even increase the performance of the CIF, especially when Sb ions are present, preferably from Sb2O3 and SbCl3.
This CIF was designed for HCl or a mixture of HCl/HF at temperatures above 121°C up to 246°C.
The phenyl ketones may be C9-20 α-alkenylphenones or hydroxyalkenylphenones and their mixtures.
Among them, the authors suggested 2-benzoyl-3-hydroxy-1-propene, 2-benzoyl-3-methoxy-1-propene, and phenyl vinyl ketone.
As a nitrogen-containing heterocyclic aromatic quaternary salt, a pyridinium, quinolinium, isoquinolinium, benzoazolinium, or benzothiazolinium salt may be used.
They especially suggested N-cyclohexylpyridinium bromide, N-octylpyridinium bromide, N-nonylpyridinium bromide, N-decylpyridinium bromide, N-dodecylpyridinium bromide, N,N-dodecyldipyridinium dibromide, N-tetradecylpyridinium bromide, N-laurylpyridinium chloride, N-dodecylbenzylpyridinium chloride, N-dodecylquinolinium bromide quinolinium-(l-naphthylenemethyl) chloride, and N-naphthylmethyl quinolinium chloride.
Of these, the authors prefer naphthylmethyl quinolinium chloride and dodecylpyridinium bromide.
Finally, it was noted in the paper that the CIF combinations of the above-mentioned inhibitors with Cu2Cl2/KI were more effective compared with combinations with Cu2Cl2/HCOOH.
Fernier and Growcock [117] also proposed a CIF composed of an α,β-unsaturated aldehyde (Fig.
5) and a surfactant which is effective for ferrous materials as well as for Al, Zn, and Cu in aqueous acids such as HCl, HF, H2SO4, H3PO4, formic acid, acetic acid, citric acid, and their mixtures.
This CIF is also effective for the above-mentioned materials in alkaline solutions and brines.
R1 in Fig.
5 represents a substituted or non-substituted saturated or unsaturated aliphatic hydrocarbon group containing from about 3 to about 12 carbon atoms with or without one or more non-interfering substituents, an aryl group (e.g.
phenyl, benzyl, or the like), or an aryl group containing one or more non-interfering substituents.
R2 in Fig.
5 represents hydrogen, a saturated or unsaturated aliphatic hydrocarbon group containing from 1 to about 5 carbon atoms with or without one or more non-interfering substituents, an aryl group, or a substituted aryl group containing one or more non-interfering substituents.
R3 in Fig.
5 represents hydrogen, a saturated or unsaturated aliphatic hydrocarbon group containing from about 3 to about 12 carbon atoms with or without one or more non-interfering substituents, and an aryl group with or without one or more non-interfering substituents.
The total number of carbon atoms in the substituents represented by R1, R2, and R3 range from 1 to about 16, and preferably from about 5 to about 10.
The non-interfering substituents which may replace hydrogen on the α- and β-carbon atoms of the aldehydes in Fig.
5, or which are found in the hydrocarbon substituents which replace hydrogen on these carbon atoms, have no adverse effect on the corrosion inhibition and are, e.g.
lower alkyl (containing from 1 to about 4 carbon atoms), lower alkoxy (containing from 1 to about 4 carbon atoms), halo, i.e.
fluoro, chloro, bromo, or iodo, hydroxyl, dialkylamino, cyano, thiocyano, N,N-dialkylcarbamoylthio, and nitro substituents.
The authors also expanded on the use of surfactants that can be employed in CIFs.
These can be of the anionic, cationic, non-ionic, and amphoteric types.
Examples of surfactants in this reference were: alkylsulphates, such as sodium alkyl sulphate, alkyl aryl sulphates, such as polypropylene benzene sulphonates, and dialkyl sodium sulphosuccinates, such as dioctyl sodium sulphosuccinate, N-cyclohexylpyridinium bromide, N-octylpyridinium bromide, N-nonylpyridinium bromide, N-decylpyridinium bromide, N-dodecylpyridinium bromide, N,N-dodecyldipyridinium dibromide, N-tetradecylpyridinium bromide, N-laurylpyridinium chloride, N-dodecylbenzylpyridinium chloride, N-dodecylquinolinium bromide quinolinium-(1-naphylenemethyl)chloride, monochloromethylated and bischloromethylated pyridinium halides, ethoxylated and propoxylated quaternary ammonium compounds, sulphated ethoxylates of alkyl phenols and primary and secondary fatty alcohols, didodecyldimethylammonium chloride, hexadecylethyldimethylammonium chloride, 2-hydroxy-3-(2-undecylamidoethylamino)-propane-l-triethylammonium hydroxide, 2-hydroxy-3-(2-heptadecylamidoethylamino)-propane-1-triethylammonium hydroxide, 2-hydroxy-3-(2-heptadecylamidoethylamino)-propane-1-triethylammonium hydroxide, primary amines, secondary amines, tertiary amines (e.g.
dodecyl dimethyl amine), ethoxylates of alkyl phenols, primary fatty alcohols, secondary fatty alcohols, polyoxyethylenepolyoxypropylene block copolymers, and coco-β-aminopropionate.
Gao et al.
[124] showed that different α,β-unsaturated carbonyl compounds (cinnamaldehyde, benzalacetone, phenyl styryl ketone) formulated with propargyl alcohol act as very efficient CIFs for N80 steel in 20% HCl at 90°C.
The authors claim that the main reason for the high η at elevated temperatures is the polymerization and adsorption of these compounds on the steel surface.
Moreover, Sastri [125] pointed out that commercial CIFs for use at high temperatures invariably contain acetylenic alcohols.
The high η of propargyl alcohol is attributed to the iron complex catalysed formation of protective polymer films, which is favoured at high temperatures.
Williams et al.
[16,64,126] claimed that a CIF containing quaternary ammonium compound, metal ions, a highly polar aprotic solvent, and a surfactant is effective in mitigating corrosion of well construction steel (N-80, Cr 2205 [64] and J-55, P-105, Cr-9, Cr-13, Cr-2205, and Cr-2250 [16,126]) during the acidizing treatment with HCl, HF, formic acid, acetic acid, and/or their mixtures.
The preferred quaternary ammonium compounds in this CIF are the following: alkyl pyridine-N-methyl chloride quaternary, alkyl pyridine-N-benzyl chloride quaternary, quinoline-N-methyl chloride quaternary, quinoline-N-benzyl chloride quaternary, quinoline-N-(chloro-benzyl chloride) quaternary, isoquinoline quaternaries, benzoquinoline quaternaries, chloromethyl naphthalene quaternaries, and chloromethyl naphthalene quinoline quaternaries.
Among the metal compounds, they conclude that they should be present at concentrations of at least 0.08wt.% (0.04wt.% in the case of Sb-compound [16,126]), Sb- [16,126], Bi-, Ca-, and Cu(I)-compounds are the most preferable.
Williams et al.
[111] also suggested a mixture of at least two metal ions, where the first metal compound is selected from an antimony, bismuth, and cuprous compound and the second metal ion is selected from Ca, AI, Mg, Zn, and Zr ions.
The suggested highly polar aprotic solvents are dimethyl formamide, dimethylsulphoxide, dimethylacetamide, 1-methyl-2-pyrrolidone, tetramethylene sulphone, and their mixtures.
These solvents may be blended with (most preferably) dimethyl formamide.
This CIF may also contain a dispersant such as an organic amine (including aromatic amines, aliphatic amines, and heterocyclic amines).
Of these, the authors prefer aminophenol, aniline, chloroaniline, toluidine, diphenyl amine, picoline, alkyl pyridine, or n-octylamine.
As a surfactant, the authors suggested ethoxylated alkyl phenols, ethoxylated aliphatic alcohols, polyethylene glycol esters of fatty, resin, and tall oil acids.
Furthermore, Williams et al.
[14] described a CIF containing bismuth compound (0.4–1.4%), a quaternary ammonium compound (0.4–2.2%), and a surfactant (0.1–1.5%) to inhibit corrosion in well construction steels (such as N-80, J-55, P-105, Cr-9, Cr-13, Cr-2205, and Cr-2250) in HCl, HF, or their mixtures.
The concentrations in brackets represent the most preferable content in the CIF.
This CIF was designed to avoid a toxic combination of Sb-compounds and acetylenic alcohols [111].
The preferred quaternary ammonium compounds in this CIF are the same as described above for [64], except for chloromethyl naphthalene quinoline quaternaries.
The authors suggested that the most preferable compounds are those containing a benzyl group.
The quaternary compound:Bi ratio may be used in molar ratios of 1:1–5:1.
The same surfactant as mentioned above is also suggested for use in this CIF [64].
Coffey et al.
[107] described two CIFs and claim that they are effective for ferrous materials in hydroxyacetic, acetic, propionic, formic, HCl, HF, H2SO4, and H3PO4 acids and their mixtures, especially in the presence of H2S.
The first CIF includes a formaldehyde or paraformaldehyde (the latter is preferred), an acetophenone or its derivatives, a cyclohexylamine or its derivatives (e.g.
2-methyl cyclohexylamine or 2,4-dimethyl cyclohexylamine) and optionally, an aliphatic carboxylic, an acid such as octanoic acid, myristic acid, pelargonic acid, lauric acid, oleic acid, and tall oil.
The second CIF includes an acetylenic alcohol or a mixture thereof (such as 1-propyn-3-ol, l-butyn-3-ol, 1-pentyn-3-ol, 1-hexyn-3-ol, 1-heptyn-3-ol, l-octyn-3-ol, 1-nonyn-3-ol, 1-decyn-3-ol, or 1-octyn-4-ethyl-3-ol), an excess of formaldehyde, and optionally a surfactant (a non-ionic one is preferred, such as ethoxylated alkanols or ethoxylated alkyl phenols) and alcohols with 1–4 carbon atoms (they preferred isopropanol).
Walker [28] described a CIF for acidic solution in acidizing subterranean formations with ferrous metal well bores at T of 65.5–260°C.
The acidic solutions that were described as mineral acids are HCl, or mixtures of HCl with HF, acetic acid, formic acid, or HF, H2SO4, formic acid, acetic acid, and their mixtures.
The formulation was based on one or more acetylenic alcohols (5–35% of the amount of the formulation) with the structure given in Fig.
6, a quaternary ammonium compound, an aromatic hydrocarbon having high oil-wetting characteristics, and any antimony compound which is capable of activation by the other constituents of the CI.
The structure of these acetylenic alcohol compounds is different than the one above in Fig.
1 proposed by Beale and Kucera [121].
Walker [28] proposed that the acetylenic alcohols employed having the general formula shown in Fig.
6, where R1, R2, and R3 are hydrogen, alkyl, phenyl, substituted phenyl, or hydroxy-alkyl radicals.
He suggested that preferably R1 comprises hydrogen, R2 comprises hydrogen, methyl, ethyl, or propyl radicals, and R3 comprises an alkyl radical having the general formula CnH2n, where n is an integer from 1 to 10.
He proposed acetylenic alcohols such as methyl butynol, methyl pentynol, hexynol, ethyl octynol, propargyl alcohol, benzylbutynol, and ethynylcyclohexanol.
The most preferable selection among them was hexynol, propargyl alcohol, methyl butynol, and ethyl octynol.
Among quaternary ammonium compounds, Walker suggested the same as Williams et al.
[64] (see above), except quinolone-N-(chloro-benzyl chloride) quaternary and chloromethyl naphthalene quinoline quaternaries.
As a hydrocarbon compound, which exhibits high oil-wetting characteristics, Walker suggested xylenes, saturated biphenyl-xylenes admixtures, HAN (heavy aromatic solvent), tetralene, tetrahydroquinoline, and tetrahydronaphthalene.
The antimony compound, preferably at 0.7–40mmol/L concentration, can be comprised of antimony trioxide, antimony pentoxide, antimony trichloride, antimony sulphide, antimony pentachloride, potassium antimony tartrate, antimony tartrate, antimony trifluoride, potassium pyroantimonate, antimony adducts of ethylene glycol, solutions containing ethylene glycol, water and the oxidized product of hydrogen peroxide, and antimony trioxide or any other trivalent antimony compound.
Walker also suggested that this CIF can be dissolved in an alkanol solvent such as methyl, ethyl, propyl, isopropyl, butyl, pentyl, hexyl, heptyl, or octyl alcohol.
This CIF can also comprise a non-ionic surfactant, which facilitates the dispersion of the CI in the acidic solution, such as ethoxylated oleate, tall oils, or ethoxylated fatty acids, preferably at volumes up to 20%.
The CI could be generated in situ in the acidic solution, if so, then Walker suggested mixing all the constituents prior to addition of the antimony compound.
Additionally, Walker [12] used the same CIF as above [28], but with the addition of a stabilizer, which substantially prevents the precipitation of solubilized antimony-containing compounds from aqueous solutions and mitigates steel corrosion at T 65.5–260°C.
The stabilizer can be one of the reactive fluoride-containing compounds, compounds having α- or β-hydroxy organic acid functional groups, or non-organic acid polyhydroxy compounds having 3–9 carbon atoms.
The examples of the fluoride-containing stabilizers are HF, ammonium bifluoride, sodium fluoride, potassium fluoride, ammonium fluoride, transition metal fluorides, rare earth fluorides, and alkaline earth fluorides.
The compounds having α-hydroxy or β-hydroxy organic acid functional groups are citric acid, citric acid salts, tartaric acid, tartaric acid salts, glycolic acid, glycolic acid salts, lactic acid, lactic acid salts, 3-hydroxyl propionic acid, 3-hydroxyl-butanoic acid, and 3,4-dihydroxy-1,6-hexanedioic acid.
The suggested non-organic acid polyhydroxy compounds are sorbitol, glycerol, glucose, mannose, ribitol, erythritol, mannitol, perseitol, iditol, altritol, and xylitol.
This stabilizer may be admixed with the acidic solution either before or after the addition of the antimony compound.
Walker [13] suggested a CIF which contains several components.
Due to the numerous compounds possible for such a CIF design, it is recommended that readers themselves review the patent published.
In general, the patent covers a CIF with the following criteria: (a) a compound having at least one reactive hydrogen atom and having no groups reactive under reaction conditions other than hydrogen, (b) a carbonyl compound having at least one hydrogen atom on the carbon atom adjacent to the carbonyl group, (c) an aldehyde, and (d) a fatty compound and an acid source which is admixed with a source of antimony ions.
The main purpose was to avoid the usage of acetylenic alcohols.
Ali et al.
[15] disclosed a treatment fluid for iron-containing materials comprising a mineral acid, a viscoelastic surfactant gelling agent, and a CI system containing at least one of the following: an alkenylphenones (Fig.
3) or α,β-unsaturated aldehyde (Fig.
5, cinnamaldehyde or its derivatives have been found to be particularly effective), an unsaturated ketone or unsaturated aldehyde other than the alkenylphenones and α,β-unsaturated aldehyde, a dispersing agent (such as an organic amine, also used before [64]), an extender (iodine) and an alcohol solvent.
This CIF may also contain an intensifier mixture of CuI and CuCl.
As the viscoelastic surfactant, they proposed erucylamidopropyl betaine surfactant.
This CIF was designed to achieve a formic acid-free mixture, which it is claimed causes potential pipeline corrosion problems.
Baddini et al.
[18] reported CIFs based on cinnamaldehyde, benzalacetone, and chalcone with propargyl alcohol, which were effective in reducing steel corrosion in 20% HCl at 90°C.
Barmatov et al.
[9] studied different CIF combinations or individual compounds and their influence on the corrosion behaviour of HS80 and HS110 LCS (low carbon steel) in 14% HCl at 78°C.
They reported that the η of the surfactant DDPB as a CI increases sharply up to approximately 1mmol/L concentration.
For the cationic surfactant benzyldimethylhexadecylammonium chloride, they found that the η increases with increased concentration and decreases with an elevation of temperature from 40°C to 78°C.
Next, they determined the CR trend of some commercially available components as 2,2′-biquinoline>tripropargyl amine>3-butyn-1-ol>3-octyn-1-ol.
For these compounds, it was reported that they all act as mixed-type inhibitors and that their η increases with increased inhibitor concentration.
Similarly as before by Gao et al.
[124] and Frenier et al.
[61], Barmatov et al.
[9] claimed that some acetylenic alcohols in combination with α-alkenylphenones and α,β-unsaturated aldehydes may initiate surface polymerization, regarding which many authors believe that currently there is no alternative for the protection of oil well equipment during acid stimulation.
Moreover, Barmatov et al.
[9] pointed out, by investigating oil soluble 1-octyn-3-ol and 4-ethyl-1-octyn-3-ol and water soluble propargyl alcohol, that η increases with increased chain length of the polymerizable acetylenic alcohols.
Additionally, they stated that acetylenic alcohols in combination with quinolone-based quaternary ammonium compounds, a surfactant, and formic acid provide acceptable corrosion control at 104–177°C Finally, they reported that propargyl alcohol or 4-ethyl-1-octyn-3-ol in combination with DDPC show strong synergism.
Fischer and Parker [104] presented anhydrides derived from tall oil fatty acids (TOFA) and claim that they provide enhanced corrosion inhibition protection compared with traditional dimer/trimer acids (composed of 36 and 54 carbon atoms, respectively), due to the more tenacious film formed in the former case.
These anhydrides are made by reacting maleic anhydride with the unsaturated fatty acids present in TOFA.
The TOFA anhydride was neutralized with a fatty acid imidazoline.
The authors reported that only one-seventh to one-tenth of the dosage of TOFA anhydride was required compared with that of an equivalent dimer/trimer-based active inhibitor to impart 90% corrosion protection in sweet and sour environments.
Otherwise, we also noted that a commercially available CIF based on tall oil and established for HCl containing 1–5% methanol, 5–10% metyl formate, 10–30% formic acid, 30–60% ethoxylated tall oil (which contains palmitic, linoleic, and oleic acids).
Nasr-El-Din et al.
[30] reported several different CIFs consisting of the mixtures of the following: (a) quaternary amines (15–40wt.%), acetylenic alcohols (1–10wt.%), prop-2-yn-1-ol (1–10wt.%), naphthalene (1–5wt.%), aliphatic hydrocarbons (30–60wt.%), and propan-2-ol (5–10wt.%), (b) a mixture containing quaternary ammonium salts (11–30wt.%), benzyl chloride quaternary ammonium compound (11–30wt.%), propargyl alcohol (1–10wt.%), dimethyl formamide (1–10wt.%), cuprous iodide (1–10wt.%), ethoxylated nonylphenol (1–10wt.%), and isopropanol solvent (10–30wt.%), and (c) a mixture containing quaternary amines (10–20wt.%), formamide (20–40wt.%), acetylenic alcohols (5–10wt.%), 2-propyn-1-ol (5–10wt.%), ethoxylated nonylphenol (5–10wt.%), pine oil (1–5wt.%), and methanol and isopropanol solvent (20–40wt.%).
The authors claim that these CIFs are commonly used for HCl stimulation procedures.
Singh and Dey [116] studied the corrosion inhibition synergistic effect of propargyl alcohol with different inorganic cations and organic compounds for CRMS in 18% HCl at 33°C and 102°C.
They reported a synergistic effect of propargyl alcohol with As3+, Cr6+, Cu2+, Ni2+, Hg2+, and Sn2+, which was dependent on both the propargyl alcohol and cation concentration.
On the other hand, solutions containing all these ions alone except Sn2+ induced a higher CR at 33°C compared with the solution containing propargyl alcohol.
A synergistic effect was also found when propargyl alcohol was formulated with phenol, formaldehyde, and sodium hypophosphide, but less when formulated with O-aminobenzoic acid.
Gao et al.
[124] reported that cinnamaldehyde, benzalacetone, and chalcone, each formulated with propargyl alcohol, show a synergistic effect for reducing N80 steel CR in 20% HCl at 90°C.
The authors claim that the main reason for the high corrosion η at elevated temperatures is the polymerization and adsorption of these compounds on the steel surface.
Kumar and Vishwanatham [127] tested three mixtures, i.e.
formaldehyde:phenol, formaldehyde:o-cresol, and formaldehyde:p-cresol (all with a ratio of 1:2) as CIFs for N80 steel in 15% HCl at 25–115°C.
They showed that all mixtures act as effective mixed-type inhibitors by predominantly reducing the rate of anodic reaction.
Moreover, by using differential scanning calorimetry, the authors pointed out that the thermal stabilities of solutions containing cresol extend up to 200°C.
It should be noted that the usage of formaldehyde products is a problem due to their environmental unacceptability.
As noted above, formaldehyde has been employed for various CIF designs in order to minimize hydrogen penetration into the steel [18].
Kumar and Vishwanatham [127], Baddini et al.
[18], and Cardoso et al.
[70] presented a few such examples.
However, Hill and Romijn [20] reported that formaldehyde is an animal carcinogen, therefore this limits its practical use.
Hill and Romijn [20] suggested the following chemistry for CIF design: (a) a mixture of phenyl vinyl ketones and acetylenic alcohols with the addition of potassium iodide and formic acid for J55, N80, and L80 steels at T up to 149°C, (b) formulation based on quaternary amine chemistry and cinnamaldehyde (also confirmed by Growcock et al.
[109] with the addition of potassium iodide and formic acid, and nonylphenol ethoxylated or ethoxylated linear alcohol-based surfactant at T up to 121°C, (c) a mixture of phenyl vinyl ketones with potassium iodide and formic acid and a nonylphenol ethoxylated-based surfactant and toluene as a solvent and used for 13Cr steel, (d) quinolinium and pyridinium salts with antimony chloride [13,62], and (e) Mannich condensation product or quaternary salt with acetylenic alcohols, such as propargyl alcohol, 1-hexyl-3-ol (suggested also by Schmitt [66] and Sastri [128]) and 4-ethyl-1-octyn-3-ol, however the former two are very toxic by skin adsorption.
Mannich bases are made by condensation of amines (mainly primary amines) with an aldehyde (mainly formaldehyde) and a ketone [118].
Sastri [128] and Schmitt [66] suggested the use of the following: (a) mixtures of N-containing compounds, acetylenic compounds, and surfactants, (b) condensation products of amines and aldehydes, (c) C12–C18–primary amines, cyclohexylamine, aniline, methylanilines, alkylpyridines, benzimidazole, and rosin amine condensed with formaldehyde, and (d) acetylenic inhibitors with Fe ions.
As briefly mentioned, several components utilized in CIF have come under scrutiny regarding environmental health and safety (EH&S) issues.
The emphasis is especially on finding environmentally acceptable acid CIs at elevated temperatures for acidizing environments.
It is important to find non-toxic chemicals, with high biodegradability and reduced bioaccumulation.
Environmental acceptability is usually assessed by the national regulations of a particular country.
In particular, the North Sea is known for having the most stringent criteria regarding chemical qualifications.
Most of the developed CIFs for the conventional acids no longer satisfy the OSPARCOM requirements, because their primary active ingredients may be harmful if discharged into the environment.
OSPRAMCOM has the ultimate goal of replacing all environmentally hazardous chemical discharges by 2020 [110].
This presents a big problem for the existing CIFs mainly used for HCl and forces industry to replace or reformulate them.
For example, CIFs containing acetylenic and antimony compounds present serious problems due to their high toxicity [64].
The goal of many research studies is to present reliable corrosion data to oilfield service companies in order to test CIF acceptability in large-scale operations in real field trials.
It has to be emphasized that oilfield service companies are very interested in using safer and more environmentally acceptable alternatives than those currently employed, especially to satisfy OSPARCOM requirements [20].
In addition, particular countries are also now generating criteria for what can be classified as environmentally friendly.
For example, WGK (German Wassergefährdungsklassen) stands for The German Water Hazard Class.
The national German regulation, VwVwS (German Verwaltungsvorschrift wassergefährdende Stoffe), describes the water hazard classification such that all substances are either classified as non-hazardous to water or assigned to one of the three classes, WGK 1, WGK 2, and WGK 3, implying increasing water hazard.
The lowest class, WGK 1, may seem relatively harmless and close to non-hazardous.
However, if only one compound in the CIF design is in the WGK 2 or WGK 3 class and the others in WGK 1, the whole solution is classified as the higher WGK rating.
The employment of more environmentally acceptable chemicals does not necessary mean that they will be less effective, but this is usually connected to the increased time needed to find a solution.
However, most of the CIs that are still in use have hazardous effects on the environment [129].
Acids frequently represent a potential danger for drilling crews and the environment.
For drilling-crews it is particularly important that fluids do not cause health problems, i.e.
dermal toxicity, eye irritation, skin sensitisation, and mutagenicity.
However, conventional acids frequently cause these problems.
For example, HCl forms calcium chloride brine, which has been reported to cause skin injuries to workers in the oil industry [116].
Moreover, inhibitors for HCl are frequently effective only at high concentrations and are extremely toxic, causing handling and waste disposal problems, and producing toxic vapours under acidizing process conditions [61].
The dangers of using HF are also well known; e.g.
the release of HF in a Nevada desert created a so-called death cloud.
It was reported that 16million Americans are potentially in a “kill zone” due to refineries using HF.
Moreover, the very low LD50 value for chloroacetic acid (76mg/kg for rats), which can also easily penetrate skin, is not safe to handle.
The disadvantage of using formic and acetic acids is their volatility, which makes them difficult to handle.
Due to the above listed potential problems and disadvantages of using conventional acids, safer, environmentally more acceptable, and less corrosive alternatives are currently being sought.
An inhibited MSA solution could be one of them.
MSA is completely miscible in water and can be applied as a liquid over a wide temperature range.
It is a strong organic acid (pKa=−1.9) and has no tendency to either oxidize or reduce organic compounds.
It has very low vapour pressure and a high boiling point, thus it is odour-free and evolves no dangerous volatiles.
MSA salts are highly soluble, therefore it can be used in the well acidizing procedure.
Moreover, MSA has low toxicity to aquatic life and is biodegradable within 28days (which is a requirement for a chemical to be used in the North Sea [20]), with only CO2 and sulphates being formed [130].
MSA is also present in the natural environment as part of the biogeochemical sulphur cycle, where atmospheric dimethyl sulphide arising from marine algae, cyanobacteria, and salt marsh plants is photochemically oxidized, leading to MSA formation.
From the environmental perspective, MSA is usually described as a “green acid” [131] and is therefore environmentally much more acceptable compared with, e.g.
HCl, HF, and chloroacetic acid.
Moreover, stainless steel materials (e.g.
duplex 22Cr and super-Cr-13) are usually used to combat H2S and CO2 corrosion.
However, they are susceptible to corrosion in HCl solution [18].
On the other hand, we have recently shown that stainless steel materials are highly passivated in MSA solution [50].
Currently, extensive research by our research group is being carried out to evaluate the acceptability of MSA and the design of its CIFs for it to be used in the oilfield industry for the first time.
MSA could be an alternative to the conventional acids currently employed due to its beneficial environmental properties and reduced hazard to the personnel involved.
Before a CI or CIF is considered for field application, a suite of laboratory tests is performed for a particular application to evaluate its suitability [39,132,133].
It is essential to perform laboratory tests under the same conditions as are in the pipe under actual conditions, for example at the same temperature and pressure, and to use the same coupon testing material as the pipe is made of.
Usually, for experimental corrosion testing, specially designed glass equipment and autoclaves are used in order to simulate conditions in wells during the acidizing process.
Sometimes in these tests, dissolved oxygen in the acid is not removed in order to simulate well stimulation [9].
The mixing order of the CIF components can also be important, e.g.
Williams et al.
[16] suggested using first the surfactant, then acetylenic alcohol, the solvent, the quaternary compound, and finally the metal intensifier.
The references below list several key factors that can influence the results of acid corrosion testing.
These include: the acid volume/sample area ratio [9,24,133,134], the surface preparation and cleaning [9,25,62,132,135–137], the contact time and sample position [13,19,23,67,133,136], the emulsion stability test and temperature requirement for the liquid phase [19], the parasitic consumption [39], the temperature and pressure [19,66,128], and the evaluation of pitting corrosion [9,24,132,138].
As the majority of oilfield testing procedures for the evaluation of CI or CIF performance are done with WL tests, some of the important criteria are discussed below.
Smith et al.
[24] reported that the major discrepancy in corrosion inhibitor test results in reporting CR may occur by varying the ratio of the volume of the inhibited acid to the steel-coupon surface area.
They observed a decrease in CR with increasing inhibited acid volume up to a ratio of (11.62mL of acid solution)/(cm2 of sample), where CR became constant.
The authors explained that with an increase in the acid volume, the amount of the inhibitor increases, leading to better protection, because the steel area remains the same.
They also suggested that the testing practise may require 25–150mL of the acid solution per square inch of the sample (6.45cm2).
It has to be pointed out that with a non-inhibited acid solution it may be just the opposite, because by decreasing the volume, saturation of the corrosion products in the acid solution may be achieved faster and the measured CR may be slower compared with the measurement in higher volumes.
Moreover, the availability of oxygen may depend on the acid volume, which also influences CR [9].
The volume of acid per sample surface area should match the real operational procedure in the pipe.
For example, a volume of solution per sample area ratio of 3.75mL/cm2 would simulate acidizing through a 15cm diameter pipe.
However, the latter example does not take into account that fresh acid is pumped into the system in the real acidizing situation.
Also, under flow conditions, saturation of the corrosion products in the corrosion test does not occur [133].
Moreover, if the CR of the measured sample is high, consumption of the acid may be significant and its concentration changes, which does not simulate the real situation (freshly pumped acid).
A drop from 28% to 19% HCl concentration after only 3h of HS80 LCS immersion (closed system) was reported by Barmatov et al.
[9].
Due to that reason, Barmatov et al.
[9] claim that in laboratory tests, the results for CR higher than 0.243kg/m2 per test period may not be accurate and should be used as a rough estimate and η calculated from WL may overestimate the inhibitors performance.
On the other hand, the ASTM G31 [134] standard recommends a minimum solution volume of 0.4mL per 1mm2 of the sample, which is quite high and makes the laboratory testing procedure impractical.
However, Barmatov et al.
[9] used 7mL of acid solution per 1cm2 of sample.
In practice, the areas of the samples used for corrosion tests differ slightly, even though the samples look alike.
Therefore, the precise area of each sample should be determined before the test and used for the calculation of the CR.
The average value of replica CRs should be calculated from these numbers (the individual CR of a particular sample) and not according to the average mass-loss of the multiple samples, which could induce systematic error.
Smith et al.
[24] claimed that metal sample surface preparation affects CR slightly.
Moreover, Papavinasam et al.
[132] reported that surface finish preparation (grinding and polishing) and slight differences in the metallurgy of the coupons have little effect on corrosion behaviour.
On the other hand, Barmatov et al.
[9] pointed out that the surface texture and roughness of LCS metallic samples may affect the CR and pitting formation, where surface stress plays an important role.
This was especially pronounced for measurements at low CI concentrations (<0.14%).
At higher CI concentrations, no significant effect of the surface texture and roughness on CR was observed.
The authors compared 4 different preparation procedures: glass bead blasted (GBB) samples, pickled samples with HCl, ground samples with 240-grit SiC paper, and ground samples with 600- and 1200-grit SiC papers and afterwards polished with silica.
For CI concentration of 0.01–0.05% in 14% HCl, the following CR order was reported: GBB>240-grit ground samples>pickled samples>polished samples.
Even though pickled samples had the highest surface roughness, their CR was slower compared with the samples with the GBB preparation procedure (the CR was also faster for the 240-grit ground samples).
This was explained by the fact that the GBB procedure introduces stresses, plastic deformation, and microstrains, and changes in the heterogeneity of the surfaces.
Moreover, in 28% HCl inhibited with 0.05% CI (which inhibitor was not reported), the GBB samples had a higher CR compared with the other 3 preparation procedures, for which the CR was similar.
For the ground and polished samples, they did not observe any pits on the surface, whereas they reported pitting indexes (the definition is given below) of 3 and 7 for pickled and GBB samples, respectively.
Coupons are usually cut from metal sheet or from a real pipeline.
In this manner, the cut edges can become sites of preferential corrosion attack, which is usually not experienced in a real pipe and the corrosion test thus would not simulate real field conditions [132].
To minimize an edge effect, it is preferred if the material is cut into sample coupons by using a water-cooled band saw to minimize changes in the material properties due to heat generated by the cutting procedure [9].
In general, to prepare a surface without deep scratches that could influence the CR and test results (especially electrochemical measurements), a circulating device is employed to grind the sample with, for example, up to 4000-grit SiC papers to ensure a uniform pattern of very shallow scratches.
The grinding direction should be turned four times by 90° to minimize abrasion [43–57].
Finally, in some cases polishing is subsequently best carried out according to the procedures provided by the company supplying the polishing material.
After grinding and polishing, the samples should be cleaned and degreased ultrasonically in a bath of acetone [25], methanol [135], or some other solution.
However, it is important that this solution is not corrosive for the sample material.
Barmatov et al.
[9] suggested cleaning the samples in acetone prior the test and then drying them.
After the preparation procedure and before the corrosion test, the samples must be stored in dry boxes (containing water adsorbent) to prevent atmospheric corrosion in the humid environment.
After corrosion tests, it is common practice to clean the specimens by rinsing with water or detergent solution, brushing with a fibre-bristle brush, and immersion in an ultrasound bath (containing special cleaning solutions).
If corrosion products are still present on the surface, a procedure including dipping for 5–10s one or more times in 10–15% HCl solution containing a CI (or possibly also Clarke solution), rinsing, and brushing is performed, which successfully removes corrosion products from the surface [136,137].
Some cleaning recommendations are provided in the ASTM G1 standard [137].
Barmatov et al.
[9] suggested rinsing the samples in acetone and scrubbing them with soap and water to remove residual inhibitor film and corrosion deposits.
Finally, they rinsed the samples in acetone before weighing.
It has to be pointed out that the cleaning procedure after the corrosion test depends on the test conditions and what happened on the surface of the samples.
Sometimes after the test the samples are “sticky” due to the inhibitor film and have to be cleaned in, e.g.
acetone [62] or petroleum ether (however, this was not recommended in [136]).
On the other hand, if a sample is covered with a lot of corrosion products, it is recommended to clean it in Clarke solution.
After cleaning, the sample must be dried with inert gas (or by rinsing with anhydrous acetone or methanol) and immediately weighed.
It is also important that during preparation, installation, and cleaning, the sample is handled with clean and dry gloves [136].
The API RP 13B-1 Standard Procedure for Field Testing Water-Based Drilling Fluids in Annex E explains that the contact time of the sample with the drilling fluid should be at least 40h and up to 100h (it is stated that 100h is the normal exposure time for such test) [136].
The time needed to reach test temperature and to cool the test equipment is usually not included, but on the other hand, it increases the contact time of the sample with the acid solution.
To decrease the influence of the cooling period, the equipment is commonly cooled down in ice, under a stream of water, or by employing an autoclave’s cooling coils.
It is important to pick the right testing period because the CR is usually fast at the beginning of the test and then decreases with an increase in the immersion time [67,136].
However, it could be just the opposite as well.
Some authors suggest duplicating the well-treating environment in autoclave corrosion tests to ensure the worst case scenario for simulating real conditions [23].
HCl acid stimulation treatments frequently take 6–8h [19] and that is most likely why Hill and Jones performed 8h autoclave tests [19].
On the other hand, Walker [13] performed tests for a period of 4–48h.
Otherwise, it is practical and lately quite common to perform 24h tests to obtain reliable CR results.
Localized attack that results in pits is the primary cause of corrosion-induced failures and thus a very important criterion in CIF design.
A WL experiment could show a small mass loss, but the pits formed can already be deep [24].
Extensive pitting corrosion of steel materials is very common in solutions containing high concentrations of chlorides.
Extensive localized attack can lead to severe damage to the components.
Special care should be taken in the case of tanks and pipes, where corrosion damage can cause the leakage of fluids or gases [46,47].
A good inhibitor must prevent significant pit development.
Usually extensive pitting occurs when inhibitor concentrations are reduced to their absolute minimum [24].
One approach commonly used in the industry is visual observation and classification from 0 to 9 of pitting probability, according to the pitting index given in Table 3.
Pitting represented by Ranks 1 through 4 is usually not considered serious [24].
On the other hand, Barmatov et al.
[9] used a pitting index of ⩽2 as a acceptability limit.
It is also a common practice to measure the 3-D surface profile with, for example, a stylus or optical profilometer and express the pitting corrosion rate with the value of the maximum pit depth and the average of the ten deepest pits [138].
The use of this technique is still limited, because the scanned area is usually small and the analysis time is quite long.
At least three repetition measurements should be performed and the standard deviation should be calculated to present data with, for example, 95% confidence intervals.
Outliers should be discarded according to statistical tests, such as the Grubbs statistical test [139].
Most of the literature quotes η and not CR (see Table 2).
However, an inhibitor with calculated η of 90%, for example, could mean two things: (a) that the CI is very effective in preventing corrosion or (b) the reference CR for the non-inhibited solution was really high (e.g.
200mm/y, which is not uncommon for CS in non-inhibited stimulation fluids), whereas the CR in the inhibited solution was slower, but still unacceptable (e.g.
20mm/y).
Moreover, in the literature the η of CIs is commonly reported to increase with increasing T (e.g.
in [85]).
However, this is usually due to an even faster CR for non-inhibited solution, whereas the CR also increases for the inhibited solution, but is slower compared with non-inhibited solution.
The problem in reporting η instead of CR is even more emphasised when calculating η from electrochemical measurements (e.g.
polarisation resistance measurements).
Moreover, the Tafel extrapolation method for the determination of the corrosion current density from which the CR is calculated also causes problems (Eqs.
(2) and (3)) [140,141].
This method was developed for kinetically controlled reactions.
However, concentration polarisation, oxide formation, preferential dissolution of one alloy component, a mixed control process (where more than one anodic or cathodic reaction occurs simultaneously), and also other effects are frequent in corrosion measurements, which cause deviation from the original Tafel theory presented in Eq.
(1) and consequently CR error.
(1)j=jcorrexp2.303E-Ecorrβa-exp-2.303E-Ecorrβcwhere j (current density) is the measured cell current, jcorr is the corrosion current density, E is the electrode potential, Ecorr is the corrosion potential, and βa and βa are anodic and cathodic Beta Tafel coefficients, respectively (determined from measured curve slopes).
Due to the problems listed above, it is recommended to report both η and CR together, if possible.
Calculation of η and CR from the Tafel plot measurements are given in Eqs.
(2) and (3) (jcorr0 and jcorri represent jcorr measured in non-inhibited and inhibited solution, respectively).
(2)η=100jcorr0-jcorrijcorr0(3)Corrosion rate=Jcorr·K1ρ·A·∑ir(niwi/Ai)where Jcorr is in amperes (J – current), K1 is the constant that defines the units for CR, ρ is density (in g/cm3), A is the sample area (in cm2), ni is the valence of the alloying element “i” in equivalent/mole, wi is the mass fraction of the alloying element “i”, Ai is the atomic mass of the element “i” in g/mol, and r is the number of elements in the alloy).
It has to be emphasized that the CR calculation in Eq.
(3) is only valid for uniform corrosion, but if localized corrosion occurs it dramatically underestimates the CR.
To conclude, we believe that it is not recommended to calculate η and CR from Tafel plot measurements.
Papavinasam et al.
[132] reported that WL measurements followed by the characterization of pits are by far the most reliable technique for monitoring the effect of CIs on uniform and pitting CRs in the oil and gas industry.
Moreover, this technique is not affected by solution conductivity.
The η and CR from WL measurements are calculated according to Eqs.
(4) and (5).
(4)η=100Δm0-ΔmiΔm0(5)Corrosion ratemmy=K2Δmρ·A·twhere K2 is a conversion constant from cmh−1 to mm y−1 (87,600), Δm is the mass change (in grams, calculated from m before and after the corrosion test, Δm0 and Δmi represent Δm measured in non-inhibited and inhibited solution, respectively), and t is the immersion time (in h).
The polarisation resistance (Rp) measurement is also a convenient method for η determination, but less reproducible than WL measurement [132].
For the determination of the absolute CR in, for example, mm/y, the value of the anodic and cathodic Tafel slopes are needed, which causes uncertainty.
It is better to use only Rp instead of calculated CR values, and compare CI performance on a relative basis instead.
Moreover, it has been reported that error-producing complications when using Rp measurements include the oxidation of electroactive species besides the corroding metal, a change in the corrosion potential during Rp measurement, the use of a potential scan over too large a potential interval, the use of an excessively fast potential sweep rate, insufficient stabilization of the electrode before the measurement, uncompensated resistance (the biggest contribution is usually due to a non- or low-conducting medium), the presence of adsorbed intermediates, non-uniform current and potential distributions, and especially, as already mentioned above, incorrect values of the assumed or determined Tafel slopes [132,142].
It has to be emphasized that Rp measurement is a non-destructive method that can measure uniform corrosion behaviour over long time intervals, but it does not provide information on localized corrosion.
η calculated with Rp measurements is expressed as (Rp0 and Rpi represent Rp measured in non-inhibited and inhibited solution, respectively):(6)η=100Rpi-Rp0Rpi The electrochemical impedance spectroscopy (EIS) technique was less recommended by Papavinasam et al.
[132] in oil and gas pipeline corrosion inhibition monitoring due to poor reproducibility, long measurement time, and the need to develop a physical model (an equivalent electrical circuit).
On the other hand, Lotz et al.
[143] claim that EIS is a powerful technique for accessing in situ CR.
The electrochemical noise (EN) technique was highly recommended by Papavinasam et al.
[132], however they reported that it is still in the developmental stage.
The authors claim that no satisfactory method has yet been developed for the presentation and interpretation of EN data.
Papavinasam et al.
[132] also reported that hydrogen permeation measurement does not correlate with real corrosion in oil and gas pipelines.
To conclude, an advantage of electrochemical over WL measurements is in obtaining other information beside CR and η values, such as the following: which reaction of the corrosion couple is primarily inhibited, pitting and crevice corrosion susceptibility, repassivation ability after pitting formation, the passivation property of the material, the thermodynamics of redox processes, the kinetics of the electron transfer, and representation of the properties of the surface structure and its corrosion phenomena by equivalent electrical circuits.
This review summarizes the corrosion inhibition of lower-grade steels in acidic media.
The focus herein was on HCl solutions and the elevated temperatures usually encountered in the well acidizing procedure.
Lower-grade steel materials are the most commonly used construction materials for oil and gas wells due to their low cost and high performance.
During the acidizing procedure these steel materials are under very corrosive conditions and need to be inhibited by means of an appropriate corrosion inhibitor.
Numerous compounds were presented which are used as corrosion inhibitors for steel materials in acid solutions.
This review should also prove useful for other fields of corrosion inhibitor research where conditions are not so severe, e.g.
acid pickling, industrial cleaning, and acid descaling.
However, at elevated temperatures individual components alone are not effective in the well acidizing procedure and they often need to be formulated with appropriate surfactants, solvents, and intensifiers to protect metal in acidizing environments.
Other components may also be present is such corrosion inhibitor formulations.
In this work, various corrosion inhibitor formulations have been presented which are effective at elevated temperatures.
It has been shown that acetylenic alcohols are the most widely used active components as corrosion inhibitors in formulation design, now for more than five decades.
This review also describes the potential danger of conventional acids commonly employed in the acidizing procedure and the use of methanesulphonic acid as an alternative acid solution for acidizing, due to its beneficial properties.
Moreover, most of the developed corrosion inhibitors or their formulations no longer meet the Oslo Paris Commission requirements because their primary active ingredients may be harmful if discharged into the environment.
Current research is thus focused on developing alternatives to those currently employed.
Finally, some recommended references are reported herein that refer to important criteria in acidizing testing.
Chaospy: An open source tool for designing methods of uncertainty quantification The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation.
The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification.
For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis.
In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.
We consider a computational science problem in space x and time t where the aim is to quantify the uncertainty in some response Y, computed by a forward model f, which depends on uncertain input parameters Q:(1)Y=f(x,t,Q).We treat Q as a vector of model parameters, and Y is normally computed as some grid function in space and time.
The uncertainty in this problem stems from the parameters Q, which are assumed to have a known joint probability density function pQ.
The challenge is that we want to quantify the uncertainty in Y, but nothing is known about its density pY.
The goal is then to either build the density pY or relevant descriptive properties of Y using the density pQ and the forward model f. For all practical purposes this must be done by a numerical procedure.
In this paper, we focus on two approaches to numerically quantify uncertainty: Monte Carlo simulation and non-intrusive global polynomial chaos expansions.
For a review of the former, there is a very useful book by Rubinstein, Reuven and Kroese [1], while for the latter, we refer to the excellent book by Xiu [2].
Note that other methods for performing uncertainty quantification also exist, such as perturbation methods, moment equations, and operator based methods.
These methods are all discussed in [2], but are less general and less widely applicable than the two addressed in this paper.
The number of toolboxes available to perform Monte Carlo simulation is vastly larger than the number of toolboxes for non-intrusive polynomial chaos expansion.
As far as the authors know, there are only a few viable options for the latter class of methods: The Dakota Project (referred to as Dakota) [3], the Opus Open Turns library (referred to as Turns) [4], Uncertenty Quantification Toolkit [5], and MIT Uncertenty Quantification Library [6].
In this paper we will focus on the former two: Dakota and Turns.
Both packages consist of libraries with extensive sets of tools, where Monte Carlo simulation and non-intrusive polynomial chaos expansion are just two tools available among several others.
It is worth noting that both Dakota and Turns can be used from two perspectives: as a user and as a developer.
Both packages are open source projects with comprehensive developer manuals.
As such, they both allow anyone to extend the software with any functionality one sees fit.
However, these extension features are not targeting the common user and require a deeper understanding of both coding practice and the underlying design of the library.
In our opinion, the threshold for a common user to extend the library is normally out of reach.
Consequently, we are in this paper only considering Dakota and Turns from the point of view of the common user.
Dakota requires the forward model f to be wrapped in a stand-alone callable executable.
The common approach is then to link this executable to the analysis software through a configuration file.
The technical steps are somewhat cumbersome, but has their advantage in that already built and installed simulation software can be used without writing a line of code.
Alternative to this direct approach is to interact with an application programming interface (API).
This approach requires the user to know how to program in the supported languages, but this also has clear benefits as an interface through a programming language allows for a deeper level of integration between the user's model and the UQ tools.
Also, exposing the software's internal components through an API allows a higher detailed control over the tools and how they can be combined in statistical algorithms.
This feature is attractive to scientists who would like the possibility to experiment with new or non-standard methods in ways not thought of before.
This approach is used by the Turns software (using the languages Python or R) and is supported in Dakota through a library mode (using C++).
For example, consider bootstrapping [7], a popular method for measuring the stability of any parameter estimation.
Neither Dakota nor Turns support bootstrapping directly.
However, since Turns exposes some of the inner components to the user, a programmer can combine these to implement a custom bootstrapping technique.
This paper describes a new, third alternative open source software package called Chaospy [8].
Like Dakota and Turns, it is a toolbox for analysing uncertainty using advanced Monte Carlo simulation and non-intrusive polynomial chaos expansions.
However, unlike the others, it aims to assist scientists in constructing tailored statistical methods by combining a lot of fundamental and advanced building blocks.
Chaospy builds upon the same philosophy as Turns in that it offers flexibility to the user, but takes it significantly further.
In Chaospy, it is possible to gain detailed control and add user defined functionality to all of the following: random variable generation, polynomial construction, sampling schemes, numerical integration rules, response evaluation, and point collocation.
The software is designed from the ground up in Python to be modular and easy to experiment with.
The number of lines of code to achieve a full uncertainty analysis is amazingly low.
It is also very easy to compare a range of methods in a given problem.
Standard statistical methods are easily accessible through a few lines of R or Pandas [9] code, and one may think of Chaospy as a tool similar to R or Pandas, just tailored to polynomial chaos expansion and Monte Carlo simulation.
Although Chaospy is designed with a large focus on modularity, flexibility, and customization, the toolbox comes with a wide range of pre-defined statistical methods.
Within the scope of Monte Carlo sampling and non-intrusive polynomial chaos expansion, Chaospy has a competitive collection of methods, comparable to both Dakota and Turns.
It also offers some novel features regarding statistical methods, first and foremostly a flexible framework for defining and handling input distributions, including dependent stochastic variables.
Detailed comparisons of features in the three packages appear throughout the paper.
The paper is structured as follows.
We start in Section 2 with a quick demonstration of how the software can be used to perform uncertainty quantification in a simple physical problem.
Section 3 addresses probability distributions and the theory relevant to perform Monte Carlo simulation.
Section 4 concerns non-intrusive polynomial chaos expansions, while conclusions and topics for further work appear in Section 5.
To demonstrate how Chaospy is used to solve an uncertainty quantification problem, we consider a simple physical example of (scaled) exponential decay with an uncertain, piecewise constant coefficient: (2)u′(x)=−c(x)u(x),u(0)=u0,c(x)=c0,x<0.5c1,0.5≤x<0.7c2,x≥0.7Such a model arises in many contexts, but we may here think of u(x) as the porosity at depth x in geological layers and ci as a (scaled) compaction constant in layer number i.
For simplicity, we consider only three layers with three uncertain constants c0, c1, and c2.
The model can easily be evaluated by solving the differential equation problem, here by a 2nd-order Runge–Kutta method on a mesh x, coded in Python as: Alternatively, the model can be implemented in some external software in another programming language.
This software can either be run as a stand-alone application, where the Python function model runs the application and communicates with it through input and output files, or the model function can communicate with the external software through function calls if a Python wrapper has been made for the software (there are numerous technologies available for creating Python wrappers for C, C++, and Fortran software).
The Chaospy package may be loaded by Each of the uncertain parameters must be assigned a probability density, and we assume that c0, c1, and c2 are stochastically independent: The sample points (c0, c1, c2) in probability space, where the model is to be evaluated, can be chosen in many ways.
Here we specify a third-order Gaussian Quadrature scheme tailored to the joint distribution: The next step is to evaluate the computational model at these sample points (object nodes): Now, samples contains a list of arrays, each array containing u values at the 101 x values for one combination (c0, c1, c2) of the input parameters.
To create a polynomial chaos expansion, we must generate orthogonal polynomials corresponding to the joint distribution.
We choose polynomials of the same order as specified in the quadrature rule, computed by the widely used three-term recurrence relation (ttr): To create an approximate solver (or surrogate model), we join the polynomial chaos expansion, the quadrature nodes and weights, and the model samples: The model_approx object can now cheaply evaluate the model at a point (c0, c1, c2) in probability space for all x points in the x array.
Built-in tools can be used to derive statistical information about the model response: The mean and deviation objects are arrays containing the mean value and standard deviation at each point in x.
A graphical illustration is shown in Fig.
1.
The accuracy of the estimation is comparable to what Dakota and Turns can provide.
Fig.
2 shows that the estimation error in the three software toolboxes are almost indistinguishable.
The error is calculated as the absolute difference between the true value and the estimated value integrated over the depth x:ɛE=∫01|E(u)−E(uapprox)|dxɛV=∫01|V(u)−V(uapprox)|dxBoth the point collocation method and the pseudo-spectral projection method are included.
The former is calculated using two times the random collocation nodes as the number of polynomials, and the latter using Gaussian quadrature integration with quadrature order equal to polynomial order.
Note that Turns does not support pseudo-spectral projection, and is therefore only compared using point collocation.
Numerical methods for uncertainty quantification need to generate pseudo-random realizations{Qk}k∈IKIK={1,…,K},from the density pQ.
Each Q∈{Qk}k∈IK is multivariate with the number of dimensions D>1.
Generating realizations from a given density pQ is often non-trivial, at least when D is large.
A very common assumption made in uncertainty quantification is that each dimension in Q consists of stochastically independent components.
Stochastic independence allows for a joint sampling scheme to be reduced to a series of univariate samplings, drastically reducing the complexity of generating a sample Q.
Unfortunately, the assumption of independence does not always hold in practice.
We have examples from many research fields where stochastic dependence must be assumed, including modelling of climate [10], iron-ore minerals [11], finance [12], and ion channel densities in detailed neuroscience models [13].
There also exists examples where introducing dependent random variables is beneficial for the modelling process, even though the original input was stochastically independent [14].
In any cases, modelling of stochastically dependent variables are required to perform uncertainty quantification adequately.
A strong feature of Chaospy is its support for stochastic dependence.
All random samples are in Chaospy generated using Rosenblatt transformations TQ [15].
It allows for a random variable U, generated uniformly on a unit hypercube [0, 1]D, to be transformed into Q=TQ−1(U), which behaves as if it were drawn from the density pQ.
It is easy to generate pseudo-random samples from a uniform distribution, and the Rosenblatt transformation can then be used as a method for generating samples from arbitrary densities.
The Rosenblatt transformation can be derived as follows.
Consider a probability decomposition, for example for a bivariate random variable Q=(Q0, Q1):(3)pQ0,Q1(q0,q1)=pQ0(q0)pQ1∣Q0(q1∣q0),were pQ0 is an marginal density function, and pQ1∣Q0 is a conditional density.
For the multivariate case, the density decomposition will have the form(4)pQ(q)=∏d=0D−1pQd′(qd′),where (5)Qd′=Qd∣Q0,…,Qd−1qd′=qd∣q0,…,qd−1denotes that Qd and Qd are dependent on all components with lower indices.
A forward Rosenblatt transformation can then be defined as(6)TQ(q)=(FQ0′(q0′),…,FQD−1′(qD−1′)),where FQd′ is the cumulative distribution function:(7)FQd′(qd′)=∫−∞qdpQd′(r∣q0,…,qd−1)dr.This transformation is bijective, so it is always possible to define the inverse Rosenblatt transformation TQ−1 in a similar fashion.
To implement the Rosenblatt transformation in practice, we need to identify the inverse transform TQ−1.
Unfortunately, TQ is often non-linear without a closed-form formula, making analytical calculations of the transformation's inverse difficult.
In the scenario where we do not have a symbolic representation of the inverse transformation, a numerical scheme has to be employed.
To the authors’ knowledge, there are no standards for defining such a numerical scheme.
The following paragraphs therefore describe our proposed method for calculating the inverse transformation numerically.
The problem of calculating the inverse transformation TQ−1 can, by decomposing the definition of the forward Rosenblatt transformation in (6), be reformulated asFQd′−1(u∣q0,…,qd−1)=r:FQd′(r∣q0,…,qd−1)=ud=0,…,D−1.In other words, the challenge of calculating the inverse transformation can be reformulated as a series of one dimensional root-finding problems.
In Chaospy, these roots are found by employing a Newton–Raphson scheme.
However, to ensure convergence, the scheme is coupled with a bisection method.
The bisection method is applicable here since the problem is one-dimensional and the functions of interest are by definition monotone.
When the Newton–Raphson method fails to converge at an increment, a bisection step gives the Newton–Raphson a new start location away from the previous location.
This algorithm ensures fast and reliable convergence towards the root.
The Newton–Raphson-bisection hybrid method is implemented as follows.
The initial values are the lower and upper bounds [lo0, up0].
If pQd′ is unbound, the interval is selected such that it approximately covers the density.
For example for a standard normal random variable, which is unbound, the interval [−7.5,7.5] will approximately cover the whole density with an error about 10−14.
The algorithm starts with a Newton–Raphson increment, using the initial value r0=(up0−lo0)u+lo0: (8)rk+1=rk−FQd′(rk∣q0,…,qd−1)−upQd′(rk∣q0,…,qd−1),where the density pQd′ can be approximated using finite differences.
If the new value does not fall in the interval [lok, upk], this proposed value is rejected, and is instead replaced with a bisection increment: (9)rk+1=upk+lok2.In either case, the bounds are updated according to (10)(lok+1,upk+1)=(lok,rk+1)FQd′(rk+1∣q0,…,qd−1)>u(rk+1,upk)FQd′(rk+1∣q0,…,qd−1)<uThe algorithm repeats the steps in (8)–(10), until the residual |FQd′(rk∣q0,…,qd−1)−u| is sufficiently small.
The described algorithm overcomes one of the challenges of implementing Rosenblatt transformations in practice: how to calculate the inverse transformation.
Another challenge is how to construct a transformation in the first place.
This is the topic of the next section.
The backbone of distributions in Chaospy is the Rosenblatt transformation TQ.
The method, as described in the previous section, assumes that pQ is known to be able to perform the transformation and its inverse.
In practice, however, we first need to construct pQ, before the transformation can be used.
This can be a challenging task, but in Chaospy a lot of effort has been put into constructing novel tools for making the process as flexible and painless as possible.
In essence, users can create their own custom multivariate distributions using a new methodology as described next.
Following the definition in (6), each Rosenblatt transformation consists of a collection of conditional distributions.
We express all conditionality through distribution parameters.
For example, the location parameter of a normal distribution can be set to be uniformly distributed, say on [−1, 1].
The following interactive Python code defines a normal variable with a normally distributed mean: We now have two stochastic variables, uniform and normal, whose joint bivariate distribution can be constructed through the cp.J function: The software will, from this minimal formulation, try to sort out the dependency ordering and construct the full Rosenblatt transformation.
The only requirement is that a decomposition as in (4) is in fact possible.
The result is a fully functioning forward and inverse Rosenblatt transformation.
The following code evaluates the forward transformation (the density) at (1, 0.9), the inverse transformation at (0.4, 0.6), and draws a random sample from the joint distribution: Distributions in higher dimensions are trivially obtained by including more arguments to the cp.J function.
As an alternative to the explicit formulation of dependency through distribution parameters, it is also possible to construct dependencies implicitly through arithmetic operators.
For example, it is possible to recreate the example above using addition of stochastic variables instead of letting a distribution parameter be stochastic.
More precisely, we have a uniform variable on [−1, 1] and a normally distributed variable with location at x=0.
Adding the uniform variable to the normal variable creates a new normal variable with stochastic location: As before, the software automatically sorts the dependency ordering from the context.
Here, since the uniform variable is present as first argument, the software recognises the second argument as a normal distribution, conditioned on the uniform distribution, and not the other way around.
Another favorable feature in Chaospy is that multiple transformations can be stacked on top of each other.
For example, consider the example of a multivariate log-normal random variable Q with three dependent components.
(Let us ignore for a moment the fact that Chaospy already offers such a distribution.)
Trying to decompose this distribution is a very cumbersome task if performed manually.
However, this process can be drastically simplified through variable transformations, for which Chaospy has strong support.
A log-normal distribution, for example, can be expressed asQ=eZL+b,where Z are standard normal variables, and L and b are predefined matrix and vector, respectively.
To implement this particular transformation, we only have to write The resulting distribution is fully functional multivariate log-normal, assuming L and b are properly defined.
One obvious prerequisite for using univariate distributions to create conditionals and multivariate distributions, is the availability of univariate distributions.
Since the univariate distribution is the fundamental building block, Chaospy offers a large collection of 64 univariate distributions.
They are all listed in Table 1.
The list also shows that Dakota's support is limited to 11 distributions, and Turns has a collection of 26 distributions.
The Chaospy software supports in addition custom distributions through the function cp.constructor.
To illustrate its use, consider the simple example of a uniform random variable on the interval [lo, up].
The minimal input to create such a distribution is Here, the two provided arguments are a cumulative distribution function (cdf), and a boundary interval function (bnd), respectively.
The cp.constructor function also takes several optional arguments to provide extra functionality.
For example, the inverse of the cumulative distribution function –the point percentile function – can be provided through the ppf keyword.
If this function is not provided, the software will automatically approximate it using the method described in Section 3.2.
Dakota and Turns do not support the Rosenblatt transformation applied to multivariate distributions with dependencies.
Instead, the two packages model dependencies using copulas [16].
A copula consists of stochastically independent multivariate distributions made dependent using a parameterized function g. Since the Rosenblatt transformation is general purpose, it is possible to construct any copula directly.
However, this can quickly become a very cumbersome task since each copula must be decomposed individually for each combination of independent distributions and parameterization of g. To simplify the user's efforts, Chaospy has dedicated constructors that can reformulate a copula coupling into a Rosenblatt transformation.
This is done following the work of Lee [17] and approximated using finite differences.
The implementation is based of the software toolbox RoseDist [18].
In practice, this approach allow copulas to be defined in a Rosenblatt transformation setting.
For example, to construct a bivariate normal distribution with a Clayton copula in Chaospy, we do the following: A list of supported copulas are listed in Table 2.
It shows that Turns supports 7 methods, Chaospy 6, while Dakota offers 1 method.
As noted in the beginning of Section 3, by generating samples {Qk}k∈IK and evaluating the response function f, it is possible to draw inference upon Y without knowledge about pY, through Monte Carlo simulation.
Unfortunately, the number of samples K to achieve reasonable accuracy can often be very high, so if f is assumed to be computationally expensive, the number of samples needed frequently make Monte Carlo simulation infeasible for practical applications.
As a way to mitigate this problem, it is possible to modify {Qk}k∈IK from traditional pseudo-random samples, so that the accuracy increases.
Schemes that select non-traditional samples for {Qk}k∈IK to increase accuracy are known as variance reduction techniques.
A list of such techniques are presented in Table 3, and it shows that Dakota, Turns and Chaospy support 4, 7, and 7 variance reduction techniques, respectively.
One of the more popular variance reduction technique is the quasi-Monte Carlo scheme [1].
The method consists of selecting the samples {Qk}k∈IK to be a low-discrepancy sequence instead of pseudo-random samples.
The idea is that samples placed with a given distance from each other increase the coverage over the sample space, requiring fewer samples to reach a given accuracy.
For example, if standard Monte Carlo requires 106 samples for a given accuracy, quasi-Monte Carlo can often get away with only 103.
Note that this would break some of the statistical properties of the samples [19].
Most of the theory on quasi-Monte Carlo methods focuses on generating samples on the unit hypercube [0, 1]N. The option to generate samples directly on to other distributions exists, but is often very limited.
To the authors’ knowledge, the only viable method for including most quasi-Monte Carlo methods into the vast majority of non-standard probability distributions, is through the Rosenblatt transformation.
Since Chaospy is built around the Rosenblatt transformation, it has the novel feature of supporting quasi-Monte Carlo methods for all probability distributions.
Turns and Dakota only support Rosenblatt transformations for independent variables and the Normal copula.
Sometimes the quasi-Monte Carlo method is infeasible because the forward model is too computationally costly.
The next section describes polynomial chaos expansions, which often require far fewer samples than the quasi-Monte Carlo method for the same amount of accuracy.
Polynomial chaos expansions represent a collection of methods that can be considered a subset of polynomial approximation methods, but particularly designed for uncertainty quantification.
A general polynomial approximation can be defined as (11)fˆ(x,t,Q)=∑n∈INcn(x,t)Φn(Q)IN={0,…,N},where {cn}n∈IN are coefficients (often known as Fourier coefficients) and {Φn}n∈IN are polynomials.
If fˆ is a good approximation of f, it is possible to either infer statistical properties of fˆ analytically or through cheap numerical computations where fˆ is used as a surrogate for f. A polynomial chaos expansion is defined as a polynomial approximation, as in (11), where the polynomials {Φn}n∈IN are orthogonal on a custom weighted function space LQ: (12)〈Φn,Φm〉=EΦn(Q)Φm(Q)=∫…∫Φn(q)Φm(q)pQ(q)dq=0n≠m.
As a side note, it is worth noting that in parallel with polynomial chaos expansions, there also exists an alternative collocation method based on multivariate Lagrange polynomials [28].
This method is supported by Dakota and Chaospy, but not Turns.
To generate a polynomial chaos expansion, we must first calculate the polynomials {Φn}n∈IN such that the orthogonality property in (12) is satisfied.
This will be the topic of Section 4.1.
In Section 4.2 we show how to estimate the coefficients {cn}n∈IN.
Last, in Section 4.7, tools used to quantify uncertainty in polynomial chaos expansions will be discussed.
From (12) it follows that the orthogonality property is not in general transferable between distributions, since a new set of polynomials has to be constructed for each pQ.
The easiest approach to construct orthogonal polynomials is to identify the probability density pQ in the so-called Askey-Wilson scheme [29].
The polynomials can then be picked from a list, or be built from analytical components.
The continuous distributions supported in the scheme include the standard normal, gamma, beta, and uniform distributions respectively through the Hermite, Laguerre, Jacobi, and Legendre polynomial expansion.
All the three mentioned software toolboxes support these expansions.
Moving beyond the standard collection of the Askey-Wilson scheme, it is possible to create custom orthogonal polynomials, both analytically and numerically.
Unfortunately, most methods involving finite precision arithmetics are ill-posed, making a numerical approach quite a challenge [30].
This section explores the various approaches for constructing polynomial expansions.
A full list of methods is found in Table 4.
It shows that Dakota, Turns and Chaospy support 4, 3 and 5 orthogonalisation methods, respectively.
Looking beyond an analytical approach, the most popular method for constructing orthogonal polynomials is the discretized Stieltjes procedure [33].
As far as the authors know, it is the only truly numerically stable method for orthogonal polynomial construction.
It is based upon one-dimensional recursion coefficients that are estimated using numerical integration.
Unfortunately, the method is only applicable in the multivariate case if the components of pQ are stochastically independent.
Generalized polynomial chaos expansions.
One approach to model densities with stochastically dependent components numerically, is to reformulate the uncertainty problem as a set of independent components through generalised polynomial chaos expansion [34].
As described in detail in Section 3.1, a Rosenblatt transformation allows for the mapping between any domain and the unit hypercube [0, 1]D. With a double transformation we can reformulate the response function f asf(x,t,Q)=f(x,t,TQ−1(TR(R)))≈fˆ(x,t,R)=∑n∈INcn(x,t)Φn(R),where R is any random variable drawn from pR, which for simplicity is chosen to consists of independent components.
Also, {Φn}n∈IN is constructed to be orthogonal with respect to LR, not LQ.
In any case, R is either selected from the Askey-Wilson scheme, or calculated using the discretized Stieltjes procedure.
We remark that the accuracy of the approximation deteriorate if the transformation composition TQ−1∘TR is not smooth [34].
Dakota, Turns, and Chaospy all support generalized polynomial chaos expansions for independent stochastic variables and the Normal/Nataf copula listed in Table 2.
Since Chaospy has the Rosenblatt transformation underlying the computational framework, generalized polynomial chaos expansions are in fact available for all densities.
The direct multivariate approach.
Given that both the density pQ has stochastically dependent components, and the transformation composition TQ−1∘TR is not smooth, it is still possible to generate orthogonal polynomials numerically.
As noted above, most methods are numerically unstable, and the accuracy in the orthogonality can deteriorate with polynomial order, but the methods can still be useful [14].
In Table 4, only Chaospy's implementation of Bertran's recursion method [31], Cholesky decomposition [35] and modified Gram-Schmidt orthogonalization [32] support construction of orthogonal polynomials for multivariate dependent densities directly.
Custom polynomial expansions.
In the most extreme cases, an automated numerical method is insufficient.
Instead, a polynomial expansion has to be constructed manually.
User-defined expansions can be created conveniently, as demonstrated in the next example involving a second-order Hermite polynomial expansion, orthogonal with respect to the normal density [29]:Φnn∈I6=1,Q0,Q1,Q02−1,Q0Q1,Q12−1The relevant Chaospy code for creating this polynomial expansion looks like Chaospy contains a collection of tools to manipulate and create polynomials, see Table 5.
One thing worth noting is that polynomial chaos expansions suffers from the curse of dimensionality: The number of terms grows exponentially with the number of dimensions [36].
As a result, Chaospy does not support neither high dimensional nor infinite dimensional problems (random fields).
One approach to address such problems with polynomial chaos expansion is to first reduce the number of dimension through techniques like Karhunen–Loeve expansions [37].
If software implementations of such methods can be provided, the user can easily extend Chaospy to high and infinite dimensional problems.
Chaospy includes operators such as the expectation operator E. This is a helpful tool to ensure that the constructed polynomials are orthogonal, as defined in (12).
To verify that two elements in phi are indeed orthogonal under the standard bivariate normal distribution, one writes More details of operators used to perform uncertainty analysis are given in Section 4.7.
There are several methodologies for estimating the coefficients {cn}n∈IN, typically categorized either as non-intrusive or intrusive, where non-intrusive means that the computational procedures only requires evaluation of f (i.e., software for f can be reused as a black box).
Intrusive methods need to incorporate information about the underlying forward model in the computation of the coefficients.
In case of forward models based on differential equations, one performs a Galerkin formulation for the coefficients in probability space, leading effectively to a D-dimensional differential equation problem in this space [38].
Back et al.
[39] demonstrated that the computational cost of such an intrusive Galerkin method in some cases was higher than some non-intrusive methods.
None of the three toolboxes discussed in this paper have support for intrusive methods.
Within the realm of non-intrusive methods, there are in principle two viable methodologies available: pseudo-spectral projection [40] and the point collocation method [41].
The former applies a numerical integration scheme to estimate Fourier coefficients, while the latter solves a linear system arising from a statistical regression formulation.
Dakota and Chaospy support both methodologies, while Turns only supports point collocation.
We shall now discuss the practical, generic implementation of these two methods in Chaospy.
The pseudo-spectral projection method is based on a standard least squares minimization in the weighted function space LQ.
Since the polynomials are orthogonal in this space, the associated linear system is diagonal, which allows a closed-form expression for the Fourier coefficients.
The expression involves high-dimensional integrals in LQ.
Numerical integration is then required, (13)cn=EYΦnEΦn2=1EΦn2∫…∫pQ(q)f(x,t,q)Φn(q)dq≈1EΦn2∑k∈IKwkpQ(qk)f(x,t,qk)Φn(qk)IK={0,…,K−1},where wk are weights and qk nodes in a quadrature scheme.
Note that f is only evaluated for the nodes qk, and these evaluations can be made once.
Thereafter, one can experiment with the polynomial order since any cn depends on the same evaluations of f. Table 6 shows the various quadrature schemes offered by Dakota and Chaospy (recall that Turns does not support pseudo-spectral projection).
All techniques for generating nodes and weights in Chaospy are available through the cp.generate_quadrature function.
Suppose we want to generate optimal Gaussian quadrature nodes for the normal distribution.
We then write Most quadrature schemes are designed for univariate problems.
To extend a univariate scheme to the multivariate case, integration rules along each axis can be combined using a tensor product.
Unfortunately, such a product suffers from the curse of dimensionality and becomes a very costly integration procedure for large D. In higher-dimensional problems one can replace the full tensor product by a Smolyak sparse grid [48].
The method works by taking multiple lower order tensor product rules and joining them together.
If the rule is nested, i.e., the same samples found at a low order are also included at higher order, the number of evaluations can be further reduced.
Another feature is to add anisotropy such that some dimensions are sampled more than others [49].
In addition to the tensor product rules, there are a few native multivariate cubature rules that allow for low order multivariate integration [43].
Both Dakota and Chaospy also support the Smolyak sparse grid and anisotropy.
Chaospy has support for construction of custom integration rules defined by the user.
The cp.rule_generator function can be used to join a list of univariate rules using tensor grid or Smolyak sparse grid.
For example, consider the trapezoid rule: The cp.rule_generator function takes positional arguments, each representing a univariate rule.
To generate a rule for the multivariate case, with the same one-dimensional rule along two axes, we do the following: Software for constructing and executing a general-purpose integration scheme is useful for several computational components in uncertainty quantification.
For example, in Section 4.1 when constructing orthogonal polynomials using raw statistical moments, or calculating discretized Stieltjes’ recurrence coefficients, numerical integration is relevant.
Like the ppf function noted in Section 3.3, the moments and recurrence coefficients can be added directly into each distribution.
However, when these are not available, Chaospy will automatically estimate missing information by quadrature rules, using the cp.generate_quadrature function described above.
To compute the Fourier coefficients and the polynomial chaos expansion, we use the cp.fit_quadrature function.
It takes four arguments: the set of orthogonal polynomials, quadrature nodes, quadrature weights, and the user's function for evaluating the forward model (to be executed at the quadrature nodes).
Note that in the case of the discretized Stieltjes method discussed in Section 4.1, the nominator EΦn2 in (13) can be calculated more accurately using recurrence coefficients [32].
Special numerical features like this can be added by including optional arguments in cp.fit_quadrature.
The other non-intrusive approach to estimate the coefficients {ck}k∈IK is the point collocation method.
One way of formulating the method is to require the polynomial expansion to equal the model evaluations at a set of collocation nodes {Qk}k∈IK, resulting in an over-determined set of linear equations for the Fourier coefficients: (14)Φ0(q0)⋯ΦN(q0)⋮⋮Φ0(qK−1)⋯ΦN(qK−1)c0⋮cN=f(q0)⋮f(qK−1)Unlike pseudo spectral projection, the locations of the collocation nodes are not required to follow any integration rule.
Hosder [41] showed that the solution using Hammersley samples from quasi-Monte Carlo samples resulted in more stable results than using conventional pseudo-random samples.
In other words, well placed collocation nodes might increase the accuracy.
In Chaospy these collocation nodes can be selected from integration rules or from pseudo-random samples from Monte Carlo simulation, as discussed in Section 3.5.
In addition, the software accepts user defined strategies for choosing the sampling points.
Turns also allows for user-defined points, while Dakota has its predefined strategies.
The obvious way to solve the over-determined system in (14) is to use least squares minimization, which resembles the standard statistical linear regression approach of fitting a polynomial to a set of data points.
However, from a numerical point of view, this might not be the best strategy.
If the numerical stability of the solution is low, it might be prudent to use Tikhonov regularization [50], or if the problem is so large that the number of coefficients is very high, it might be useful to force some of the coefficients to be zero through least angle regression [51].
Being able to run and compare alternative methods is important in many problems to see if numerical stability is a potential problem.
Table 7 lists the regression methods offered by Dakota, Turns, and Chaospy.
Generating a polynomial chaos expansion using linear regression is done using Chaospy's cp.fit_regression function.
It takes the same arguments as cp.fit_quadrature, except that quadrature weights are omitted, and optional arguments define the rule used to optimize (14).
Irrespectively of the method used to estimate the coefficients ck, the user is left with the job to evaluate the forward model (response function) f, which is normally by far the most computing-intensive part in uncertainty quantification.
Chaospy does not impose any restriction on the simulation code used to compute the forward model.
The only requirement is that the user can provide an array of values of f at the quadrature or collocation nodes.
Chaospy users will usually wrap any complex simulation code for f in a Python function f(q), where q is a node in probability space (i.e., q contains values of the uncertain parameters in the problem).
For example, for pseudo-spectral projection, samples of f can be created as or perhaps done in parallel if f is time consuming to evaluate:  The evaluation of all the f values can also be done in parallel with MPI in a distributed way on a cluster using the Python module like mpi4py.
Both Dakota and Turns support parallel evaluation of f values, but the feature is embeded into the code, potentially limiting the customization options of the parallelization.
There is much literature that extends on the theory of polynomial chaos expansion [36].
For example, Isukapalli showed that the accuracy of a polynomial expansion could be increased by using partial derivatives of the model response [59].
This theory is only directly supported by Dakota.
In Turns and Chaospy the support is indirect by allowing the user to add the feature manually.
To be able to incorporate partial derivatives of the response, the partial derivative of the polynomial expansion must be available as well.
In both Turns and Chaospy, the derivative of a polynomial can be generated easily.
This derivative can then be added to the expansion, allowing us to incorporate Isukapalli's theory in practice.
This is just an example on how manipulation of the polynomial expansions and model approximations can overcome the lack of support for a particular feature from the literature.
To be able to support many current and possible future extensions of polynomial chaos, a large collection of tools for manipulating polynomials must be available.
In Dakota, no such tools exist from a user perspective.
In Turns, there is support for some arithmetic operators in addition to the derivative.
In Chaospy, however, the polynomial generated for the model response is of the same type as the polynomials generated in Sections 4.1 and 4.2, and the rich set of manipulations of polynomials is then available for fˆ as well.
Beyond the analytical tools for statistical analysis of fˆ, either from the toolbox or custom ones by the user, there are many statistical metrics that cannot easily be expressed as simple closed-form formulas.
Such metrics include confidence intervals, sensitivity indices, p-values in hypothesis testing, to mention a few.
In those scenarios, it makes sense to perform a secondary uncertainty analysis through Monte Carlo simulation.
Evaluating the approximation fˆ is normally computationally much cheaper than evaluating the full forward model f, thus allowing a large number of Monte Carlo samples within a cheap computational budget.
This type of secondary simulations are done automatically in the background in Dakota and Turns, while Chaospy does not feature automated tools for secondary Monte Carlo simulation.
Instead, Chaospy allows for simple and computationally cheap generation of pseudo-random samples, as described in Section 3.5, such that the user can easily put together a tailored Monte Carlo simulation to meet the needs at hand.
Within a few lines of Python code, the samples can be analyzed with the standard Numpy and the Scipy libraries [60] or with more specialized statistical libraries like Pandas [9], Scikit-learn [61], Scikit-statsmodel [62], and Python's interface to the rich R environment for statistical computing.
For example, for the specific fˆ function illustrated above, the following code computes a 90 percent confidence interval, based on 105 pseudo-random samples and Numpy's functionality for finding percentiles in discrete data: Since the type of statistical analysis of fˆ often strongly depends on the physical problem at hand, we believe that the ability to quickly compose custom solutions by putting together basic building blocks is very useful in uncertainty quantification.
This is yet another example of the need for a package with a strong focus on easy customization.
The last step in uncertainty quantification based on polynomial chaos expansions is to quantify the uncertainty.
In polynomial chaos expansion this is done by using the uncertainty in the model approximation f_approx as a substiute for the uncertainty in the model f. For the most popular statistical metrics, like mean, variance, correlation, a polynomial chaos expansion allows for analytical analysis, which is easy to calculate and has high accuracy.
This property is reflected in all the three toolboxes.
To calculate the expected value, variance and correlation of a simple (here univariate) polynomial approximation f_approx, with a normally distributed ξ0 variable, we can with Chaospy write A list of supported analytical metrics is listed in Table 8.
Until now there have only been a few real software alternatives for implementing non-intrusive polynomial chaos expansions.
Two of the more popular implementations, Dakota and Turns, are both high-quality software that can be applied to a large array of problems.
The present paper has introduced a new alternative: Chaospy.
Its aim is to be an experimental foundry for scientists.
Besides featuring a vast library of state-of-the-art tools, Chaospy allows for a high degree of customization in a user-friendly way.
Within a few lines of high-level Python code, the user can play around with custom distributions, custom polynomials, custom integration schemes, custom sampling schemes, and custom statistical analysis of the result.
Throughout the text we have compared the built-in functionality of the three packages, and Chaospy do very well in this comparison, which is summarized in Table 9.
But the primary advantage of the package is the strong emphasis on offering well-designed software building blocks, with a high abstraction level, that can easily be combined to create tailored uncertainty quantification algorithms for new problems.
Although the primary aim of the software is to construct polynomial chaos expansions, the software is also a state-of-the-art toolbox for performing Monte Carlo simulation, either directly on the forward model or in combination with polynomial chaos expansions.
Variance reduction techniques are included to speed up the convergence, and because Chaospy is based on Rosenblatt transformations, efficient quasi-Monte Carlo sampling is available for any distribution.
Another novel feature of Chaospy is the ability to handle stochastically dependent input variables through a new mathematical technique.
The work is supported by funding from Statoil ASA through the Simula School of Research and Innovation, and by a Center of Excellence grant from the Research Council of Norway through the Center for Biomedical Computing.
Thanks also go to Vinzenz Gregor Eck, Stuart Clark, Karoline Hagane, Samwell Tarly, and a random distribution of unnamed bug fixers for their contributions.
Whole cell tracking through the optimal control of geometric evolution laws Cell tracking algorithms which automate and systematise the analysis of time lapse image data sets of cells are an indispensable tool in the modelling and understanding of cellular phenomena.
In this study we present a theoretical framework and an algorithm for whole cell tracking.
Within this work we consider that “tracking” is equivalent to a dynamic reconstruction of the whole cell data (morphologies) from static image data sets.
The novelty of our work is that the tracking algorithm is driven by a model for the motion of the cell.
This model may be regarded as a simplification of a recently developed physically meaningful model for cell motility.
The resulting problem is the optimal control of a geometric evolution law and we discuss the formulation and numerical approximation of the optimal control problem.
The overall goal of this work is to design a framework for cell tracking within which the recovered data reflects the physics of the forward model.
A number of numerical simulations are presented that illustrate the applicability of our approach.
Cell migration is a fundamental process in cell biology and is tightly linked to many important physiological and pathological events such as the immune response, wound healing, tissue differentiation, metastasis, embryogenesis, inflammation and tumour invasion [1].
Experimental advances provide techniques to observe migrating cells both in vivo and in vitro.
Inferring dynamic quantities from this static data is an important task that has many applications in biology and related fields.
The field of cell tracking arose from this need and is concerned with the development of methods to track and analyse dynamic cell shape changes from a series of still images captured within a time frame (see for example [2,3] for reviews).
On the other hand, a major focus of current research is the derivation of mathematical models for cell migration based on physical principles, e.g., [4].
Furthermore, such models appear to show good qualitative and quantitative agreement with experimental observations of migrating cells.
Despite this, very little research has focused on incorporating these mathematical modelling advances into appropriate cell tracking algorithms.
In a related work, we investigated fitting parameters in models for cell motility to experimental image data sets of migrating cells where observations of both the position of the cells and the concentrations of cell-resident proteins related to motility were available [5].
In this study we present a first step towards the development of a framework for cell tracking based on novel models of cell motility.
Specifically, we propose a cell tracking algorithm which can be thought of as fitting a simplified, yet physically meaningful, model for cell migration to experimental observations and data.
We focus on the setting, prevalent in cell tracking problems, where only the position of the cell at a series of discrete times is available and no further biological information is given.
We present a mathematical model based on physical principles for the cell movement that consists of a geometric evolution equation.
We then formulate an inverse problem, which takes the form of a PDE constrained optimisation problem, for fitting the model to the static experimental observations.
To solve the optimisation problem we propose an algorithm based on the optimal control of geometric evolution laws [6,7].
The objective of this study is to serve as a useful first step in the development of cell tracking algorithms in which the underlying model for the evolution is based on physical principles, rather than purely geometric considerations.
In this setting, one hopes to attain estimates of motility-related features such as trajectories, velocities, persistence lengths, circularity, etc., which reflect the physics underlying the model.
We illustrate the fact that the tracking procedure we propose allows us to incorporate physically important aspects of cell migration by including volume conservation in the model for the evolution.
This is motivated by the observation that, for many cells, while the surface area of the cell membrane may change significantly during migration the volume enclosed by the cell remains roughly constant [8].
Of course other physical aspects of the migration could be included in the model, such as a spontaneous curvature of the membrane which is relevant for more complex models of cell motility involving the Helfrich model [9].
The remainder of our discussion proceeds as follows.
In Section 2 we briefly describe the problem of cell tracking and introduce our approach to cell tracking, which may be regarded as fitting a mathematical model to experimental image data sets.
We present the geometric evolution law model we seek to fit, which is a simplification of recently developed models in the literature that show good agreement with experiments [8,10–12,4,13,9].
We finish Section 2 by reformulating our model into the phase field framework, which appears more suitable for the problem in hand, and we formulate the cell tracking problem as a PDE constrained optimisation problem.
In Section 3 we propose an algorithm for the resolution of the PDE constrained optimisation problem and we discuss some practical aspects related to the implementation.
In particular we note that the theoretical and computational framework may be applied directly to multi-cell image data sets and raw image data sets (of sufficient quality) without segmentation.
In Section 4 we present some numerical examples for the case of 2d single and multi-cell image data sets.
Finally in Section 5 we present some conclusions of our study and discuss future extensions and applications of the work.
Our focus is on developing whole cell tracking algorithms which track the morphology of the cell rather than particle tracking in which particles such as the cell centroid or cell resident proteins or (macro-)molecules are tracked [14].
A number of approaches have proved successful in cell tracking with level-set [15] or electrostatic based methods among the most widely used [16].
One feature of such methods is that the trajectories they generate are not physical in nature rather they are designed with the goal of achieving nice geometric properties, e.g., equidistribution of vertices, smoothness of the trajectories and so on.
Our approach differs to these purely geometric approaches in that we start with a model derived from physical principles and it is this model for the evolution that drives the tracking algorithm.
In this sense our approach is similar in spirit to the parameter identification procedure described in [5] as in both studies the goal may be regarded as fitting a mathematical model to experimental image data sets.
We now summarise the main problems that must be addressed by a cell tracking algorithm.
In general cell tracking consists of three main steps1.Segmentation: In this step the raw image data set is processed and the cells are separated from the background in each frame.
Matching: The cells segmented in the first step must then be associated from frame to frame (note this is only relevant in the case of multiple cell image data sets) such that where possible (in practice cells may disappear or spontaneously appear in images) there is a one-to-one map that uniquely associates individual cells from one frame to the next.
Linking: Finally the linking step consists of estimating dynamic data from the associated segmented static cells.
As mentioned above in contrast to many of the existing approaches for cell tracking, the framework we propose in this study is based on fitting a model, derived from physical principles, for the motion of the cell to experimental image data.
The general class of models to which our approach is applicable are PDE based models for the motion, where the movement of the cell membrane is described by a geometric evolution law.
We introduce some notation for the formulation of the model.
We denote by Γ the cell membrane, which is assumed to be a closed smooth oriented d−1 dimensional hypersurface in Rd, d=2,3, with outward pointing unit normal ν.
Given a function η defined in a neighbourhood of Γ, the tangential or surface gradient of η denoted by ∇Γ is defined as(2.1)∇Γη:=∇η−∇η⋅νν, where ∇ denotes the Cartesian gradient in Rd.
The Laplace–Beltrami operator ΔΓ is defined as the tangential divergence of the tangential gradient, i.e.,(2.2)ΔΓη:=∇Γ⋅(∇Γη).
The mean curvature H of Γ with respect to the normal ν is defined as(2.3)H:=∇Γ⋅ν.
In this study we model the evolution of the cell membrane as being governed by volume conserved mean curvature flow with forcing, given by(2.4){V(x,t)=(−σH(x,t)+η(x,t)+λV(t))ν(x,t)on Γ(t),t∈(0,T],Γ(0)=Γ0, where Γ is the closed surface that represents the cell membrane, V is the material velocity of Γ, σ is the surface tension and λV(t) is a spatially uniform force accounting for volume conservation, physically this may be thought of as an interior pressure.
The forcing function η is the main driver of the directed migration.
The model we present is phenomenological and hence it is difficult to directly relate η to biophysical processes.
However, as positive values of η correspond to protrusive forces and negative values of η correspond to contractile forces one interpretation of the forcing function η is that it accounts for both protrusive forces generated by polymerisation of actin at the leading edge of the cell and contractile forces generated by the action of myosin motors at the rear of the cell.
The evolution law (2.4) is a simplification of a large class of models that arise in the modelling of cell motility which take the following form(2.5){V(x,t)=(g1(H(x,t))+g2(a(x,t))+λV(t))ν(x,t)on Γ(t),t∈(0,T],Γ(0)=Γ0, where g1 models the dependence of the evolution on geometric quantities, such as resistance of the membrane to stretching which could be modelled by mean curvature terms as in (2.4) or bending which can be modelled through the inclusion of Willmore or Helfrich flow type terms.
The function g2 appearing in (2.5) captures the dependence of the evolution on a vector of bulk and/or surface resident species a.
The surface resident species a could, for example, satisfy another PDE such as a surface reaction–diffusion system(2.6){∂V•a+a∇Γ(t)⋅V−DΔΓ(t)a=f(a)on Γ(t),t∈(0,T],a(⋅,0)=a0(⋅)on Γ(0), where a=(a1,…,ana)T, na is the number of chemical species involved, ai denotes the density of the ith chemical species, V is the material velocity of the surface,(2.7)∂V•a:=∂ta+V⋅∇a, is the material derivative with respect to the velocity V, D is a diagonal matrix of positive diffusion coefficients and f(a) is the nonlinear reaction.
Models of the form (2.5)–(2.6) have been used successfully to model cell motility in [4,17,11,12] while models coupling evolution laws of the form (2.5) to bulk PDEs (i.e., equations posed in the cell interior) have been considered in [13,9,10].
Despite its simplicity the evolution law (2.4) may be regarded as a prototype of the more complex models for cell motility of the form (2.5)–(2.6).
The geometric evolution component (2.5) is often the most challenging component of the model to solve numerically and developing an understanding of how to construct cell tracking algorithms assuming a geometric evolution law based model for the motion is an important first step towards developing tracking algorithms based on more realistic physical models.
In many applications it is also the case that the only information available from the data is the position of the cell membrane and no adequate model for the biochemistry of the motility related species involved is available.
Without any knowledge of the relevant biochemistry it is difficult to identify which motility related species should influence the evolution let alone propose how the evolution depends on their distribution (i.e., a g2 in (2.5)) or a model for the species dynamics (i.e., an equation such as (2.6)).
Nevertheless one may still wish to extract dynamic quantities from static image data sets, in this setting it may be reasonable to consider the evolution law (2.4) as a stand alone model for the motion as at least the mechanical aspects of the membrane evolution are accounted for through a physical model derived from basic physical principles.
The cell tracking approach we consider in this study corresponds to the following problem.
2.4.
ProblemCell trackingGiven an initial cell membrane position Γ0 and an observation of the position Γobs, find a space–time distributed forcing η such that the evolution of the cell membrane, Γ(t),t∈[0,T] satisfies (2.4) with Γ(0)=Γ0 and Γ(T) the position of the cell membrane at time t=T, is close to Γobs.
As the volume enclosed by the cell may vary over the images it is inappropriate to enforce conservation of a constant volume.
Instead we enforce, with the help of a Lagrange multiplier λV(t), that the volume enclosed by the cell is given by V˜(t)=V0+tT(Vobs−V0), i.e.
that the volume of the cell is a time-dependent linear interpolant of the volumes of the data.
Problem 2.4 is an optimal control of a free boundary problem, where the free moving boundary problem is that of forced mean curvature flow and the control variable is the space–time distributed forcing.
The theory of optimal control of geometric evolution laws is in its infancy, in fact only recently has progress been made on the optimal control of parabolic equations on evolving surfaces even in the case of prescribed evolution [18].
On the other hand the theory for the optimal control of semilinear parabolic equations is more mature (see, for example, [19]).
We wish to exploit this fact and to this end we consider the phase field approximation of (2.4) given by the Allen–Cahn equation;(2.8){∂tφ(x,t)=Δφ(x,t)−1ε2G′(φ(x,t))−1ε(cGη(x,t)−λ(t)) in Ω×(0,T],∇φ⋅νΩ=0 on ∂Ω×(0,T],φ(⋅,0)=φ0(⋅) in Ω, where Ω⊂Rd is a bulk time-independent domain, with normal νΩ, that contains Γ(t), φ0 is a diffuse interface representation of Γ0 and ε>0 is a small parameter which governs the width of the diffuse interface.
For details on the asymptotic analysis of (2.8) and the convergence (as ε→0) to a solution of (2.4) we refer the reader, for example, to [20–23] and references therein.
The function G appearing in (2.8) is a double well potential, for example the quartic potential(2.9)G(φ)=14(φ2−1)2 which has minima at ±1.
The constant cG=12∫−11G(r)1/2dr appearing in (2.8) is a scaling constant that depends on the double well potential.
We enforce the time-dependent volume constraint following the approach of [21].
Specifically our diffuse interface formulation of the constraint on the enclosed volume is given by a constraint on ∫Ω[φ(x,t)]+dx, where [a]+=max(a,0).
We define Mφ, the linear interpolant of ∫Ω[φ(x,t)]+dx of the initial and target diffuse interface data byMφ(t):=∫Ω[φ0]++tT([φobs]+−[φ0]+)dx, and determine λ(t) in (2.8) such that Mφ(t)=∫Ω[φ(x,t)]+dx.
We have used λ (rather than λV) for the Lagrange multiplier in (2.8) to reflect the fact that our constraint is on ∫Ω[φ(x,t)]+dx.
However we shall refer to this constraint as a volume constraint in order to highlight the physical feature the constraint is intended to model.
We also investigated an alternative approach to enforcing the volume constraint via penalising deviations from a target volume following [24] (see also [9]), in our numerical tests this strategy proved less robust than the volume constraint proposed above.
To formulate the cell tracking problem as a PDE constrained optimal control problem we define the objective functional we shall seek to minimise as follows(2.10)J(φ,η)=12∫Ω(φ(x,T)−φobs(x))2dx+θ2∫0T∫Ωη(x,t)2dxdt, where φobs is a diffuse interface representation of the observation Γobs and θ>0 is a regularisation parameter.
The first term on the right of (2.10) is the so-called fidelity term that measures the distance between the solution to the model and the target data and the second term is the regularisation which is necessary to ensure a well-posed problem (for example see [19]).
Our optimal control approach to the cell tracking problem may now be stated as the following minimisation problem.
2.5.
ProblemOptimal control problemGiven an initial diffuse interface representation of the cell membrane position φ0 and an observation of the position φobs, find a space–time distributed forcing η⁎:Ω×[0,T]→R such that with φ a solution of (2.8) with initial condition φ(⋅,0)=φ0, the forcing η⁎ solves the minimisation problem(2.11)minη⁡J(φ,η),with J given by (2.10).
To apply the theory of optimal control of semilinear PDEs for the solution of the tracking problem, we briefly outline the derivation of the optimality conditions, for further details see for example [25,19].
Introducing the Lagrange multiplier (adjoint state) p, we define the Lagrangian functional(2.12)L(φ,η,p)=J(φ,η)−∫0T∫Ω(∂tφ(x,t)−Δφ(x,t)+1ε2G′(φ(x,t))+1ε(cGη(x,t)−λ(t)))p(x,t)dxdt.
Requiring stationarity of the Lagrangian with respect to the adjoint state yields the state equation (2.8) and requiring stationarity of the Lagrangian, at the optimal control η⁎ and associated optimal state φ⁎, with respect to the state and the control, yields the (formal) first order optimality conditions(2.13)δφL(φ⁎,η⁎,p)φ=0,∀φ:φ(x,0)=0,(2.14)δηL(φ⁎,η⁎,p)η=0,∀η.
Condition (2.13) yields the adjoint equation, which is the following linear parabolic PDE for the adjoint state p,(2.15){∂tp(x,t)=−Δp(x,t)+1ε2G″(φ(x,t))p(x,t) in Ω×(0,T],∇p⋅νΩ=0 on ∂Ω×(0,T],p(x,T)=φ(x,T)−φobs(x) in Ω.
Note that Eq.
(2.15) is posed backwards in time and hence is equipped with terminal conditions.
Condition (2.14) together with the Riesz representation theorem yields the optimality condition (cf., [19])(2.16)δηL(φ⁎,η⁎,p)=θη⁎+cGεp=0.
2.7.
RemarkChoice of the potentialWe note that our approach to the optimal control problem involving the formulation of the adjoint problem appears to require a smooth potential G (cf., (2.9)).
The formulation of the adjoint problem is to our best knowledge an open problem for other widely used, but non-smooth or unbounded, potentials such as the obstacle or logarithmic potential.
As is standard, we use the optimality conditions to construct an iterative optimisation loop to solve the optimal control problem, Problem 2.5.
The basic idea is that in each step of the loop we first solve the state equation (2.8) with a given control, then solve the adjoint equation (2.15) with the computed states and then update the control using the optimality condition (2.16).
For this initial study to ensure robustness of the algorithm and to aid in the clarity of the exposition we employ a simple gradient based update of the control [26].
Given ηk and p we compute the updated control ηk+1 via steepest descent.
That is we choose as an update direction the negative gradient, the formula for the update of the control is(3.1)ηk+1(x,t)=ηk(x,t)−α(θηk(x,t)+cGεp(x,t)),(x,t)∈Ω×[0,T), where α is a step size.
For simplicity in this study we take a constant step size of α=0.01.
For the termination criteria for the algorithm we stop if the objective functional J is less than a given tolerance tolJ, the update in the control is less than a given tolerance, i.e., if ‖α(θηk+cGεp)‖L2(Ω×[0,T))<tolη or if a maximum number of iterations Kmax is reached.
In practice the forward (2.8) and adjoint equations (2.15) must be approximated and to do this we utilise a finite element method, using continuous piecewise linear elements.
The details of the numerical method for the approximation of the forward and adjoint equations are given in Appendix A.
The cell tracking algorithm we propose may now be stated in pseudocode as follows (the notation employed is as in Appendix A).
3.2.
RemarkSegmentation and image dataAn important aspect of any cell tracking algorithm is its ability to extract data suitable for the tracking algorithm from the experimental image data set.
In many cases the experimental image data set is grayscale data with the intensity (brightness) indicating whether a point is in the interior of a cell, i.e., points inside the cell appear bright for example and points outside appear dark.
For many tracking algorithms this intensity data is then post processed via a segmentation algorithm (e.g., active contour methods [27,28]) to yield sharp interface representations of the cell membrane.
Assuming a sharp interface representation of the cell membrane is available, diffuse interface representations may be easily initialised (see for example [5]).
We note however that the raw intensity data produced by many imaging procedures may already be close to a diffuse interface representation of the cell, this is typically the case when the data is relatively free of noise and the contrast between the cell and the background is high.
In this case one may wish to exploit this fact in the algorithm and work with the raw image data set itself (or a post processed e.g., thresholded version), thus circumventing the extra error induced by segmentation.
3.3.
RemarkObservations at multiple points in timeFor clarity of exposition we focus on the case of fitting to a single observation.
The approach generalises straightforwardly to multiple observations with the first term in (2.10) simply replaced by a sum over the distinct times at which the observations are taken of the difference between the solution (at the appropriate time) and the target data.
3.4.
RemarkMultiple cells and matching problemsAs mentioned above a major focus of many cell tracking algorithms is to track multiple cells in the same image and the resolution of the so-called matching problem.
Our approach can be applied to multi-cell image data.
Here φ0 and φobs would be diffuse interface representations of the multi-cell image data set and the diffuse interfaces would consist of multiple disjoint phases.
The remaining aspects of the approach remain unchanged and the matching problem is solved implicitly in the computation of the optimal control.
There are however multiple practical issues which arise in this setting related to the separation between distinct cells, which affects the choice of ε, and the fact that the evolution law (2.8) allows changes in the topology of the phases which may lead to cell splitting, the annihilation of a phase (which would correspond to the disappearance of a cell) or the nucleation of a phase (i.e., the spontaneous appearance of a cell).
We intend to comment on practical approaches to multi-cell tracking elsewhere.
We now present some benchmark numerical examples illustrating the application of the algorithm to artificial image data sets.
For all the simulations we report on in this section, in the state equation (2.8) we set ε=0.1, and we took the end-time T=0.4.
As mentioned previously the parameter ε governs the width of the diffuse interface and should in general be taken as small as is computationally feasible, smaller values of ε necessitate a finer grid, in this initial study with uniform grids we set ε=0.1 as the CPU times become prohibitive for smaller values of ε.
The end time T corresponds to the nondimensionalised time between snapshots and could in principle be related to an acquisition time between images given real biological data.
For each of the experiments, apart from those of Section 4.4, we set the initial guess for the control to be constant in space and time (zero in the single cell case and one for the multi-cell examples).
For the approximation of the forward and adjoint equations we used triangulations with 8321 DOFs in all the simulations, apart from those of Section 4.3, and selected a uniform timestep τ=1×10−3.
The same numerical parameters for the optimisation algorithm were used for all the experiments and are given in Table 1.
In every example we report on the algorithm terminated due to the update of the control being less than the prescribed tolerance.
The technical details of the hardware used to carry out the simulations are given in Remark 4.1.
4.1.
RemarkHardware detailsAll the numerical experiments have been performed on the high performance cluster (HPC) at the University of Sussex.
Each of the simulations was carried out in serial using a single core of the cluster.
The HPC cluster currently consists of 3140 cores with an even mixture of Intel and AMD CPUs.
The majority of the cluster are 64 core AMD nodes with 256 GB RAM per node, and a smaller number of 512 GB RAM nodes.
The cluster uses the high-performance Lustre clustered-filesystem for I/O, and currently stands at 298TB of storage for research use.
Here we apply the algorithm to a single synthetic cell data set taken from the PhagoSight website http://www.phagosight.org/synData.php.
The synthetic cell was generated as a mixture of Gaussians with Poisson noise that varied over time to simulate the displacement and change of shape of a neutrophil as observed in a Zebrafish embryo.
The data for analysis consisted of points on the synthetic cell membrane at a series of times (for simplicity we used 2d data, i.e., the cell membrane was a 1d curve embedded in R2).
The initial and target curves we took as test data for the algorithm are shown in Fig.
1(a).
To apply our algorithm, based on diffuse interface representations, we define the domain Ω:=[0,8]×[0,6] which was such that both the initial and target curves were contained in the domain.
We then constructed diffuse interface representations of the target data following the procedure described in [5].
Figs.
1(b) and 1(c) show the diffuse interface representations of the initial and target data respectively.
In order to investigate the influence of the volume constraint on the computed cell morphologies we performed two experiments, in the first we simply considered the forced Allen–Cahn model for the evolution with no volume constraint, i.e., (2.8) with λ=0, and in the second we included the volume constraint as described in Section 2.
The algorithm took 1996 iterations to meet the stopping criteria with no volume constraint and 2479 iterations with the volume constraint, corresponding to CPU times of 25 238 and 119 216 s respectively.
Fig.
2 shows the value of the objective functional against the number of iterations of the optimisation algorithm with and without the volume constraint.
We observe similar behaviour in both cases with an initial rapid decrease in the objective functional followed by a more gradual reduction with each iteration as we approach the minimum.
Fig.
3 shows the zero level-set of the computed solution using the optimal control at the final time with and without the volume constraint.
The curve corresponding to the zero level set is shaded by the value of the computed optimal control.
The background shading corresponds to the target data.
In both cases the position of the zero level-set of the computed solution shows good agreement with the target data.
Qualitatively we observe cells with a clearly defined “front” and “rear”, with the computed control corresponding to protrusive forces at the front and contractive forces at the rear.
Fig.
4 shows the area enclosed by the zero level-set of the solution with the optimal control with and without the volume constraint together with the linear interpolant of the areas of the data.
We see that without the volume constraint the area initially decreases then rapidly increases as we approach the final time whilst with the volume constraint (note the constraint is actually on the mass rather than the volume) the area is close to the linear interpolant of the areas of the data.
In terms of the computed cell morphologies, Fig.
5 shows snapshots of the computed cell membranes (zero level-sets) for the two different cases.
We clearly observe that the intermediate snapshot (blue curve) encloses a much smaller area if the volume constraint is not included in the algorithm.
In Fig.
6 we report on the trajectory and speed (magnitude of the velocity) of the centroid (center of mass) of the zero level-set of the computed solution with the optimal control, with and without the volume constraint.
We observe similar trajectories with and without the volume constraint and in both cases we observe an increase in the speed as we approach the end-time.
However in the case of no volume constraint this increase is more marked with a sharp spike in the centroid velocity observed close to the final time.
The increasing centroid speed we observe may be unphysical and if a (roughly) constant centroid velocity is desired one strategy may be to impose pointwise constraints on the control, this would prevent the large increase in the maximum and minimum values of the control observed during the simulations as we approach the final time as shown in Fig.
7.
Another possible strategy would be to modify the regularisation in (2.10).
Fig.
8 shows the fidelity term ‖φobs(x)−φ(x,T)k‖L2(Ω) with and without the volume constraint versus the number of iterations (where k corresponds to the optimisation iteration number).
The fidelity term may be considered as a quantitative measure for the “goodness of fit” of the computed data to the observations.
We observe a steady decay in the fidelity term as we approach the optimal control in both cases.
In this section we investigate the effect of the mesh-size on the results by refining the mesh whilst keeping the time step τ constant.
We report on the value of the fidelity term computed using ϕ⁎, i.e., the forward state computed with the optimal control.
The initial and target data are taken to be the same as in Section 4.2 and we employ the algorithm with the volume constraint.
The initial guess for the control is taken to be zero in each case.
The results from the mesh refinements are presented in Table 2.
We observe a reduction in the fidelity term as we refine the mesh which implies an improved fit to the observed data.
Although in principle it would be interesting to investigate the influence of refining both the timestep and mesh-size on the computed results, our tests indicate that the algorithm breaks down for time steps significantly larger than 0.01 and hence, refinement of the timestep and mesh-size together becomes computationally prohibitive.
Here we apply the algorithm with the volume constraint on the simple example of a translated circle to illustrate the effect that the choice of the initial guess for the control η has on the solution of the problem.
To apply our algorithm we define the domain Ω to be [−3,6]×[−3,3] with a triangulation of 8321 grid points.
We selected a uniform timestep τ=1×10−3 and set the interfacial thickness ε=0.1.
We took the end-time T=0.8.
The remaining numerical parameters for the optimisation algorithm are as given in Table 1.
The initial data was taken to be a smoothed (by running a few steps of the Allen–Cahn solver) version of the function taking the value 1 inside B1(0,0) (a circle of radius 1 centred at the origin) and −1 in Ω/B1(0,0).
The target data was taken to be a smoothed (by running a few steps of the Allen–Cahn solver) version of the function taking the value 1 inside B1(3,0) and −1 in Ω/B1(0,0).
Fig.
9 shows the initial and target diffuse interface data.
To illustrate the effect of the choice of initial guess on the algorithm, we consider two different values for the initial guess, firstly we set η=0 and secondly we set η=c⋅∇φ, where c=(2.5,0), i.e., in the latter case the initial guess depends on the solution to the Allen–Cahn equation.
In both cases we used the algorithm with the volume constraints.
With the zero initial guess the algorithm took 3262 iterations to meet the stopping criteria corresponding to a CPU time of 320 433 s. With the second choice of initial guess the algorithm took 2056 iterations to meet the stopping criteria corresponding to a CPU time of 228 173 s respectively.
Fig.
10 shows the zero level-set of the computed solution using the optimal control at the final time.
The curve corresponding to the zero level-set is shaded by the value of the control with the background shading corresponding to the target data.
In both cases the position of the computed curve (zero level-set) with the optimal control shows good agreement with the target data.
Fig.
11 shows snapshots of the computed zero level-sets with the two different initial guesses.
For the case with the initial value of η=0, we observe in Fig.
11(a) that the interface remains close to the initial position for most of the time of the simulation, and at the very last moment it shrinks to a point with a new phase nucleated at the position of the target data corresponding to a change in topology.
With the second choice of initial guess (η=c⋅∇φ) we observe in Fig.
11(b) that there is a gradual motion towards the target position with no changes in topology.
Fig.
12 shows the area enclosed by the zero level-set of the computed solution with the optimal control with the two different initial guesses together with the linear interpolant of the areas of the data.
We observe a sharp increase in area towards the end of the time interval with the zero initial guess as the new phase is nucleated.
With the second choice of initial guess, the area of the computed curve exhibits a good fit to the linear interpolant of the areas of the data.
We now apply the algorithm to the case of multi-cell image data sets.
As a proof-of-concept we consider the simplest possible scenario where we have an initial and desired data set both consisting of two cells that are well separated.
For the first experiment we defined the initial data and target data as follows.
Defining the domain Ω to be [−2,8]×[−2,2] we defined the subdomains Ω1, Ω2, Ω3 and Ω4 to be the simply connected bounded domains with boundary curves Γ1,Γ2,Γ3 and Γ4 defined by (the curves Γ1,Γ2 and Γ3 and Γ4 are the zero level-sets of the diffuse interfaces shown in Figs.
13(a) and 13(b) respectively)Γ1:={x∈Ω|x12+x22−0.82+0.1sin⁡(4x1)+0.1sin⁡(3x2)=0},Γ2:={x∈Ω|(x12−2)2+(x2−0.6)2−0.72+0.1sin⁡(5x12)+0.3sin⁡(2x2)=0},Γ3:={x∈Ω|(x1−0.4)2+(x2−0.5)2−0.82+0.1sin⁡(6x1)+0.1sin⁡(7x2)=0},Γ4:={x∈Ω|(x12−2.5)2+(x2−1)2−0.72+0.1sin⁡(7x12)+0.1sin⁡(1.5x2)=0}.
We then set the initial and target data to be a smoothed (by running a few steps of the Allen–Cahn solver) version of the functionφ0={1 for x∈Ω1∪Ω2,−1 for x∈Ω/(Ω1∪Ω2), and φobs={1 for x∈Ω3∪Ω4,−1 for x∈Ω/(Ω3∪Ω4).
Fig.
13 shows the initial and target diffuse interface data.
As previously, we compare the results of the algorithm with and without the volume constraint.
For this experiment, the algorithm took 2035 iterations to meet the stopping criteria with no volume constraint and 2199 iterations with the volume constraint, corresponding to CPU times of 28 608 and 105 750 s respectively.
Fig.
14 shows the value of the objective functional against the number of iterations of the optimisation algorithm with and without the volume constraint.
Fig.
15 shows the zero level-set of the computed solution using the optimal control at the final time with and without the volume constraint shaded by the value of the control with the background shading corresponding to the target data.
The results are similar to the single cell simulations of Section 4.2 with an initial rapid decrease in the cost followed by a subsequent gradual decrease.
The cells (zero level-sets) computed with the optimal control show good agreement with the target data for both versions of the algorithm and for both cells.
For each of the versions of the algorithm, both of the computed cells again posses a clearly defined “front” and “rear” similar to the single cell case.
Fig.
16 shows the area enclosed by the zero-level set of the computed solution with the optimal control with and without the volume constraint together with the linear interpolant of the areas of the data.
We observe analogous behaviour to the single cell.
In terms of the computed cell morphologies, Fig.
17 shows snapshots of the computed zero level-sets for the two different versions of the algorithm.
We see that in this multi-cell setting the algorithm has implicitly solved the matching problem by generating two disjoint cells whose topology remains fixed throughout the evolution.
We observe that the loss of volume in the case of no volume constraint corresponds to one of the cells in the intermediate snapshot (blue curve) enclosing a much smaller area.
Of course, in general our algorithm may generate cells whose topology is not fixed as in Section 4.4.
To this end we report on another experiment.
Defining the domain Ω to be [−2,6.3]×[−2.5,2.5] we defined the subdomains Ω1, Ω2, Ω3 and Ω4 to be the simply connected bounded domains with boundary curves Γ1,Γ2,Γ3 and Γ4 defined by (see Fig.
18)Γ1:={x∈Ω|x12+x22−0.92+0.1sin⁡(4.5x1)+0.11sin⁡(3x2))=0},Γ2:={x∈Ω|(x1−5)2+x22−0.72+0.1sin⁡(5x12)+0.3sin⁡(2x2)=0},Γ3:={x∈Ω|(x1−0.35)2+(x2−0.7)2−0.82+0.1sin⁡(6x1)+0.1sin⁡(7x2)=0},Γ4:={x∈Ω|(x1−0.3)2+(x2−1.1)2−0.72−0.1sin⁡(7x12)+0.1sin⁡(1.5x2)=0}.
We then set the initial and target data to be a smoothed (by running a few steps of the Allen–Cahn solver) version of the functionφ0={1 for x∈Ω1∪Ω2,−1 for x∈Ω/(Ω1∪Ω2), and φobs={1 for x∈Ω3∪Ω4,−1 for x∈Ω/(Ω3∪Ω4).
Fig.
18 shows the initial and target diffuse interface data.
As previously, we compare the results of the algorithm with and without the volume constraint.
For this experiment, the algorithm took 1960 iterations to meet the stopping criteria with no volume constraint and 1937 iterations with the volume constraint, corresponding to CPU times of 27 553 and 93 150 s respectively.
Fig.
19 shows the value of the objective functional against the number of iterations of the optimisation algorithm with and without the volume constraint.
Fig.
20 shows the zero level-set of the computed solution using the optimal control at the final time with and without the volume constraint shaded by the value of the control with the background shading corresponding to the target data.
The results are similar to the previous simulations with an initial rapid decrease in the cost followed by a subsequent gradual decrease and good agreement with the target data for both versions of the algorithm and for both cells.
For each of the versions of the algorithm, both of the computed cells again posses a clearly defined “front” and “rear”.
Fig.
21 shows the area of the domain in which the computed solution is positive with and without the volume constraint together with the linear interpolant of the areas of the data.
For this experiment we observe that the area enclosed by the computed cells differs significantly from the linear interpolant areas of the data both with and without the volume constraint.
This may be due to the change in topology of the interface during the evolution.
Fig.
22 shows snapshots of the computed zero level-sets for the two different versions of the algorithm.
Unlike the previous examples we see that for this particular choice of initial and target data, the algorithm yields cells which change in topology with one of the curves shrinking until it disappears whilst the other splits eventually becoming two disjoint curves.
Thus our algorithm generates trajectories corresponding to the annihilation (via shrinking) of one cell whilst the other cell splits to form the two cells observed in the image data set.
The CPU times for each of the experiments is of the order of hours.
For all the experiments the number of iterations required before the stopping criteria is met are similar, however this leads to simulations with the volume constraint taking approximately four times as long (in terms of CPU time) as those without the volume constraint.
This is due to the iterative nature of the algorithm used to compute the Lagrange multiplier cf., Appendix A which necessitates multiple solves per timestep.
We note that the CPU times of the algorithms may be too large for many applications.
In light of this we mention that the stopping criteria we have used may be too strict for many applications and that a significant decrease in the cost function together with a reasonable fit to the data is observed after as few as 50 iterations (which reduces the CPU time by a factor of around 40) and that for many applications this level of fit may be sufficient hence the stopping criteria could be relaxed.
Finally we mention that the current solution procedure based on uniform grids and serial solution of the forward and adjoint problems may be improved and we are currently investigating combining adaptive grids with a parallel solver for the forward and adjoint problems which gives a significant speed up but presents new technical challenges which we wish to avoid in this paper to maintain clarity of exposition.
In this study we presented a first step towards the development of cell tracking algorithms based on physical models for cell migration.
The presented algorithm seeks to track whole cell morphologies and is applicable to single cell or multi-cell image data sets.
Our approach may be regarded as a model fitting procedure in which a physically derived model for the evolution of the cell or cells is fitted to experimental image data sets.
The algorithm is based on the theory of optimal control of PDEs and full details of the derivation and implementation of the algorithm are given.
We also present a number of numerical experiments illustrating the performance of the algorithm with synthetic representative single cell and multi-cell image data sets.
The key novelty of our approach is that the model for the evolution of the cell (or cells), which drives the tracking procedure, is based on a relevant simplification of existing physically derived models for cell motility that reproduce many experimentally observed aspects of cell migration (e.g., [4]).
Thus this study is an important step towards the development of cell tracking algorithms in which the recovered trajectories are physically meaningful.
This is in contrast to the majority of existing algorithms for whole cell tracking in which the models for the evolution of the cell that underly the tracking procedure are purely geometric in nature neglecting completely the physics of cell migration [15,16,29].
One significant advantage of the approach to cell tracking we propose, is that the physics of the model driving the evolution of the cell are reflected in the recovered dynamic data.
Thus it is possible to encode physical features of cell migration into the tracking procedure.
We illustrate this fact by including volume conservation in the model.
Comparing the results of the tracking algorithm with and without volume conservation, we observe, that in a number of simulations neglecting volume conservation leads to physically unrealistic cell morphologies with a significant reduction of the cell volume in the recovered morphologies whilst this undesirable effect is no longer evident if volume conservation is included.
We note that volume conservation is more physically relevant in three dimensions.
This is since large changes in volume in two-dimensional imaging data of cells (i.e., the area of the projections of the cell on to a two-dimensional plane) are often observed in experimental data despite the cells conserving their enclosed (three-dimensional) volume.
Of course volume conservation is only an example of the kind of biology or physics one may wish to encode in the algorithm.
A number of models that fit into our framework have been proposed incorporating more complex biophysics such as spontaneous curvatures [9], for Helfrich type models, adhesion for the migration of cells on substrates or in the ECM [13], cell–cell or cell–obstacle interactions [4] and chemotaxis [12], these models are thus potential candidates for the model driving the evolution in our tracking algorithm.
We work with diffuse interface representations of the cell membrane to make use of the mature theory for the optimal control of semilinear PDEs.
One attractive aspect of this approach is that, as we do not require sharp interface representations of the cell membrane, it may be possible therefore to work directly with the raw experimental image data set without any need for segmentation.
However the diffuse interface or phase field framework we employ does make the algorithm computationally intensive as evidenced by the relatively large CPU times for our experiments and a key area for future work is to investigate improvements in the computational efficiency of the algorithm.
This need is especially evident if one wishes to track cells in 3d, as although our theoretical framework applies equally to this setting the computational cost becomes prohibitive.
Computational aspects under investigation include•Spatial and temporal adaptivity which is challenging in this setting as the solution of the state equation enters the adjoint equation.
Alternative update schemes for the control to the simple yet robust gradient based update considered in this study.
Parallelisation and the development of fast solvers for the solution of the state and adjoint equations.
Investigating the performance of the algorithm with real biological data for different cell types and in different environments is an important and worthwhile task.
We are currently applying the algorithm to the tracking of in vivo neutrophil migration and intend to report on this elsewhere.
As mentioned previously one interpretation of the forcing η⁎ is that it accounts for both protrusive forces generated by polymerisation of actin at the leading edge of the cell together with contractile forces generated by the action of myosin motors at the cell rear.
Thus a potential avenue for assessing the plausibility of the cell tracks computed with our algorithm would be to compare the computed η⁎ with experimental imaging data on the location of polymerised actin and myosin-II on the cell membrane with the expectation being that regions in which the computed forcing η⁎ is positive would correspond to regions rich in polymerised actin and regions in which the computed forcing η⁎ is negative would correspond to regions rich in myosin-II.
There are also many extensions of our approach which are likely to prove useful in applications.
Our algorithm could equally be applied to the identification of (possibly time-dependent) parameters in models for cell migration (e.g., a spatially constant forcing or material parameters such as surface tension or bending rigidity) however in this case it is likely that the sharp interface approach we propose in [5] will be more efficient.
As observed in some of the experiments we report on, the framework we employ allows changes in topology of the cells.
Whilst this may be desirable for some applications, e.g., tracking cells beyond cell division or cell fusion, in many biological experiments the topology of the cells is fixed.
Our experiments suggest that topological changes arise primarily in the case of multi-cell image data sets.
In this setting it should be possible to track the evolution of certain topological invariants (or more specifically diffuse interface representations of such invariants) and use these as an indicator for when the computed cells are changing in topology.
The user could then manually reduce the multi-cell tracking problem to multiple single cell (or smaller scale multi-cell) tracking problems by specifying the correspondence between cells in different frames, with the hope that changes in topology do not occur for these new problems.
The model we propose for the evolution in this study is a simplification of more general physically relevant models in which bulk or surface PDEs for the biochemistry are coupled to a geometric evolution law for the motion.
An important area for future work is the extension of the framework to this more general setting.
We note that the phase field approach we employ makes it computationally straightforward to couple the geometric evolution law for the motion to bulk PDEs (posed either within the cell or in the extra-cellular matrix) [10,8,13].
We hope that our optimal control-model fitting based framework is a useful first step towards incorporating advances in the modelling of cell migration into cell tracking algorithms.
This work (A.M., V.S.
and C.V.) is supported by the Engineering and Physical Sciences Research Council, UK grant (EP/J016780/1) and the Leverhulme Trust Research Project Grant (RPG-2014-149).
K.B.
was partially supported by the Embirikion Foundation Grant (2011-2014) – Greece.
The computations were carried out using the computational resources of the School of Mathematics and Physical Sciences at the University of Sussex.
The only data associated with the article was obtained from the phagosight website, www.phagosight.org, and is freely available for general use.
We introduce the variational form for the forward problem (2.8) defined as follows.
Find (φ,λ)∈L2([0,T];H1(Ω))×L2(0,T) such that∫Ω∂tφψdx+∫Ω∇φ⋅∇ψdx=1ε∫Ω(cGη−λ)ψdx−1ε2∫ΩG′(φ)ψdx∀ψ∈H1(Ω).
Let T be a decomposition of Ω into simplexes (for simplicity we assume Ω is polygonal).
We define the finite element space(A.1)V:={ψh∈H1(Ω)∩C0(Ω):ψh|k∈P1∀k∈T}.
For the time discretisation we employ an implicit–explicit method where the diffusive term is treated implicitly and the reaction terms explicitly.
Introducing the shorthand for a time discrete sequence fn:=f(tn) and a uniform timestep τ with T=Mτ,M∈N, the fully discrete scheme reads, for n=0,…,M−1, given φhn,ηhn∈V find (φhn+1,λn+1)∈V×R such that1τ∫Ω(φhn+1−φhn)ψhdx+∫Ω∇φhn+1⋅∇ψhdx=1ε∫Ω(cGηhn−λn+1)ψhdx−1ε2∫ΩΛh(G′(φhn))ψhdx∀ψh∈V, where Λh:C0(Ω)→V denotes the Lagrange interpolant.
We solve the above problem using the iterative technique introduced and studied in [21], which uses a bisection method for the Lagrange multiplier.
In particular we seek an iterative sequence {ϕhn+1,k,λn+1,k}k≥1 where ϕhn+1,k solves1τ∫Ω(φhn+1,k−φhn)ψhdx+∫Ω∇φhn+1,k⋅∇ψhdx=1ε∫Ω(cGηhn−λn+1,k)ψhdx−1ε2∫ΩΛh(G′(φhn))ψhdx∀ψh∈V, with λn+1,1=−2ετ+1, λn+1,2=2ετ−1 and {λn+1,k+1}k≥2 satisfyingλn+1,k+1=λn+1,k+(λn+1,k−λn+1,k−1)(Mφn+1−∫Ω[φhn+1,k]+)(∫Ω[φhn+1,k]+−∫Ω[φhn+1,k−1]+), where we recallMφn+1:=∫Ω[φh0]++(n+1)τT([φobs]+−[φh0]+)dx.
We deem this iteration to have converged when |λn+1,k+1−λn+1,k|<tol.
The discretisation of the forward problem without the volume constraint is as above with λ=0.
For the discretisation of the adjoint problem we employ a standard semi-implicit finite element approximation.
Optical switching of radical pair conformation enhances magnetic sensitivity The yield of radical pair reactions is influenced by magnetic fields well beyond the levels expected from energy considerations.
This dependence can be traced back to the microscopic dynamics of electron spins and constitutes the basis of chemical compasses.
Here we propose a new experimental approach based on molecular photoswitches to achieve additional control on the chemical reaction and allow short-time resolution of the spin dynamics.
Our proposal enables experiments to test some of the standard assumptions of the radical pair model and improves the sensitivity of a paradigmatic model of chemical magnetometer by up to two orders of magnitude.
Radical pair (RP) reactions have recently raised attention as one of the best characterized instances in the field of quantum biology [1].
Due to the strong dependence of the products’ chemical yield on the external magnetic field, they constitute one of the two main hypotheses to explain how certain species of birds use the Earth’s magnetic field to orient during migration.
Recent advancements include the identification of a suitable protein, which is present in the retina and possesses the properties required by the avian magnetoreception [2,3], and the experimental demonstration that a RP reaction can be sensitive to magnetic fields as weak as the Earth’s one [4].
The formulation of an additional quantum physics perspective boosted the interest towards the study of entanglement, its decoherence and the implementation of quantum control techniques [5–8].
From a more technological perspective, these aspects are also relevant to the implementation of information processing in molecular spintronics [9,10], i.e.
in electron spin systems.
In both contexts, it is necessary to investigate the established RP model [11,12] on the short timescale of the electron spin dynamics and to extend the experimental studies from an ensemble approach towards the single-molecule level.
Here, we propose a new experimental setup which allows sub-nanosecond time resolution and is suitable to reduce the stochastic nature of ensemble experiments by effectively engineering the reaction kinetics.
We suggest to connect the radicals via a molecular switch presenting two isomeric forms and capable of undergoing conformational changes upon the absorption of a photon.
In this way, one can optically manipulate the distance between the radicals and effectively determine the instant of their re-encounter.
Since the chemical reaction takes place at the moment of re-encounter, we achieve control on the reaction kinetics.
First, we present the experimental setup and discuss its feasibility with state-of-the-art technology.
The consistency of timescales and the efficiency of the procedure are analyzed for the case of azobenzene as the photo-controlled bridge.
Second, we show how the additional control can be exploited to improve the performance of sensors based on the RP mechanism, and in particular we apply our scheme to a specific chemical magnetometer to boost its sensitivity by two orders of magnitude.
From the broad class of chemical reactions involving intermediate radicals [13], we consider those reactions that take place in solution and involve two different chemical compounds (named A and D for acceptor and donor, respectively) free to diffuse.
Typically, one of the molecules is photoexcited and, if A and D are close enough, a fast electron transfer establishes the radical pair D•+–A•-.
The electron spins are assumed to be in the singlet state S=12(↑↓-↓↑), while, at room temperature, the nuclear spins are in the completely mixed state.
The radicals diffuse away from each other to such distances that the electron–electron interactions become negligible and the electron spin dynamics is determined only by the external magnetic field and the interaction of each electron with the surrounding nuclear spins on the respective radical [13]:(1)H=∑m=A,DHm=-γeB→·∑mS→m+|γe|∑m,jS→m·αˆmj·I→mjwhere γe=-geμB is the electron gyromagnetic ratio, S→m,I→mj are the electron and nuclear spin operators respectively, B→ is the external magnetic field, αˆmj denote the hyperfine coupling tensors, and we fix ℏ=1.
In case of isotropic interactions, the tensors αˆmj reduce to scalars.
Geminate re-encounter of the two radicals happens after a random walk in solution, and backward electron transfer completes the chemical reaction for radicals that are in a singlet state.
Triplet radicals recombine to distinct triplet products.
The number of radical pairs which recombine through the singlet reaction channel is directly proportional to the time integral of the fraction of radicals in a singlet state fs(t) weighted by the probability of (diffusive) re-encounter: Φs=∫0∞pre(t)fs(t)dt, with pre(t) the re-encounter probability and fs(t)=Sρel(t)S the overlap between the singlet state and the electron spin state ρel(t) at time t. Thus, we are facing a system in which two distinct, but concurrent dynamics are taking place: The quantum evolution of the electron spin state, and the classical diffusion of the molecules D+ and A-.
The latter one stochastically determines the duration of the former.
In the literature, the probability distribution for the re-encounter time has phenomenologically been described as a single exponential pre(t)=ke-kt with re-encounter rate k [13,14].
It is pre(t) that we aim to control in order to modify Φs and all the related quantities.
A large rate k∼1ns−1 indicates that most of the geminate re-encounters happen in the first few nanoseconds after the initial electron transfer, and so it becomes hard to study the electron spin dynamics at later times or to observe the dependence of the entanglement lifetime on B as predicted in Ref.
[5].
Furthermore, a weak magnetic field effectively has no time to produce any appreciable effect and this limits the sensitivity of chemical magnetometers or compasses.
In the context of avian magnetoreception, the RP is actually required [2,15] to have a small re-encounter rate k∼1μs−1.
For chemical reactions involving bi-ionic radicals in solution, however, the Coulomb attraction leads to a faster re-encounter.
Thus several experimental efforts have been undertaken to modify the distribution of the RP re-encounter time, mainly by imposing constraints on the geometry of the system by linking the radicals with a flexible chain [16,17] or restricting the available diffusion volume using micelles [18].
Although these experimental studies confirm a substantial modification of pre(t) in these systems, they are still interpreted within the phenomenological exponential model with rate k∼0.1–1μs−1 essentially determined by properties (like solvent viscosity or chain length) that cannot be changed continuously or even independently, and whose effect on the rate k is not easy to predict quantitatively.
To overcome these difficulties, we propose to use optical switches, in this way combining geometrical constraints with the possibility of optically controlling them.
The distance of the radicals strongly affects both the possibility of charge recombination and the electron spin dynamics.
At small separations, the electron transfer is favored by the proximity of the radicals, while the direct electron–electron coupling inhibits the spin dynamics.
In fact, the exchange interaction determines an energy gap between the singlet state and the triplet states which suppresses the spin interconversion.
Since such interaction decreases exponentially with increasing distance between the radicals, for the moderate separation reached in bi-ionic reaction in solution they are typically neglected (as expressed in Eq.
1).
Consistently, at large distances the dynamics is determined by the Zeeman and hyperfine interaction only, whereas the electron transfer is practically forbidden.
Here, we propose to optically control the inter-radical separation by chemically attaching A and D to the two endings of a molecule exhibiting two isomeric forms and an isomerization pathway which is photo-activated.
Such molecules possess the property of reversibly switching between a straight and a contracted structure upon the absorption of a photon of suitable wavelength.
The two isomeric forms effectively impose a ‘closed’, respectively ‘open’, configuration for the radicals, see Figure 1, that we are able to change in a reversible way by means of short laser pulses.
Examples of photoswitchable compounds are fulgides, diarylethenes or azobenzene derivatives [19].
Via the optical manipulation of the opening and closing of the linking bridge, it is in principle possible to directly control the instant of RP creation, the exact duration of the quantum dynamics for the electron spins, and the instant of the final recombination.
The implementation of such a scheme for an ensemble of molecules is ideally as follows: initially all the bridges are closed and the radicals not yet formed, then the RP are created by excitation via a laser pulse (standard technique applied for example in Ref.
[4,20]), and immediately after that, a second laser pulse triggers the isomerization and separates the radicals.
The electron spin state can then evolve according to the Hamiltonian (1) until a third laser pulse will close the bridges and lead to the final recombination.
In this scheme, every molecule undergoes a single cycle of isomerization: From the closed form to the open one, and back to the closed form again.
It is desirable, but not strictly necessary, that all the three processes (i.e.
the radical pair creation and the two opposite isomerization processes) could be selectively triggered by laser pulses of different wavelengths.
For pulse durations much shorter than the timescale of the electron spin evolution, the re-encounter probability can be described by a peaked distribution pre(t)∝δ(t-τre), with τre being the time elapsed between the second and third pulse and ‘∝’ indicating the proportionality with respect to the actual photoisomerization yield.
Such peaked distribution allows to monitor the electron spin dynamics at the precise instant τre, which can be chosen in a wide interval ranging from a few nanoseconds to several tens of microseconds (for later times one has to take into account spin–lattice relaxation and other decoherence sources).
The proposed scheme can be straightforwardly adapted to engineer more complicated and structured re-encounter probability distributions: a series of pulses creates a re-encounter probability exhibiting separated peaks, while continuous illumination gives rise to an exponential distribution (here not phenomenologically assumed, but externally imposed) whose rate is directly proportional to the light intensity.
Combining laser pulses, dark intervals, and continuous illumination with adjustable light intensity, the ability of engineering a specific pre(t) is practically limited by technical restrictions (e.g.
the laser intensity or the speed of light modulation) and by the absorption and isomerization properties of the photoswitches (e.g.
the selectivity with which an ‘opening pulse’ does not cause spurious ‘closing events’, and vice versa).
Realistically, a fraction of the photoswitches may not undergo photoisomerization, but, for thermally stable isomers, this means that the corresponding radicals simply do not contribute to the chemical reaction.
An alternative scenario is given by photoswitches that do not change their conformation, but rather change their conductance, passing from an insulating to a conductive state and vice versa [21,22].
In the case of backward electron transfer through the linker, the isomerization of the linker corresponds to switching on/off the coupling between the two radicals at its ends.
To explore the actual feasibility of our proposal, we consider the concrete example of azobenzene as the photoswitching molecular bridge (for more details see supplementary material).
Azobenzene is an organic molecule that occurs in two isomeric forms, trans and cis azobenzene, having a stretched, respectively contracted, structure.
The synthesis of azobenzene derivatives having different chemical functional groups is routinely performed [23–25].
Preliminary checks will have to verify that the chemistry of the RP is not altered by the presence of the azobenzene and that the constraints on the RP distance are satisfied.
The processes involved in our proposal need to satisfy a hierarchy of timescales: the electron spin dynamics determines the reference timescale to be ∼1–10ns (for typical values of the external magnetic field and hyperfine couplings, i.e.
∼0.1–1mT) with respect to which the spin relaxation (either dissipation or dephasing) and the thermally driven isomerization have to be slow, while the radical creation and photoisomerization mechanism have to be fast.
Typically, spin–lattice relaxation takes ∼1–10μs at room temperature, while photo-excitation of the donor and successive electron transfer happens in a few picoseconds.
Radicals were generated by a 7ns pulse in [4], but even RP creation in less than one picosecond has been achieved in experiments with femtosecond laser pulses [26].
In the case of azobenzene, the isomers can be considered completely stable at room temperature [27] whereas the photoisomerization takes only a few picoseconds [28,29].
The timescale over which we can modulate the re-encounter probability depends on the strength and duration of the optical pulses: Estimates in Ref.
[30] suggest that a single pulse of a duration of ≃10ps achieves photoisomerization yields of 30–40%.
The control of the RP re-encounter probability finds a direct application to improve the performance of chemical devices.
Here, we show how a simple-to-implement control scheme highly enhances the sensitivity of a model chemical magnetometer by up to two orders of magnitude.
The basic idea behind a chemical magnetometer is that, since a change in the magnetic field modifies the amount of singlet products, one can reverse the reasoning and measure the chemical yield to estimate B.
Intuitively, the magnetic sensitivity is high when a small change in the magnetic field intensity produces large effects on the singlet yield.
Formally, it is defined as:(2)Λs(B)≡∂Φs(B)∂B=∫0∞pre(t)gs(B,t)dt,with gs(B,t)≡∂fs(B,t)∂B being the instantaneous magnetic sensitivity.
The functional form of fs(B,t)=Sρel(t)S strongly depends on the specific realization of the radical pair, in particular on the number of the surrounding nuclear spins.
Here, we consider a radical pair in which the first electron spin is devoid of hyperfine interactions, while the second electron spin interacts isotropically with one spin-1 nucleus, e.g.
nitrogen.
In the context of the chemical compass (i.e.
when the task is determining the magnetic field direction through anisotropic hyperfine interactions), an analogous configuration (with only one spin-1/2 nucleus) has been proposed [3], and numerically characterized [8], as being optimal: Additional nuclear spins would perturb the intuitive ‘reference and probe’ picture.
The Hamiltonian then simplifies to H=-γeB(S1(z)+S2(z))+|γe|αS→2·I→, where α is the isotropic hyperfine coupling.
We evaluate the sensitivity in the regime of long spin–lattice relaxation times and weak magnetic fields, i.e.
for B≪α, noting that, in practical applications in which the B field varies around a large ‘off-set’ value, one can reproduce such conditions by applying a suitable external field for compensation.
For the realistic choice α≃1.0mT, the Earth magnetic field is indeed weak: BEarth≃α/20.
We have obtained an analytical expression for the instantaneous magnetic sensitivity gs(B,t) as given in the supplementary material, whose behavior, in the regime B≪α, is shown in Figure 2.
When integrated on timescales longer than the effective ‘period’ of the envelope, τenvel=3π|γe|B, the positive and negative contributions cancel each other and drastically reduce the ultimate sensitivity Λs(B).
A solution is to engineer the re-encounter probability in such a way that the radical pairs are allowed to recombine only when gs(B,t)⩾0.
Such desirable pre(t) is obtained applying weak light pulses (for the final step of ‘closing’ the photoswitchable bridge) interspersed with equally long dark intervals, both of duration τ≃τenvel/2.
Considering the realistic parameters of Figure 2, one obtains τ≈0.54μs corresponding to a laser repetition of around 1MHz.
The corresponding re-encounter probability is a piecewise exponential function with decay rate kprot directly proportional to the light intensity.
The phenomenological and controlled re-encounter probabilities are shown in Figure 3 together with the corresponding integrand involved in the evaluation of the magnetic sensitivity (2).
The experimental protocol that we suggest is characterized by the sole parameter τ, which represents both the pulse duration and the waiting time between two consecutive pulses.
In Figure 4 (left) we plot the (integrated) magnetic sensitivity as a function of τ, and observe the presence of a sharp peak located in correspondence to the resonance condition τ′≃τenvel/2.
As expected, a weaker resonance gives rise to a second peak at τ≃3τ′.
The choice of a suitable τ is then essential to obtain an enhancement of the magnetic sensitivity: Since the appropriate τ′ is inversely proportional to the intensity of the external field, a rough estimate of it is required.
If an approximated value Bapp≃B of the magnetic field is known, one can run the protocol with the corresponding value of τ′≃3π2|γe|Bapp.
Otherwise, if the value of the external magnetic field is completely unknown, we suggest a series of experimental runs which scans over a suitable range of pulse durations.
From an irregular landscape, a sharp peak centered around a certain τ′ will emerge, providing a first estimate of the field intensity: Consistently with the expressions given above, one obtains Bestim=3π2|γe|τ′.
In both cases, the precise value of B is determined by applying pulses of duration τ′.
Figure 4 (right) predicts that the optical control of pre(t) enhances the magnetic sensitivity up to two orders of magnitude with respect to the phenomenological exponential model over a broad range of rate k. We observe that similar enhancements are obtained for several kprot satisfying kprot⩽τ-1, and for a single nuclear spin-1/2 together with a suitably modified protocol (see supplementary material).
A few remarks are necessary.
In principle, to associate a unique magnetic field B to the experimentally observed singlet yield, the function Φs(B) must be invertible or, equivalently in this case, strictly monotonic.
Any specific realization of a chemical magnetometer presents several functional windows associated with a range of magnetic field intensities in which the derivative ∂Φs(B)∂B does not change sign.
Inside any functional window, the uncertainty in the estimation of B is inversely proportional to the corresponding magnetic sensitivity Λs(B).
Since the singlet yield Φs(B) takes only values in [0,1], the area below the sensitivity curve must sum up to |ΔΦs|⩽1 when integrated on any functional window.
Therefore, in the best case, the range of field intensities that lead to high sensitivity (and in which, we remind the reader, the sensitivity should not change its sign) has a width that is inversely proportional to the average value of sensitivity achieved.
For realistic functional forms of Λs(B), it is also inversely proportional to the maximum value.
For a chemical magnetometer, a high sensitivity requires a correspondingly ‘narrow’ functional range.
The good news is that, with our protocol, we can move such window and center it on the actual magnetic field.
Finally, let us estimate how the sensitivity in Figure 4 (right) compares to the best performance achievable by a chemical magnetometer, possibly in connection with an arbitrarily complicated pulse scheme: For a functional range between 0.048mT and 0.052mT, i.e.
of width 0.004mT, as taken from the figure, the average sensitivity cannot be greater than 250mT−1.
The average sensitivity in the given range amounts to about 25mT−1 which means that our magnetometer is optimal within a factor of 10.
This is remarkable since the proposed pulse sequence is characterized by the sole parameter τ and because such time is much longer than the timescale of the electron spin dynamics.
As a matter of fact, it is comparable to 1|γeB| which is the longest timescale involved.
We have proposed a new experimental approach which enables the control of the re-encounter probability of radical pairs in solution via light pulses.
This allows one to study the electron spin dynamics on the short timescale of its evolution and enlarges the information accessible in spin chemistry experiments.
Reducing the stochastic nature of ensemble and time averages will contribute to a better understanding of the radical pair mechanism itself.
So far, two types of quantum control schemes have been explored in chemistry.
(i) In addition to static magnetic fields, the spin chemistry of radical pairs has been controlled by resonant microwave fields (reaction yield detected magnetic resonance, RYDMR) [31,32,20,33].
This type of control coherently interacts with the spin degrees of freedom of radical pairs.
Recent theoretical work has added other variants to this type of control with direct manipulation of the electron spins [5,7,8].
(ii) Photoreactions, in particular photodissociation reactions of small molecules, have been subject to coherent laser control with shaped fs-pulses [34,35].
In this situation, it is the quantum dynamics of valence electrons and nuclear motion coupled to them that is directly acted on.
The present scheme represents a novel type of quantum control in chemistry.
It does not interfere directly with the electronic or spin degrees of freedom of a radical pair, but it switches parts of its Hamiltonian (exchange interaction) and the reaction constant.
In our model, the control is achieved through the photo-chemical effect of light on the conformation of the radical pair, i.e.
in principle by a purely ‘mechanical’ effect that could be also obtained by some other kind of micro-mechanics, perhaps in an STM tip.
Recently, Wasielewski and co-workers [36] have experimentally demonstrated similar effects using a completely different approach, in which the distance between the two electrons was changed by triggering a secondary electron transfer.
Regarding technological applications, the proposed experimental scheme offers an approach to increase the sensitivity of man-made magnetic-field sensors based on chemical reactions.
This we have demonstrated with a suitable radical pair connected by a photoswitchable bridge.
We have presented a simple protocol, which enhances the sensitivity of the chemical magnetometer by up to two orders of magnitude.
Remarkably, this scheme can be implemented using weak laser pulses, which have a duration much longer than the timescale given by the electron spin dynamics and, indeed, comparable to the longest one as defined by the weak magnetic field 1|γe|B.
This research was supported by the Austrian Science Fund (FWF): F04011, F04012.
Supplementary data associated with this letter can be found, in the online version, at http://dx.doi.org/10.1016/j.cplett.2013.04.010.